<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T10:08:34+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "ro"
}
-->
# Secțiunea 1: Fundamentele EdgeAI

EdgeAI reprezintă o schimbare de paradigmă în implementarea inteligenței artificiale, aducând capabilitățile AI direct pe dispozitivele de la marginea rețelei, în loc să se bazeze exclusiv pe procesarea în cloud. Este important să înțelegem cum EdgeAI permite procesarea locală a AI pe dispozitive cu resurse limitate, menținând în același timp o performanță rezonabilă și abordând provocări precum confidențialitatea, latența și funcționalitatea offline.

## Introducere

În această lecție, vom explora EdgeAI și conceptele sale fundamentale. Vom acoperi paradigma tradițională de calcul AI, provocările calculului la marginea rețelei, tehnologiile cheie care permit EdgeAI și aplicațiile practice în diverse industrii.

## Obiective de învățare

La finalul acestei lecții, veți putea:

- Înțelege diferența dintre abordările tradiționale bazate pe cloud și cele bazate pe EdgeAI.
- Identifica tehnologiile cheie care permit procesarea AI pe dispozitivele de la marginea rețelei.
- Recunoaște beneficiile și limitările implementărilor EdgeAI.
- Aplica cunoștințele despre EdgeAI în scenarii și cazuri de utilizare din lumea reală.

## Înțelegerea paradigmei tradiționale de calcul AI

În mod tradițional, aplicațiile AI generative se bazează pe infrastructuri de calcul de înaltă performanță pentru a rula eficient modele de limbaj mari (LLMs). Organizațiile implementează de obicei aceste modele pe clustere GPU în medii cloud, accesând capabilitățile lor prin interfețe API.

Acest model centralizat funcționează bine pentru multe aplicații, dar are limitări inerente în scenariile de calcul la marginea rețelei. Abordarea convențională implică trimiterea interogărilor utilizatorului către servere la distanță, procesarea acestora folosind hardware puternic și returnarea rezultatelor prin internet. Deși această metodă oferă acces la modele de ultimă generație, creează dependențe de conectivitatea la internet, introduce preocupări legate de latență și ridică probleme de confidențialitate atunci când datele sensibile trebuie transmise către servere externe.

Există câteva concepte de bază pe care trebuie să le înțelegem atunci când lucrăm cu paradigmele tradiționale de calcul AI, și anume:

- **☁️ Procesare bazată pe cloud**: Modelele AI rulează pe infrastructuri server puternice cu resurse computaționale ridicate.
- **🔌 Acces bazat pe API**: Aplicațiile accesează capabilitățile AI prin apeluri API la distanță, mai degrabă decât prin procesare locală.
- **🎛️ Gestionarea centralizată a modelelor**: Modelele sunt întreținute și actualizate centralizat, asigurând consistența, dar necesitând conectivitate la rețea.
- **📈 Scalabilitatea resurselor**: Infrastructura cloud poate scala dinamic pentru a face față cerințelor computaționale variabile.

## Provocările calculului la marginea rețelei

Dispozitivele de la marginea rețelei, cum ar fi laptopurile, telefoanele mobile și dispozitivele Internet of Things (IoT), precum Raspberry Pi și NVIDIA Orin Nano, prezintă constrângeri computaționale unice. Aceste dispozitive au, de obicei, putere de procesare, memorie și resurse energetice limitate comparativ cu infrastructura centrelor de date.

Rularea LLM-urilor tradiționale pe astfel de dispozitive a fost istoric o provocare din cauza acestor limitări hardware. Cu toate acestea, nevoia de procesare AI la marginea rețelei a devenit din ce în ce mai importantă în diverse scenarii. Luați în considerare situațiile în care conectivitatea la internet este nesigură sau indisponibilă, cum ar fi site-uri industriale izolate, vehicule în tranzit sau zone cu acoperire slabă a rețelei. În plus, aplicațiile care necesită standarde ridicate de securitate, cum ar fi dispozitivele medicale, sistemele financiare sau aplicațiile guvernamentale, pot necesita procesarea datelor sensibile local pentru a menține confidențialitatea și cerințele de conformitate.

### Constrângeri cheie ale calculului la marginea rețelei

Mediile de calcul la marginea rețelei se confruntă cu mai multe constrângeri fundamentale pe care soluțiile tradiționale de AI bazate pe cloud nu le întâmpină:

- **Putere de procesare limitată**: Dispozitivele de la marginea rețelei au, de obicei, mai puține nuclee CPU și viteze de ceas mai mici comparativ cu hardware-ul de nivel server.
- **Constrângeri de memorie**: RAM-ul disponibil și capacitatea de stocare sunt semnificativ reduse pe dispozitivele de la marginea rețelei.
- **Limitări energetice**: Dispozitivele alimentate de baterii trebuie să echilibreze performanța cu consumul de energie pentru o funcționare extinsă.
- **Gestionarea termică**: Formele compacte limitează capacitățile de răcire, afectând performanța susținută sub sarcină.

## Ce este EdgeAI?

### Concept: Definirea Edge AI

Edge AI se referă la implementarea și execuția algoritmilor de inteligență artificială direct pe dispozitivele de la marginea rețelei—hardware-ul fizic care există la "marginea" rețelei, aproape de locul unde sunt generate și colectate datele. Aceste dispozitive includ smartphone-uri, senzori IoT, camere inteligente, vehicule autonome, dispozitive purtabile și echipamente industriale. Spre deosebire de sistemele AI tradiționale care se bazează pe servere cloud pentru procesare, Edge AI aduce inteligența direct la sursa datelor.

În esență, Edge AI înseamnă decentralizarea procesării AI, mutând-o departe de centrele de date centralizate și distribuind-o pe vasta rețea de dispozitive care alcătuiesc ecosistemul nostru digital. Aceasta reprezintă o schimbare arhitecturală fundamentală în modul în care sunt proiectate și implementate sistemele AI.

Pilonii conceptuali cheie ai Edge AI includ:

- **Procesare de proximitate**: Computația are loc fizic aproape de locul unde se originează datele.
- **Inteligență descentralizată**: Capacitățile de luare a deciziilor sunt distribuite pe mai multe dispozitive.
- **Suveranitatea datelor**: Informațiile rămân sub control local, deseori fără a părăsi dispozitivul.
- **Funcționare autonomă**: Dispozitivele pot funcționa inteligent fără a necesita conectivitate constantă.
- **AI integrat**: Inteligența devine o capacitate intrinsecă a dispozitivelor de zi cu zi.

### Vizualizarea arhitecturii Edge AI

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI reprezintă o schimbare de paradigmă în implementarea inteligenței artificiale, aducând capabilitățile AI direct pe dispozitivele de la marginea rețelei, în loc să se bazeze exclusiv pe procesarea în cloud. Această abordare permite modelelor AI să ruleze local pe dispozitive cu resurse computaționale limitate, oferind capabilități de inferență în timp real fără a necesita conectivitate constantă la internet.

EdgeAI cuprinde diverse tehnologii și tehnici concepute pentru a face modelele AI mai eficiente și potrivite pentru implementarea pe dispozitive cu resurse limitate. Scopul este de a menține o performanță rezonabilă, reducând semnificativ cerințele computaționale și de memorie ale modelelor AI.

Să analizăm abordările fundamentale care permit implementările EdgeAI pe diferite tipuri de dispozitive și cazuri de utilizare.

### Principii fundamentale ale EdgeAI

EdgeAI se bazează pe mai multe principii fundamentale care îl diferențiază de AI-ul tradițional bazat pe cloud:

- **Procesare locală**: Inferența AI are loc direct pe dispozitivul de la marginea rețelei, fără a necesita conectivitate externă.
- **Optimizarea resurselor**: Modelele sunt optimizate special pentru constrângerile hardware ale dispozitivelor țintă.
- **Performanță în timp real**: Procesarea are loc cu latență minimă pentru aplicațiile sensibile la timp.
- **Confidențialitate prin design**: Datele sensibile rămân pe dispozitiv, îmbunătățind securitatea și conformitatea.

## Tehnologii cheie care permit EdgeAI

### Cuantificarea modelelor

Una dintre cele mai importante tehnici în EdgeAI este cuantificarea modelelor. Acest proces implică reducerea preciziei parametrilor modelului, de obicei de la numere în virgulă mobilă pe 32 de biți la întregi pe 8 biți sau chiar formate de precizie mai redusă. Deși această reducere a preciziei poate părea îngrijorătoare, cercetările au arătat că multe modele AI pot menține performanța chiar și cu o precizie semnificativ redusă.

Cuantificarea funcționează prin maparea intervalului de valori în virgulă mobilă la un set mai mic de valori discrete. De exemplu, în loc să folosească 32 de biți pentru a reprezenta fiecare parametru, cuantificarea poate folosi doar 8 biți, rezultând o reducere de 4x a cerințelor de memorie și, adesea, conducând la timpi de inferență mai rapizi.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Diferite tehnici de cuantificare includ:

- **Cuantificare post-antrenament (PTQ)**: Aplicată după antrenarea modelului, fără a necesita reantrenare.
- **Antrenament conștient de cuantificare (QAT)**: Încorporează efectele cuantificării în timpul antrenamentului pentru o acuratețe mai bună.
- **Cuantificare dinamică**: Cuantifică greutățile la int8, dar calculează activările dinamic.
- **Cuantificare statică**: Pre-calculă toți parametrii de cuantificare atât pentru greutăți, cât și pentru activări.

Pentru implementările EdgeAI, selectarea strategiei de cuantificare adecvate depinde de arhitectura specifică a modelului, cerințele de performanță și capacitățile hardware ale dispozitivului țintă.

### Compresia și optimizarea modelelor

Dincolo de cuantificare, diverse tehnici de compresie ajută la reducerea dimensiunii modelului și a cerințelor computaționale. Acestea includ:

**Pruning**: Această tehnică elimină conexiunile sau neuronii inutili din rețelele neuronale. Prin identificarea și eliminarea parametrilor care contribuie puțin la performanța modelului, pruning-ul poate reduce semnificativ dimensiunea modelului, menținând în același timp acuratețea.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Distilarea cunoștințelor**: Această abordare implică antrenarea unui model "student" mai mic pentru a imita comportamentul unui model "teacher" mai mare. Modelul student învață să aproximeze ieșirile teacher-ului, obținând adesea performanțe similare cu semnificativ mai puțini parametri.

**Optimizarea arhitecturii modelului**: Cercetătorii au dezvoltat arhitecturi specializate concepute special pentru implementarea la marginea rețelei, cum ar fi MobileNets, EfficientNets și alte arhitecturi ușoare care echilibrează performanța cu eficiența computațională.

### Modele de limbaj mici (SLMs)

O tendință emergentă în EdgeAI este dezvoltarea modelelor de limbaj mici (SLMs). Aceste modele sunt concepute de la zero pentru a fi compacte și eficiente, oferind în același timp capabilități semnificative de procesare a limbajului natural. SLM-urile realizează acest lucru prin alegeri arhitecturale atente, tehnici eficiente de antrenament și antrenament concentrat pe domenii sau sarcini specifice.

Spre deosebire de abordările tradiționale care implică comprimarea modelelor mari, SLM-urile sunt adesea antrenate cu seturi de date mai mici și arhitecturi optimizate concepute special pentru implementarea la marginea rețelei. Această abordare poate duce la modele care nu doar că sunt mai mici, dar și mai eficiente pentru cazuri de utilizare specifice.

## Accelerarea hardware pentru EdgeAI

Dispozitivele moderne de la marginea rețelei includ din ce în ce mai mult hardware specializat conceput pentru a accelera sarcinile AI:

### Unități de procesare neuronală (NPUs)

NPUs sunt procesoare specializate concepute special pentru calculele rețelelor neuronale. Aceste cipuri pot efectua sarcini de inferență AI mult mai eficient decât procesoarele tradiționale, adesea cu un consum mai redus de energie. Multe smartphone-uri moderne, laptopuri și dispozitive IoT includ acum NPUs pentru a permite procesarea AI pe dispozitiv.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Dispozitive cu NPUs includ:

- **Apple**: Cipurile din seria A și M cu Neural Engine
- **Qualcomm**: Procesoarele Snapdragon cu Hexagon DSP/NPU
- **Samsung**: Procesoarele Exynos cu NPU
- **Intel**: VPUs Movidius și acceleratoare Habana Labs
- **Microsoft**: PC-uri Windows Copilot+ cu NPUs

### 🎮 Accelerarea GPU

Deși dispozitivele de la marginea rețelei pot să nu aibă GPU-uri puternice precum cele din centrele de date, multe includ totuși GPU-uri integrate sau discrete care pot accelera sarcinile AI. GPU-urile mobile moderne și procesoarele grafice integrate pot oferi îmbunătățiri semnificative ale performanței pentru sarcinile de inferență AI.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### Optimizarea CPU

Chiar și dispozitivele care au doar CPU pot beneficia de EdgeAI prin implementări optimizate. CPU-urile moderne includ instrucțiuni specializate pentru sarcinile AI, iar cadrele software au fost dezvoltate pentru a maximiza performanța CPU pentru inferența AI.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Pentru inginerii software care lucrează cu EdgeAI, înțelegerea modului de a valorifica aceste opțiuni de accelerare hardware este esențială pentru optimizarea performanței inferenței și eficienței energetice pe dispozitivele țintă.

## Beneficiile EdgeAI

### Confidențialitate și securitate

Unul dintre cele mai mari avantaje ale EdgeAI este îmbunătățirea confidențialității și securității. Prin procesarea datelor local pe dispozitiv, informațiile sensibile nu părăsesc niciodată controlul utilizatorului. Acest lucru este deosebit de important pentru aplicațiile care gestionează date personale, informații medicale sau date confidențiale de afaceri.

### Reducerea latenței

EdgeAI elimină necesitatea de a trimite date către servere la distanță pentru procesare, reducând semnificativ latența. Acest lucru este crucial pentru aplicațiile în timp real, cum ar fi vehiculele autonome, automatizarea industrială sau aplicațiile interactive care necesită răspunsuri imediate.

### Funcționalitate offline

EdgeAI permite funcționalitatea AI chiar și atunci când conectivitatea la internet nu este disponibilă. Acest lucru este valoros pentru aplicațiile din locații izolate, în timpul călătoriilor sau în situații în care fiabilitatea rețelei este o preocupare.

### Eficiență costurilor

Prin reducerea dependenței de serviciile AI bazate pe cloud, EdgeAI poate ajuta la reducerea costurilor operaționale, mai ales pentru aplicațiile cu volume mari de utilizare. Organizațiile pot evita costurile API recurente și pot reduce cerințele de lățime de bandă.

### Scalabilitate

EdgeAI distribuie sarcina computațională pe dispozitivele de la marginea rețelei, în loc să o centralizeze în centrele de date. Acest lucru poate ajuta la reducerea costurilor infrastructurii și la îmbunătățirea scalabilității generale a sistemului.

## Aplicații ale EdgeAI

### Dispozitive inteligente și IoT

EdgeAI alimentează multe funcții ale dispozitivelor inteligente, de la asistenți vocali care pot procesa comenzi local până la camere inteligente care pot identifica obiecte și persoane fără a trimite videoclipuri în cloud. Dispozitivele IoT utilizează EdgeAI pentru întreținere predictivă, monitorizarea mediului și luarea deciziilor automate.

### Aplicații mobile

Smartphone-urile și tabletele utilizează EdgeAI pentru diverse funcții, inclusiv îmbunătățirea fotografiilor, traducerea în timp real, realitatea augmentată și recomandările personalizate. Aceste aplicații beneficiază de avantajele latenței reduse și confidențialității procesării locale.

### Aplicații industriale

Mediile de producție și industriale utilizează EdgeAI pentru controlul calității, întreținerea predictivă și optimizarea proceselor. Aceste aplicații necesită adesea procesare în timp real și pot func
- [02: Aplicații EdgeAI](02.RealWorldCaseStudies.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să fiți conștienți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa maternă ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de oameni. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.