<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:59:50+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "ro"
}
-->
# Secțiunea 7: Qualcomm QNN (Qualcomm Neural Network) Optimization Suite

## Cuprins
1. [Introducere](../../../Module04)
2. [Ce este Qualcomm QNN?](../../../Module04)
3. [Instalare](../../../Module04)
4. [Ghid de început rapid](../../../Module04)
5. [Exemplu: Conversia și optimizarea modelelor cu QNN](../../../Module04)
6. [Utilizare avansată](../../../Module04)
7. [Cele mai bune practici](../../../Module04)
8. [Depanare](../../../Module04)
9. [Resurse suplimentare](../../../Module04)

## Introducere

Qualcomm QNN (Qualcomm Neural Network) este un cadru complet de inferență AI conceput pentru a valorifica la maximum potențialul acceleratoarelor hardware AI ale Qualcomm, inclusiv Hexagon NPU, Adreno GPU și Kryo CPU. Indiferent dacă vizați dispozitive mobile, platforme de calcul la margine sau sisteme auto, QNN oferă capabilități de inferență optimizate care utilizează unitățile de procesare AI specializate ale Qualcomm pentru performanță maximă și eficiență energetică.

## Ce este Qualcomm QNN?

Qualcomm QNN este un cadru unificat de inferență AI care permite dezvoltatorilor să implementeze modele AI eficient pe arhitectura de calcul eterogenă a Qualcomm. Acesta oferă o interfață de programare unificată pentru accesarea Hexagon NPU (Unitatea de Procesare Neurală), Adreno GPU și Kryo CPU, selectând automat unitatea de procesare optimă pentru diferite straturi și operații ale modelului.

### Caracteristici cheie

- **Calcul eterogen**: Acces unificat la NPU, GPU și CPU cu distribuție automată a sarcinilor
- **Optimizare conștientă de hardware**: Optimizări specializate pentru platformele Snapdragon ale Qualcomm
- **Suport pentru cuantizare**: Tehnici avansate de cuantizare INT8, INT16 și cu precizie mixtă
- **Instrumente de conversie a modelelor**: Suport direct pentru modele TensorFlow, PyTorch, ONNX și Caffe
- **Optimizat pentru AI la margine**: Conceput special pentru scenarii de implementare mobilă și la margine, cu accent pe eficiența energetică

### Beneficii

- **Performanță maximă**: Valorifică hardware-ul AI specializat pentru îmbunătățiri de performanță de până la 15x
- **Eficiență energetică**: Optimizat pentru dispozitive mobile și alimentate de baterii, cu gestionare inteligentă a energiei
- **Latentă redusă**: Inferență accelerată hardware cu suprasarcină minimă pentru aplicații în timp real
- **Implementare scalabilă**: De la smartphone-uri la platforme auto în ecosistemul Qualcomm
- **Pregătit pentru producție**: Cadru testat în milioane de dispozitive implementate

## Instalare

### Cerințe preliminare

- Qualcomm QNN SDK (necesită înregistrare la Qualcomm)
- Python 3.7 sau mai recent
- Hardware compatibil Qualcomm sau simulator
- Android NDK (pentru implementare mobilă)
- Mediu de dezvoltare Linux sau Windows

### Configurarea SDK QNN

1. **Înregistrare și descărcare**: Vizitați Qualcomm Developer Network pentru a vă înregistra și a descărca SDK QNN
2. **Extrageți SDK-ul**: Despachetați SDK-ul QNN în directorul de dezvoltare
3. **Setați variabilele de mediu**: Configurați căile pentru instrumentele și bibliotecile QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Configurarea mediului Python

Creați și activați un mediu virtual:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Instalați pachetele Python necesare:

```bash
pip install numpy tensorflow torch onnx
```

### Verificați instalarea

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Dacă instalarea este reușită, ar trebui să vedeți informații de ajutor pentru fiecare instrument QNN.

## Ghid de început rapid

### Prima conversie a unui model

Să convertim un model simplu PyTorch pentru a rula pe hardware-ul Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Conversia ONNX în format QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generarea bibliotecii de modele QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Ce face acest proces

Fluxul de lucru de optimizare implică: conversia modelului original în format ONNX, traducerea ONNX în reprezentarea intermediară QNN, aplicarea optimizărilor specifice hardware-ului și generarea unei biblioteci de modele compilate pentru implementare.

### Explicația parametrilor cheie

- `--input_network`: Fișierul sursă al modelului ONNX
- `--output_path`: Fișierul sursă C++ generat
- `--input_dim`: Dimensiunile tensorului de intrare pentru optimizare
- `--quantization_overrides`: Configurație personalizată de cuantizare
- `-t x86_64-linux-clang`: Arhitectura țintă și compilatorul

## Exemplu: Conversia și optimizarea modelelor cu QNN

### Pasul 1: Conversie avansată a modelului cu cuantizare

Iată cum să aplicați o cuantizare personalizată în timpul conversiei:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Convertiți cu cuantizare personalizată:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Pasul 2: Optimizare multi-backend

Configurați pentru execuție eterogenă pe NPU, GPU și CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Pasul 3: Crearea unui binar de context pentru implementare

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Pasul 4: Inferență cu runtime-ul QNN

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Structura rezultatelor

După optimizare, directorul de implementare va conține:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Utilizare avansată

### Configurarea backend-ului personalizat

Configurați optimizări specifice backend-ului:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Cuantizare dinamică

Aplicați cuantizarea în timpul execuției pentru o mai bună acuratețe:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profilarea performanței

Monitorizați performanța pe diferite backend-uri:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Selectarea automată a backend-ului

Implementați selecția inteligentă a backend-ului pe baza caracteristicilor modelului:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Cele mai bune practici

### 1. Optimizarea arhitecturii modelului
- **Fuziunea straturilor**: Combinați operații precum Conv+BatchNorm+ReLU pentru o mai bună utilizare a NPU
- **Convoluții separabile pe adâncime**: Preferate în locul convoluțiilor standard pentru implementare mobilă
- **Designuri prietenoase cu cuantizarea**: Utilizați activări ReLU și evitați operațiile care nu se cuantizează bine

### 2. Strategia de cuantizare
- **Cuantizare post-antrenament**: Începeți cu aceasta pentru o implementare rapidă
- **Set de date de calibrare**: Utilizați date reprezentative care acoperă toate variațiile de intrare
- **Precizie mixtă**: Utilizați INT8 pentru majoritatea straturilor, păstrați straturile critice la o precizie mai mare

### 3. Ghiduri pentru selecția backend-ului
- **NPU (HTP)**: Cel mai bun pentru sarcini CNN, modele cuantizate și aplicații sensibile la energie
- **GPU**: Optim pentru operații intensive de calcul, modele mai mari și precizie FP16
- **CPU**: Soluție de rezervă pentru operații neacceptate și depanare

### 4. Optimizarea performanței
- **Dimensiunea lotului**: Utilizați dimensiunea lotului 1 pentru aplicații în timp real, loturi mai mari pentru debit
- **Preprocesarea intrării**: Minimizați suprasarcina de copiere și conversie a datelor
- **Reutilizarea contextului**: Pre-compilați contexte pentru a evita suprasarcina de compilare în timpul execuției

### 5. Gestionarea memoriei
- **Alocarea tensorului**: Utilizați alocarea statică atunci când este posibil pentru a evita suprasarcina în timpul execuției
- **Pool-uri de memorie**: Implementați pool-uri de memorie personalizate pentru tensori alocați frecvent
- **Reutilizarea bufferelor**: Reutilizați bufferele de intrare/ieșire între apelurile de inferență

### 6. Optimizarea energiei
- **Moduri de performanță**: Utilizați moduri de performanță adecvate în funcție de constrângerile termice
- **Scalarea dinamică a frecvenței**: Permiteți sistemului să scaleze frecvența în funcție de sarcină
- **Gestionarea stării de inactivitate**: Eliberați resursele corespunzător atunci când nu sunt utilizate

## Depanare

### Probleme comune

#### 1. Probleme de instalare SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Erori de conversie a modelului
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Probleme de cuantizare
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Probleme de performanță
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Probleme de memorie
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Compatibilitate backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Depanarea performanței

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Obținerea ajutorului

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Documentația QNN**: Disponibilă în pachetul SDK
- **Forumuri comunitare**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Asistență tehnică**: Prin portalul dezvoltatorilor Qualcomm

## Resurse suplimentare

### Linkuri oficiale
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Platforme Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portalul dezvoltatorilor**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **Motor AI**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Resurse de învățare
- **Ghid de început**: Disponibil în documentația SDK QNN
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Ghid de optimizare**: Documentația SDK include ghiduri cuprinzătoare de optimizare
- **Tutoriale video**: [Canalul YouTube Qualcomm Developer Network](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Instrumente de integrare
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Modele pre-optimizate pentru hardware-ul Qualcomm
- **API Neural Networks Android**: Integrare cu Android NNAPI
- **Delegate TensorFlow Lite**: Delegate Qualcomm pentru TFLite

### Benchmarks de performanță
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Cercetare AI Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Exemple comunitare
- **Aplicații de exemplu**: Disponibile în directorul de exemple SDK QNN
- **Repozitorii GitHub**: Exemple și instrumente contribuie de comunitate
- **Bloguri tehnice**: [Blogul dezvoltatorilor Qualcomm](https://developer.qualcomm.com/blog)

### Instrumente conexe
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Tehnici avansate de cuantizare și compresie
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Pentru comparație și implementare de rezervă
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Motor de inferență cross-platform

### Specificații hardware
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Platforme Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Ce urmează

Continuați călătoria AI la margine explorând [Modulul 5: SLMOps și implementarea în producție](../Module05/README.md) pentru a învăța despre aspectele operaționale ale gestionării ciclului de viață al modelelor de limbaj mici.

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să fiți conștienți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa maternă ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de oameni. Nu ne asumăm responsabilitatea pentru neînțelegerile sau interpretările greșite care pot apărea din utilizarea acestei traduceri.