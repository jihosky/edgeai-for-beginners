<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T14:30:32+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ro"
}
-->
# Secțiunea 03 - Integrarea Protocolului de Context al Modelului (MCP)

## Introducere în MCP (Protocolul de Context al Modelului)

Protocolul de Context al Modelului (MCP) este un standard open-source pentru conectarea aplicațiilor AI la sisteme externe. Folosind MCP, aplicații AI precum Claude sau ChatGPT pot fi conectate la surse de date (de exemplu, fișiere locale, baze de date), instrumente (de exemplu, motoare de căutare, calculatoare) și fluxuri de lucru (de exemplu, prompturi specializate)—permițând accesul la informații esențiale și realizarea de sarcini.

Gândiți-vă la MCP ca la un **port USB-C pentru aplicațiile AI**. Așa cum USB-C oferă o modalitate standardizată de a conecta dispozitive electronice, MCP oferă o modalitate standardizată de a conecta aplicațiile AI la sisteme externe.

### Ce poate permite MCP?

MCP deblochează capabilități puternice pentru aplicațiile AI:

- **Asistenți AI personalizați**: Agenții pot accesa Google Calendar și Notion, acționând ca un asistent AI mai personalizat
- **Generare avansată de cod**: Claude Code poate genera o aplicație web completă folosind un design Figma
- **Integrare de date la nivel de întreprindere**: Chatbot-uri pentru întreprinderi pot conecta mai multe baze de date dintr-o organizație, oferind utilizatorilor posibilitatea de a analiza datele prin chat
- **Fluxuri de lucru creative**: Modelele AI pot crea designuri 3D în Blender și le pot imprima folosind o imprimantă 3D
- **Acces la informații în timp real**: Conectarea la surse de date externe pentru informații actualizate
- **Operațiuni complexe în mai mulți pași**: Realizarea de fluxuri de lucru sofisticate care combină mai multe instrumente și sisteme

### De ce este important MCP?

MCP oferă beneficii în întregul ecosistem:

**Pentru dezvoltatori**: MCP reduce timpul și complexitatea dezvoltării atunci când se construiește sau se integrează o aplicație sau un agent AI.

**Pentru aplicațiile AI**: MCP oferă acces la un ecosistem de surse de date, instrumente și aplicații care îmbunătățesc capabilitățile și experiența utilizatorului final.

**Pentru utilizatorii finali**: MCP duce la aplicații sau agenți AI mai capabili, care pot accesa datele dvs. și pot lua măsuri în numele dvs., atunci când este necesar.

## Modele de limbaj mici (SLMs) în MCP

Modelele de limbaj mici reprezintă o abordare eficientă pentru implementarea AI, oferind mai multe avantaje:

### Beneficiile SLM-urilor
- **Eficiență resurselor**: Cerințe computaționale reduse
- **Timpuri de răspuns mai rapide**: Latență redusă pentru aplicații în timp real  
- **Eficiență costurilor**: Necesită infrastructură minimă
- **Confidențialitate**: Pot funcționa local fără transmiterea datelor
- **Personalizare**: Mai ușor de ajustat pentru domenii specifice

### De ce SLM-urile funcționează bine cu MCP

SLM-urile asociate cu MCP creează o combinație puternică în care capacitățile de raționament ale modelului sunt completate de instrumente externe, compensând numărul mai mic de parametri prin funcționalitate extinsă.

## Prezentare generală a SDK-ului MCP pentru Python

SDK-ul MCP pentru Python oferă fundația pentru construirea aplicațiilor compatibile cu MCP. SDK-ul include:

- **Biblioteci client**: Pentru conectarea la serverele MCP
- **Framework server**: Pentru crearea de servere MCP personalizate
- **Gestionare a protocolului**: Pentru administrarea comunicării
- **Integrare cu instrumente**: Pentru executarea funcțiilor externe

## Implementare practică: Clientul MCP Phi-4

Să explorăm o implementare reală folosind modelul mini Phi-4 de la Microsoft, integrat cu capabilități MCP.

### Prezentare generală a arhitecturii MCP

MCP urmează o **arhitectură client-server**, unde un host MCP (o aplicație AI precum Claude Code sau Claude Desktop) stabilește conexiuni cu unul sau mai multe servere MCP. Host-ul MCP realizează acest lucru prin crearea unui client MCP pentru fiecare server MCP.

#### Participanți cheie

- **Host MCP**: Aplicația AI care coordonează și gestionează unul sau mai mulți clienți MCP
- **Client MCP**: Un component care menține o conexiune cu un server MCP și obține context de la serverul MCP pentru ca host-ul MCP să îl utilizeze
- **Server MCP**: Un program care oferă context clienților MCP

#### Arhitectură pe două niveluri

MCP constă în două niveluri distincte:

**Nivelul de date**: Definește protocolul bazat pe JSON-RPC pentru comunicarea client-server, incluzând:
- Gestionarea ciclului de viață (inițializarea conexiunii, negocierea capabilităților)
- Primitive de bază (instrumente, resurse, prompturi)
- Funcții client (sampling, elicitation, logging)
- Funcții utilitare (notificări, urmărirea progresului)

**Nivelul de transport**: Definește mecanismele și canalele de comunicare:
- **Transport STDIO**: Utilizează fluxurile de intrare/ieșire standard pentru procese locale (performanță optimă, fără suprasarcină de rețea)
- **Transport HTTP Streamable**: Utilizează HTTP POST cu opțional Server-Sent Events pentru servere la distanță (suportă autentificarea standard HTTP)

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### Primitivele de bază ale MCP

MCP definește primitive care specifică tipurile de informații contextuale ce pot fi partajate cu aplicațiile AI și gama de acțiuni care pot fi realizate.

#### Primitivele serverului

MCP definește trei primitive de bază pe care serverele le pot expune:

**Instrumente**: Funcții executabile pe care aplicațiile AI le pot invoca pentru a realiza acțiuni
- Exemple: operațiuni pe fișiere, apeluri API, interogări în baze de date
- Metode: `tools/list`, `tools/call`
- Suportă descoperirea și executarea dinamică

**Resurse**: Surse de date care oferă informații contextuale aplicațiilor AI
- Exemple: conținutul fișierelor, înregistrări din baze de date, răspunsuri API
- Metode: `resources/list`, `resources/read`
- Permite accesul la date structurate

**Prompturi**: Șabloane reutilizabile care ajută la structurarea interacțiunilor cu modelele de limbaj
- Exemple: prompturi de sistem, exemple few-shot
- Metode: `prompts/list`, `prompts/get`
- Standardizează modelele de interacțiune AI

#### Primitivele clientului

MCP definește, de asemenea, primitive pe care clienții le pot expune pentru a permite interacțiuni mai bogate:

**Sampling**: Permite serverelor să solicite completări ale modelului de limbaj de la aplicația AI a clientului
- Metodă: `sampling/complete`
- Permite dezvoltarea serverului independent de model
- Oferă acces la modelul de limbaj al host-ului

**Elicitation**: Permite serverelor să solicite informații suplimentare de la utilizatori
- Metodă: `elicitation/request`
- Permite interacțiunea și confirmarea utilizatorului
- Suportă colectarea dinamică a informațiilor

**Logging**: Permite serverelor să trimită mesaje de logare către clienți
- Utilizat pentru depanare și monitorizare
- Oferă vizibilitate asupra operațiunilor serverului

### Ciclul de viață al protocolului MCP

#### Inițializare și negocierea capabilităților

MCP este un protocol cu stare care necesită gestionarea ciclului de viață. Procesul de inițializare servește mai multor scopuri critice:

1. **Negocierea versiunii protocolului**: Asigură că atât clientul, cât și serverul utilizează versiuni compatibile ale protocolului (de exemplu, "2025-06-18")
2. **Descoperirea capabilităților**: Fiecare parte declară funcțiile și primitivele suportate
3. **Schimb de identitate**: Oferă informații de identificare și versiune

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Descoperirea și executarea instrumentelor

După inițializare, clienții pot descoperi și executa instrumente:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Notificări în timp real

MCP suportă notificări în timp real pentru actualizări dinamice:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## Începeți: Ghid pas cu pas

### Pasul 1: Configurarea mediului

Instalați dependențele necesare:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Pasul 2: Configurare de bază

Configurați variabilele de mediu:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Pasul 3: Rularea primului client MCP

**Configurare de bază Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Utilizarea backend-ului vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Conexiune Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Server MCP personalizat:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Pasul 4: Utilizare programatică

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Funcționalități avansate

### Suport pentru mai multe backend-uri

Implementarea suportă atât backend-uri Ollama, cât și vLLM, permițându-vă să alegeți în funcție de cerințele dvs.:

- **Ollama**: Mai bun pentru dezvoltare locală și testare
- **vLLM**: Optimizat pentru scenarii de producție și volum mare de date

### Protocoale de conexiune flexibile

Sunt suportate două moduri de conexiune:

**Modul STDIO**: Comunicare directă între procese
- Latență redusă
- Potrivit pentru instrumente locale
- Configurare simplă

**Modul SSE**: Streaming bazat pe HTTP
- Capabil de rețea
- Mai bun pentru sisteme distribuite
- Actualizări în timp real

### Capacități de integrare a instrumentelor

Sistemul poate fi integrat cu diverse instrumente:
- Automatizare web (Playwright)
- Operațiuni pe fișiere
- Interacțiuni API
- Comenzi de sistem
- Funcții personalizate

## Gestionarea erorilor și bune practici

### Gestionarea cuprinzătoare a erorilor

Implementarea include gestionarea robustă a erorilor pentru:

**Erori de conexiune:**
- Defecțiuni ale serverului MCP
- Timeout-uri de rețea
- Probleme de conectivitate

**Erori de execuție a instrumentelor:**
- Instrumente lipsă
- Validarea parametrilor
- Eșecuri de execuție

**Erori de procesare a răspunsurilor:**
- Probleme de parsare JSON
- Inconsistențe de format
- Anomalii în răspunsurile LLM

### Bune practici

1. **Gestionarea resurselor**: Utilizați manageri de context asincron
2. **Gestionarea erorilor**: Implementați blocuri try-catch cuprinzătoare
3. **Logare**: Activați niveluri de logare adecvate
4. **Securitate**: Validați intrările și curățați ieșirile
5. **Performanță**: Utilizați pooling de conexiuni și caching

## Aplicații reale

### Automatizare web
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Procesare de date
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integrare API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optimizarea performanței

### Gestionarea memoriei
- Gestionarea eficientă a istoricului mesajelor
- Curățarea corespunzătoare a resurselor
- Pooling de conexiuni

### Optimizarea rețelei
- Operațiuni HTTP asincrone
- Timeout-uri configurabile
- Recuperare grațioasă a erorilor

### Procesare concurentă
- I/O non-blocant
- Execuție paralelă a instrumentelor
- Modele asincrone eficiente

## Considerații de securitate

### Protecția datelor
- Gestionarea securizată a cheilor API
- Validarea intrărilor
- Curățarea ieșirilor

### Securitatea rețelei
- Suport HTTPS
- Configurații implicite pentru endpoint-uri locale
- Gestionarea securizată a token-urilor

### Siguranța execuției
- Filtrarea instrumentelor
- Medii sandbox
- Logare de audit

## Ecosistemul MCP și dezvoltarea

### Domeniul proiectului MCP

Ecosistemul Protocolului de Context al Modelului include mai multe componente cheie:

- **[Specificația MCP](https://modelcontextprotocol.io/specification/latest)**: Specificația oficială care detaliază cerințele de implementare pentru clienți și servere
- **[SDK-uri MCP](https://modelcontextprotocol.io/docs/sdk)**: SDK-uri pentru diferite limbaje de programare care implementează MCP
- **Instrumente de dezvoltare MCP**: Instrumente pentru dezvoltarea serverelor și clienților MCP, inclusiv [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[Implementări de referință ale serverelor MCP](https://github.com/modelcontextprotocol/servers)**: Implementări de referință ale serverelor MCP

### Începerea dezvoltării MCP

Pentru a începe să construiți cu MCP:

**Construiți servere**: [Creați servere MCP](https://modelcontextprotocol.io/docs/develop/build-server) pentru a expune datele și instrumentele dvs.

**Construiți clienți**: [Dezvoltați aplicații](https://modelcontextprotocol.io/docs/develop/build-client) care se conectează la serverele MCP

**Înțelegeți conceptele**: [Înțelegeți conceptele de bază](https://modelcontextprotocol.io/docs/learn/architecture) și arhitectura MCP

## Concluzie

SLM-urile integrate cu MCP reprezintă o schimbare de paradigmă în dezvoltarea aplicațiilor AI. Prin combinarea eficienței modelelor mici cu puterea instrumentelor externe, dezvoltatorii pot crea sisteme inteligente care sunt atât eficiente din punct de vedere al resurselor, cât și extrem de capabile.

Protocolul de Context al Modelului oferă o modalitate standardizată de a conecta aplicațiile AI la sisteme externe, la fel cum USB-C oferă un standard universal de conectare pentru dispozitive electronice. Această standardizare permite:

- **Integrare fără probleme**: Conectarea modelelor AI la surse de date și instrumente diverse
- **Creșterea ecosistemului**: Construirea o singură dată, utilizarea în mai multe aplicații AI
- **Capabilități extinse**: Completarea SLM-urilor cu funcționalități externe
- **Actualizări în timp real**: Suport pentru aplicații AI dinamice și receptive

Aspecte cheie:
- MCP este un standard deschis care conectează aplicațiile AI și sistemele externe
- Protocolul suportă instrumente, resurse și prompturi ca primitive de bază
- Notificările în timp real permit aplicații dinamice și receptive
- Gestionarea corespunzătoare a ciclului de viață și a erorilor este esențială pentru utilizarea în producție
- Ecosistemul oferă SDK-uri cuprinzătoare și instrumente de dezvoltare

## Referințe și lecturi suplimentare

### Documentația oficială MCP

- **[Site-ul oficial al Protocolului de Context al Modelului](https://modelcontextprotocol.io/)** - Documentație completă și specificații
- **[Ghidul de început MCP](https://modelcontextprotocol.io/docs/getting-started/intro)** - Introducere și concepte de bază
- **[Prezentare generală a arhitecturii MCP](https://modelcontextprotocol.io/docs/learn/architecture)** - Arhitectură tehnică detaliată
- **[Specificația MCP](https://modelcontextprotocol.io/specification/latest)** - Specificația oficială a protocolului
- **[Documentația SDK-urilor MCP](https://modelcontextprotocol.io/docs/sdk)** - Ghiduri SDK specifice limbajului

### Resurse de dezvoltare

- **[MCP pentru începători](https://aka.ms/mcp-for-beginners)** - Ghid cuprinzător pentru începători despre Protocolul de Context al Modelului
- **[Organizația GitHub MCP](https://github.com/modelcontextprotocol)** - Repozitorii oficiale și exemple
- **[Repozitoriu server MCP](https://github.com/modelcontextprotocol/servers)** - Implementări de referință ale serverelor
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - Instrument de dezvoltare și depanare
- **[Ghid pentru construirea serverelor MCP](https://modelcontextprotocol.io/docs/develop/build-server)** - Tutorial pentru dezvoltarea serverelor
- **[Ghid pentru construirea clienților MCP](https://modelcontextprotocol.io/docs/develop/build-client)** - Tutorial pentru dezvoltarea clienților

### Modele de limbaj mici și AI la margine

- **[Modelele Phi de la Microsoft](https://
- **[Documentația Ollama](https://ollama.ai/docs)** - Platformă pentru implementarea locală a LLM
- **[Documentația vLLM](https://docs.vllm.ai/)** - Servire LLM de înaltă performanță

### Standarde tehnice și protocoale

- **[Specificația JSON-RPC 2.0](https://www.jsonrpc.org/)** - Protocolul RPC de bază utilizat de MCP
- **[JSON Schema](https://json-schema.org/)** - Standard de definire a schemelor pentru instrumentele MCP
- **[Specificația OpenAPI](https://swagger.io/specification/)** - Standard pentru documentația API
- **[Evenimente trimise de server (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Standard web pentru actualizări în timp real

### Dezvoltarea agenților AI

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Dezvoltare de agenți pregătiți pentru producție
- **[Documentația LangChain](https://docs.langchain.com/)** - Cadru pentru integrarea agenților și instrumentelor
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - SDK-ul Microsoft pentru orchestrarea AI

### Rapoarte și cercetări din industrie

- **[Anunțul Protocolului de Context al Modelului de la Anthropic](https://www.anthropic.com/news/model-context-protocol)** - Introducerea originală a MCP
- **[Studiu despre Modele de Limbaj Mici](https://arxiv.org/abs/2410.20011)** - Studiu academic despre cercetarea SLM
- **[Analiza pieței AI la margine](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Tendințe și previziuni din industrie
- **[Cele mai bune practici pentru dezvoltarea agenților AI](https://arxiv.org/abs/2309.02427)** - Cercetare despre arhitecturile agenților

Această secțiune oferă baza pentru construirea propriilor aplicații MCP alimentate de SLM, deschizând posibilități pentru automatizare, procesarea datelor și integrarea sistemelor inteligente.

## ➡️ Ce urmează

- [Modulul 7. Exemple de AI la margine](../Module07/README.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să fiți conștienți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa maternă ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de oameni. Nu ne asumăm responsabilitatea pentru neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.