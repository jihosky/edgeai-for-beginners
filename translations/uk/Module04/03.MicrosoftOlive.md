<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T15:16:07+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "uk"
}
-->
# Розділ 3: Microsoft Olive Optimization Suite

## Зміст
1. [Вступ](../../../Module04)
2. [Що таке Microsoft Olive?](../../../Module04)
3. [Встановлення](../../../Module04)
4. [Швидкий старт](../../../Module04)
5. [Приклад: Конвертація Qwen3 до ONNX INT4](../../../Module04)
6. [Розширене використання](../../../Module04)
7. [Репозиторій рецептів Olive](../../../Module04)
8. [Найкращі практики](../../../Module04)
9. [Вирішення проблем](../../../Module04)
10. [Додаткові ресурси](../../../Module04)

## Вступ

Microsoft Olive — це потужний, простий у використанні інструмент оптимізації моделей, орієнтований на апаратне забезпечення, який спрощує процес оптимізації моделей машинного навчання для розгортання на різних апаратних платформах. Незалежно від того, чи ви працюєте з CPU, GPU або спеціалізованими AI-акселераторами, Olive допомагає досягти оптимальної продуктивності, зберігаючи точність моделі.

## Що таке Microsoft Olive?

Olive — це простий у використанні інструмент оптимізації моделей, орієнтований на апаратне забезпечення, який об'єднує провідні галузеві методи стиснення, оптимізації та компіляції моделей. Він працює з ONNX Runtime як комплексне рішення для оптимізації інференсу.

### Основні функції

- **Оптимізація, орієнтована на апаратне забезпечення**: Автоматично вибирає найкращі методи оптимізації для вашого цільового апаратного забезпечення
- **40+ вбудованих компонентів оптимізації**: Включає стиснення моделей, квантування, оптимізацію графів та інше
- **Простий CLI інтерфейс**: Легкі команди для поширених завдань оптимізації
- **Підтримка багатьох фреймворків**: Працює з PyTorch, моделями Hugging Face та ONNX
- **Підтримка популярних моделей**: Olive може автоматично оптимізувати популярні архітектури моделей, такі як Llama, Phi, Qwen, Gemma тощо, "з коробки"

### Переваги

- **Скорочення часу розробки**: Не потрібно вручну експериментувати з різними методами оптимізації
- **Підвищення продуктивності**: Значне покращення швидкості (до 6 разів у деяких випадках)
- **Кросплатформне розгортання**: Оптимізовані моделі працюють на різних апаратних платформах і операційних системах
- **Збереження точності**: Оптимізації зберігають якість моделі, покращуючи продуктивність

## Встановлення

### Попередні вимоги

- Python 3.8 або новіший
- Менеджер пакетів pip
- Віртуальне середовище (рекомендується)

### Базове встановлення

Створіть і активуйте віртуальне середовище:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Встановіть Olive з функціями автоматичної оптимізації:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Додаткові залежності

Olive пропонує різні додаткові залежності для розширених функцій:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Перевірка встановлення

```bash
olive --help
```

Якщо все успішно, ви побачите повідомлення довідки Olive CLI.

## Швидкий старт

### Ваша перша оптимізація

Оптимізуємо невелику мовну модель за допомогою функції автоматичної оптимізації Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Що робить ця команда

Процес оптимізації включає: отримання моделі з локального кешу, захоплення ONNX Graph і збереження ваг у файлі даних ONNX, оптимізацію ONNX Graph і квантування моделі до int4 за допомогою методу RTN.

### Пояснення параметрів команди

- `--model_name_or_path`: Ідентифікатор моделі Hugging Face або локальний шлях
- `--output_path`: Каталог, де буде збережена оптимізована модель
- `--device`: Цільовий пристрій (cpu, gpu)
- `--provider`: Провайдер виконання (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Використовувати ONNX Runtime Generate AI для інференсу
- `--precision`: Точність квантування (int4, int8, fp16)
- `--log_level`: Рівень деталізації журналу (0=мінімальний, 1=детальний)

## Приклад: Конвертація Qwen3 до ONNX INT4

На основі прикладу Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), ось як оптимізувати модель Qwen3:

### Крок 1: Завантаження моделі (опціонально)

Щоб мінімізувати час завантаження, кешуйте лише необхідні файли:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Крок 2: Оптимізація моделі Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Крок 3: Тестування оптимізованої моделі

Створіть простий Python-скрипт для тестування вашої оптимізованої моделі:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Структура вихідних даних

Після оптимізації ваш каталог вихідних даних буде містити:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Розширене використання

### Файли конфігурації

Для більш складних робочих процесів оптимізації можна використовувати JSON-файли конфігурації:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Запуск з конфігурацією:

```bash
olive run --config config.json
```

### Оптимізація GPU

Для оптимізації CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Для DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Тонке налаштування з Olive

Olive також підтримує тонке налаштування моделей:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Найкращі практики

### 1. Вибір моделі
- Починайте з менших моделей для тестування (наприклад, 0.5B-7B параметрів)
- Переконайтеся, що архітектура вашої цільової моделі підтримується Olive

### 2. Апаратні аспекти
- Виберіть ціль оптимізації відповідно до вашого апаратного забезпечення
- Використовуйте оптимізацію GPU, якщо у вас є апаратне забезпечення, сумісне з CUDA
- Розгляньте DirectML для Windows-машин з інтегрованою графікою

### 3. Вибір точності
- **INT4**: Максимальне стиснення, невелика втрата точності
- **INT8**: Хороший баланс між розміром і точністю
- **FP16**: Мінімальна втрата точності, помірне зменшення розміру

### 4. Тестування та валідація
- Завжди тестуйте оптимізовані моделі для ваших конкретних випадків використання
- Порівнюйте показники продуктивності (затримка, пропускна здатність, точність)
- Використовуйте репрезентативні вхідні дані для оцінки

### 5. Ітеративна оптимізація
- Починайте з автоматичної оптимізації для швидких результатів
- Використовуйте файли конфігурації для детального контролю
- Експериментуйте з різними проходами оптимізації

## Вирішення проблем

### Поширені проблеми

#### 1. Проблеми з встановленням
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Проблеми з CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Проблеми з пам'яттю
- Використовуйте менші розміри пакетів під час оптимізації
- Спробуйте квантування з більшою точністю спочатку (int8 замість int4)
- Переконайтеся, що є достатньо місця на диску для кешування моделі

#### 4. Помилки завантаження моделі
- Перевірте шлях до моделі та права доступу
- Переконайтеся, що модель потребує `trust_remote_code=True`
- Переконайтеся, що всі необхідні файли моделі завантажені

### Отримання допомоги

- **Документація**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Приклади**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Репозиторій рецептів Olive

### Вступ до рецептів Olive

Репозиторій [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) доповнює основний інструмент Olive, надаючи широкий набір готових до використання рецептів оптимізації для популярних AI-моделей. Цей репозиторій слугує практичним довідником як для оптимізації загальнодоступних моделей, так і для створення робочих процесів оптимізації для власних моделей.

### Основні функції

- **100+ готових рецептів**: Готові конфігурації оптимізації для популярних моделей
- **Підтримка багатьох архітектур**: Охоплює трансформерні моделі, моделі зору та мультимодальні архітектури
- **Оптимізації, орієнтовані на апаратне забезпечення**: Рецепти, адаптовані для CPU, GPU та спеціалізованих акселераторів
- **Популярні сімейства моделей**: Включає Phi, Llama, Qwen, Gemma, Mistral та багато інших

### Підтримувані сімейства моделей

Репозиторій включає рецепти оптимізації для:

#### Мовні моделі
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, серія Qwen2.5 (0.5B до 14B)
- **Google Gemma**: Різні конфігурації моделей Gemma
- **Mistral AI**: Серія Mistral-7B
- **DeepSeek**: Моделі серії R1-Distill

#### Моделі зору та мультимодальні моделі
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: Різні конфігурації CLIP-ViT
- **ResNet**: Оптимізації ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Спеціалізовані моделі
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Базові та багатомовні варіанти
- **Sentence Transformers**: all-MiniLM-L6-v2

### Використання рецептів Olive

#### Метод 1: Клонування конкретного рецепта

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Метод 2: Використання рецепта як шаблону

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Структура рецепта

Кожен каталог рецепта зазвичай містить:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Приклад: Використання рецепта Phi-4-mini

Розглянемо рецепт Phi-4-mini як приклад:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Файл конфігурації зазвичай включає:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Налаштування рецептів

#### Зміна цільового апаратного забезпечення

Щоб змінити цільове апаратне забезпечення, оновіть розділ `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Налаштування параметрів оптимізації

Змініть розділ `passes` для різних рівнів оптимізації:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Створення власного рецепта

1. **Почніть зі схожої моделі**: Знайдіть рецепт для моделі зі схожою архітектурою
2. **Оновіть конфігурацію моделі**: Змініть назву/шлях моделі в конфігурації
3. **Налаштуйте параметри**: Змініть параметри оптимізації за потреби
4. **Тестуйте та перевіряйте**: Запустіть оптимізацію та перевірте результати
5. **Внесіть свій внесок**: Розгляньте можливість додати свій рецепт до репозиторію

### Переваги використання рецептів

#### 1. **Перевірені конфігурації**
- Перевірені налаштування оптимізації для конкретних моделей
- Уникає проб і помилок у пошуку оптимальних параметрів

#### 2. **Тонке налаштування для апаратного забезпечення**
- Попередньо оптимізовані для різних провайдерів виконання
- Готові конфігурації для цілей CPU, GPU та NPU

#### 3. **Широке охоплення**
- Підтримка найпопулярніших моделей з відкритим кодом
- Регулярні оновлення з новими випусками моделей

#### 4. **Внески спільноти**
- Спільна розробка з AI-спільнотою
- Спільний досвід і найкращі практики

### Внесок у рецепти Olive

Якщо ви оптимізували модель, яка не охоплена в репозиторії:

1. **Форк репозиторію**: Створіть власний форк olive-recipes
2. **Створіть каталог рецепта**: Додайте новий каталог для вашої моделі
3. **Додайте конфігурацію**: Додайте olive_config.json та супровідні файли
4. **Документуйте використання**: Надайте чіткий README з інструкціями
5. **Надішліть Pull Request**: Внесіть свій внесок у спільноту

### Оцінка продуктивності

Багато рецептів включають оцінку продуктивності, яка показує:
- **Покращення затримки**: Зазвичай 2-6 разів швидше за базовий рівень
- **Зменшення використання пам'яті**: Зниження використання пам'яті на 50-75% завдяки квантуванню
- **Збереження точності**: Збереження точності на рівні 95-99%

### Інтеграція з AI Toolkit

Рецепти безперешкодно працюють з:
- **VS Code AI Toolkit**: Пряма інтеграція для оптимізації моделей
- **Azure Machine Learning**: Хмарні робочі процеси оптимізації
- **ONNX Runtime**: Оптимізоване розгортання інференсу

## Додаткові ресурси

### Офіційні посилання
- **GitHub Репозиторій**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Репозиторій рецептів Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Документація ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Приклад Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Приклади спільноти
- **Jupyter Notebooks**: Доступні в репозиторії Olive GitHub — https://github.com/microsoft/Olive/tree/main/examples
- **Розширення VS Code**: Огляд AI Toolkit для VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Блог-пости**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Суміжні інструменти
- **ONNX Runtime**: Високопродуктивний інференс-двигун — https://onnxruntime.ai/
- **Hugging Face Transformers**: Джерело багатьох сумісних моделей — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Хмарні робочі процеси оптимізації — https://learn.microsoft.com/azure/machine-learning/


## ➡️ Що далі

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.