<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T16:02:36+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "uk"
}
-->
# Розділ 7: Qualcomm QNN (Qualcomm Neural Network) Optimization Suite

## Зміст
1. [Вступ](../../../Module04)
2. [Що таке Qualcomm QNN?](../../../Module04)
3. [Встановлення](../../../Module04)
4. [Швидкий старт](../../../Module04)
5. [Приклад: Конвертація та оптимізація моделей за допомогою QNN](../../../Module04)
6. [Розширене використання](../../../Module04)
7. [Найкращі практики](../../../Module04)
8. [Вирішення проблем](../../../Module04)
9. [Додаткові ресурси](../../../Module04)

## Вступ

Qualcomm QNN (Qualcomm Neural Network) — це комплексна платформа для виконання AI, створена для максимального використання апаратних прискорювачів Qualcomm, таких як Hexagon NPU, Adreno GPU та Kryo CPU. Незалежно від того, чи ви працюєте з мобільними пристроями, платформами для обчислень на краю мережі або автомобільними системами, QNN забезпечує оптимізоване виконання AI, використовуючи спеціалізовані AI-обчислювальні блоки Qualcomm для досягнення максимальної продуктивності та енергоефективності.

## Що таке Qualcomm QNN?

Qualcomm QNN — це уніфікована платформа для виконання AI, яка дозволяє розробникам ефективно впроваджувати AI-моделі на гетерогенній обчислювальній архітектурі Qualcomm. Вона забезпечує уніфікований програмний інтерфейс для доступу до Hexagon NPU (Neural Processing Unit), Adreno GPU та Kryo CPU, автоматично вибираючи оптимальний обчислювальний блок для різних шарів моделі та операцій.

### Основні функції

- **Гетерогенне обчислення**: Уніфікований доступ до NPU, GPU та CPU з автоматичним розподілом навантаження
- **Оптимізація з урахуванням апаратного забезпечення**: Спеціалізовані оптимізації для платформ Qualcomm Snapdragon
- **Підтримка квантування**: Розширені техніки квантування INT8, INT16 та змішаної точності
- **Інструменти конвертації моделей**: Пряма підтримка моделей TensorFlow, PyTorch, ONNX та Caffe
- **Оптимізація для Edge AI**: Розроблено спеціально для мобільних та краєвих сценаріїв з акцентом на енергоефективність

### Переваги

- **Максимальна продуктивність**: Використання спеціалізованого AI-обладнання для досягнення до 15-кратного покращення продуктивності
- **Енергоефективність**: Оптимізовано для мобільних пристроїв та пристроїв з живленням від батареї з інтелектуальним управлінням енергією
- **Низька затримка**: Виконання з апаратним прискоренням з мінімальними накладними витратами для реального часу
- **Масштабоване впровадження**: Від смартфонів до автомобільних платформ у екосистемі Qualcomm
- **Готовність до виробництва**: Перевірена платформа, яка використовується в мільйонах пристроїв

## Встановлення

### Попередні вимоги

- Qualcomm QNN SDK (потрібна реєстрація на Qualcomm)
- Python 3.7 або новіший
- Сумісне апаратне забезпечення Qualcomm або симулятор
- Android NDK (для мобільного впровадження)
- Середовище розробки на Linux або Windows

### Налаштування QNN SDK

1. **Реєстрація та завантаження**: Відвідайте Qualcomm Developer Network для реєстрації та завантаження QNN SDK
2. **Розпакування SDK**: Розпакуйте QNN SDK у вашу директорію розробки
3. **Налаштування змінних середовища**: Налаштуйте шляхи для інструментів та бібліотек QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Налаштування Python середовища

Створіть та активуйте віртуальне середовище:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Встановіть необхідні пакети Python:

```bash
pip install numpy tensorflow torch onnx
```

### Перевірка встановлення

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Якщо все успішно, ви побачите інформацію про допомогу для кожного інструменту QNN.

## Швидкий старт

### Ваша перша конвертація моделі

Давайте конвертуємо просту модель PyTorch для запуску на апаратному забезпеченні Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Конвертація ONNX у формат QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Генерація бібліотеки моделі QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Що робить цей процес

Процес оптимізації включає: конвертацію оригінальної моделі у формат ONNX, перетворення ONNX у проміжне представлення QNN, застосування оптимізацій, специфічних для апаратного забезпечення, та генерацію скомпільованої бібліотеки моделі для впровадження.

### Пояснення ключових параметрів

- `--input_network`: Вихідний файл моделі ONNX
- `--output_path`: Згенерований файл вихідного коду C++
- `--input_dim`: Розміри вхідного тензора для оптимізації
- `--quantization_overrides`: Налаштування квантування
- `-t x86_64-linux-clang`: Цільова архітектура та компілятор

## Приклад: Конвертація та оптимізація моделей за допомогою QNN

### Крок 1: Розширена конвертація моделі з квантуванням

Ось як застосувати налаштування квантування під час конвертації:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Конвертація з налаштуванням квантування:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Крок 2: Оптимізація для багатобекендного виконання

Налаштування для гетерогенного виконання на NPU, GPU та CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Крок 3: Створення бінарного контексту для впровадження

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Крок 4: Виконання з QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Структура вихідних даних

Після оптимізації ваша директорія впровадження міститиме:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Розширене використання

### Налаштування бекенду

Налаштуйте специфічні оптимізації бекенду:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Динамічне квантування

Застосуйте квантування під час виконання для покращення точності:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Профілювання продуктивності

Моніторинг продуктивності на різних бекендах:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Автоматичний вибір бекенду

Реалізуйте інтелектуальний вибір бекенду на основі характеристик моделі:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Найкращі практики

### 1. Оптимізація архітектури моделі
- **Злиття шарів**: Об'єднуйте операції, такі як Conv+BatchNorm+ReLU, для кращого використання NPU
- **Глибинно-сепарабельні згортки**: Віддавайте перевагу їм над стандартними згортками для мобільного впровадження
- **Дизайн, дружній до квантування**: Використовуйте активації ReLU та уникайте операцій, які погано квантуються

### 2. Стратегія квантування
- **Квантування після навчання**: Почніть з цього для швидкого впровадження
- **Калібрувальний набір даних**: Використовуйте репрезентативні дані, що охоплюють всі варіації вхідних даних
- **Змішана точність**: Використовуйте INT8 для більшості шарів, залишаючи критичні шари у вищій точності

### 3. Рекомендації щодо вибору бекенду
- **NPU (HTP)**: Найкраще для CNN-навантажень, квантуваних моделей та енергоефективних застосувань
- **GPU**: Оптимально для обчислювально-інтенсивних операцій, більших моделей та точності FP16
- **CPU**: Резерв для непідтримуваних операцій та налагодження

### 4. Оптимізація продуктивності
- **Розмір пакету**: Використовуйте розмір пакету 1 для застосувань у реальному часі, більші пакети для пропускної здатності
- **Попередня обробка введення**: Мінімізуйте накладні витрати на копіювання та конвертацію даних
- **Повторне використання контексту**: Попередньо компілюйте контексти, щоб уникнути накладних витрат на компіляцію під час виконання

### 5. Управління пам'яттю
- **Розподіл тензорів**: Використовуйте статичний розподіл, коли це можливо, щоб уникнути накладних витрат під час виконання
- **Пули пам'яті**: Реалізуйте власні пули пам'яті для часто розподілюваних тензорів
- **Повторне використання буферів**: Повторно використовуйте буфери введення/виведення між викликами виконання

### 6. Оптимізація енергоспоживання
- **Режими продуктивності**: Використовуйте відповідні режими продуктивності залежно від теплових обмежень
- **Динамічне масштабування частоти**: Дозвольте системі масштабувати частоту залежно від навантаження
- **Управління станом простою**: Правильно звільняйте ресурси, коли вони не використовуються

## Вирішення проблем

### Поширені проблеми

#### 1. Проблеми встановлення SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Помилки конвертації моделі
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Проблеми з квантуванням
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Проблеми продуктивності
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Проблеми з пам'яттю
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Сумісність бекенду
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Налагодження продуктивності

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Отримання допомоги

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Документація QNN**: Доступна в пакеті SDK
- **Форуми спільноти**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Технічна підтримка**: Через портал розробників Qualcomm

## Додаткові ресурси

### Офіційні посилання
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon Platforms**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Портал розробників**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Навчальні ресурси
- **Посібник для початківців**: Доступний у документації QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Посібник з оптимізації**: Документація SDK включає детальні рекомендації з оптимізації
- **Відеоуроки**: [YouTube-канал Qualcomm Developer Network](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Інтеграційні інструменти
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Попередньо оптимізовані моделі для апаратного забезпечення Qualcomm
- **Android Neural Networks API**: Інтеграція з Android NNAPI
- **TensorFlow Lite Delegate**: Делегат Qualcomm для TFLite

### Бенчмарки продуктивності
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Приклади спільноти
- **Зразки застосувань**: Доступні в директорії прикладів QNN SDK
- **Репозиторії GitHub**: Приклади та інструменти, створені спільнотою
- **Технічні блоги**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Супутні інструменти
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Розширені техніки квантування та стиснення
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Для порівняння та резервного впровадження
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Кросплатформенний механізм виконання

### Технічні характеристики обладнання
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon Platforms**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Що далі

Продовжуйте свою подорож у Edge AI, досліджуючи [Модуль 5: SLMOps та впровадження у виробництво](../Module05/README.md), щоб дізнатися про операційні аспекти управління життєвим циклом малих мовних моделей.

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.