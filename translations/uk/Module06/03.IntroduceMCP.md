<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T15:15:30+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "uk"
}
-->
# Розділ 03 - Інтеграція протоколу контексту моделі (MCP)

## Вступ до MCP (Протокол контексту моделі)

Протокол контексту моделі (MCP) — це стандарт з відкритим кодом для підключення AI-додатків до зовнішніх систем. Використовуючи MCP, AI-додатки, такі як Claude або ChatGPT, можуть підключатися до джерел даних (наприклад, локальних файлів, баз даних), інструментів (наприклад, пошукових систем, калькуляторів) і робочих процесів (наприклад, спеціалізованих запитів), що дозволяє їм отримувати ключову інформацію та виконувати завдання.

Уявіть MCP як **USB-C порт для AI-додатків**. Як USB-C забезпечує стандартизований спосіб підключення електронних пристроїв, MCP забезпечує стандартизований спосіб підключення AI-додатків до зовнішніх систем.

### Що може забезпечити MCP?

MCP відкриває потужні можливості для AI-додатків:

- **Персоналізовані AI-асистенти**: Агенти можуть отримувати доступ до вашого Google Календаря та Notion, діючи як більш персоналізований AI-асистент
- **Розширене генерування коду**: Claude Code може створити цілий веб-додаток, використовуючи дизайн Figma
- **Інтеграція даних підприємства**: Чат-боти для підприємств можуть підключатися до кількох баз даних організації, дозволяючи користувачам аналізувати дані через чат
- **Творчі робочі процеси**: AI-моделі можуть створювати 3D-дизайни в Blender і друкувати їх на 3D-принтері
- **Доступ до інформації в реальному часі**: Підключення до зовнішніх джерел даних для отримання актуальної інформації
- **Складні багатокрокові операції**: Виконання складних робочих процесів, що поєднують кілька інструментів і систем

### Чому MCP важливий?

MCP забезпечує переваги для всіх учасників екосистеми:

**Для розробників**: MCP скорочує час і складність розробки при створенні або інтеграції AI-додатків чи агентів.

**Для AI-додатків**: MCP забезпечує доступ до екосистеми джерел даних, інструментів і додатків, що покращує функціональність і досвід кінцевого користувача.

**Для кінцевих користувачів**: MCP дозволяє створювати більш функціональні AI-додатки або агенти, які можуть отримувати доступ до ваших даних і виконувати дії від вашого імені, коли це необхідно.

## Малі мовні моделі (SLMs) у MCP

Малі мовні моделі представляють ефективний підхід до розгортання AI, пропонуючи кілька переваг:

### Переваги SLMs
- **Ефективність ресурсів**: Менші вимоги до обчислювальних ресурсів
- **Швидший час відповіді**: Зменшена затримка для додатків у реальному часі  
- **Економічність**: Мінімальні потреби в інфраструктурі
- **Конфіденційність**: Можливість локального запуску без передачі даних
- **Налаштування**: Легше адаптувати до конкретних доменів

### Чому SLMs добре працюють із MCP

SLMs у поєднанні з MCP створюють потужну комбінацію, де здатність моделі до логічного мислення доповнюється зовнішніми інструментами, компенсуючи меншу кількість параметрів за рахунок розширеної функціональності.

## Огляд Python MCP SDK

Python MCP SDK забезпечує основу для створення додатків із підтримкою MCP. SDK включає:

- **Бібліотеки клієнта**: Для підключення до серверів MCP
- **Фреймворк сервера**: Для створення власних серверів MCP
- **Обробники протоколу**: Для управління комунікацією
- **Інтеграція інструментів**: Для виконання зовнішніх функцій

## Практична реалізація: клієнт Phi-4 MCP

Розглянемо реальну реалізацію з використанням міні-моделі Phi-4 від Microsoft, інтегрованої з можливостями MCP.

### Огляд архітектури MCP

MCP використовує **архітектуру клієнт-сервер**, де хост MCP (AI-додаток, наприклад, Claude Code або Claude Desktop) встановлює з'єднання з одним або кількома серверами MCP. Хост MCP робить це, створюючи одного клієнта MCP для кожного сервера MCP.

#### Основні учасники

- **Хост MCP**: AI-додаток, який координує та управляє одним або кількома клієнтами MCP
- **Клієнт MCP**: Компонент, який підтримує з'єднання із сервером MCP і отримує контекст від сервера MCP для використання хостом MCP
- **Сервер MCP**: Програма, яка надає контекст клієнтам MCP

#### Дворівнева архітектура

MCP складається з двох окремих рівнів:

**Рівень даних**: Визначає протокол на основі JSON-RPC для комунікації клієнт-сервер, включаючи:
- Управління життєвим циклом (ініціалізація з'єднання, узгодження можливостей)
- Основні примітиви (інструменти, ресурси, запити)
- Функції клієнта (семплінг, отримання інформації, логування)
- Утилітарні функції (сповіщення, відстеження прогресу)

**Транспортний рівень**: Визначає механізми та канали комунікації:
- **Транспорт STDIO**: Використовує стандартні потоки вводу/виводу для локальних процесів (оптимальна продуктивність, відсутність мережевих витрат)
- **Транспорт HTTP Streamable**: Використовує HTTP POST із опціональними Server-Sent Events для віддалених серверів (підтримує стандартну HTTP-аутентифікацію)

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### Основні примітиви MCP

MCP визначає примітиви, які вказують типи контекстної інформації, що можуть бути передані AI-додаткам, і діапазон дій, які можуть бути виконані.

#### Примітиви сервера

MCP визначає три основні примітиви, які сервери можуть надавати:

**Інструменти**: Виконувані функції, які AI-додатки можуть викликати для виконання дій
- Приклади: операції з файлами, виклики API, запити до баз даних
- Методи: `tools/list`, `tools/call`
- Підтримують динамічне виявлення та виконання

**Ресурси**: Джерела даних, які надають контекстну інформацію AI-додаткам
- Приклади: вміст файлів, записи баз даних, відповіді API
- Методи: `resources/list`, `resources/read`
- Забезпечують доступ до структурованих даних

**Запити**: Шаблони, які допомагають структурувати взаємодію з мовними моделями
- Приклади: системні запити, приклади few-shot
- Методи: `prompts/list`, `prompts/get`
- Стандартизують шаблони взаємодії AI

#### Примітиви клієнта

MCP також визначає примітиви, які клієнти можуть надавати для забезпечення більш багатих взаємодій:

**Семплінг**: Дозволяє серверам запитувати завершення мовної моделі від AI-додатка клієнта
- Метод: `sampling/complete`
- Забезпечує незалежну від моделі розробку серверів
- Надає доступ до мовної моделі хоста

**Отримання інформації**: Дозволяє серверам запитувати додаткову інформацію від користувачів
- Метод: `elicitation/request`
- Забезпечує взаємодію з користувачем і підтвердження
- Підтримує динамічне збирання інформації

**Логування**: Дозволяє серверам надсилати повідомлення журналу клієнтам
- Використовується для налагодження та моніторингу
- Забезпечує видимість операцій сервера

### Життєвий цикл протоколу MCP

#### Ініціалізація та узгодження можливостей

MCP — це протокол із станом, який потребує управління життєвим циклом. Процес ініціалізації виконує кілька критичних завдань:

1. **Узгодження версії протоколу**: Забезпечує використання сумісних версій протоколу клієнтом і сервером (наприклад, "2025-06-18")
2. **Виявлення можливостей**: Кожна сторона декларує підтримувані функції та примітиви
3. **Обмін ідентифікацією**: Надає інформацію про ідентифікацію та версії

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Виявлення та виконання інструментів

Після ініціалізації клієнти можуть виявляти та виконувати інструменти:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Сповіщення в реальному часі

MCP підтримує сповіщення в реальному часі для динамічних оновлень:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## Початок роботи: покроковий посібник

### Крок 1: Налаштування середовища

Встановіть необхідні залежності:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Крок 2: Базова конфігурація

Налаштуйте змінні середовища:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Крок 3: Запуск першого клієнта MCP

**Базове налаштування Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Використання бекенду vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Підключення через Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Користувацький сервер MCP:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Крок 4: Програмне використання

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Розширені функції

### Підтримка кількох бекендів

Реалізація підтримує як Ollama, так і vLLM бекенди, дозволяючи вибирати залежно від ваших потреб:

- **Ollama**: Краще для локальної розробки та тестування
- **vLLM**: Оптимізовано для виробництва та сценаріїв із високою пропускною здатністю

### Гнучкі протоколи підключення

Підтримуються два режими підключення:

**Режим STDIO**: Пряме спілкування процесів
- Менша затримка
- Підходить для локальних інструментів
- Просте налаштування

**Режим SSE**: Потокове HTTP-з'єднання
- Мережеві можливості
- Краще для розподілених систем
- Оновлення в реальному часі

### Можливості інтеграції інструментів

Система може інтегруватися з різними інструментами:
- Веб-автоматизація (Playwright)
- Операції з файлами
- Взаємодія з API
- Системні команди
- Користувацькі функції

## Обробка помилок і найкращі практики

### Комплексне управління помилками

Реалізація включає надійне управління помилками для:

**Помилки підключення:**
- Збої серверів MCP
- Тайм-аути мережі
- Проблеми з підключенням

**Помилки виконання інструментів:**
- Відсутні інструменти
- Перевірка параметрів
- Збої виконання

**Помилки обробки відповідей:**
- Проблеми з розбором JSON
- Непослідовності формату
- Аномалії відповідей LLM

### Найкращі практики

1. **Управління ресурсами**: Використовуйте асинхронні контекстні менеджери
2. **Обробка помилок**: Реалізуйте комплексні блоки try-catch
3. **Логування**: Увімкніть відповідні рівні логування
4. **Безпека**: Перевіряйте введення та очищуйте виведення
5. **Продуктивність**: Використовуйте пулінг з'єднань і кешування

## Реальні застосування

### Веб-автоматизація
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Обробка даних
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Інтеграція API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Оптимізація продуктивності

### Управління пам'яттю
- Ефективна обробка історії повідомлень
- Правильне очищення ресурсів
- Пулінг з'єднань

### Оптимізація мережі
- Асинхронні HTTP-операції
- Налаштовувані тайм-аути
- Гнучке відновлення після помилок

### Конкурентна обробка
- Неблокуючий ввід/вивід
- Паралельне виконання інструментів
- Ефективні асинхронні шаблони

## Міркування щодо безпеки

### Захист даних
- Безпечне управління ключами API
- Перевірка введення
- Очищення виведення

### Мережева безпека
- Підтримка HTTPS
- Локальні налаштування за замовчуванням
- Безпечне управління токенами

### Безпека виконання
- Фільтрація інструментів
- Пісочниця для середовищ
- Логування аудиту

## Екосистема MCP та розробка

### Обсяг проекту MCP

Екосистема протоколу контексту моделі включає кілька ключових компонентів:

- **[Специфікація MCP](https://modelcontextprotocol.io/specification/latest)**: Офіційна специфікація, що описує вимоги до реалізації клієнтів і серверів
- **[SDK MCP](https://modelcontextprotocol.io/docs/sdk)**: SDK для різних мов програмування, які реалізують MCP
- **Інструменти розробки MCP**: Інструменти для розробки серверів і клієнтів MCP, включаючи [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[Референсні реалізації серверів MCP](https://github.com/modelcontextprotocol/servers)**: Референсні реалізації серверів MCP

### Початок розробки з MCP

Щоб почати створювати з MCP:

**Створюйте сервери**: [Створіть сервери MCP](https://modelcontextprotocol.io/docs/develop/build-server), щоб надати ваші дані та інструменти

**Створюйте клієнти**: [Розробляйте додатки](https://modelcontextprotocol.io/docs/develop/build-client), які підключаються до серверів MCP

**Вивчайте концепції**: [Зрозумійте основні концепції](https://modelcontextprotocol.io/docs/learn/architecture) та архітектуру MCP

## Висновок

SLMs, інтегровані з MCP, представляють зміну парадигми в розробці AI-додатків. Поєднуючи ефективність малих моделей із потужністю зовнішніх інструментів, розробники можуть створювати інтелектуальні системи, які є одночасно ресурсоефективними та високофункціональними.

Протокол контексту моделі забезпечує стандартизований спосіб підключення AI-додатків до зовнішніх систем, як USB-C забезпечує універсальний стандарт підключення для електронних пристроїв. Ця стандартизація дозволяє:

- **Безшовна інтеграція**: Підключення AI-моделей до різноманітних джерел даних та інструментів
- **Розвиток екосистеми**: Створюйте один раз, використовуйте в кількох AI-додатках
- **Розширені можливості**: Доповнюйте SLMs зовнішньою функціональністю
- **Оновлення в реальному часі**: Підтримуйте динамічні, чутливі AI-додатки

Основні висновки:
- MCP — це відкритий стандарт, який об'єднує AI-додатки та зовнішні системи
- Прот
- **[Документація Ollama](https://ollama.ai/docs)** - Платформа для локального розгортання LLM
- **[Документація vLLM](https://docs.vllm.ai/)** - Високопродуктивне обслуговування LLM

### Технічні стандарти та протоколи

- **[Специфікація JSON-RPC 2.0](https://www.jsonrpc.org/)** - Основний RPC-протокол, який використовується MCP
- **[JSON Schema](https://json-schema.org/)** - Стандарт визначення схем для інструментів MCP
- **[Специфікація OpenAPI](https://swagger.io/specification/)** - Стандарт документації API
- **[Події, що надсилаються сервером (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Веб-стандарт для оновлень у реальному часі

### Розробка AI-агентів

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Готовий до використання фреймворк для розробки агентів
- **[Документація LangChain](https://docs.langchain.com/)** - Фреймворк для інтеграції агентів та інструментів
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - SDK для оркестрації AI від Microsoft

### Галузеві звіти та дослідження

- **[Оголошення про протокол контексту моделі від Anthropic](https://www.anthropic.com/news/model-context-protocol)** - Початкове представлення MCP
- **[Огляд малих мовних моделей](https://arxiv.org/abs/2410.20011)** - Академічний огляд досліджень SLM
- **[Аналіз ринку Edge AI](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Тенденції та прогнози галузі
- **[Найкращі практики розробки AI-агентів](https://arxiv.org/abs/2309.02427)** - Дослідження архітектур агентів

Цей розділ забезпечує основу для створення власних додатків MCP на базі SLM, відкриваючи можливості для автоматизації, обробки даних та інтеграції інтелектуальних систем.

## ➡️ Що далі

- [Модуль 7. Зразки Edge AI](../Module07/README.md)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.