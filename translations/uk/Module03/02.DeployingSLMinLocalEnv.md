<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T10:09:14+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "uk"
}
-->
# Розділ 2: Розгортання в локальному середовищі - рішення з пріоритетом конфіденційності

Локальне розгортання малих мовних моделей (SLM) представляє зміну парадигми у напрямку конфіденційних, економічно ефективних AI-рішень. Цей детальний посібник досліджує два потужні фреймворки — Ollama та Microsoft Foundry Local — які дозволяють розробникам використовувати весь потенціал SLM, зберігаючи повний контроль над своїм середовищем розгортання.

## Вступ

У цьому уроці ми розглянемо передові стратегії розгортання малих мовних моделей у локальних середовищах. Ми охопимо основні концепції локального AI-розгортання, дослідимо дві провідні платформи (Ollama та Microsoft Foundry Local) і надамо практичні рекомендації щодо впровадження рішень, готових до виробництва.

## Цілі навчання

До кінця цього уроку ви зможете:

- Зрозуміти архітектуру та переваги фреймворків локального розгортання SLM.
- Реалізувати розгортання, готове до виробництва, використовуючи Ollama та Microsoft Foundry Local.
- Порівняти та вибрати відповідну платформу на основі конкретних вимог і обмежень.
- Оптимізувати локальні розгортання для продуктивності, безпеки та масштабованості.

## Розуміння архітектури локального розгортання SLM

Локальне розгортання SLM представляє фундаментальну зміну від хмарних AI-сервісів до локальних рішень, які зберігають конфіденційність. Цей підхід дозволяє організаціям зберігати повний контроль над своєю AI-інфраструктурою, забезпечуючи суверенітет даних та операційну незалежність.

### Класифікація фреймворків розгортання

Розуміння різних підходів до розгортання допомагає вибрати правильну стратегію для конкретних випадків використання:

- **Орієнтовані на розробку**: Спрощене налаштування для експериментів і прототипування.
- **Корпоративного рівня**: Рішення, готові до виробництва, з можливостями інтеграції в корпоративне середовище.
- **Кросплатформні**: Універсальна сумісність з різними операційними системами та обладнанням.

### Основні переваги локального розгортання SLM

Локальне розгортання SLM пропонує кілька фундаментальних переваг, які роблять його ідеальним для корпоративних і конфіденційних застосувань:

**Конфіденційність і безпека**: Локальна обробка гарантує, що конфіденційні дані ніколи не залишають інфраструктуру організації, забезпечуючи відповідність GDPR, HIPAA та іншим нормативним вимогам. Можливі розгортання в ізольованих мережах для класифікованих середовищ, а повні журнали аудиту забезпечують контроль безпеки.

**Економічна ефективність**: Усунення моделей ціноутворення за токен значно знижує операційні витрати. Зменшення вимог до пропускної здатності та залежності від хмари забезпечує передбачувані структури витрат для корпоративного бюджету.

**Продуктивність і надійність**: Швидший час інференції без затримок мережі дозволяє реалізувати додатки в реальному часі. Офлайн-функціональність забезпечує безперервну роботу незалежно від підключення до Інтернету, а оптимізація локальних ресурсів забезпечує стабільну продуктивність.

## Ollama: Універсальна платформа локального розгортання

### Основна архітектура та філософія

Ollama розроблена як універсальна, зручна для розробників платформа, яка демократизує локальне розгортання LLM на різних апаратних конфігураціях і операційних системах.

**Технічна основа**: Побудована на надійному фреймворку llama.cpp, Ollama використовує ефективний формат моделі GGUF для оптимальної продуктивності. Кросплатформна сумісність забезпечує стабільну роботу на Windows, macOS і Linux, а інтелектуальне управління ресурсами оптимізує використання CPU, GPU та пам'яті.

**Філософія дизайну**: Ollama надає пріоритет простоті без шкоди для функціональності, пропонуючи розгортання без налаштувань для негайної продуктивності. Платформа підтримує широку сумісність моделей, забезпечуючи стабільні API для різних архітектур моделей.

### Розширені функції та можливості

**Відмінне управління моделями**: Ollama забезпечує комплексне управління життєвим циклом моделей з автоматичним завантаженням, кешуванням і версіонуванням. Платформа підтримує широкий екосистему моделей, включаючи Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral та спеціалізовані моделі для вбудовування.

**Налаштування через Modelfiles**: Досвідчені користувачі можуть створювати власні конфігурації моделей зі специфічними параметрами, системними підказками та модифікаціями поведінки. Це дозволяє оптимізувати для конкретних доменів і спеціалізованих вимог додатків.

**Оптимізація продуктивності**: Ollama автоматично виявляє та використовує доступне апаратне прискорення, включаючи NVIDIA CUDA, Apple Metal та OpenCL. Інтелектуальне управління пам'яттю забезпечує оптимальне використання ресурсів на різних апаратних конфігураціях.

### Стратегії впровадження у виробництво

**Встановлення та налаштування**: Ollama забезпечує спрощене встановлення на різних платформах через нативні інсталятори, менеджери пакетів (WinGet, Homebrew, APT) та Docker-контейнери для контейнеризованих розгортань.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Основні команди та операції**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Розширене налаштування**: Modelfiles дозволяють складне налаштування для корпоративних вимог:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Приклади інтеграції для розробників

**Інтеграція через Python API**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Інтеграція через JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Використання RESTful API через cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Налаштування продуктивності та оптимізація

**Конфігурація пам'яті та потоків**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Вибір квантування для різного обладнання**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Платформа корпоративного Edge AI

### Архітектура корпоративного рівня

Microsoft Foundry Local представляє комплексне корпоративне рішення, спеціально розроблене для розгортання Edge AI з глибокою інтеграцією в екосистему Microsoft.

**Основа на ONNX**: Побудована на стандартному ONNX Runtime, Foundry Local забезпечує оптимізовану продуктивність на різних апаратних архітектурах. Платформа використовує інтеграцію Windows ML для нативної оптимізації Windows, зберігаючи кросплатформну сумісність.

**Відмінне апаратне прискорення**: Foundry Local має інтелектуальне виявлення та оптимізацію апаратного забезпечення для CPU, GPU та NPU. Глибока співпраця з постачальниками обладнання (AMD, Intel, NVIDIA, Qualcomm) забезпечує оптимальну продуктивність на корпоративних апаратних конфігураціях.

### Розширений досвід розробників

**Доступ через кілька інтерфейсів**: Foundry Local надає комплексні інтерфейси розробки, включаючи потужний CLI для управління моделями та розгортання, багатомовні SDK (Python, NodeJS) для нативної інтеграції та RESTful API з сумісністю OpenAI для безперешкодної міграції.

**Інтеграція з Visual Studio**: Платформа безперешкодно інтегрується з AI Toolkit для VS Code, надаючи інструменти для конвертації моделей, квантування та оптимізації в середовищі розробки. Ця інтеграція прискорює робочі процеси розробки та знижує складність розгортання.

**Пайплайн оптимізації моделей**: Інтеграція Microsoft Olive дозволяє складні робочі процеси оптимізації моделей, включаючи динамічне квантування, оптимізацію графів та налаштування для конкретного обладнання. Можливості хмарної конвертації через Azure ML забезпечують масштабовану оптимізацію для великих моделей.

### Стратегії впровадження у виробництво

**Встановлення та конфігурація**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Операції управління моделями**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Розширена конфігурація розгортання**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Інтеграція в корпоративну екосистему

**Безпека та відповідність**: Foundry Local забезпечує функції безпеки корпоративного рівня, включаючи контроль доступу на основі ролей, ведення журналів аудиту, звітність про відповідність та зашифроване зберігання моделей. Інтеграція з інфраструктурою безпеки Microsoft забезпечує дотримання політик безпеки підприємства.

**Вбудовані AI-сервіси**: Платформа пропонує готові до використання AI-можливості, включаючи Phi Silica для локальної обробки мов, AI Imaging для покращення та аналізу зображень, а також спеціалізовані API для поширених завдань корпоративного AI.

## Порівняльний аналіз: Ollama vs Foundry Local

### Порівняння технічної архітектури

| **Аспект** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Формат моделі** | GGUF (через llama.cpp) | ONNX (через ONNX Runtime) |
| **Фокус платформи** | Універсальна кросплатформна | Оптимізація для Windows/корпоративного середовища |
| **Інтеграція обладнання** | Загальна підтримка GPU/CPU | Глибока підтримка Windows ML, NPU |
| **Оптимізація** | Квантування llama.cpp | Microsoft Olive + ONNX Runtime |
| **Корпоративні функції** | Орієнтована на спільноту | Корпоративного рівня зі SLA |

### Характеристики продуктивності

**Сильні сторони продуктивності Ollama**:
- Виняткова продуктивність CPU завдяки оптимізації llama.cpp.
- Стабільна поведінка на різних платформах та обладнанні.
- Ефективне використання пам'яті з інтелектуальним завантаженням моделей.
- Швидкий час запуску для сценаріїв розробки та тестування.

**Переваги продуктивності Foundry Local**:
- Відмінне використання NPU на сучасному обладнанні Windows.
- Оптимізоване прискорення GPU через партнерство з постачальниками.
- Моніторинг продуктивності корпоративного рівня та оптимізація.
- Масштабовані можливості розгортання для виробничих середовищ.

### Аналіз досвіду розробників

**Досвід розробників Ollama**:
- Мінімальні вимоги до налаштування з негайною продуктивністю.
- Інтуїтивно зрозумілий інтерфейс командного рядка для всіх операцій.
- Широка підтримка спільноти та документація.
- Гнучке налаштування через Modelfiles.

**Досвід розробників Foundry Local**:
- Комплексна інтеграція IDE з екосистемою Visual Studio.
- Робочі процеси корпоративної розробки з функціями командної співпраці.
- Професійні канали підтримки з підтримкою Microsoft.
- Розширені інструменти налагодження та оптимізації.

### Оптимізація випадків використання

**Виберіть Ollama, якщо**:
- Розробляєте кросплатформні додатки, які потребують стабільної поведінки.
- Пріоритетом є прозорість відкритого коду та внески спільноти.
- Працюєте з обмеженими ресурсами або бюджетними обмеженнями.
- Створюєте експериментальні або дослідницькі додатки.
- Потрібна широка сумісність моделей для різних архітектур.

**Виберіть Foundry Local, якщо**:
- Розгортаєте корпоративні додатки з суворими вимогами до продуктивності.
- Використовуєте оптимізації обладнання, специфічні для Windows (NPU, Windows ML).
- Потрібна корпоративна підтримка, SLA та функції відповідності.
- Створюєте виробничі додатки з інтеграцією в екосистему Microsoft.
- Потрібні розширені інструменти оптимізації та професійні робочі процеси розробки.

## Розширені стратегії розгортання

### Шаблони контейнеризованого розгортання

**Контейнеризація Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Корпоративне розгортання Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Техніки оптимізації продуктивності

**Стратегії оптимізації Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Оптимізація Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Міркування щодо безпеки та відповідності

### Реалізація безпеки корпоративного рівня

**Найкращі практики безпеки Ollama**:
- Ізоляція мережі за допомогою правил брандмауера та доступу через VPN.
- Аутентифікація через інтеграцію з реверс-проксі.
- Перевірка цілісності моделей та безпечний розподіл моделей.
- Ведення журналів аудиту для доступу до API та операцій з моделями.

**Безпека корпоративного рівня Foundry Local**:
- Вбудований контроль доступу на основі ролей з інтеграцією Active Directory.
- Комплексні журнали аудиту з звітністю про відповідність.
- Зашифроване зберігання моделей та безпечне розгортання моделей.
- Інтеграція з інфраструктурою безпеки Microsoft.

### Вимоги до відповідності та регулювання

Обидві платформи підтримують відповідність нормативним вимогам через:
- Контроль місцезнаходження даних, що забезпечує локальну обробку.
- Ведення журналів аудиту для вимог звітності.
- Контроль доступу для обробки конфіденційних даних.
- Шифрування даних у стані спокою та під час передачі для захисту даних.

## Найкращі практики для розгортання у виробництві

### Моніторинг та спостереження

**Ключові метрики для моніторингу**:
- Затримка інференції моделі та пропускна здатність.
- Використання ресурсів (CPU, GPU, пам'ять).
- Час відповіді API та рівень помилок.
- Точність моделі та відхилення продуктивності.

**Реалізація моніторингу**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Безперервна інтеграція та розгортання

**Інтеграція CI/CD пайплайнів**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Майбутні тенденції та міркування

### Нові технології

Ландшафт локального розгортання SLM продовжує розвиватися з кількома ключовими тенденціями:

**Розширені архітектури моделей**: З'являються моделі наступного покоління з покращеною ефективністю та співвідношенням можливостей, включаючи моделі з сумішшю експертів для динамічного масштабування та спеціалізовані архітектури для розгортання на периферії.

**Інтеграція обладнання**: Глибша інтеграція зі спеціалізованим AI-обладнанням, включаючи NPU, спеціальні чипи та прискорювачі

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.