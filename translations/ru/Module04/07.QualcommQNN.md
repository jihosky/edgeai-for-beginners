<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:45:16+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "ru"
}
-->
# Раздел 7: Qualcomm QNN (Qualcomm Neural Network) Optimization Suite

## Содержание
1. [Введение](../../../Module04)
2. [Что такое Qualcomm QNN?](../../../Module04)
3. [Установка](../../../Module04)
4. [Краткое руководство](../../../Module04)
5. [Пример: Конвертация и оптимизация моделей с помощью QNN](../../../Module04)
6. [Расширенное использование](../../../Module04)
7. [Лучшие практики](../../../Module04)
8. [Устранение неполадок](../../../Module04)
9. [Дополнительные ресурсы](../../../Module04)

## Введение

Qualcomm QNN (Qualcomm Neural Network) — это комплексная платформа для выполнения AI-инференса, разработанная для максимального использования потенциала аппаратных ускорителей AI от Qualcomm, включая Hexagon NPU, Adreno GPU и Kryo CPU. Независимо от того, нацелены ли вы на мобильные устройства, платформы периферийных вычислений или автомобильные системы, QNN предоставляет оптимизированные возможности инференса, которые используют специализированные модули обработки AI от Qualcomm для достижения максимальной производительности и энергоэффективности.

## Что такое Qualcomm QNN?

Qualcomm QNN — это унифицированная платформа для выполнения AI-инференса, которая позволяет разработчикам эффективно развертывать AI-модели на гетерогенной вычислительной архитектуре Qualcomm. Она предоставляет унифицированный программный интерфейс для доступа к Hexagon NPU (нейронный процессор), Adreno GPU и Kryo CPU, автоматически выбирая оптимальный процессор для различных слоев и операций модели.

### Основные характеристики

- **Гетерогенные вычисления**: Унифицированный доступ к NPU, GPU и CPU с автоматическим распределением нагрузки
- **Оптимизация с учетом аппаратного обеспечения**: Специализированные оптимизации для платформ Qualcomm Snapdragon
- **Поддержка квантизации**: Продвинутые техники квантизации INT8, INT16 и смешанной точности
- **Инструменты конвертации моделей**: Прямая поддержка моделей TensorFlow, PyTorch, ONNX и Caffe
- **Оптимизация для Edge AI**: Разработано специально для мобильных и периферийных сценариев с акцентом на энергоэффективность

### Преимущества

- **Максимальная производительность**: Использование специализированного AI-оборудования для повышения производительности до 15 раз
- **Энергоэффективность**: Оптимизация для мобильных устройств и устройств с батарейным питанием с интеллектуальным управлением энергопотреблением
- **Минимальная задержка**: Инференс с аппаратным ускорением и минимальными накладными расходами для приложений в реальном времени
- **Масштабируемое развертывание**: От смартфонов до автомобильных платформ в экосистеме Qualcomm
- **Готовность к производству**: Проверенная на практике платформа, используемая в миллионах устройств

## Установка

### Предварительные требования

- Qualcomm QNN SDK (требуется регистрация на сайте Qualcomm)
- Python версии 3.7 или выше
- Совместимое оборудование Qualcomm или симулятор
- Android NDK (для развертывания на мобильных устройствах)
- Среда разработки на Linux или Windows

### Настройка QNN SDK

1. **Регистрация и загрузка**: Посетите Qualcomm Developer Network для регистрации и загрузки QNN SDK
2. **Распаковка SDK**: Распакуйте QNN SDK в вашу рабочую директорию
3. **Настройка переменных окружения**: Настройте пути для инструментов и библиотек QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Настройка Python-окружения

Создайте и активируйте виртуальное окружение:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Установите необходимые пакеты Python:

```bash
pip install numpy tensorflow torch onnx
```

### Проверка установки

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Если установка прошла успешно, вы увидите справочную информацию для каждого инструмента QNN.

## Краткое руководство

### Первая конвертация модели

Давайте конвертируем простую модель PyTorch для работы на оборудовании Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Конвертация ONNX в формат QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Генерация библиотеки моделей QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Что делает этот процесс

Процесс оптимизации включает: конвертацию оригинальной модели в формат ONNX, преобразование ONNX в промежуточное представление QNN, применение оптимизаций, специфичных для оборудования, и генерацию скомпилированной библиотеки моделей для развертывания.

### Объяснение ключевых параметров

- `--input_network`: Исходный файл модели ONNX
- `--output_path`: Сгенерированный исходный файл C++
- `--input_dim`: Размеры входного тензора для оптимизации
- `--quantization_overrides`: Настройка квантизации
- `-t x86_64-linux-clang`: Целевая архитектура и компилятор

## Пример: Конвертация и оптимизация моделей с помощью QNN

### Шаг 1: Продвинутая конвертация модели с квантизацией

Вот как применить пользовательскую квантизацию во время конвертации:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Конвертация с пользовательской квантизацией:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Шаг 2: Оптимизация для нескольких бэкендов

Настройка для гетерогенного выполнения на NPU, GPU и CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Шаг 3: Создание бинарного контекста для развертывания

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Шаг 4: Инференс с использованием QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Структура вывода

После оптимизации ваша директория развертывания будет содержать:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Расширенное использование

### Настройка пользовательских бэкендов

Настройка оптимизаций для конкретных бэкендов:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Динамическая квантизация

Применение квантизации во время выполнения для повышения точности:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Профилирование производительности

Мониторинг производительности на различных бэкендах:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Автоматический выбор бэкенда

Реализация интеллектуального выбора бэкенда на основе характеристик модели:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Лучшие практики

### 1. Оптимизация архитектуры модели
- **Слияние слоев**: Объединяйте операции, такие как Conv+BatchNorm+ReLU, для лучшего использования NPU
- **Глубинно-разделенные свертки**: Предпочитайте их стандартным сверткам для мобильного развертывания
- **Дизайн, дружественный к квантизации**: Используйте активации ReLU и избегайте операций, которые плохо поддаются квантизации

### 2. Стратегия квантизации
- **Квантизация после обучения**: Начните с этого для быстрого развертывания
- **Калибровочный набор данных**: Используйте репрезентативные данные, охватывающие все вариации входных данных
- **Смешанная точность**: Используйте INT8 для большинства слоев, оставляя критические слои в более высокой точности

### 3. Рекомендации по выбору бэкенда
- **NPU (HTP)**: Идеально для CNN, квантизированных моделей и приложений с низким энергопотреблением
- **GPU**: Оптимально для вычислительно интенсивных операций, больших моделей и точности FP16
- **CPU**: Резерв для неподдерживаемых операций и отладки

### 4. Оптимизация производительности
- **Размер пакета**: Используйте размер пакета 1 для приложений в реальном времени, большие пакеты — для повышения пропускной способности
- **Предобработка входных данных**: Минимизируйте накладные расходы на копирование и преобразование данных
- **Повторное использование контекста**: Предварительно компилируйте контексты, чтобы избежать накладных расходов на компиляцию во время выполнения

### 5. Управление памятью
- **Выделение тензоров**: Используйте статическое выделение, чтобы избежать накладных расходов во время выполнения
- **Пулы памяти**: Реализуйте пользовательские пулы памяти для часто выделяемых тензоров
- **Повторное использование буферов**: Повторно используйте буферы ввода/вывода между вызовами инференса

### 6. Оптимизация энергопотребления
- **Режимы производительности**: Используйте подходящие режимы производительности в зависимости от тепловых ограничений
- **Динамическое масштабирование частоты**: Позвольте системе изменять частоту в зависимости от нагрузки
- **Управление состоянием простоя**: Корректно освобождайте ресурсы, когда они не используются

## Устранение неполадок

### Распространенные проблемы

#### 1. Проблемы с установкой SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Ошибки конвертации модели
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Проблемы с квантизацией
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Проблемы с производительностью
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Проблемы с памятью
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Совместимость бэкендов
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Отладка производительности

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Получение помощи

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Документация QNN**: Доступна в пакете SDK
- **Форумы сообщества**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Техническая поддержка**: Через портал разработчиков Qualcomm

## Дополнительные ресурсы

### Официальные ссылки
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Платформы Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Портал разработчиков**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Учебные материалы
- **Руководство для начинающих**: Доступно в документации QNN SDK
- **Модельный зоопарк**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Руководство по оптимизации**: Документация SDK включает подробные рекомендации по оптимизации
- **Видеоуроки**: [YouTube-канал Qualcomm Developer Network](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Интеграционные инструменты
- **SNPE (устаревший)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Предварительно оптимизированные модели для оборудования Qualcomm
- **Android Neural Networks API**: Интеграция с Android NNAPI
- **TensorFlow Lite Delegate**: Делегат Qualcomm для TFLite

### Бенчмарки производительности
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Примеры из сообщества
- **Примерные приложения**: Доступны в директории примеров QNN SDK
- **Репозитории GitHub**: Примеры и инструменты, созданные сообществом
- **Технические блоги**: [Блог разработчиков Qualcomm](https://developer.qualcomm.com/blog)

### Связанные инструменты
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Продвинутые техники квантизации и сжатия
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Для сравнения и резервного развертывания
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Кроссплатформенный движок инференса

### Характеристики оборудования
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Платформы Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Что дальше

Продолжите свое путешествие в области Edge AI, изучив [Модуль 5: SLMOps и развертывание в производстве](../Module05/README.md), чтобы узнать об операционных аспектах управления жизненным циклом малых языковых моделей.

---

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Хотя мы стремимся к точности, пожалуйста, учитывайте, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.