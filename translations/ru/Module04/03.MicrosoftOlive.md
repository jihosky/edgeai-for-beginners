<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T10:47:49+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ru"
}
-->
# Раздел 3: Microsoft Olive Optimization Suite

## Содержание
1. [Введение](../../../Module04)
2. [Что такое Microsoft Olive?](../../../Module04)
3. [Установка](../../../Module04)
4. [Краткое руководство](../../../Module04)
5. [Пример: Конвертация Qwen3 в ONNX INT4](../../../Module04)
6. [Расширенное использование](../../../Module04)
7. [Репозиторий рецептов Olive](../../../Module04)
8. [Лучшие практики](../../../Module04)
9. [Устранение неполадок](../../../Module04)
10. [Дополнительные ресурсы](../../../Module04)

## Введение

Microsoft Olive — это мощный и простой в использовании инструмент оптимизации моделей, учитывающий особенности оборудования. Он упрощает процесс оптимизации моделей машинного обучения для развертывания на различных аппаратных платформах. Независимо от того, нацелены ли вы на процессоры, графические процессоры или специализированные AI-ускорители, Olive помогает достичь оптимальной производительности, сохраняя точность модели.

## Что такое Microsoft Olive?

Olive — это инструмент оптимизации моделей, учитывающий особенности оборудования, который объединяет передовые методы сжатия, оптимизации и компиляции моделей. Он работает с ONNX Runtime как решение для оптимизации вывода от начала до конца.

### Основные функции

- **Оптимизация с учетом оборудования**: Автоматически выбирает лучшие методы оптимизации для вашего целевого оборудования
- **40+ встроенных компонентов оптимизации**: Включает сжатие моделей, квантование, оптимизацию графов и многое другое
- **Простой интерфейс CLI**: Простые команды для выполнения задач оптимизации
- **Поддержка нескольких фреймворков**: Работает с PyTorch, моделями Hugging Face и ONNX
- **Поддержка популярных моделей**: Olive может автоматически оптимизировать популярные архитектуры моделей, такие как Llama, Phi, Qwen, Gemma и другие

### Преимущества

- **Сокращение времени разработки**: Нет необходимости вручную экспериментировать с различными методами оптимизации
- **Увеличение производительности**: Значительное ускорение работы (до 6 раз в некоторых случаях)
- **Кроссплатформенное развертывание**: Оптимизированные модели работают на различных аппаратных платформах и операционных системах
- **Сохранение точности**: Оптимизация сохраняет качество модели, улучшая производительность

## Установка

### Предварительные требования

- Python 3.8 или выше
- Менеджер пакетов pip
- Виртуальная среда (рекомендуется)

### Базовая установка

Создайте и активируйте виртуальную среду:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Установите Olive с функциями автооптимизации:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Дополнительные зависимости

Olive предлагает различные дополнительные зависимости для расширенных функций:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Проверка установки

```bash
olive --help
```

Если установка прошла успешно, вы должны увидеть сообщение справки CLI Olive.

## Краткое руководство

### Ваша первая оптимизация

Давайте оптимизируем небольшую языковую модель с помощью функции автооптимизации Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Что делает эта команда

Процесс оптимизации включает: получение модели из локального кэша, захват графа ONNX и сохранение весов в файле данных ONNX, оптимизацию графа ONNX и квантование модели до int4 с использованием метода RTN.

### Объяснение параметров команды

- `--model_name_or_path`: Идентификатор модели Hugging Face или локальный путь
- `--output_path`: Директория, где будет сохранена оптимизированная модель
- `--device`: Целевое устройство (cpu, gpu)
- `--provider`: Провайдер выполнения (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Использовать ONNX Runtime Generate AI для вывода
- `--precision`: Точность квантования (int4, int8, fp16)
- `--log_level`: Уровень детализации логов (0=минимальный, 1=подробный)

## Пример: Конвертация Qwen3 в ONNX INT4

На основе предоставленного примера Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), вот как оптимизировать модель Qwen3:

### Шаг 1: Загрузка модели (опционально)

Чтобы минимизировать время загрузки, кэшируйте только необходимые файлы:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Шаг 2: Оптимизация модели Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Шаг 3: Тестирование оптимизированной модели

Создайте простой Python-скрипт для тестирования вашей оптимизированной модели:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Структура вывода

После оптимизации ваша выходная директория будет содержать:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Расширенное использование

### Конфигурационные файлы

Для более сложных рабочих процессов оптимизации можно использовать JSON-конфигурационные файлы:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Запуск с конфигурацией:

```bash
olive run --config config.json
```

### Оптимизация для GPU

Для оптимизации CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Для DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Тонкая настройка с Olive

Olive также поддерживает тонкую настройку моделей:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Лучшие практики

### 1. Выбор модели
- Начинайте с небольших моделей для тестирования (например, 0.5B-7B параметров)
- Убедитесь, что архитектура вашей целевой модели поддерживается Olive

### 2. Учет оборудования
- Соотнесите цель оптимизации с вашим оборудованием для развертывания
- Используйте оптимизацию для GPU, если у вас есть оборудование, совместимое с CUDA
- Рассмотрите DirectML для компьютеров с Windows и встроенной графикой

### 3. Выбор точности
- **INT4**: Максимальное сжатие, небольшая потеря точности
- **INT8**: Хороший баланс размера и точности
- **FP16**: Минимальная потеря точности, умеренное уменьшение размера

### 4. Тестирование и проверка
- Всегда тестируйте оптимизированные модели с вашими конкретными сценариями использования
- Сравнивайте метрики производительности (задержка, пропускная способность, точность)
- Используйте репрезентативные входные данные для оценки

### 5. Итеративная оптимизация
- Начинайте с автооптимизации для быстрого результата
- Используйте конфигурационные файлы для тонкой настройки
- Экспериментируйте с различными этапами оптимизации

## Устранение неполадок

### Распространенные проблемы

#### 1. Проблемы с установкой
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Проблемы с CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Проблемы с памятью
- Используйте меньшие размеры пакетов во время оптимизации
- Попробуйте квантование с более высокой точностью сначала (int8 вместо int4)
- Убедитесь, что достаточно места на диске для кэширования модели

#### 4. Ошибки загрузки модели
- Проверьте путь к модели и права доступа
- Убедитесь, что для модели требуется `trust_remote_code=True`
- Убедитесь, что все необходимые файлы модели загружены

### Получение помощи

- **Документация**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Примеры**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Репозиторий рецептов Olive

### Введение в рецепты Olive

Репозиторий [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) дополняет основной инструмент Olive, предоставляя обширную коллекцию готовых к использованию рецептов оптимизации для популярных моделей AI. Этот репозиторий служит практическим справочником как для оптимизации общедоступных моделей, так и для создания рабочих процессов оптимизации для собственных моделей.

### Основные функции

- **100+ готовых рецептов**: Готовые конфигурации оптимизации для популярных моделей
- **Поддержка различных архитектур**: Охватывает трансформеры, модели для обработки изображений и мультимодальные архитектуры
- **Оптимизация для конкретного оборудования**: Рецепты, адаптированные для CPU, GPU и специализированных ускорителей
- **Популярные семейства моделей**: Включает Phi, Llama, Qwen, Gemma, Mistral и многие другие

### Поддерживаемые семейства моделей

Репозиторий включает рецепты оптимизации для:

#### Языковые модели
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, серия Qwen2.5 (0.5B до 14B)
- **Google Gemma**: Различные конфигурации моделей Gemma
- **Mistral AI**: Серия Mistral-7B
- **DeepSeek**: Модели серии R1-Distill

#### Модели для обработки изображений и мультимодальные модели
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: Различные конфигурации CLIP-ViT
- **ResNet**: Оптимизации ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Специализированные модели
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Базовые и многоязычные варианты
- **Sentence Transformers**: all-MiniLM-L6-v2

### Использование рецептов Olive

#### Метод 1: Клонирование конкретного рецепта

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Метод 2: Использование рецепта как шаблона

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Структура рецепта

Каждая директория рецепта обычно содержит:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Пример: Использование рецепта Phi-4-mini

Давайте используем рецепт Phi-4-mini в качестве примера:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Файл конфигурации обычно включает:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Настройка рецептов

#### Изменение целевого оборудования

Чтобы изменить целевое оборудование, обновите раздел `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Настройка параметров оптимизации

Измените раздел `passes` для различных уровней оптимизации:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Создание собственного рецепта

1. **Начните с похожей модели**: Найдите рецепт для модели с похожей архитектурой
2. **Обновите конфигурацию модели**: Измените имя/путь модели в конфигурации
3. **Настройте параметры**: Измените параметры оптимизации по необходимости
4. **Тестируйте и проверяйте**: Проведите оптимизацию и проверьте результаты
5. **Внесите вклад**: Рассмотрите возможность добавления вашего рецепта в репозиторий

### Преимущества использования рецептов

#### 1. **Проверенные конфигурации**
- Проверенные настройки оптимизации для конкретных моделей
- Избегание метода проб и ошибок при поиске оптимальных параметров

#### 2. **Тонкая настройка для оборудования**
- Предварительно оптимизировано для различных провайдеров выполнения
- Готовые конфигурации для целей на CPU, GPU и NPU

#### 3. **Широкий охват**
- Поддержка самых популярных моделей с открытым исходным кодом
- Регулярные обновления с новыми релизами моделей

#### 4. **Вклад сообщества**
- Совместная разработка с AI-сообществом
- Обмен знаниями и лучшими практиками

### Внесение вклада в рецепты Olive

Если вы оптимизировали модель, которая не охвачена в репозитории:

1. **Сделайте форк репозитория**: Создайте собственный форк olive-recipes
2. **Создайте директорию рецепта**: Добавьте новую директорию для вашей модели
3. **Добавьте конфигурацию**: Вставьте olive_config.json и сопутствующие файлы
4. **Документируйте использование**: Предоставьте понятный README с инструкциями
5. **Отправьте Pull Request**: Внесите вклад в сообщество

### Эталонные показатели производительности

Многие рецепты включают эталонные показатели производительности, показывающие:
- **Улучшение задержки**: Типичное ускорение в 2-6 раз по сравнению с базовым уровнем
- **Снижение памяти**: Уменьшение использования памяти на 50-75% благодаря квантованию
- **Сохранение точности**: Сохранение точности на уровне 95-99%

### Интеграция с AI Toolkit

Рецепты легко интегрируются с:
- **VS Code AI Toolkit**: Прямая интеграция для оптимизации моделей
- **Azure Machine Learning**: Облачные рабочие процессы оптимизации
- **ONNX Runtime**: Оптимизированное развертывание вывода

## Дополнительные ресурсы

### Официальные ссылки
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Репозиторий рецептов Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Документация ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Пример Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Примеры сообщества
- **Jupyter Notebooks**: Доступны в репозитории Olive на GitHub — https://github.com/microsoft/Olive/tree/main/examples
- **Расширение VS Code**: Обзор AI Toolkit для VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Блог**: Блог Microsoft Open Source — https://opensource.microsoft.com/blog/

### Связанные инструменты
- **ONNX Runtime**: Высокопроизводительный движок вывода — https://onnxruntime.ai/
- **Hugging Face Transformers**: Источник многих совместимых моделей — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Облачные рабочие процессы оптимизации — https://learn.microsoft.com/azure/machine-learning/


## ➡️ Что дальше

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Отказ от ответственности**:  
Этот документ был переведен с использованием сервиса автоматического перевода [Co-op Translator](https://github.com/Azure/co-op-translator). Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.