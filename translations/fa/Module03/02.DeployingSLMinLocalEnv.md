<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:10:47+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "fa"
}
-->
# بخش ۲: استقرار در محیط محلی - راه‌حل‌های مبتنی بر حفظ حریم خصوصی

استقرار مدل‌های زبان کوچک (SLM) در محیط‌های محلی، یک تغییر اساسی به سمت راه‌حل‌های هوش مصنوعی مقرون‌به‌صرفه و حفظ حریم خصوصی است. این راهنمای جامع دو چارچوب قدرتمند—Ollama و Microsoft Foundry Local—را بررسی می‌کند که به توسعه‌دهندگان امکان می‌دهد تا از تمام قابلیت‌های SLM‌ها بهره‌مند شوند و کنترل کامل بر محیط استقرار خود داشته باشند.

## مقدمه

در این درس، استراتژی‌های پیشرفته استقرار مدل‌های زبان کوچک در محیط‌های محلی را بررسی خواهیم کرد. مفاهیم اساسی استقرار هوش مصنوعی محلی را پوشش می‌دهیم، دو پلتفرم پیشرو (Ollama و Microsoft Foundry Local) را بررسی می‌کنیم و راهنمایی‌های عملی برای پیاده‌سازی راه‌حل‌های آماده تولید ارائه می‌دهیم.

## اهداف آموزشی

در پایان این درس، شما قادر خواهید بود:

- معماری و مزایای چارچوب‌های استقرار SLM محلی را درک کنید.
- استقرارهای آماده تولید را با استفاده از Ollama و Microsoft Foundry Local پیاده‌سازی کنید.
- پلتفرم مناسب را بر اساس نیازها و محدودیت‌های خاص انتخاب کنید.
- استقرارهای محلی را برای عملکرد، امنیت و مقیاس‌پذیری بهینه کنید.

## درک معماری‌های استقرار SLM محلی

استقرار SLM محلی یک تغییر اساسی از خدمات هوش مصنوعی وابسته به ابر به راه‌حل‌های حفظ حریم خصوصی و درون‌سازمانی است. این رویکرد به سازمان‌ها امکان می‌دهد کنترل کامل بر زیرساخت‌های هوش مصنوعی خود داشته باشند و در عین حال حاکمیت داده‌ها و استقلال عملیاتی را تضمین کنند.

### طبقه‌بندی چارچوب‌های استقرار

درک رویکردهای مختلف استقرار به انتخاب استراتژی مناسب برای موارد استفاده خاص کمک می‌کند:

- **متمرکز بر توسعه**: راه‌اندازی ساده برای آزمایش و نمونه‌سازی
- **در سطح سازمانی**: راه‌حل‌های آماده تولید با قابلیت‌های یکپارچه‌سازی سازمانی  
- **چند پلتفرمی**: سازگاری جهانی با سیستم‌عامل‌ها و سخت‌افزارهای مختلف

### مزایای کلیدی استقرار SLM محلی

استقرار SLM محلی چندین مزیت اساسی ارائه می‌دهد که آن را برای برنامه‌های سازمانی و حساس به حریم خصوصی ایده‌آل می‌کند:

**حریم خصوصی و امنیت**: پردازش محلی تضمین می‌کند که داده‌های حساس هرگز زیرساخت سازمان را ترک نمی‌کنند، و امکان رعایت مقرراتی مانند GDPR، HIPAA و سایر الزامات قانونی را فراهم می‌کند. استقرارهای ایزوله برای محیط‌های طبقه‌بندی‌شده امکان‌پذیر است، در حالی که ردپای کامل نظارت امنیتی را حفظ می‌کند.

**مقرون‌به‌صرفه بودن**: حذف مدل‌های قیمت‌گذاری بر اساس تعداد توکن‌ها، هزینه‌های عملیاتی را به طور قابل توجهی کاهش می‌دهد. نیازهای پهنای باند کمتر و کاهش وابستگی به ابر، ساختارهای هزینه قابل پیش‌بینی برای بودجه‌بندی سازمانی فراهم می‌کند.

**عملکرد و قابلیت اطمینان**: زمان‌های استنتاج سریع‌تر بدون تأخیر شبکه، امکان برنامه‌های کاربردی بلادرنگ را فراهم می‌کند. عملکرد آفلاین تضمین می‌کند که عملیات بدون توجه به اتصال اینترنت ادامه دارد، در حالی که بهینه‌سازی منابع محلی عملکردی پایدار ارائه می‌دهد.

## Ollama: پلتفرم استقرار محلی جهانی

### معماری و فلسفه اصلی

Ollama به عنوان یک پلتفرم جهانی و کاربرپسند طراحی شده است که استقرار LLM محلی را در پیکربندی‌های سخت‌افزاری و سیستم‌عامل‌های متنوع دموکراتیک می‌کند.

**پایه فنی**: بر اساس چارچوب قدرتمند llama.cpp ساخته شده است، Ollama از فرمت مدل GGUF برای عملکرد بهینه استفاده می‌کند. سازگاری چند پلتفرمی رفتار سازگار در محیط‌های Windows، macOS و Linux را تضمین می‌کند، در حالی که مدیریت منابع هوشمند استفاده بهینه از CPU، GPU و حافظه را فراهم می‌کند.

**فلسفه طراحی**: Ollama سادگی را بدون قربانی کردن عملکرد اولویت می‌دهد و استقرار بدون پیکربندی را برای بهره‌وری فوری ارائه می‌دهد. این پلتفرم سازگاری گسترده مدل را حفظ می‌کند و API‌های سازگار در معماری‌های مختلف مدل ارائه می‌دهد.

### ویژگی‌ها و قابلیت‌های پیشرفته

**مدیریت عالی مدل**: Ollama مدیریت جامع چرخه عمر مدل را با کشیدن خودکار، ذخیره‌سازی و نسخه‌بندی ارائه می‌دهد. این پلتفرم از یک اکوسیستم مدل گسترده شامل Llama 3.2، Google Gemma 2، Microsoft Phi-4، Qwen 2.5، DeepSeek، Mistral و مدل‌های جاسازی تخصصی پشتیبانی می‌کند.

**سفارشی‌سازی از طریق Modelfiles**: کاربران پیشرفته می‌توانند پیکربندی‌های مدل سفارشی با پارامترهای خاص، درخواست‌های سیستمی و تغییرات رفتاری ایجاد کنند. این امکان بهینه‌سازی‌های خاص دامنه و نیازهای برنامه‌های تخصصی را فراهم می‌کند.

**بهینه‌سازی عملکرد**: Ollama به طور خودکار شتاب سخت‌افزاری موجود از جمله NVIDIA CUDA، Apple Metal و OpenCL را تشخیص داده و استفاده می‌کند. مدیریت حافظه هوشمند استفاده بهینه از منابع را در پیکربندی‌های سخت‌افزاری مختلف تضمین می‌کند.

### استراتژی‌های پیاده‌سازی تولید

**نصب و راه‌اندازی**: Ollama نصب ساده در پلتفرم‌های مختلف از طریق نصب‌کننده‌های بومی، مدیران بسته (WinGet، Homebrew، APT) و کانتینرهای Docker برای استقرار کانتینری ارائه می‌دهد.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**دستورات و عملیات ضروری**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**پیکربندی پیشرفته**: Modelfiles امکان سفارشی‌سازی پیچیده برای نیازهای سازمانی را فراهم می‌کنند:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### نمونه‌های یکپارچه‌سازی توسعه‌دهنده

**یکپارچه‌سازی API پایتون**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**یکپارچه‌سازی JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**استفاده از API RESTful با cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### تنظیم و بهینه‌سازی عملکرد

**پیکربندی حافظه و رشته‌ها**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**انتخاب کمیت‌سازی برای سخت‌افزارهای مختلف**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: پلتفرم هوش مصنوعی لبه سازمانی

### معماری در سطح سازمانی

Microsoft Foundry Local یک راه‌حل جامع سازمانی است که به طور خاص برای استقرار هوش مصنوعی لبه تولید با یکپارچگی عمیق در اکوسیستم Microsoft طراحی شده است.

**پایه مبتنی بر ONNX**: بر اساس ONNX Runtime استاندارد صنعتی ساخته شده است، Foundry Local عملکرد بهینه در معماری‌های سخت‌افزاری متنوع ارائه می‌دهد. این پلتفرم از یکپارچگی Windows ML برای بهینه‌سازی بومی Windows استفاده می‌کند و در عین حال سازگاری چند پلتفرمی را حفظ می‌کند.

**برتری شتاب سخت‌افزاری**: Foundry Local دارای تشخیص سخت‌افزار هوشمند و بهینه‌سازی در CPUها، GPUها و NPUها است. همکاری عمیق با فروشندگان سخت‌افزار (AMD، Intel، NVIDIA، Qualcomm) عملکرد بهینه در پیکربندی‌های سخت‌افزاری سازمانی را تضمین می‌کند.

### تجربه پیشرفته توسعه‌دهنده

**دسترسی چند رابط**: Foundry Local رابط‌های توسعه جامع از جمله CLI قدرتمند برای مدیریت و استقرار مدل، SDKهای چند زبانه (Python، NodeJS) برای یکپارچگی بومی و API‌های RESTful با سازگاری OpenAI برای مهاجرت بدون مشکل ارائه می‌دهد.

**یکپارچگی با Visual Studio**: این پلتفرم به طور یکپارچه با AI Toolkit برای VS Code یکپارچه می‌شود و ابزارهای تبدیل مدل، کمیت‌سازی و بهینه‌سازی را در محیط توسعه فراهم می‌کند. این یکپارچگی جریان‌های کاری توسعه را تسریع می‌کند و پیچیدگی استقرار را کاهش می‌دهد.

**خط لوله بهینه‌سازی مدل**: یکپارچگی با Microsoft Olive امکان جریان‌های کاری بهینه‌سازی مدل پیچیده از جمله کمیت‌سازی پویا، بهینه‌سازی گراف و تنظیم سخت‌افزار خاص را فراهم می‌کند. قابلیت‌های تبدیل مبتنی بر ابر از طریق Azure ML بهینه‌سازی مقیاس‌پذیر برای مدل‌های بزرگ ارائه می‌دهد.

### استراتژی‌های پیاده‌سازی تولید

**نصب و پیکربندی**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**عملیات مدیریت مدل**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**پیکربندی پیشرفته استقرار**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### یکپارچگی اکوسیستم سازمانی

**امنیت و رعایت قوانین**: Foundry Local ویژگی‌های امنیتی در سطح سازمانی از جمله کنترل دسترسی مبتنی بر نقش، ثبت گزارش‌های حسابرسی، گزارش‌دهی رعایت قوانین و ذخیره‌سازی مدل رمزگذاری‌شده ارائه می‌دهد. یکپارچگی با زیرساخت امنیتی Microsoft رعایت سیاست‌های امنیتی سازمانی را تضمین می‌کند.

**خدمات هوش مصنوعی داخلی**: این پلتفرم قابلیت‌های هوش مصنوعی آماده استفاده از جمله Phi Silica برای پردازش زبان محلی، AI Imaging برای بهبود و تحلیل تصویر و API‌های تخصصی برای وظایف رایج هوش مصنوعی سازمانی ارائه می‌دهد.

## تحلیل مقایسه‌ای: Ollama در مقابل Foundry Local

### مقایسه معماری فنی

| **جنبه** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **فرمت مدل** | GGUF (از طریق llama.cpp) | ONNX (از طریق ONNX Runtime) |
| **تمرکز پلتفرم** | سازگاری چند پلتفرمی جهانی | بهینه‌سازی Windows/سازمانی |
| **یکپارچگی سخت‌افزار** | پشتیبانی عمومی GPU/CPU | پشتیبانی عمیق Windows ML، NPU |
| **بهینه‌سازی** | کمیت‌سازی llama.cpp | Microsoft Olive + ONNX Runtime |
| **ویژگی‌های سازمانی** | مبتنی بر جامعه | در سطح سازمانی با SLAs |

### ویژگی‌های عملکردی

**مزایای عملکرد Ollama**:
- عملکرد استثنایی CPU از طریق بهینه‌سازی llama.cpp
- رفتار سازگار در پلتفرم‌ها و سخت‌افزارهای مختلف
- استفاده کارآمد از حافظه با بارگذاری هوشمند مدل
- زمان‌های شروع سریع برای سناریوهای توسعه و آزمایش

**مزایای عملکرد Foundry Local**:
- استفاده برتر از NPU در سخت‌افزارهای مدرن Windows
- شتاب GPU بهینه از طریق همکاری با فروشندگان
- نظارت و بهینه‌سازی عملکرد در سطح سازمانی
- قابلیت‌های استقرار مقیاس‌پذیر برای محیط‌های تولید

### تحلیل تجربه توسعه‌دهنده

**تجربه توسعه‌دهنده Ollama**:
- نیازهای راه‌اندازی حداقلی با بهره‌وری فوری
- رابط خط فرمان شهودی برای تمام عملیات
- پشتیبانی گسترده جامعه و مستندات
- سفارشی‌سازی انعطاف‌پذیر از طریق Modelfiles

**تجربه توسعه‌دهنده Foundry Local**:
- یکپارچگی جامع IDE با اکوسیستم Visual Studio
- جریان‌های کاری توسعه سازمانی با ویژگی‌های همکاری تیمی
- کانال‌های پشتیبانی حرفه‌ای با پشتیبانی Microsoft
- ابزارهای پیشرفته اشکال‌زدایی و بهینه‌سازی

### بهینه‌سازی موارد استفاده

**انتخاب Ollama زمانی که**:
- توسعه برنامه‌های چند پلتفرمی با رفتار سازگار
- اولویت دادن به شفافیت منبع باز و مشارکت‌های جامعه
- کار با منابع محدود یا محدودیت‌های بودجه
- ساخت برنامه‌های آزمایشی یا متمرکز بر تحقیق
- نیاز به سازگاری گسترده مدل در معماری‌های مختلف

**انتخاب Foundry Local زمانی که**:
- استقرار برنامه‌های سازمانی با الزامات عملکرد سخت‌گیرانه
- استفاده از بهینه‌سازی سخت‌افزار خاص Windows (NPU، Windows ML)
- نیاز به پشتیبانی سازمانی، SLAs و ویژگی‌های رعایت قوانین
- ساخت برنامه‌های تولیدی با یکپارچگی اکوسیستم Microsoft
- نیاز به ابزارهای بهینه‌سازی پیشرفته و جریان‌های کاری توسعه حرفه‌ای

## استراتژی‌های پیشرفته استقرار

### الگوهای استقرار کانتینری

**کانتینری‌سازی Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**استقرار سازمانی Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### تکنیک‌های بهینه‌سازی عملکرد

**استراتژی‌های بهینه‌سازی Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**بهینه‌سازی Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## ملاحظات امنیتی و رعایت قوانین

### پیاده‌سازی امنیت سازمانی

**بهترین روش‌های امنیتی Ollama**:
- ایزوله‌سازی شبکه با قوانین فایروال و دسترسی VPN
- احراز هویت از طریق یکپارچگی پروکسی معکوس
- تأیید یکپارچگی مدل و توزیع امن مدل
- ثبت گزارش‌های حسابرسی برای دسترسی API و عملیات مدل

**امنیت سازمانی Foundry Local**:
- کنترل دسترسی مبتنی بر نقش داخلی با یکپارچگی Active Directory
- ردپای جامع حسابرسی با گزارش‌دهی رعایت قوانین
- ذخیره‌سازی مدل رمزگذاری‌شده و استقرار امن مدل
- یکپارچگی با زیرساخت امنیتی Microsoft

### الزامات رعایت قوانین و مقررات

هر دو پلتفرم از رعایت قوانین از طریق موارد زیر پشتیبانی می‌کنند:
- کنترل‌های محل اقامت داده‌ها که پردازش محلی را تضمین می‌کنند
- ثبت گزارش‌های حسابرسی برای الزامات گزارش‌دهی قانونی
- کنترل‌های دسترسی برای مدیریت داده‌های حساس
- رمزگذاری در حالت استراحت و انتقال برای حفاظت از داده‌ها

## بهترین روش‌ها برای استقرار تولید

### نظارت و مشاهده‌پذیری

**معیارهای کلیدی برای نظارت**:
- تأخیر و توان عملیاتی استنتاج مدل
- استفاده از منابع (CPU، GPU، حافظه)
- زمان‌های پاسخ API و نرخ خطا
- دقت مدل و انحراف عملکرد

**پیاده‌سازی نظارت**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### یکپارچگی و استقرار مداوم

**یکپارچگی خط لوله CI/CD**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## روندها و ملاحظات آینده

### فناوری‌های نوظهور

چشم‌انداز استقرار SLM محلی با چندین روند کلیدی همچنان در حال تکامل است:

**معماری‌های مدل پیشرفته**: مدل‌های SLM نسل بعدی با نسبت‌های کارایی و قابلیت بهبود یافته در حال ظهور هستند، از جمله مدل‌های ترکیبی از متخصصان برای مقیاس‌گذاری پویا و معماری‌های تخصصی برای استقرار لبه.

**یکپارچگی سخت‌افزار**: یکپارچگی عمیق‌تر با سخت‌افزار تخصصی هوش مصنوعی از جمله NPUها، سیلیکون سفارشی و شتاب‌دهنده‌های محاسبات لبه، قابلیت‌های عملکردی پیشرفته‌ای را فراهم خواهد کرد.

**تکامل اکوسیستم**: تلاش‌های استانداردسازی در پلتفرم‌های استقرار و بهبود قابلیت همکاری بین چارچوب‌های مختلف، استقرار چند پلتفرمی را ساده‌تر خواهد کرد.

### الگوهای پذیرش صنعت

**پذیرش سازمانی**: افزایش پذیرش سازمانی به دلیل الزامات حفظ حریم خصوصی، بهینه‌سازی هزینه و نیازهای رعایت قوانین. بخش‌های دولتی و دفاعی به ویژه بر استقرارهای ایزوله تمرکز دارند.

**ملاحظات جهانی**: الزامات حاکمیت داده‌های بین‌المللی پذیرش استقرار محلی را به ویژه در مناطقی با مقررات سخت‌گیرانه حفاظت از داده‌ها هدایت می‌کنند.

## چالش‌ها و ملاحظات

### چالش‌های فنی

**الزامات زیرساختی**: استقرار محلی نیاز به برنامه‌ریزی دقیق ظرفیت و انتخاب سخت‌افزار دارد. سازمان‌ها باید الزامات عملکرد را با محدودیت‌های هزینه متعادل کنند و در عین حال مقیاس‌پذیری برای حجم‌های کاری در حال رشد را تضمین کنند.

**🔧 نگهداری و به‌روزرسانی‌ها**: به‌روزرسانی‌های منظم مدل، وصله‌های امنیتی و بهینه‌سازی عملکرد نیاز به منابع و تخصص اختصاصی دارند. خطوط لوله استقرار خودکار برای محیط‌های تولید ضروری می‌شوند.

### ملاحظات امنیتی

**امنیت مدل**: حفاظت از مدل‌های اختصاصی در برابر دسترسی یا استخراج غیرمجاز نیاز به اقدامات امنیتی جامع از جمله رمزگذاری، کنترل‌های دسترسی و ثبت گزارش‌های حسابرسی دارد.

**حفاظت از داده‌ها**: اطمینان از مدیریت امن داده‌ها در سراسر خط لوله استنتاج در حالی که استانداردهای عملکرد و قابلیت استفاده را حفظ می‌کند.

## چک‌لیست پیاده‌سازی عملی

### ✅ ارزیابی پیش از استقرار

- [ ] تحلیل الزامات سخت‌افزاری و برنامه‌ریزی ظرفیت
- [ ] تعریف معماری شبکه و الزامات امنیتی
- [ ] انتخاب مدل و ارزیابی عملکرد
- [ ] اعتبارسنجی الزامات رعایت قوانین و مقررات

### ✅ پیاده‌سازی استقرار

- [ ] انتخاب پلتفرم بر اساس تحلیل نیازها
- [ ] نصب و پیکربندی پلتفرم انتخاب‌شده
- [ ] پیاده‌سازی بهینه‌سازی و کمیت‌سازی مدل

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.