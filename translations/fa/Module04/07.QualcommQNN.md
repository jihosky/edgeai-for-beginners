<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:45:59+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "fa"
}
-->
# بخش ۷: مجموعه بهینه‌سازی Qualcomm QNN (شبکه عصبی Qualcomm)

## فهرست مطالب
1. [مقدمه](../../../Module04)
2. [Qualcomm QNN چیست؟](../../../Module04)
3. [نصب](../../../Module04)
4. [راهنمای شروع سریع](../../../Module04)
5. [مثال: تبدیل و بهینه‌سازی مدل‌ها با QNN](../../../Module04)
6. [استفاده پیشرفته](../../../Module04)
7. [بهترین روش‌ها](../../../Module04)
8. [رفع مشکلات](../../../Module04)
9. [منابع اضافی](../../../Module04)

## مقدمه

Qualcomm QNN (شبکه عصبی Qualcomm) یک چارچوب جامع برای استنتاج هوش مصنوعی است که برای بهره‌برداری کامل از شتاب‌دهنده‌های سخت‌افزاری هوش مصنوعی Qualcomm، از جمله Hexagon NPU، Adreno GPU و Kryo CPU طراحی شده است. چه هدف شما دستگاه‌های موبایل، پلتفرم‌های محاسباتی لبه یا سیستم‌های خودرویی باشد، QNN قابلیت‌های استنتاج بهینه‌ای را ارائه می‌دهد که از واحدهای پردازش تخصصی هوش مصنوعی Qualcomm برای عملکرد و بهره‌وری انرژی حداکثری استفاده می‌کند.

## Qualcomm QNN چیست؟

Qualcomm QNN یک چارچوب استنتاج هوش مصنوعی یکپارچه است که به توسعه‌دهندگان امکان می‌دهد مدل‌های هوش مصنوعی را به‌طور کارآمد در معماری محاسباتی ناهمگن Qualcomm اجرا کنند. این چارچوب یک رابط برنامه‌نویسی یکپارچه برای دسترسی به Hexagon NPU (واحد پردازش عصبی)، Adreno GPU و Kryo CPU فراهم می‌کند و به‌طور خودکار واحد پردازش بهینه را برای لایه‌ها و عملیات مختلف مدل انتخاب می‌کند.

### ویژگی‌های کلیدی

- **محاسبات ناهمگن**: دسترسی یکپارچه به NPU، GPU و CPU با توزیع خودکار بار کاری
- **بهینه‌سازی سخت‌افزار محور**: بهینه‌سازی‌های تخصصی برای پلتفرم‌های Snapdragon Qualcomm
- **پشتیبانی از کوانتیزاسیون**: تکنیک‌های پیشرفته کوانتیزاسیون INT8، INT16 و دقت ترکیبی
- **ابزارهای تبدیل مدل**: پشتیبانی مستقیم از مدل‌های TensorFlow، PyTorch، ONNX و Caffe
- **بهینه‌سازی برای هوش مصنوعی لبه**: طراحی شده برای سناریوهای استقرار موبایل و لبه با تمرکز بر بهره‌وری انرژی

### مزایا

- **عملکرد حداکثری**: استفاده از سخت‌افزار تخصصی هوش مصنوعی برای بهبود عملکرد تا ۱۵ برابر
- **بهره‌وری انرژی**: بهینه‌سازی شده برای دستگاه‌های موبایل و باتری‌دار با مدیریت هوشمند انرژی
- **تاخیر کم**: استنتاج شتاب‌یافته سخت‌افزاری با کمترین سربار برای برنامه‌های زمان واقعی
- **استقرار مقیاس‌پذیر**: از گوشی‌های هوشمند تا پلتفرم‌های خودرویی در سراسر اکوسیستم Qualcomm
- **آماده تولید**: چارچوب آزمایش‌شده در میلیون‌ها دستگاه مستقر

## نصب

### پیش‌نیازها

- SDK Qualcomm QNN (نیاز به ثبت‌نام در Qualcomm دارد)
- Python نسخه ۳.۷ یا بالاتر
- سخت‌افزار سازگار Qualcomm یا شبیه‌ساز
- Android NDK (برای استقرار موبایل)
- محیط توسعه لینوکس یا ویندوز

### تنظیم SDK QNN

1. **ثبت‌نام و دانلود**: به شبکه توسعه‌دهندگان Qualcomm مراجعه کنید تا SDK QNN را ثبت‌نام و دانلود کنید.
2. **استخراج SDK**: SDK QNN را در دایرکتوری توسعه خود باز کنید.
3. **تنظیم متغیرهای محیطی**: مسیرهای ابزارها و کتابخانه‌های QNN را پیکربندی کنید.

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### تنظیم محیط Python

یک محیط مجازی ایجاد و فعال کنید:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

بسته‌های مورد نیاز Python را نصب کنید:

```bash
pip install numpy tensorflow torch onnx
```

### تأیید نصب

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

اگر موفقیت‌آمیز باشد، باید اطلاعات راهنما برای هر ابزار QNN را مشاهده کنید.

## راهنمای شروع سریع

### اولین تبدیل مدل شما

بیایید یک مدل ساده PyTorch را برای اجرا روی سخت‌افزار Qualcomm تبدیل کنیم:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### تبدیل ONNX به فرمت QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### تولید کتابخانه مدل QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### این فرآیند چه کاری انجام می‌دهد؟

جریان کاری بهینه‌سازی شامل: تبدیل مدل اصلی به فرمت ONNX، ترجمه ONNX به نمایش میانی QNN، اعمال بهینه‌سازی‌های خاص سخت‌افزار و تولید یک کتابخانه مدل کامپایل شده برای استقرار است.

### توضیح پارامترهای کلیدی

- `--input_network`: فایل مدل ONNX منبع
- `--output_path`: فایل منبع C++ تولید شده
- `--input_dim`: ابعاد تنسور ورودی برای بهینه‌سازی
- `--quantization_overrides`: پیکربندی کوانتیزاسیون سفارشی
- `-t x86_64-linux-clang`: معماری هدف و کامپایلر

## مثال: تبدیل و بهینه‌سازی مدل‌ها با QNN

### مرحله ۱: تبدیل پیشرفته مدل با کوانتیزاسیون

در اینجا نحوه اعمال کوانتیزاسیون سفارشی در طول تبدیل آمده است:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

تبدیل با کوانتیزاسیون سفارشی:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### مرحله ۲: بهینه‌سازی چند بک‌اند

پیکربندی برای اجرای ناهمگن در NPU، GPU و CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### مرحله ۳: ایجاد باینری کانتکست برای استقرار

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### مرحله ۴: استنتاج با زمان اجرای QNN

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### ساختار خروجی

پس از بهینه‌سازی، دایرکتوری استقرار شما شامل موارد زیر خواهد بود:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## استفاده پیشرفته

### پیکربندی بک‌اند سفارشی

بهینه‌سازی‌های خاص بک‌اند را پیکربندی کنید:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### کوانتیزاسیون پویا

کوانتیزاسیون را در زمان اجرا برای دقت بهتر اعمال کنید:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### پروفایل عملکرد

عملکرد را در بک‌اندهای مختلف نظارت کنید:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### انتخاب خودکار بک‌اند

انتخاب هوشمند بک‌اند را بر اساس ویژگی‌های مدل پیاده‌سازی کنید:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## بهترین روش‌ها

### ۱. بهینه‌سازی معماری مدل
- **ادغام لایه‌ها**: عملیات‌هایی مانند Conv+BatchNorm+ReLU را برای استفاده بهتر از NPU ترکیب کنید.
- **کانولوشن‌های جداشدنی عمقی**: این‌ها را به جای کانولوشن‌های استاندارد برای استقرار موبایل ترجیح دهید.
- **طراحی‌های سازگار با کوانتیزاسیون**: از فعال‌سازی‌های ReLU استفاده کنید و از عملیات‌هایی که به‌خوبی کوانتیزه نمی‌شوند اجتناب کنید.

### ۲. استراتژی کوانتیزاسیون
- **کوانتیزاسیون پس از آموزش**: با این روش برای استقرار سریع شروع کنید.
- **مجموعه داده کالیبراسیون**: از داده‌های نماینده که تمام تغییرات ورودی را پوشش می‌دهد استفاده کنید.
- **دقت ترکیبی**: از INT8 برای اکثر لایه‌ها استفاده کنید، لایه‌های حیاتی را در دقت بالاتر نگه دارید.

### ۳. دستورالعمل‌های انتخاب بک‌اند
- **NPU (HTP)**: بهترین گزینه برای بارهای کاری CNN، مدل‌های کوانتیزه و برنامه‌های حساس به انرژی
- **GPU**: بهینه برای عملیات‌های محاسباتی سنگین، مدل‌های بزرگ‌تر و دقت FP16
- **CPU**: جایگزین برای عملیات‌های پشتیبانی نشده و اشکال‌زدایی

### ۴. بهینه‌سازی عملکرد
- **اندازه دسته**: از اندازه دسته ۱ برای برنامه‌های زمان واقعی استفاده کنید، دسته‌های بزرگ‌تر برای توان عملیاتی
- **پیش‌پردازش ورودی**: سربار کپی و تبدیل داده‌ها را به حداقل برسانید.
- **استفاده مجدد از کانتکست**: کانتکست‌ها را پیش‌کامپایل کنید تا از سربار کامپایل در زمان اجرا جلوگیری شود.

### ۵. مدیریت حافظه
- **اختصاص تنسور**: از تخصیص ایستا در صورت امکان استفاده کنید تا از سربار زمان اجرا جلوگیری شود.
- **حوضچه‌های حافظه**: حوضچه‌های حافظه سفارشی برای تنسورهای مکرراً تخصیص‌یافته پیاده‌سازی کنید.
- **استفاده مجدد از بافرها**: بافرهای ورودی/خروجی را در تماس‌های استنتاج استفاده مجدد کنید.

### ۶. بهینه‌سازی انرژی
- **حالت‌های عملکرد**: از حالت‌های عملکرد مناسب بر اساس محدودیت‌های حرارتی استفاده کنید.
- **مقیاس‌بندی فرکانس پویا**: اجازه دهید سیستم فرکانس را بر اساس بار کاری تنظیم کند.
- **مدیریت حالت بیکار**: منابع را به‌درستی در زمان عدم استفاده آزاد کنید.

## رفع مشکلات

### مشکلات رایج

#### ۱. مشکلات نصب SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### ۲. خطاهای تبدیل مدل
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### ۳. مشکلات کوانتیزاسیون
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### ۴. مشکلات عملکرد
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### ۵. مشکلات حافظه
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### ۶. سازگاری بک‌اند
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### اشکال‌زدایی عملکرد

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### دریافت کمک

- **شبکه توسعه‌دهندگان Qualcomm**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **مستندات QNN**: موجود در بسته SDK
- **انجمن‌های جامعه**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **پشتیبانی فنی**: از طریق پورتال توسعه‌دهندگان Qualcomm

## منابع اضافی

### لینک‌های رسمی
- **مرکز هوش مصنوعی Qualcomm**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **پلتفرم‌های Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **پورتال توسعه‌دهندگان**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **موتور هوش مصنوعی**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### منابع آموزشی
- **راهنمای شروع**: موجود در مستندات SDK QNN
- **مدل‌های آماده**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **راهنمای بهینه‌سازی**: مستندات SDK شامل دستورالعمل‌های جامع بهینه‌سازی است.
- **آموزش‌های ویدیویی**: [کانال یوتیوب توسعه‌دهندگان Qualcomm](https://www.youtube.com/c/QualcommDeveloperNetwork)

### ابزارهای یکپارچه‌سازی
- **SNPE (قدیمی)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: مدل‌های پیش‌بهینه‌سازی شده برای سخت‌افزار Qualcomm
- **API شبکه‌های عصبی Android**: یکپارچه‌سازی با NNAPI Android
- **نماینده TensorFlow Lite**: نماینده Qualcomm برای TFLite

### معیارهای عملکرد
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **تحقیقات هوش مصنوعی Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### مثال‌های جامعه
- **برنامه‌های نمونه**: موجود در دایرکتوری مثال‌های SDK QNN
- **مخازن GitHub**: مثال‌ها و ابزارهای ارائه‌شده توسط جامعه
- **وبلاگ‌های فنی**: [وبلاگ توسعه‌دهندگان Qualcomm](https://developer.qualcomm.com/blog)

### ابزارهای مرتبط
- **ابزار کارایی مدل هوش مصنوعی Qualcomm (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - تکنیک‌های پیشرفته کوانتیزاسیون و فشرده‌سازی
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - برای مقایسه و استقرار جایگزین
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - موتور استنتاج چندپلتفرمی

### مشخصات سخت‌افزاری
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **پلتفرم‌های Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ مرحله بعدی

سفر خود در هوش مصنوعی لبه را با بررسی [ماژول ۵: SLMOps و استقرار تولیدی](../Module05/README.md) ادامه دهید تا جنبه‌های عملیاتی مدیریت چرخه عمر مدل‌های کوچک زبان را بیاموزید.

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.