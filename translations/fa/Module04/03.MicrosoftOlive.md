<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T10:59:24+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fa"
}
-->
# بخش ۳: مجموعه بهینه‌سازی Microsoft Olive

## فهرست مطالب
1. [مقدمه](../../../Module04)
2. [Microsoft Olive چیست؟](../../../Module04)
3. [نصب](../../../Module04)
4. [راهنمای شروع سریع](../../../Module04)
5. [مثال: تبدیل Qwen3 به ONNX INT4](../../../Module04)
6. [استفاده پیشرفته](../../../Module04)
7. [مخزن دستورالعمل‌های Olive](../../../Module04)
8. [بهترین روش‌ها](../../../Module04)
9. [رفع مشکلات](../../../Module04)
10. [منابع اضافی](../../../Module04)

## مقدمه

Microsoft Olive یک ابزار قدرتمند و آسان برای بهینه‌سازی مدل‌های یادگیری ماشین است که فرآیند بهینه‌سازی مدل‌ها برای اجرا در پلتفرم‌های سخت‌افزاری مختلف را ساده می‌کند. چه هدف شما CPU، GPU یا شتاب‌دهنده‌های تخصصی هوش مصنوعی باشد، Olive به شما کمک می‌کند تا عملکرد بهینه را بدون کاهش دقت مدل به دست آورید.

## Microsoft Olive چیست؟

Olive یک ابزار بهینه‌سازی مدل سخت‌افزار محور است که تکنیک‌های پیشرو در صنعت را در زمینه فشرده‌سازی، بهینه‌سازی و کامپایل مدل ترکیب می‌کند. این ابزار با ONNX Runtime به عنوان یک راه‌حل بهینه‌سازی کامل برای استنتاج کار می‌کند.

### ویژگی‌های کلیدی

- **بهینه‌سازی سخت‌افزار محور**: به طور خودکار بهترین تکنیک‌های بهینه‌سازی را برای سخت‌افزار هدف شما انتخاب می‌کند.
- **بیش از ۴۰ مؤلفه بهینه‌سازی داخلی**: شامل فشرده‌سازی مدل، کوانتیزاسیون، بهینه‌سازی گراف و موارد دیگر.
- **رابط خط فرمان آسان**: دستورات ساده برای وظایف بهینه‌سازی رایج.
- **پشتیبانی از چندین چارچوب**: سازگار با PyTorch، مدل‌های Hugging Face و ONNX.
- **پشتیبانی از مدل‌های محبوب**: Olive می‌تواند مدل‌های معماری محبوب مانند Llama، Phi، Qwen، Gemma و غیره را به صورت خودکار بهینه کند.

### مزایا

- **کاهش زمان توسعه**: نیازی به آزمایش دستی تکنیک‌های مختلف بهینه‌سازی نیست.
- **افزایش عملکرد**: بهبودهای قابل توجه در سرعت (تا ۶ برابر در برخی موارد).
- **اجرای چند پلتفرمی**: مدل‌های بهینه‌سازی شده در سخت‌افزارها و سیستم‌عامل‌های مختلف کار می‌کنند.
- **حفظ دقت**: بهینه‌سازی‌ها کیفیت مدل را حفظ کرده و عملکرد را بهبود می‌بخشند.

## نصب

### پیش‌نیازها

- Python نسخه ۳.۸ یا بالاتر
- مدیر بسته pip
- محیط مجازی (توصیه می‌شود)

### نصب پایه

ایجاد و فعال‌سازی محیط مجازی:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

نصب Olive با ویژگی‌های بهینه‌سازی خودکار:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### وابستگی‌های اختیاری

Olive وابستگی‌های اختیاری مختلفی برای ویژگی‌های اضافی ارائه می‌دهد:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### تأیید نصب

```bash
olive --help
```

اگر موفقیت‌آمیز باشد، باید پیام راهنمای CLI Olive را مشاهده کنید.

## راهنمای شروع سریع

### اولین بهینه‌سازی شما

بیایید یک مدل زبان کوچک را با استفاده از ویژگی بهینه‌سازی خودکار Olive بهینه کنیم:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### این دستور چه کاری انجام می‌دهد؟

فرآیند بهینه‌سازی شامل: دریافت مدل از حافظه محلی، گرفتن گراف ONNX و ذخیره وزن‌ها در یک فایل داده ONNX، بهینه‌سازی گراف ONNX و کوانتیزه کردن مدل به int4 با استفاده از روش RTN است.

### توضیح پارامترهای دستور

- `--model_name_or_path`: شناسه مدل Hugging Face یا مسیر محلی
- `--output_path`: دایرکتوری که مدل بهینه‌سازی شده در آن ذخیره می‌شود
- `--device`: دستگاه هدف (cpu، gpu)
- `--provider`: ارائه‌دهنده اجرا (CPUExecutionProvider، CUDAExecutionProvider، DmlExecutionProvider)
- `--use_ort_genai`: استفاده از ONNX Runtime Generate AI برای استنتاج
- `--precision`: دقت کوانتیزاسیون (int4، int8، fp16)
- `--log_level`: سطح گزارش‌دهی (۰=حداقل، ۱=مفصل)

## مثال: تبدیل Qwen3 به ONNX INT4

بر اساس مثال ارائه شده در Hugging Face در [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)، در اینجا نحوه بهینه‌سازی مدل Qwen3 آمده است:

### مرحله ۱: دانلود مدل (اختیاری)

برای کاهش زمان دانلود، فقط فایل‌های ضروری را کش کنید:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### مرحله ۲: بهینه‌سازی مدل Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### مرحله ۳: آزمایش مدل بهینه‌سازی شده

یک اسکریپت ساده Python برای آزمایش مدل بهینه‌سازی شده ایجاد کنید:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### ساختار خروجی

پس از بهینه‌سازی، دایرکتوری خروجی شما شامل موارد زیر خواهد بود:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## استفاده پیشرفته

### فایل‌های پیکربندی

برای جریان‌های کاری پیچیده‌تر بهینه‌سازی، می‌توانید از فایل‌های پیکربندی JSON استفاده کنید:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

اجرا با پیکربندی:

```bash
olive run --config config.json
```

### بهینه‌سازی GPU

برای بهینه‌سازی CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

برای DirectML (ویندوز):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### تنظیم دقیق با Olive

Olive همچنین از تنظیم دقیق مدل‌ها پشتیبانی می‌کند:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## بهترین روش‌ها

### ۱. انتخاب مدل
- با مدل‌های کوچک‌تر برای آزمایش شروع کنید (مثلاً ۰.۵B-7B پارامتر)
- اطمینان حاصل کنید که معماری مدل هدف شما توسط Olive پشتیبانی می‌شود

### ۲. ملاحظات سخت‌افزاری
- هدف بهینه‌سازی خود را با سخت‌افزار اجرایی خود مطابقت دهید
- اگر سخت‌افزار سازگار با CUDA دارید، از بهینه‌سازی GPU استفاده کنید
- DirectML را برای ماشین‌های ویندوز با گرافیک یکپارچه در نظر بگیرید

### ۳. انتخاب دقت
- **INT4**: فشرده‌سازی حداکثری، کاهش جزئی دقت
- **INT8**: تعادل خوب بین اندازه و دقت
- **FP16**: کاهش حداقلی دقت، کاهش متوسط اندازه

### ۴. آزمایش و اعتبارسنجی
- همیشه مدل‌های بهینه‌سازی شده را با موارد استفاده خاص خود آزمایش کنید
- معیارهای عملکرد (زمان تأخیر، توان عملیاتی، دقت) را مقایسه کنید
- از داده‌های ورودی نماینده برای ارزیابی استفاده کنید

### ۵. بهینه‌سازی تکراری
- با بهینه‌سازی خودکار برای نتایج سریع شروع کنید
- از فایل‌های پیکربندی برای کنترل دقیق استفاده کنید
- با گذرهای مختلف بهینه‌سازی آزمایش کنید

## رفع مشکلات

### مشکلات رایج

#### ۱. مشکلات نصب
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### ۲. مشکلات CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ۳. مشکلات حافظه
- از اندازه‌های دسته کوچک‌تر در طول بهینه‌سازی استفاده کنید
- ابتدا کوانتیزاسیون با دقت بالاتر را امتحان کنید (int8 به جای int4)
- فضای دیسک کافی برای کش مدل فراهم کنید

#### ۴. خطاهای بارگذاری مدل
- مسیر مدل و مجوزهای دسترسی را بررسی کنید
- بررسی کنید که آیا مدل نیاز به `trust_remote_code=True` دارد
- اطمینان حاصل کنید که همه فایل‌های مورد نیاز مدل دانلود شده‌اند

### دریافت کمک

- **مستندات**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **مشکلات GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **مثال‌ها**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## مخزن دستورالعمل‌های Olive

### معرفی دستورالعمل‌های Olive

مخزن [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) مجموعه اصلی ابزار Olive را با ارائه مجموعه‌ای جامع از دستورالعمل‌های بهینه‌سازی آماده برای مدل‌های محبوب هوش مصنوعی تکمیل می‌کند. این مخزن به عنوان یک مرجع عملی برای بهینه‌سازی مدل‌های عمومی و ایجاد جریان‌های کاری بهینه‌سازی برای مدل‌های اختصاصی عمل می‌کند.

### ویژگی‌های کلیدی

- **بیش از ۱۰۰ دستورالعمل آماده**: پیکربندی‌های بهینه‌سازی آماده برای مدل‌های محبوب
- **پشتیبانی از چند معماری**: شامل مدل‌های ترانسفورمر، مدل‌های بینایی و معماری‌های چندوجهی
- **بهینه‌سازی‌های خاص سخت‌افزار**: دستورالعمل‌هایی که برای CPU، GPU و شتاب‌دهنده‌های تخصصی طراحی شده‌اند
- **خانواده‌های مدل محبوب**: شامل Phi، Llama، Qwen، Gemma، Mistral و بسیاری دیگر

### خانواده‌های مدل پشتیبانی شده

مخزن شامل دستورالعمل‌های بهینه‌سازی برای:

#### مدل‌های زبانی
- **Microsoft Phi**: Phi-3-mini، Phi-3.5-mini، Phi-4-mini، Phi-4-reasoning
- **Meta Llama**: Llama-2-7b، Llama-3.1-8B، Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B، Qwen2-7B، سری Qwen2.5 (0.5B تا 14B)
- **Google Gemma**: پیکربندی‌های مختلف مدل Gemma
- **Mistral AI**: سری Mistral-7B
- **DeepSeek**: مدل‌های سری R1-Distill

#### مدل‌های بینایی و چندوجهی
- **Stable Diffusion**: v1.4، XL-base-1.0
- **مدل‌های CLIP**: پیکربندی‌های مختلف CLIP-ViT
- **ResNet**: بهینه‌سازی‌های ResNet-50
- **ترانسفورمرهای بینایی**: ViT-base-patch16-224

#### مدل‌های تخصصی
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: نسخه‌های پایه و چندزبانه
- **ترانسفورمرهای جمله**: all-MiniLM-L6-v2

### استفاده از دستورالعمل‌های Olive

#### روش ۱: کلون کردن دستورالعمل خاص

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### روش ۲: استفاده از دستورالعمل به عنوان قالب

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### ساختار دستورالعمل

هر دایرکتوری دستورالعمل معمولاً شامل موارد زیر است:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### مثال: استفاده از دستورالعمل Phi-4-mini

بیایید از دستورالعمل Phi-4-mini به عنوان مثال استفاده کنیم:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

فایل پیکربندی معمولاً شامل موارد زیر است:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### سفارشی‌سازی دستورالعمل‌ها

#### تغییر سخت‌افزار هدف

برای تغییر سخت‌افزار هدف، بخش `systems` را به‌روزرسانی کنید:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### تنظیم پارامترهای بهینه‌سازی

بخش `passes` را برای سطوح مختلف بهینه‌سازی تغییر دهید:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### ایجاد دستورالعمل خودتان

1. **شروع با مدل مشابه**: یک دستورالعمل برای مدلی با معماری مشابه پیدا کنید.
2. **به‌روزرسانی پیکربندی مدل**: نام/مسیر مدل را در پیکربندی تغییر دهید.
3. **تنظیم پارامترها**: پارامترهای بهینه‌سازی را به دلخواه تغییر دهید.
4. **آزمایش و اعتبارسنجی**: بهینه‌سازی را اجرا کرده و نتایج را اعتبارسنجی کنید.
5. **بازگشت به جامعه**: دستورالعمل خود را به مخزن اضافه کنید.

### مزایای استفاده از دستورالعمل‌ها

#### ۱. **پیکربندی‌های اثبات‌شده**
- تنظیمات بهینه‌سازی آزمایش شده برای مدل‌های خاص.
- جلوگیری از آزمون و خطا در یافتن پارامترهای بهینه.

#### ۲. **تنظیمات خاص سخت‌افزار**
- از پیش بهینه‌سازی شده برای ارائه‌دهندگان مختلف اجرا.
- پیکربندی‌های آماده برای اهداف CPU، GPU و NPU.

#### ۳. **پوشش جامع**
- پشتیبانی از محبوب‌ترین مدل‌های متن‌باز.
- به‌روزرسانی‌های منظم با انتشار مدل‌های جدید.

#### ۴. **مشارکت‌های جامعه**
- توسعه مشترک با جامعه هوش مصنوعی.
- اشتراک دانش و بهترین روش‌ها.

### مشارکت در دستورالعمل‌های Olive

اگر مدلی را بهینه‌سازی کرده‌اید که در مخزن پوشش داده نشده است:

1. **فورک کردن مخزن**: فورک خودتان از olive-recipes ایجاد کنید.
2. **ایجاد دایرکتوری دستورالعمل**: یک دایرکتوری جدید برای مدل خود اضافه کنید.
3. **شامل پیکربندی**: olive_config.json و فایل‌های پشتیبانی را اضافه کنید.
4. **مستندسازی استفاده**: README واضح با دستورالعمل‌ها ارائه دهید.
5. **ارسال درخواست کشش**: به جامعه بازگردید.

### معیارهای عملکرد

بسیاری از دستورالعمل‌ها شامل معیارهای عملکرد هستند که نشان می‌دهند:
- **بهبود زمان تأخیر**: معمولاً ۲-۶ برابر سرعت بیشتر نسبت به حالت اولیه.
- **کاهش حافظه**: کاهش ۵۰-۷۵٪ استفاده از حافظه با کوانتیزاسیون.
- **حفظ دقت**: حفظ دقت ۹۵-۹۹٪.

### یکپارچه‌سازی با ابزار هوش مصنوعی

دستورالعمل‌ها به طور یکپارچه با موارد زیر کار می‌کنند:
- **VS Code AI Toolkit**: یکپارچه‌سازی مستقیم برای بهینه‌سازی مدل.
- **Azure Machine Learning**: جریان‌های کاری بهینه‌سازی مبتنی بر ابر.
- **ONNX Runtime**: استنتاج بهینه‌سازی شده برای اجرا.

## منابع اضافی

### لینک‌های رسمی
- **مخزن GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **مخزن دستورالعمل‌های Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **مستندات ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **مثال Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### مثال‌های جامعه
- **دفترچه‌های Jupyter**: موجود در مخزن GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **افزونه VS Code**: نمای کلی ابزار هوش مصنوعی برای VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **پست‌های وبلاگ**: وبلاگ متن‌باز Microsoft — https://opensource.microsoft.com/blog/

### ابزارهای مرتبط
- **ONNX Runtime**: موتور استنتاج با عملکرد بالا — https://onnxruntime.ai/
- **Hugging Face Transformers**: منبع بسیاری از مدل‌های سازگار — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: جریان‌های کاری بهینه‌سازی مبتنی بر ابر — https://learn.microsoft.com/azure/machine-learning/

## ➡️ مرحله بعدی

- [04: مجموعه بهینه‌سازی OpenVINO Toolkit](./04.openvino.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.