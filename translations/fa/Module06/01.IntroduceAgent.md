<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T10:57:27+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "fa"
}
-->
# عوامل هوش مصنوعی و مدل‌های زبان کوچک: راهنمای جامع

## مقدمه

در این آموزش، عوامل هوش مصنوعی و مدل‌های زبان کوچک (SLMs) و استراتژی‌های پیشرفته پیاده‌سازی آن‌ها برای محیط‌های محاسباتی لبه را بررسی خواهیم کرد. مفاهیم بنیادی هوش مصنوعی عامل‌محور، تکنیک‌های بهینه‌سازی SLM، استراتژی‌های عملی برای استقرار در دستگاه‌های محدود منابع و چارچوب عامل مایکروسافت برای ساخت سیستم‌های عامل آماده تولید را پوشش خواهیم داد.

چشم‌انداز هوش مصنوعی در سال ۲۰۲۵ دچار یک تغییر پارادایم شده است. در حالی که سال ۲۰۲۳ سال چت‌بات‌ها و سال ۲۰۲۴ شاهد رونق کوپایلوت‌ها بود، سال ۲۰۲۵ متعلق به عوامل هوش مصنوعی است — سیستم‌های هوشمندی که فکر می‌کنند، استدلال می‌کنند، برنامه‌ریزی می‌کنند، از ابزارها استفاده می‌کنند و وظایف را با کمترین ورودی انسانی اجرا می‌کنند، که به طور فزاینده‌ای توسط مدل‌های زبان کوچک کارآمد قدرت می‌گیرند. چارچوب عامل مایکروسافت به عنوان یک راه‌حل پیشرو برای ساخت این سیستم‌های هوشمند با قابلیت‌های مبتنی بر لبه آفلاین ظاهر شده است.

## اهداف آموزشی

در پایان این آموزش، شما قادر خواهید بود:

- 🤖 مفاهیم بنیادی عوامل هوش مصنوعی و سیستم‌های عامل‌محور را درک کنید
- 🔬 مزایای مدل‌های زبان کوچک نسبت به مدل‌های زبان بزرگ در کاربردهای عامل‌محور را شناسایی کنید
- 🚀 استراتژی‌های پیشرفته استقرار SLM برای محیط‌های محاسباتی لبه را یاد بگیرید
- 📱 عوامل عملی مبتنی بر SLM را برای کاربردهای دنیای واقعی پیاده‌سازی کنید
- 🏗️ عوامل آماده تولید را با استفاده از چارچوب عامل مایکروسافت بسازید
- 🌐 عوامل مبتنی بر لبه آفلاین را با ادغام LLM و SLM محلی مستقر کنید
- 🔧 چارچوب عامل مایکروسافت را با Foundry Local برای استقرار لبه ادغام کنید

## درک عوامل هوش مصنوعی: مبانی و طبقه‌بندی‌ها

### تعریف و مفاهیم اصلی

عامل هوش مصنوعی (AI) به سیستمی یا برنامه‌ای اشاره دارد که قادر است به طور خودکار وظایفی را به نمایندگی از یک کاربر یا سیستم دیگر انجام دهد، با طراحی جریان کاری خود و استفاده از ابزارهای موجود. برخلاف هوش مصنوعی سنتی که فقط به سوالات شما پاسخ می‌دهد، یک عامل می‌تواند به طور مستقل برای دستیابی به اهداف عمل کند.

### چارچوب طبقه‌بندی عامل

درک مرزهای عامل به انتخاب انواع مناسب عامل برای سناریوهای مختلف محاسباتی کمک می‌کند:

- **🔬 عوامل بازتاب ساده**: سیستم‌های مبتنی بر قوانین که به ادراکات فوری پاسخ می‌دهند (ترموستات‌ها، اتوماسیون پایه)
- **📱 عوامل مبتنی بر مدل**: سیستم‌هایی که حالت داخلی و حافظه را حفظ می‌کنند (جاروبرقی‌های رباتیک، سیستم‌های ناوبری)
- **⚖️ عوامل مبتنی بر هدف**: سیستم‌هایی که برای دستیابی به اهداف، توالی‌ها را برنامه‌ریزی و اجرا می‌کنند (برنامه‌ریزان مسیر، زمان‌بندهای وظیفه)
- **🧠 عوامل یادگیری**: سیستم‌های تطبیقی که عملکرد را در طول زمان بهبود می‌بخشند (سیستم‌های توصیه‌گر، دستیارهای شخصی)

### مزایای کلیدی عوامل هوش مصنوعی

عوامل هوش مصنوعی چندین مزیت بنیادی ارائه می‌دهند که آن‌ها را برای کاربردهای محاسباتی لبه ایده‌آل می‌کند:

**خودمختاری عملیاتی**: عوامل اجرای وظایف مستقل را بدون نظارت مداوم انسانی فراهم می‌کنند، که آن‌ها را برای کاربردهای بلادرنگ ایده‌آل می‌کند. آن‌ها به نظارت حداقلی نیاز دارند و در عین حال رفتار تطبیقی را حفظ می‌کنند، که امکان استقرار در دستگاه‌های محدود منابع با کاهش سربار عملیاتی را فراهم می‌کند.

**انعطاف‌پذیری استقرار**: این سیستم‌ها قابلیت‌های هوش مصنوعی در دستگاه را بدون نیاز به اتصال اینترنت فعال می‌کنند، حریم خصوصی و امنیت را از طریق پردازش محلی افزایش می‌دهند، می‌توانند برای کاربردهای خاص دامنه سفارشی شوند و برای محیط‌های مختلف محاسباتی لبه مناسب هستند.

**مقرون‌به‌صرفه بودن**: سیستم‌های عامل استقرار مقرون‌به‌صرفه‌ای را در مقایسه با راه‌حل‌های مبتنی بر ابر ارائه می‌دهند، با کاهش هزینه‌های عملیاتی و نیازهای پهنای باند کمتر برای کاربردهای لبه.

## استراتژی‌های پیشرفته مدل زبان کوچک

### مبانی SLM (مدل زبان کوچک)

مدل زبان کوچک (SLM) مدلی زبانی است که می‌تواند بر روی یک دستگاه الکترونیکی مصرف‌کننده معمولی قرار گیرد و استنتاج را با تأخیر کافی کم برای پاسخگویی به درخواست‌های عامل‌محور یک کاربر انجام دهد. به طور عملی، SLM‌ها معمولاً مدل‌هایی با کمتر از ۱۰ میلیارد پارامتر هستند.

**ویژگی‌های کشف فرمت**: SLM‌ها پشتیبانی پیشرفته‌ای برای سطوح مختلف کمیت‌سازی، سازگاری بین پلتفرمی، بهینه‌سازی عملکرد بلادرنگ و قابلیت‌های استقرار لبه ارائه می‌دهند. کاربران می‌توانند از حریم خصوصی پیشرفته از طریق پردازش محلی و پشتیبانی WebGPU برای استقرار مبتنی بر مرورگر بهره‌مند شوند.

**مجموعه‌های سطح کمیت‌سازی**: فرمت‌های محبوب SLM شامل Q4_K_M برای فشرده‌سازی متعادل در کاربردهای موبایل، سری Q5_K_S برای استقرار لبه با تمرکز بر کیفیت، Q8_0 برای دقت نزدیک به اصل در دستگاه‌های قدرتمند لبه و فرمت‌های آزمایشی مانند Q2_K برای سناریوهای منابع فوق‌العاده کم هستند.

### GGUF (فرمت عمومی جهانی GGML) برای استقرار SLM

GGUF به عنوان فرمت اصلی برای استقرار SLM‌های کمیت‌سازی شده بر روی CPU و دستگاه‌های لبه، به طور خاص برای کاربردهای عامل‌محور بهینه شده است:

**ویژگی‌های بهینه‌شده برای عامل**: این فرمت منابع جامعی برای تبدیل و استقرار SLM با پشتیبانی پیشرفته برای فراخوانی ابزار، تولید خروجی ساختاریافته و مکالمات چند نوبتی ارائه می‌دهد. سازگاری بین پلتفرمی رفتار عامل سازگار را در دستگاه‌های مختلف لبه تضمین می‌کند.

**بهینه‌سازی عملکرد**: GGUF استفاده کارآمد از حافظه برای جریان‌های کاری عامل، پشتیبانی از بارگذاری پویا مدل برای سیستم‌های چند عاملی و استنتاج بهینه برای تعاملات بلادرنگ عامل را امکان‌پذیر می‌کند.

### چارچوب‌های بهینه‌شده برای SLM در لبه

#### بهینه‌سازی Llama.cpp برای عوامل

Llama.cpp تکنیک‌های کمیت‌سازی پیشرفته‌ای را به طور خاص برای استقرار عامل‌محور SLM ارائه می‌دهد:

**کمیت‌سازی خاص عامل**: این چارچوب از Q4_0 (بهینه برای استقرار عامل موبایل با کاهش اندازه ۷۵٪)، Q5_1 (کیفیت-فشرده‌سازی متعادل برای عوامل استنتاج لبه) و Q8_0 (کیفیت نزدیک به اصل برای سیستم‌های عامل تولیدی) پشتیبانی می‌کند. فرمت‌های پیشرفته عوامل فوق‌العاده فشرده را برای سناریوهای لبه شدید امکان‌پذیر می‌کنند.

**مزایای پیاده‌سازی**: استنتاج بهینه‌شده برای CPU با شتاب SIMD اجرای عامل حافظه‌کارآمد را فراهم می‌کند. سازگاری بین پلتفرمی در معماری‌های x86، ARM و Apple Silicon قابلیت‌های استقرار جهانی عامل را امکان‌پذیر می‌کند.

#### چارچوب Apple MLX برای عوامل SLM

Apple MLX بهینه‌سازی بومی را به طور خاص برای عوامل مبتنی بر SLM در دستگاه‌های Apple Silicon طراحی کرده است:

**بهینه‌سازی عامل Apple Silicon**: این چارچوب از معماری حافظه یکپارچه با ادغام Metal Performance Shaders، دقت مختلط خودکار برای استنتاج عامل و پهنای باند حافظه بهینه برای سیستم‌های چند عاملی استفاده می‌کند. عوامل SLM عملکرد استثنایی را بر روی تراشه‌های سری M نشان می‌دهند.

**ویژگی‌های توسعه**: پشتیبانی API Python و Swift با بهینه‌سازی‌های خاص عامل، تفکیک خودکار برای یادگیری عامل و ادغام بی‌دردسر با ابزارهای توسعه Apple محیط‌های جامع توسعه عامل را فراهم می‌کند.

#### ONNX Runtime برای عوامل SLM چند پلتفرمی

ONNX Runtime یک موتور استنتاج جهانی ارائه می‌دهد که امکان اجرای عوامل SLM را به طور سازگار در پلتفرم‌های سخت‌افزاری و سیستم‌عامل‌های مختلف فراهم می‌کند:

**استقرار جهانی**: ONNX Runtime رفتار سازگار عامل SLM را در پلتفرم‌های Windows، Linux، macOS، iOS و Android تضمین می‌کند. این سازگاری بین پلتفرمی به توسعه‌دهندگان امکان می‌دهد یک بار بنویسند و در همه جا مستقر کنند، که به طور قابل توجهی هزینه‌های توسعه و نگهداری را برای کاربردهای چند پلتفرمی کاهش می‌دهد.

**گزینه‌های شتاب سخت‌افزاری**: این چارچوب ارائه‌دهندگان اجرای بهینه برای پیکربندی‌های سخت‌افزاری مختلف از جمله CPU (Intel، AMD، ARM)، GPU (NVIDIA CUDA، AMD ROCm) و شتاب‌دهنده‌های تخصصی (Intel VPU، Qualcomm NPU) ارائه می‌دهد. عوامل SLM می‌توانند به طور خودکار از بهترین سخت‌افزار موجود بدون تغییر کد استفاده کنند.

**ویژگی‌های آماده تولید**: ONNX Runtime ویژگی‌های درجه سازمانی ضروری برای استقرار عامل تولیدی از جمله بهینه‌سازی گراف برای استنتاج سریع‌تر، مدیریت حافظه برای محیط‌های محدود منابع و ابزارهای جامع پروفایلینگ برای تحلیل عملکرد ارائه می‌دهد. این چارچوب از API‌های Python و C++ برای ادغام انعطاف‌پذیر پشتیبانی می‌کند.

## SLM در مقابل LLM در سیستم‌های عامل‌مح
- آزمایش یکپارچگی Microsoft Agent Framework  
- بررسی قابلیت‌های عملکرد آفلاین  
- آزمایش سناریوهای خرابی و مدیریت خطا  
- اعتبارسنجی جریان‌های کاری کامل عامل  

**مقایسه با Foundry Local**:

| ویژگی | Foundry Local | Ollama |
|-------|---------------|--------|
| **مورد استفاده هدف** | تولید سازمانی | توسعه و جامعه |
| **اکوسیستم مدل** | انتخاب شده توسط مایکروسافت | جامعه گسترده |
| **بهینه‌سازی سخت‌افزار** | خودکار (CUDA/NPU/CPU) | تنظیم دستی |
| **ویژگی‌های سازمانی** | نظارت و امنیت داخلی | ابزارهای جامعه |
| **پیچیدگی استقرار** | ساده (نصب با winget) | ساده (نصب با curl) |
| **سازگاری API** | OpenAI + افزونه‌ها | استاندارد OpenAI |
| **پشتیبانی** | رسمی مایکروسافت | مبتنی بر جامعه |
| **بهترین برای** | عوامل تولیدی | نمونه‌سازی اولیه و تحقیق |

**زمان انتخاب Ollama**:
- **توسعه و نمونه‌سازی اولیه**: آزمایش سریع با مدل‌های مختلف  
- **مدل‌های جامعه**: دسترسی به جدیدترین مدل‌های ارائه شده توسط جامعه  
- **استفاده آموزشی**: یادگیری و آموزش توسعه عوامل هوش مصنوعی  
- **پروژه‌های تحقیقاتی**: تحقیقات آکادمیک با نیاز به دسترسی به مدل‌های متنوع  
- **مدل‌های سفارشی**: ساخت و آزمایش مدل‌های تنظیم‌شده سفارشی  

### VLLM: استنتاج عامل SLM با عملکرد بالا

VLLM (استنتاج مدل زبان بسیار بزرگ) یک موتور استنتاج با توان بالا و کارآمد از نظر حافظه است که به طور خاص برای استقرارهای تولیدی SLM در مقیاس بهینه شده است. در حالی که Foundry Local بر سهولت استفاده تمرکز دارد و Ollama بر مدل‌های جامعه تأکید دارد، VLLM در سناریوهای با عملکرد بالا که نیاز به حداکثر توان و استفاده کارآمد از منابع دارند، برجسته است.

**معماری و ویژگی‌های اصلی**:
- **PagedAttention**: مدیریت حافظه انقلابی برای محاسبات توجه کارآمد  
- **Batching پویا**: دسته‌بندی هوشمند درخواست‌ها برای توان بهینه  
- **بهینه‌سازی GPU**: هسته‌های پیشرفته CUDA و پشتیبانی از موازی‌سازی تنسور  
- **سازگاری با OpenAI**: سازگاری کامل API برای یکپارچگی بی‌دردسر  
- **رمزگشایی حدسی**: تکنیک‌های پیشرفته تسریع استنتاج  
- **پشتیبانی از کمینه‌سازی**: کمینه‌سازی INT4، INT8 و FP16 برای کارایی حافظه  

#### نصب و راه‌اندازی

**گزینه‌های نصب**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**شروع سریع برای توسعه عامل**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### یکپارچگی با چارچوب عامل

**VLLM با Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**راه‌اندازی چند عاملی با توان بالا**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### الگوهای استقرار تولیدی

**خدمات تولیدی VLLM سازمانی**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### ویژگی‌های سازمانی و نظارت

**نظارت پیشرفته عملکرد VLLM**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### تنظیمات و بهینه‌سازی پیشرفته

**الگوهای تنظیمات تولیدی VLLM**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**چک‌لیست استقرار تولیدی برای VLLM**:

✅ **بهینه‌سازی سخت‌افزار**:
- تنظیم موازی‌سازی تنسور برای تنظیمات چند GPU  
- فعال‌سازی کمینه‌سازی (AWQ/GPTQ) برای کارایی حافظه  
- تنظیم استفاده بهینه از حافظه GPU (85-95%)  
- تنظیم اندازه دسته‌های مناسب برای توان  

✅ **تنظیم عملکرد**:
- فعال‌سازی کش پیشوند برای پرسش‌های تکراری  
- تنظیم پیش‌پر کردن قطعه‌ای برای دنباله‌های طولانی  
- راه‌اندازی رمزگشایی حدسی برای استنتاج سریع‌تر  
- بهینه‌سازی max_num_seqs بر اساس سخت‌افزار  

✅ **ویژگی‌های تولیدی**:
- راه‌اندازی نظارت بر سلامت و جمع‌آوری معیارها  
- تنظیم راه‌اندازی مجدد خودکار و خرابی  
- پیاده‌سازی صف درخواست و تعادل بار  
- راه‌اندازی ثبت جامع و هشداردهی  

✅ **امنیت و قابلیت اطمینان**:
- تنظیم قوانین فایروال و کنترل‌های دسترسی  
- راه‌اندازی محدودیت نرخ API و احراز هویت  
- پیاده‌سازی خاموشی و پاکسازی آرام  
- تنظیم پشتیبان‌گیری و بازیابی در برابر بلایا  

✅ **آزمایش یکپارچگی**:
- آزمایش یکپارچگی Microsoft Agent Framework  
- اعتبارسنجی سناریوهای با توان بالا  
- آزمایش خرابی و روش‌های بازیابی  
- ارزیابی عملکرد تحت بار  

**مقایسه با سایر راه‌حل‌ها**:

| ویژگی | VLLM | Foundry Local | Ollama |
|-------|------|---------------|--------|
| **مورد استفاده هدف** | تولید با توان بالا | سهولت استفاده سازمانی | توسعه و جامعه |
| **عملکرد** | توان حداکثری | متعادل | خوب |
| **کارایی حافظه** | بهینه‌سازی PagedAttention | بهینه‌سازی خودکار | استاندارد |
| **پیچیدگی تنظیمات** | بالا (پارامترهای زیاد) | پایین (خودکار) | پایین (ساده) |
| **مقیاس‌پذیری** | عالی (موازی‌سازی تنسور/خط لوله) | خوب | محدود |
| **کمینه‌سازی** | پیشرفته (AWQ، GPTQ، FP8) | خودکار | استاندارد GGUF |
| **ویژگی‌های سازمانی** | نیاز به پیاده‌سازی سفارشی | داخلی | ابزارهای جامعه |
| **بهترین برای** | عوامل تولیدی در مقیاس بالا | تولید سازمانی | توسعه |

**زمان انتخاب VLLM**:
- **نیازهای توان بالا**: پردازش صدها درخواست در ثانیه  
- **استقرارهای بزرگ‌مقیاس**: تنظیمات چند GPU، چند گره  
- **عملکرد حیاتی**: زمان پاسخ زیر ثانیه در مقیاس  
- **بهینه‌سازی پیشرفته**: نیاز به کمینه‌سازی و دسته‌بندی سفارشی  
- **کارایی منابع**: استفاده حداکثری از سخت‌افزار GPU گران‌قیمت  

## کاربردهای واقعی عوامل SLM

### عوامل SLM خدمات مشتری
- **قابلیت‌های SLM**: جستجوی حساب‌ها، تنظیم مجدد رمز عبور، بررسی وضعیت سفارش  
- **مزایای هزینه**: کاهش 10 برابری هزینه‌های استنتاج نسبت به عوامل LLM  
- **عملکرد**: زمان‌های پاسخ سریع‌تر با کیفیت ثابت برای پرسش‌های معمول  

### عوامل SLM فرآیندهای کسب‌وکار
- **عوامل پردازش فاکتور**: استخراج داده‌ها، اعتبارسنجی اطلاعات، ارسال برای تأیید  
- **عوامل مدیریت ایمیل**: دسته‌بندی، اولویت‌بندی، پیش‌نویس پاسخ‌ها به صورت خودکار  
- **عوامل زمان‌بندی**: هماهنگی جلسات، مدیریت تقویم‌ها، ارسال یادآوری‌ها  

### دستیارهای دیجیتال شخصی SLM
- **عوامل مدیریت وظایف**: ایجاد، به‌روزرسانی، سازماندهی لیست‌های کارها به صورت کارآمد  
- **عوامل جمع‌آوری اطلاعات**: تحقیق در مورد موضوعات، خلاصه‌سازی یافته‌ها به صورت محلی  
- **عوامل ارتباطی**: پیش‌نویس ایمیل‌ها، پیام‌ها، پست‌های شبکه‌های اجتماعی به صورت خصوصی  

### عوامل SLM مالی و تجاری
- **عوامل نظارت بر بازار**: ردیابی قیمت‌ها، شناسایی روندها در زمان واقعی  
- **عوامل تولید گزارش**: ایجاد خلاصه‌های روزانه/هفتگی به صورت خودکار  
- **عوامل ارزیابی ریسک**: ارزیابی موقعیت‌های پرتفوی با استفاده از داده‌های محلی  

### عوامل SLM پشتیبانی سلامت
- **عوامل زمان‌بندی بیماران**: هماهنگی قرار ملاقات‌ها، ارسال یادآوری‌های خودکار  
- **عوامل مستندسازی**: تولید خلاصه‌های پزشکی، گزارش‌ها به صورت محلی  
- **عوامل مدیریت نسخه‌ها**: ردیابی تجدیدها، بررسی تعاملات به صورت خصوصی  

## Microsoft Agent Framework: توسعه عامل آماده تولید

### مرور کلی و معماری

Microsoft Agent Framework یک پلتفرم جامع و درجه سازمانی برای ساخت، استقرار و مدیریت عوامل هوش مصنوعی ارائه می‌دهد که می‌توانند هم در فضای ابری و هم در محیط‌های لبه آفلاین عمل کنند. این چارچوب به طور خاص برای کار با مدل‌های زبان کوچک و سناریوهای محاسبات لبه طراحی شده است، که آن را برای استقرارهای حساس به حریم خصوصی و محدود به منابع ایده‌آل می‌کند.

**اجزای اصلی چارچوب**:
- **زمان اجرای عامل**: محیط اجرای سبک بهینه شده برای دستگاه‌های لبه  
- **سیستم یکپارچگی ابزار**: معماری افزونه قابل توسعه برای اتصال خدمات و APIهای خارجی  
- **مدیریت حالت**: حافظه پایدار عامل و مدیریت زمینه در جلسات  
- **لایه امنیتی**: کنترل‌های امنیتی داخلی برای استقرار سازمانی  
- **موتور ارکستراسیون**: هماهنگی چند عاملی و مدیریت جریان کاری  

### ویژگی‌های کلیدی برای استقرار لبه

**معماری آفلاین-اول**: Microsoft Agent Framework با اصول آفلاین-اول طراحی شده است، که به عوامل امکان می‌دهد بدون اتصال دائمی به اینترنت به طور مؤثر عمل کنند. این شامل استنتاج مدل محلی، پایگاه‌های دانش ذخیره شده، اجرای ابزار آفلاین، و کاهش تدریجی زمانی که خدمات ابری در دسترس نیستند.

**بهینه‌سازی منابع**: این چارچوب مدیریت منابع هوشمند را با بهینه‌سازی خودکار حافظه برای SLMها، تعادل بار CPU/GPU برای دستگاه‌های لبه، انتخاب تطبیقی مدل بر اساس منابع موجود، و الگوهای استنتاج کم‌مصرف برای استقرار موبایل ارائه می‌دهد.

**امنیت و حریم خصوصی**: ویژگی‌های امنیتی درجه سازمانی شامل پردازش داده‌های محلی برای حفظ حریم خصوصی، کانال‌های ارتباطی رمزگذاری شده عامل، کنترل‌های دسترسی مبتنی بر نقش برای قابلیت‌های عامل، و ثبت حسابرسی برای الزامات انطباق است.

### یکپارچگی با Foundry Local

Microsoft Agent Framework به طور یکپارچه با Foundry Local ادغام می‌شود تا یک راه‌حل کامل هوش مصنوعی لبه ارائه دهد:

**کشف خودکار مدل**: این چارچوب به طور خودکار نمونه‌های Foundry Local را شناسایی و متصل می‌کند، مدل‌های SLM موجود را کشف می‌کند، و مدل‌های بهینه را بر اساس نیازهای عامل و قابلیت‌های سخت‌افزاری انتخاب می‌کند.

**بارگذاری پویا مدل**: عوامل می‌توانند مدل‌های مختلف SLM را برای وظایف خاص به صورت پویا بارگذاری کنند، که امکان سیستم‌های چند مدلی عامل را فراهم می‌کند که مدل‌های مختلف درخواست‌های مختلف را مدیریت می‌کنند، و خرابی خودکار بین مدل‌ها بر اساس دسترسی و عملکرد.

**بهینه‌سازی عملکرد**: مکانیزم‌های کش یکپارچه زمان‌های بارگذاری مدل را کاهش می‌دهند، اتصال استخر تماس‌های API به Foundry Local را بهینه می‌کند، و دسته‌بندی هوشمند توان را برای درخواست‌های چند عاملی بهبود می‌بخشد.

### ساخت عوامل با Microsoft Agent Framework

#### تعریف و تنظیم عامل

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### یکپارچگی ابزار برای سناریوهای لبه

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### ارکستراسیون چند عاملی

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### الگوهای استقرار پیشرفته لبه

#### معماری عامل سلسله‌مراتبی

**خوشه‌های عامل محلی**: استقرار چندین عامل SLM تخصصی بر روی دستگاه‌های لبه، هر کدام برای وظایف خاص بهینه شده‌اند. استفاده از مدل‌های سبک مانند Qwen2.5-0.5B برای مسیریابی و زمان‌بندی ساده، مدل‌های متوسط مانند Phi-4-Mini برای خدمات مشتری و مستندسازی، و مدل‌های بزرگ‌تر برای استدلال پیچیده زمانی که منابع اجازه می‌دهند.

**هماهنگی لبه-به-ابر**: پیاده‌سازی الگوهای تشدید هوشمند که در آن عوامل محلی وظایف معمول را مدیریت می‌کنند، عوامل ابری استدلال پیچیده را زمانی که اتصال اجازه می‌دهد ارائه می‌دهند، و انتقال بی‌وقفه بین پردازش لبه و ابر تداوم را حفظ می‌کند.

#### تنظیمات استقرار

**استقرار دستگاه واحد**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**استقرار لبه توزیع‌شده**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### بهینه‌سازی عملکرد برای عوامل لبه

#### استراتژی‌های انتخاب مدل

**تخصیص مدل مبتنی بر وظیفه**: Microsoft Agent Framework امکان انتخاب هوشمند مدل بر اساس پیچیدگی وظیفه و نیازها را فراهم می‌کند:

- **وظایف ساده** (پرسش و پاسخ، مسیریابی): Qwen2.5-0.5B (500MB، پاسخ <100ms)  
- **وظایف متوسط** (خدمات مشتری، زمان‌بندی): Phi-4-Mini (2.4GB، پاسخ 200-500ms)  
- **وظایف پیچیده** (تحلیل فنی، برنامه‌ریزی): Phi-4 (7GB، پاسخ 1-3s زمانی که منابع اجازه می‌دهند)  

**تغییر مدل پویا**: عوامل می‌توانند بر اساس بار فعلی سیستم، ارزیابی پیچیدگی وظیفه، سطح اولویت کاربر، و منابع سخت‌افزاری موجود بین مدل‌ها تغییر کنند.

#### مدیریت حافظه و منابع

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  

### الگوهای یکپارچگی سازمانی

#### امنیت و انطباق

**پردازش داده‌های محلی**: تمام پردازش عامل به صورت محلی انجام می‌شود، که اطمینان می‌دهد داده‌های حساس هرگز دستگاه لبه را ترک نمی‌کنند. این شامل حفاظت از اطلاعات مشتری، انطباق HIPAA برای عوامل سلامت، امنیت داده‌های مالی برای عوامل بانکی، و انطباق GDPR برای استقرارهای اروپایی است.

**کنترل دسترسی**: مجوزهای مبتنی بر نقش کنترل می‌کنند که کدام ابزارها عوامل می‌توانند به آن‌ها دسترسی داشته باشند، احراز هویت کاربر برای تعاملات عامل، و مسیرهای حسابرسی برای تمام اقدامات و تصمیمات عامل.

#### نظارت و مشاهده‌پذیری

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```
  

### نمونه‌های پیاده‌سازی واقعی

#### سیستم عامل لبه خرده‌فروشی

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```
  
#### عامل پشتیبانی سلامت

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```
  

### بهترین روش‌ها برای Microsoft Agent Framework

#### دستورالعمل‌های توسعه

1. **شروع ساده**: ابتدا با سناریوهای تک‌عاملی شروع کنید قبل از ساخت سیستم‌های پیچیده چند عاملی  
2. **اندازه‌گیری مدل مناسب**: کوچک‌ترین مدلی را انتخاب کنید که نیازهای دقت شما را برآورده کند  
3. **طراحی ابزار**: ابزارهای متمرکز و تک‌منظوره ایجاد کنید به جای ابزارهای چندمنظوره پیچیده  
4. **مدیریت خطا**: کاهش تدریجی برای سناریوهای آفلاین و خرابی مدل پیاده‌سازی کنید  
5. **آزمایش**: عوامل را به طور گسترده در شرایط آفلاین و محیط‌های محدود به منابع آزمایش کنید  

#### بهترین روش‌های استقرار

1. **استقرار تدریجی**: ابتدا به گروه‌های کوچک کاربران استقرار دهید، معیارهای عملکرد را به دقت نظارت کنید  
2. **نظارت بر منابع**: هشدارهایی برای آستانه‌های حافظه، CPU، و زمان پاسخ تنظیم کنید  
3. **استراتژی‌های پشتیبان**: همیشه برنامه‌های پشتیبان برای خرابی مدل یا خستگی منابع داشته باشید  
4. **امنیت اولویت دارد**: کنترل‌های امنیتی را از ابتدا پیاده‌سازی کنید، نه به عنوان یک فکر بعدی  
5. **مستندسازی**: مستندات واضحی از قابلیت‌ها و محدودیت‌های عامل حفظ کنید  

### نقشه راه آینده و یکپارچگی

Microsoft Agent Framework به تکامل خود با بهینه‌سازی SLM پیشرفته، ابزارهای استقرار لبه بهبود یافته، مدیریت منابع بهتر برای محیط‌های محدود به منابع، و اکوسیستم ابزار گسترش‌یافته برای سناریوهای معمول سازمانی ادامه می‌دهد.

**ویژگی‌های آینده**:
- **AutoML برای بهینه‌سازی عامل**: تنظیم خودکار SLMها برای وظایف خاص عامل  
- **شبکه‌سازی مش لبه**: هماهنگی بین استقرارهای متعدد عامل لبه  
- **تله‌متری پیشرفته**: نظارت و تحلیل‌های بهبود یافته برای عملکرد عامل  
- **سازنده عامل بصری**: ابزارهای توسعه عامل کم‌کد/بدون کد  

## بهترین روش‌ها برای پیاده‌سازی عامل SLM

### دستورالعمل‌های انتخاب SLM برای عوامل

هنگام انتخاب SLMها برای استقرار عامل، عوامل زیر را در نظر بگیرید:

**ملاحظات اندازه مدل**: مدل‌های فوق فشرده مانند Q2_K را برای برنامه‌های عامل موبایل بسیار محدود، مدل‌های متعادل مانند Q4_K_M را برای سناریوهای عمومی عامل، و مدل‌های با دقت بالاتر مانند Q8_0 را برای برنامه‌های عامل حساس به کیفیت انتخاب کنید.

**هم‌راستایی مورد استفاده عامل**: قابلیت‌های SLM را با نیازهای خاص عامل مطابقت دهید، با در نظر گرفتن عواملی مانند حفظ دقت برای تصمیمات عامل، سرعت استنتاج برای تعاملات عامل در زمان واقعی، محدودیت‌های حافظه برای استقرار عامل لبه، و نیازهای عملکرد آفلاین برای عوامل متمرکز بر ح
**انتخاب چارچوب برای استقرار عامل**: چارچوب‌های بهینه‌سازی را بر اساس سخت‌افزار هدف و نیازهای عامل انتخاب کنید. از Llama.cpp برای استقرار عامل بهینه‌شده برای CPU، Apple MLX برای برنامه‌های عامل در Apple Silicon و ONNX برای سازگاری عامل در پلتفرم‌های مختلف استفاده کنید.

## تبدیل عملی عامل SLM و موارد استفاده

### سناریوهای واقعی استقرار عامل

**برنامه‌های عامل موبایل**: فرمت‌های Q4_K در برنامه‌های عامل گوشی‌های هوشمند با حداقل مصرف حافظه عالی عمل می‌کنند، در حالی که Q8_0 عملکرد متعادلی برای سیستم‌های عامل مبتنی بر تبلت ارائه می‌دهد. فرمت‌های Q5_K کیفیت برتری برای عوامل بهره‌وری موبایل دارند.

**رایانه‌های رومیزی و محاسبات عامل لبه**: Q5_K عملکرد بهینه‌ای برای برنامه‌های عامل رایانه‌های رومیزی ارائه می‌دهد، Q8_0 استنتاج با کیفیت بالا را برای محیط‌های عامل ایستگاه کاری فراهم می‌کند و Q4_K پردازش کارآمدی را در دستگاه‌های عامل لبه امکان‌پذیر می‌سازد.

**عوامل تحقیقاتی و آزمایشی**: فرمت‌های پیشرفته کوانتیزاسیون امکان بررسی استنتاج عامل با دقت فوق‌العاده پایین را برای تحقیقات آکادمیک و برنامه‌های عامل اثبات مفهوم که نیاز به محدودیت‌های شدید منابع دارند، فراهم می‌کنند.

### معیارهای عملکرد عامل SLM

**سرعت استنتاج عامل**: Q4_K سریع‌ترین زمان پاسخ عامل را بر روی CPUهای موبایل ارائه می‌دهد، Q5_K نسبت سرعت-کیفیت متعادلی را برای برنامه‌های عامل عمومی فراهم می‌کند، Q8_0 کیفیت برتری را برای وظایف پیچیده عامل ارائه می‌دهد و فرمت‌های آزمایشی حداکثر توان را برای سخت‌افزارهای تخصصی عامل فراهم می‌کنند.

**نیازهای حافظه عامل**: سطوح کوانتیزاسیون برای عوامل از Q2_K (کمتر از 500MB برای مدل‌های عامل کوچک) تا Q8_0 (تقریباً 50% اندازه اصلی) متغیر است، با پیکربندی‌های آزمایشی که حداکثر فشرده‌سازی را برای محیط‌های عامل با منابع محدود فراهم می‌کنند.

## چالش‌ها و ملاحظات برای عوامل SLM

### مصالحه‌های عملکرد در سیستم‌های عامل

استقرار عامل SLM شامل بررسی دقیق مصالحه‌ها بین اندازه مدل، سرعت پاسخ عامل و کیفیت خروجی است. در حالی که Q4_K سرعت و کارایی استثنایی را برای عوامل موبایل ارائه می‌دهد، Q8_0 کیفیت برتری را برای وظایف پیچیده عامل فراهم می‌کند. Q5_K تعادل مناسبی را برای اکثر برنامه‌های عامل عمومی ایجاد می‌کند.

### سازگاری سخت‌افزاری برای عوامل SLM

دستگاه‌های لبه مختلف قابلیت‌های متفاوتی برای استقرار عامل SLM دارند. Q4_K بر روی پردازنده‌های ساده برای عوامل ساده به خوبی اجرا می‌شود، Q5_K منابع محاسباتی متوسطی را برای عملکرد متعادل عامل نیاز دارد و Q8_0 از سخت‌افزارهای پیشرفته‌تر برای قابلیت‌های پیشرفته عامل بهره می‌برد.

### امنیت و حریم خصوصی در سیستم‌های عامل SLM

در حالی که عوامل SLM پردازش محلی را برای افزایش حریم خصوصی امکان‌پذیر می‌سازند، باید اقدامات امنیتی مناسب برای حفاظت از مدل‌های عامل و داده‌ها در محیط‌های لبه اجرا شود. این امر به ویژه هنگام استقرار فرمت‌های عامل با دقت بالا در محیط‌های سازمانی یا فرمت‌های عامل فشرده در برنامه‌هایی که با داده‌های حساس سروکار دارند، اهمیت دارد.

## روندهای آینده در توسعه عوامل SLM

چشم‌انداز عوامل SLM با پیشرفت در تکنیک‌های فشرده‌سازی، روش‌های بهینه‌سازی و استراتژی‌های استقرار لبه همچنان در حال تحول است. توسعه‌های آینده شامل الگوریتم‌های کوانتیزاسیون کارآمدتر برای مدل‌های عامل، روش‌های فشرده‌سازی بهبود یافته برای جریان‌های کاری عامل و ادغام بهتر با شتاب‌دهنده‌های سخت‌افزاری لبه برای پردازش عامل خواهد بود.

**پیش‌بینی‌های بازار برای عوامل SLM**: بر اساس تحقیقات اخیر، اتوماسیون مبتنی بر عامل می‌تواند 40–60% از وظایف شناختی تکراری را در جریان‌های کاری سازمانی تا سال 2027 حذف کند، با SLM‌ها که این تحول را به دلیل کارایی هزینه و انعطاف‌پذیری استقرار هدایت می‌کنند.

**روندهای فناوری در عوامل SLM**:
- **عوامل SLM تخصصی**: مدل‌های خاص دامنه که برای وظایف و صنایع خاص عامل آموزش دیده‌اند
- **محاسبات عامل لبه**: قابلیت‌های عامل بهبود یافته در دستگاه با حریم خصوصی بهتر و کاهش تأخیر
- **هماهنگی عامل**: هماهنگی بهتر بین عوامل SLM متعدد با مسیریابی پویا و تعادل بار
- **دموکراتیزه‌سازی**: انعطاف‌پذیری SLM امکان مشارکت گسترده‌تر در توسعه عامل را در سراسر سازمان‌ها فراهم می‌کند

## شروع کار با عوامل SLM

### مرحله 1: راه‌اندازی محیط چارچوب عامل Microsoft

**نصب وابستگی‌ها**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**راه‌اندازی Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### مرحله 2: انتخاب SLM برای برنامه‌های عامل
گزینه‌های محبوب برای چارچوب عامل Microsoft:
- **Microsoft Phi-4 Mini (3.8B)**: عالی برای وظایف عمومی عامل با عملکرد متعادل
- **Qwen2.5-0.5B (0.5B)**: فوق‌العاده کارآمد برای عوامل مسیریابی و طبقه‌بندی ساده
- **Qwen2.5-Coder-0.5B (0.5B)**: تخصصی برای وظایف عامل مرتبط با کدنویسی
- **Phi-4 (7B)**: استدلال پیشرفته برای سناریوهای پیچیده لبه زمانی که منابع اجازه می‌دهند

### مرحله 3: ایجاد اولین عامل خود با چارچوب عامل Microsoft

**راه‌اندازی اولیه عامل**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### مرحله 4: تعریف دامنه و نیازهای عامل
با برنامه‌های عامل متمرکز و به خوبی تعریف شده با استفاده از چارچوب عامل Microsoft شروع کنید:
- **عوامل دامنه واحد**: خدمات مشتری یا زمان‌بندی یا تحقیق
- **اهداف واضح عامل**: اهداف خاص و قابل اندازه‌گیری برای عملکرد عامل
- **ادغام ابزار محدود**: حداکثر 3-5 ابزار برای استقرار اولیه عامل
- **مرزهای تعریف شده عامل**: مسیرهای تصاعدی واضح برای سناریوهای پیچیده
- **طراحی اول لبه**: اولویت‌بندی عملکرد آفلاین و پردازش محلی

### مرحله 5: اجرای استقرار لبه با چارچوب عامل Microsoft

**پیکربندی منابع**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**استقرار اقدامات ایمنی برای عوامل لبه**:
- **اعتبارسنجی ورودی محلی**: بررسی درخواست‌ها بدون وابستگی به ابر
- **فیلتر کردن خروجی آفلاین**: اطمینان از اینکه پاسخ‌ها استانداردهای کیفیت را به صورت محلی برآورده می‌کنند
- **کنترل‌های امنیتی لبه**: اجرای امنیت بدون نیاز به اتصال اینترنت
- **نظارت محلی**: ردیابی عملکرد و علامت‌گذاری مشکلات با استفاده از تله‌متری لبه

### مرحله 6: اندازه‌گیری و بهینه‌سازی عملکرد عامل لبه
- **نرخ تکمیل وظایف عامل**: نظارت بر نرخ موفقیت در سناریوهای آفلاین
- **زمان‌های پاسخ عامل**: اطمینان از زمان‌های پاسخ زیر یک ثانیه برای استقرار لبه
- **استفاده از منابع**: ردیابی حافظه، CPU و مصرف باتری در دستگاه‌های لبه
- **کارایی هزینه**: مقایسه هزینه‌های استقرار لبه با جایگزین‌های مبتنی بر ابر
- **قابلیت اطمینان آفلاین**: اندازه‌گیری عملکرد عامل در طول قطعی‌های شبکه

## نکات کلیدی برای اجرای عامل SLM

1. **SLM‌ها برای عوامل کافی هستند**: برای اکثر وظایف عامل، مدل‌های کوچک به اندازه مدل‌های بزرگ عمل می‌کنند و در عین حال مزایای قابل توجهی ارائه می‌دهند
2. **کارایی هزینه در عوامل**: اجرای عوامل SLM 10-30 برابر ارزان‌تر است، که آنها را از نظر اقتصادی برای استقرار گسترده قابل اجرا می‌کند
3. **تخصص در عوامل کار می‌کند**: SLM‌های تنظیم‌شده اغلب در برنامه‌های خاص عامل از LLM‌های عمومی بهتر عمل می‌کنند
4. **معماری عامل ترکیبی**: استفاده از SLM‌ها برای وظایف معمول عامل، LLM‌ها برای استدلال پیچیده در صورت لزوم
5. **چارچوب عامل Microsoft امکان استقرار تولید را فراهم می‌کند**: ابزارهای درجه سازمانی برای ساخت، استقرار و مدیریت عوامل لبه ارائه می‌دهد
6. **اصول طراحی اول لبه**: عوامل آفلاین با پردازش محلی حریم خصوصی و قابلیت اطمینان را تضمین می‌کنند
7. **ادغام Foundry Local**: اتصال بدون درز بین چارچوب عامل Microsoft و استنتاج مدل محلی
8. **آینده عوامل SLM است**: مدل‌های زبان کوچک با چارچوب‌های تولید آینده هوش مصنوعی عامل هستند، که استقرار عامل را دموکراتیزه و کارآمد می‌کنند

## منابع و مطالعات بیشتر

### مقالات تحقیقاتی اصلی و انتشارات

#### عوامل هوش مصنوعی و سیستم‌های عامل
- **"عوامل زبان به عنوان گراف‌های بهینه‌پذیر"** (2024) - تحقیق بنیادی در مورد معماری عامل و استراتژی‌های بهینه‌سازی
  - نویسندگان: Wenyue Hua، Lishan Yang و دیگران
  - لینک: https://arxiv.org/abs/2402.16823
  - نکات کلیدی: طراحی عامل مبتنی بر گراف و استراتژی‌های بهینه‌سازی

- **"ظهور و پتانسیل عوامل مبتنی بر مدل‌های زبان بزرگ"** (2023)
  - نویسندگان: Zhiheng Xi، Wenxiang Chen و دیگران
  - لینک: https://arxiv.org/abs/2309.07864
  - نکات کلیدی: بررسی جامع قابلیت‌ها و کاربردهای عوامل مبتنی بر LLM

- **"معماری‌های شناختی برای عوامل زبان"** (2024)
  - نویسندگان: Theodore Sumers، Shunyu Yao و دیگران
  - لینک: https://arxiv.org/abs/2309.02427
  - نکات کلیدی: چارچوب‌های شناختی برای طراحی عوامل هوشمند

#### مدل‌های زبان کوچک و بهینه‌سازی
- **"گزارش فنی Phi-3: یک مدل زبان بسیار توانمند به صورت محلی بر روی گوشی شما"** (2024)
  - نویسندگان: تیم تحقیقاتی Microsoft
  - لینک: https://arxiv.org/abs/2404.14219
  - نکات کلیدی: اصول طراحی SLM و استراتژی‌های استقرار موبایل

- **"گزارش فنی Qwen2.5"** (2024)
  - نویسندگان: تیم Alibaba Cloud
  - لینک: https://arxiv.org/abs/2407.10671
  - نکات کلیدی: تکنیک‌های پیشرفته آموزش SLM و بهینه‌سازی عملکرد

- **"TinyLlama: یک مدل زبان کوچک متن‌باز"** (2024)
  - نویسندگان: Peiyuan Zhang، Guangtao Zeng و دیگران
  - لینک: https://arxiv.org/abs/2401.02385
  - نکات کلیدی: طراحی مدل فوق‌العاده فشرده و کارایی آموزش

### مستندات رسمی و چارچوب‌ها

#### چارچوب عامل Microsoft
- **مستندات رسمی**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **مخزن GitHub**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **مخزن اصلی**: https://github.com/microsoft/foundry-local
- **مستندات**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **مخزن اصلی**: https://github.com/vllm-project/vllm
- **مستندات**: https://docs.vllm.ai/


#### Ollama
- **وب‌سایت رسمی**: https://ollama.ai/
- **مخزن GitHub**: https://github.com/ollama/ollama

### چارچوب‌های بهینه‌سازی مدل

#### Llama.cpp
- **مخزن**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **مستندات**: https://microsoft.github.io/Olive/
- **مخزن GitHub**: https://github.com/microsoft/Olive

#### OpenVINO
- **سایت رسمی**: https://docs.openvino.ai/

#### Apple MLX
- **مخزن**: https://github.com/ml-explore/mlx

### گزارش‌های صنعتی و تحلیل بازار

#### تحقیقات بازار عوامل هوش مصنوعی
- **"وضعیت عوامل هوش مصنوعی 2025"** - موسسه جهانی McKinsey
  - لینک: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - نکات کلیدی: روندهای بازار و الگوهای پذیرش سازمانی

#### معیارهای فنی

- **"معیارهای استنتاج هوش مصنوعی لبه"** - MLPerf
  - لینک: https://mlcommons.org/en/inference-edge/
  - نکات کلیدی: معیارهای عملکرد استاندارد برای استقرار لبه

### استانداردها و مشخصات

#### فرمت‌ها و استانداردهای مدل
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - فرمت مدل چند پلتفرمی برای قابلیت همکاری
- **مشخصات GGUF**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - فرمت مدل کوانتیزه برای استنتاج CPU
- **مشخصات API OpenAI**: https://platform.openai.com/docs/api-reference
  - فرمت استاندارد API برای ادغام مدل زبان

#### امنیت و انطباق
- **چارچوب مدیریت ریسک هوش مصنوعی NIST**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - سیستم‌های هوش مصنوعی**: چارچوبی برای سیستم‌های هوش مصنوعی و ایمنی
- **استانداردهای IEEE برای هوش مصنوعی**: https://standards.ieee.org/industry-connections/ai/

تغییر به سمت عوامل مبتنی بر SLM نمایانگر تغییر بنیادی در نحوه برخورد ما با استقرار هوش مصنوعی است. چارچوب عامل Microsoft، همراه با پلتفرم‌های محلی و مدل‌های زبان کوچک کارآمد، راه‌حل کاملی برای ساخت عوامل آماده تولید فراهم می‌کند که به طور مؤثر در محیط‌های لبه عمل می‌کنند. با تمرکز بر کارایی، تخصص و کاربرد عملی، این مجموعه فناوری عوامل هوش مصنوعی را برای برنامه‌های واقعی در هر صنعت و محیط محاسبات لبه قابل دسترس‌تر، مقرون‌به‌صرفه‌تر و مؤثرتر می‌کند.

همان‌طور که به سال 2025 پیش می‌رویم، ترکیب مدل‌های کوچک‌تر و توانمندتر، چارچوب‌های عامل پیچیده مانند چارچوب عامل Microsoft و پلتفرم‌های استقرار لبه قوی، امکانات جدیدی را برای سیستم‌های خودمختار باز خواهد کرد که می‌توانند به طور مؤثر بر روی دستگاه‌های لبه عمل کنند و در عین حال حریم خصوصی را حفظ کنند، هزینه‌ها را کاهش دهند و تجربه‌های کاربری استثنایی ارائه دهند.

**گام‌های بعدی برای اجرا**:
1. **بررسی فراخوانی عملکرد**: یاد بگیرید که چگونه SLM‌ها ادغام ابزار و خروجی‌های ساختاریافته را مدیریت می‌کنند
2. **تسلط بر پروتکل زمینه مدل (MCP)**: الگوهای ارتباطی پیشرفته عامل را درک کنید
3. **ساخت عوامل تولیدی**: از چارچوب عامل Microsoft برای استقرارهای درجه سازمانی استفاده کنید
4. **بهینه‌سازی برای لبه**: تکنیک‌های بهینه‌سازی پیشرفته را برای محیط‌های با منابع محدود اعمال کنید


## ➡️ مرحله بعدی

- [02: فراخوانی عملکرد در مدل‌های زبان کوچک (SLMs)](./02.FunctionCalling.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.