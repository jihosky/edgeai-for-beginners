<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T09:10:55+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "fa"
}
-->
# بخش ۱: اصول EdgeAI

EdgeAI یک تغییر اساسی در نحوه استفاده از هوش مصنوعی ایجاد کرده است، به طوری که قابلیت‌های هوش مصنوعی را مستقیماً به دستگاه‌های لبه منتقل می‌کند و دیگر تنها به پردازش مبتنی بر ابر متکی نیست. مهم است که بدانیم چگونه EdgeAI پردازش محلی هوش مصنوعی را بر روی دستگاه‌های محدود از نظر منابع امکان‌پذیر می‌کند، در حالی که عملکرد قابل قبولی را حفظ کرده و چالش‌هایی مانند حریم خصوصی، تأخیر و قابلیت‌های آفلاین را برطرف می‌کند.

## مقدمه

در این درس، به بررسی EdgeAI و مفاهیم اساسی آن خواهیم پرداخت. ما به پارادایم سنتی محاسبات هوش مصنوعی، چالش‌های محاسبات لبه، فناوری‌های کلیدی که EdgeAI را ممکن می‌سازند و کاربردهای عملی در صنایع مختلف خواهیم پرداخت.

## اهداف یادگیری

در پایان این درس، شما قادر خواهید بود:

- تفاوت بین رویکردهای سنتی هوش مصنوعی مبتنی بر ابر و EdgeAI را درک کنید.
- فناوری‌های کلیدی که پردازش هوش مصنوعی را بر روی دستگاه‌های لبه امکان‌پذیر می‌کنند شناسایی کنید.
- مزایا و محدودیت‌های پیاده‌سازی‌های EdgeAI را بشناسید.
- دانش خود از EdgeAI را در سناریوها و موارد استفاده واقعی به کار ببرید.

## درک پارادایم سنتی محاسبات هوش مصنوعی

به طور سنتی، برنامه‌های هوش مصنوعی مولد برای اجرای مدل‌های زبان بزرگ (LLMs) به زیرساخت‌های محاسباتی با عملکرد بالا متکی هستند. سازمان‌ها معمولاً این مدل‌ها را در خوشه‌های GPU در محیط‌های ابری مستقر کرده و از طریق رابط‌های API به قابلیت‌های آن‌ها دسترسی پیدا می‌کنند.

این مدل متمرکز برای بسیاری از برنامه‌ها به خوبی کار می‌کند، اما محدودیت‌های ذاتی در سناریوهای محاسبات لبه دارد. رویکرد معمول شامل ارسال پرسش‌های کاربر به سرورهای راه دور، پردازش آن‌ها با استفاده از سخت‌افزار قدرتمند و بازگرداندن نتایج از طریق اینترنت است. در حالی که این روش دسترسی به مدل‌های پیشرفته را فراهم می‌کند، وابستگی به اتصال اینترنت ایجاد می‌کند، نگرانی‌های تأخیر را معرفی می‌کند و مسائل مربوط به حریم خصوصی را در زمانی که داده‌های حساس باید به سرورهای خارجی منتقل شوند، مطرح می‌کند.

برخی مفاهیم اصلی که باید هنگام کار با پارادایم‌های سنتی محاسبات هوش مصنوعی درک کنیم عبارتند از:

- **☁️ پردازش مبتنی بر ابر**: مدل‌های هوش مصنوعی بر روی زیرساخت‌های سرور قدرتمند با منابع محاسباتی بالا اجرا می‌شوند.
- **🔌 دسترسی مبتنی بر API**: برنامه‌ها از طریق تماس‌های API راه دور به قابلیت‌های هوش مصنوعی دسترسی پیدا می‌کنند، نه پردازش محلی.
- **🎛️ مدیریت متمرکز مدل**: مدل‌ها به صورت متمرکز نگهداری و به‌روزرسانی می‌شوند، که سازگاری را تضمین می‌کند اما نیاز به اتصال شبکه دارد.
- **📈 مقیاس‌پذیری منابع**: زیرساخت ابری می‌تواند به صورت پویا برای مدیریت نیازهای محاسباتی متغیر مقیاس‌بندی شود.

## چالش محاسبات لبه

دستگاه‌های لبه مانند لپ‌تاپ‌ها، تلفن‌های همراه و دستگاه‌های اینترنت اشیا (IoT) مانند Raspberry Pi و NVIDIA Orin Nano محدودیت‌های محاسباتی منحصر به فردی دارند. این دستگاه‌ها معمولاً قدرت پردازش، حافظه و منابع انرژی کمتری نسبت به زیرساخت‌های مراکز داده دارند.

اجرای مدل‌های LLM سنتی بر روی چنین دستگاه‌هایی به دلیل این محدودیت‌های سخت‌افزاری به طور تاریخی چالش‌برانگیز بوده است. با این حال، نیاز به پردازش هوش مصنوعی لبه در سناریوهای مختلف به طور فزاینده‌ای مهم شده است. به عنوان مثال، در شرایطی که اتصال اینترنت غیرقابل اعتماد یا در دسترس نیست، مانند سایت‌های صنعتی دورافتاده، وسایل نقلیه در حال حرکت، یا مناطقی با پوشش شبکه ضعیف. علاوه بر این، برنامه‌هایی که نیاز به استانداردهای امنیتی بالا دارند، مانند دستگاه‌های پزشکی، سیستم‌های مالی یا برنامه‌های دولتی، ممکن است نیاز به پردازش داده‌های حساس به صورت محلی داشته باشند تا حریم خصوصی و الزامات انطباق را حفظ کنند.

### محدودیت‌های کلیدی محاسبات لبه

محیط‌های محاسبات لبه با چندین محدودیت اساسی مواجه هستند که راه‌حل‌های سنتی هوش مصنوعی مبتنی بر ابر با آن‌ها روبرو نمی‌شوند:

- **قدرت پردازش محدود**: دستگاه‌های لبه معمولاً هسته‌های CPU کمتری و سرعت کلاک پایین‌تری نسبت به سخت‌افزارهای درجه سرور دارند.
- **محدودیت‌های حافظه**: ظرفیت RAM و ذخیره‌سازی موجود در دستگاه‌های لبه به طور قابل توجهی کاهش یافته است.
- **محدودیت‌های انرژی**: دستگاه‌های باتری‌دار باید عملکرد را با مصرف انرژی برای عملیات طولانی‌مدت متعادل کنند.
- **مدیریت حرارتی**: فرم‌فاکتورهای فشرده قابلیت‌های خنک‌کننده را محدود می‌کنند و بر عملکرد پایدار تحت بار تأثیر می‌گذارند.

## EdgeAI چیست؟

### مفهوم: تعریف Edge AI

Edge AI به استقرار و اجرای الگوریتم‌های هوش مصنوعی مستقیماً بر روی دستگاه‌های لبه اشاره دارد—سخت‌افزار فیزیکی که در "لبه" شبکه، نزدیک به جایی که داده تولید و جمع‌آوری می‌شود، قرار دارد. این دستگاه‌ها شامل تلفن‌های هوشمند، حسگرهای اینترنت اشیا، دوربین‌های هوشمند، وسایل نقلیه خودران، پوشیدنی‌ها و تجهیزات صنعتی هستند. برخلاف سیستم‌های هوش مصنوعی سنتی که برای پردازش به سرورهای ابری متکی هستند، Edge AI هوش را مستقیماً به منبع داده می‌آورد.

در اصل، Edge AI به معنای غیرمتمرکز کردن پردازش هوش مصنوعی است، انتقال آن از مراکز داده متمرکز و توزیع آن در شبکه گسترده‌ای از دستگاه‌هایی که اکوسیستم دیجیتال ما را تشکیل می‌دهند. این یک تغییر اساسی در نحوه طراحی و استقرار سیستم‌های هوش مصنوعی است.

ستون‌های مفهومی کلیدی Edge AI شامل موارد زیر است:

- **پردازش نزدیک**: محاسبات به صورت فیزیکی نزدیک به جایی که داده‌ها منشأ می‌گیرند انجام می‌شود.
- **هوش غیرمتمرکز**: قابلیت‌های تصمیم‌گیری در میان دستگاه‌های متعدد توزیع می‌شود.
- **حاکمیت داده‌ها**: اطلاعات تحت کنترل محلی باقی می‌مانند و اغلب هرگز دستگاه را ترک نمی‌کنند.
- **عملیات خودمختار**: دستگاه‌ها می‌توانند بدون نیاز به اتصال دائمی به صورت هوشمند عمل کنند.
- **هوش جاسازی‌شده**: هوش به یک قابلیت ذاتی دستگاه‌های روزمره تبدیل می‌شود.

### تصویری از معماری Edge AI

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI یک تغییر اساسی در نحوه استفاده از هوش مصنوعی ایجاد کرده است، به طوری که قابلیت‌های هوش مصنوعی را مستقیماً به دستگاه‌های لبه منتقل می‌کند و دیگر تنها به پردازش مبتنی بر ابر متکی نیست. این رویکرد امکان اجرای مدل‌های هوش مصنوعی به صورت محلی بر روی دستگاه‌هایی با منابع محاسباتی محدود را فراهم می‌کند و قابلیت‌های استنتاج بلادرنگ را بدون نیاز به اتصال دائمی به اینترنت ارائه می‌دهد.

EdgeAI شامل فناوری‌ها و تکنیک‌های مختلفی است که برای کارآمدتر کردن مدل‌های هوش مصنوعی و مناسب‌تر کردن آن‌ها برای استقرار بر روی دستگاه‌های محدود از نظر منابع طراحی شده‌اند. هدف این است که عملکرد قابل قبولی را حفظ کرده و در عین حال نیازهای محاسباتی و حافظه مدل‌های هوش مصنوعی را به طور قابل توجهی کاهش دهد.

بیایید به رویکردهای اساسی که امکان پیاده‌سازی EdgeAI را در انواع مختلف دستگاه‌ها و موارد استفاده فراهم می‌کنند، نگاهی بیندازیم.

### اصول اصلی EdgeAI

EdgeAI بر اساس چندین اصل بنیادی ساخته شده است که آن را از هوش مصنوعی سنتی مبتنی بر ابر متمایز می‌کند:

- **پردازش محلی**: استنتاج هوش مصنوعی مستقیماً بر روی دستگاه لبه انجام می‌شود و نیازی به اتصال خارجی ندارد.
- **بهینه‌سازی منابع**: مدل‌ها به طور خاص برای محدودیت‌های سخت‌افزاری دستگاه‌های هدف بهینه‌سازی می‌شوند.
- **عملکرد بلادرنگ**: پردازش با حداقل تأخیر برای برنامه‌های حساس به زمان انجام می‌شود.
- **حریم خصوصی در طراحی**: داده‌های حساس بر روی دستگاه باقی می‌مانند و امنیت و انطباق را افزایش می‌دهند.

## فناوری‌های کلیدی که EdgeAI را ممکن می‌سازند

### کوانتیزه کردن مدل

یکی از مهم‌ترین تکنیک‌ها در EdgeAI کوانتیزه کردن مدل است. این فرآیند شامل کاهش دقت پارامترهای مدل، معمولاً از اعداد اعشاری ۳۲ بیتی به اعداد صحیح ۸ بیتی یا حتی فرمت‌های دقت پایین‌تر است. در حالی که این کاهش دقت ممکن است نگران‌کننده به نظر برسد، تحقیقات نشان داده‌اند که بسیاری از مدل‌های هوش مصنوعی می‌توانند عملکرد خود را حتی با دقت به طور قابل توجهی کاهش‌یافته حفظ کنند.

کوانتیزه کردن با نگاشت محدوده مقادیر اعشاری به مجموعه‌ای کوچکتر از مقادیر گسسته کار می‌کند. به عنوان مثال، به جای استفاده از ۳۲ بیت برای نمایش هر پارامتر، کوانتیزه کردن ممکن است فقط از ۸ بیت استفاده کند، که منجر به کاهش ۴ برابری در نیازهای حافظه شده و اغلب زمان‌های استنتاج سریع‌تر را به همراه دارد.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

تکنیک‌های مختلف کوانتیزه کردن شامل موارد زیر هستند:

- **کوانتیزه کردن پس از آموزش (PTQ)**: پس از آموزش مدل اعمال می‌شود و نیازی به آموزش مجدد ندارد.
- **آموزش آگاه به کوانتیزه کردن (QAT)**: اثرات کوانتیزه کردن را در طول آموزش برای دقت بهتر در نظر می‌گیرد.
- **کوانتیزه کردن پویا**: وزن‌ها را به int8 کوانتیزه می‌کند اما فعال‌سازی‌ها را به صورت پویا محاسبه می‌کند.
- **کوانتیزه کردن ایستا**: تمام پارامترهای کوانتیزه کردن را برای وزن‌ها و فعال‌سازی‌ها از پیش محاسبه می‌کند.

برای استقرارهای EdgeAI، انتخاب استراتژی کوانتیزه کردن مناسب به معماری خاص مدل، نیازهای عملکرد و قابلیت‌های سخت‌افزاری دستگاه هدف بستگی دارد.

### فشرده‌سازی و بهینه‌سازی مدل

فراتر از کوانتیزه کردن، تکنیک‌های مختلف فشرده‌سازی به کاهش اندازه مدل و نیازهای محاسباتی کمک می‌کنند. این تکنیک‌ها شامل موارد زیر هستند:

**هرس کردن**: این تکنیک اتصالات یا نورون‌های غیرضروری را از شبکه‌های عصبی حذف می‌کند. با شناسایی و حذف پارامترهایی که سهم کمی در عملکرد مدل دارند، هرس کردن می‌تواند اندازه مدل را به طور قابل توجهی کاهش دهد و در عین حال دقت را حفظ کند.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**استخراج دانش**: این رویکرد شامل آموزش یک مدل "دانش‌آموز" کوچکتر برای تقلید رفتار یک مدل "معلم" بزرگتر است. مدل دانش‌آموز یاد می‌گیرد خروجی‌های معلم را تقریب بزند و اغلب عملکرد مشابهی را با پارامترهای به طور قابل توجهی کمتر به دست می‌آورد.

**بهینه‌سازی معماری مدل**: محققان معماری‌های تخصصی طراحی کرده‌اند که به طور خاص برای استقرار لبه طراحی شده‌اند، مانند MobileNets، EfficientNets و سایر معماری‌های سبک که عملکرد را با کارایی محاسباتی متعادل می‌کنند.

### مدل‌های زبان کوچک (SLMs)

یک روند نوظهور در EdgeAI توسعه مدل‌های زبان کوچک (SLMs) است. این مدل‌ها از ابتدا برای کوچک و کارآمد بودن طراحی شده‌اند و در عین حال قابلیت‌های معنادار زبان طبیعی را ارائه می‌دهند. SLMها این هدف را از طریق انتخاب‌های دقیق معماری، تکنیک‌های آموزش کارآمد و آموزش متمرکز بر دامنه‌ها یا وظایف خاص به دست می‌آورند.

برخلاف رویکردهای سنتی که شامل فشرده‌سازی مدل‌های بزرگ هستند، SLMها اغلب با مجموعه داده‌های کوچکتر و معماری‌های بهینه‌سازی شده که به طور خاص برای استقرار لبه طراحی شده‌اند آموزش داده می‌شوند. این رویکرد می‌تواند منجر به مدل‌هایی شود که نه تنها کوچکتر هستند بلکه برای موارد استفاده خاص نیز کارآمدتر هستند.

## شتاب‌دهی سخت‌افزاری برای EdgeAI

دستگاه‌های لبه مدرن به طور فزاینده‌ای شامل سخت‌افزار تخصصی طراحی شده برای شتاب‌دهی بارهای کاری هوش مصنوعی هستند:

### واحدهای پردازش عصبی (NPUs)

NPUs پردازنده‌های تخصصی هستند که به طور خاص برای محاسبات شبکه عصبی طراحی شده‌اند. این تراشه‌ها می‌توانند وظایف استنتاج هوش مصنوعی را بسیار کارآمدتر از CPUهای سنتی انجام دهند، اغلب با مصرف انرژی کمتر. بسیاری از تلفن‌های هوشمند، لپ‌تاپ‌ها و دستگاه‌های اینترنت اشیا مدرن اکنون شامل NPUs هستند تا پردازش هوش مصنوعی روی دستگاه را امکان‌پذیر کنند.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

دستگاه‌هایی با NPUs شامل موارد زیر هستند:

- **Apple**: تراشه‌های سری A و M با Neural Engine
- **Qualcomm**: پردازنده‌های Snapdragon با Hexagon DSP/NPU
- **Samsung**: پردازنده‌های Exynos با NPU
- **Intel**: VPUs Movidius و شتاب‌دهنده‌های Habana Labs
- **Microsoft**: رایانه‌های Windows Copilot+ با NPUs

### 🎮 شتاب‌دهی GPU

در حالی که دستگاه‌های لبه ممکن است GPUهای قدرتمند موجود در مراکز داده را نداشته باشند، بسیاری هنوز شامل GPUهای یکپارچه یا مجزا هستند که می‌توانند بارهای کاری هوش مصنوعی را شتاب دهند. GPUهای موبایل مدرن و پردازنده‌های گرافیکی یکپارچه می‌توانند بهبودهای عملکرد قابل توجهی برای وظایف استنتاج هوش مصنوعی ارائه دهند.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### بهینه‌سازی CPU

حتی دستگاه‌های فقط CPU می‌توانند از EdgeAI از طریق پیاده‌سازی‌های بهینه‌شده بهره‌مند شوند. CPUهای مدرن شامل دستورالعمل‌های تخصصی برای بارهای کاری هوش مصنوعی هستند و چارچوب‌های نرم‌افزاری برای به حداکثر رساندن عملکرد CPU برای استنتاج هوش مصنوعی توسعه یافته‌اند.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

برای مهندسان نرم‌افزار که با EdgeAI کار می‌کنند، درک نحوه استفاده از این گزینه‌های شتاب‌دهی سخت‌افزاری برای بهینه‌سازی عملکرد استنتاج و بهره‌وری انرژی بر روی دستگاه‌های هدف بسیار مهم است.

## مزایای EdgeAI

### حریم خصوصی و امنیت

یکی از مهم‌ترین مزایای EdgeAI افزایش حریم خصوصی و امنیت است. با پردازش داده‌ها به صورت محلی بر روی دستگاه، اطلاعات حساس هرگز از کنترل کاربر خارج نمی‌شود. این امر به ویژه برای برنامه‌هایی که با داده‌های شخصی، اطلاعات پزشکی یا داده‌های محرمانه تجاری سروکار دارند، مهم است.

### کاهش تأخیر

EdgeAI نیاز به ارسال داده‌ها به سرورهای راه دور برای پردازش را حذف می‌کند و تأخیر را به طور قابل توجهی کاهش می‌دهد. این امر برای برنامه‌های بلادرنگ مانند وسایل نقلیه خودران، اتوماسیون صنعتی یا برنامه‌های تعاملی که نیاز به پاسخ‌های فوری دارند، حیاتی است.

### قابلیت آفلاین

EdgeAI امکان عملکرد هوش مصنوعی را حتی زمانی که اتصال اینترنت در دسترس نیست فراهم می‌کند. این ویژگی برای برنامه‌ها در مکان‌های دورافتاده، هنگام سفر یا در شرایطی که قابلیت اطمینان شبکه مورد توجه است، ارزشمند است.

### صرفه‌جویی در هزینه

با کاهش وابستگی به خدمات هوش مصنوعی مبتنی بر ابر، EdgeAI می‌تواند به کاهش هزینه‌های عملیاتی کمک کند، به ویژه برای برنامه‌هایی با حجم استفاده بالا. سازمان‌ها می‌توانند از هزینه‌های مداوم API اجتناب کرده و نیازهای پهنای باند را کاهش دهند.

### مقیاس‌پذیری

EdgeAI بار محاسباتی را در میان دستگاه‌های لبه توزیع می‌کند، نه اینکه آن را در مراکز داده متمرکز کند. این امر می‌تواند به کاهش هزینه‌های زیرساخت و بهبود مقیاس‌پذیری کلی سیستم کمک کند.

## کاربردهای EdgeAI

### دستگاه‌های هوشمند و اینترنت اشیا

EdgeAI بسیاری از ویژگی‌های دستگاه‌های هوشمند را تأمین می‌کند، از دستیارهای صوتی که می‌توانند دستورات را به صورت محلی پردازش کنند تا دوربین‌های هوشمندی که می‌توانند اشیاء و افراد را بدون ارسال ویدئو به ابر شناسایی کنند. دستگاه‌های اینترنت اشیا از EdgeAI برای نگهداری پیش‌بینی‌کننده، نظارت بر محیط و تصمیم‌گیری خودکار استفاده می‌کنند.

### برنامه‌های موبایل

تلفن‌های هوشمند و تبلت‌ها از EdgeAI برای ویژگی‌های مختلفی استفاده می‌کنند، از جمله بهبود عکس، ترجمه بلادرنگ، واقعیت افزوده و توصیه‌های شخصی‌سازی شده. این برنامه‌ها از مزایای تأخیر کم و حریم خصوصی پردازش محلی بهره‌مند می‌شوند.

### کاربردهای صنعتی

محیط‌های تولید و صنعتی از EdgeAI برای کنترل کیفیت، نگهداری پیش‌بینی‌کننده و بهینه‌سازی فرآیند
- [02: کاربردهای EdgeAI](02.RealWorldCaseStudies.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.