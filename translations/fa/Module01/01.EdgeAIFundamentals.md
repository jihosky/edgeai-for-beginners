<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:32:37+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "fa"
}
-->
# بخش ۱: اصول اولیه EdgeAI

EdgeAI یک تغییر اساسی در نحوه استفاده از هوش مصنوعی ایجاد کرده است، به طوری که قابلیت‌های هوش مصنوعی را مستقیماً به دستگاه‌های لبه منتقل می‌کند و دیگر تنها به پردازش مبتنی بر ابر متکی نیست. مهم است که درک کنیم چگونه EdgeAI پردازش محلی هوش مصنوعی را بر روی دستگاه‌های با منابع محدود امکان‌پذیر می‌کند، در حالی که عملکرد قابل قبولی را حفظ کرده و چالش‌هایی مانند حریم خصوصی، تأخیر و قابلیت‌های آفلاین را برطرف می‌کند.

## مقدمه

در این درس، ما به بررسی EdgeAI و مفاهیم اساسی آن خواهیم پرداخت. ما به پارادایم سنتی محاسبات هوش مصنوعی، چالش‌های محاسبات لبه، فناوری‌های کلیدی که امکان‌پذیری EdgeAI را فراهم می‌کنند و کاربردهای عملی در صنایع مختلف خواهیم پرداخت.

## اهداف یادگیری

در پایان این درس، شما قادر خواهید بود:

- تفاوت بین رویکردهای سنتی هوش مصنوعی مبتنی بر ابر و EdgeAI را درک کنید.
- فناوری‌های کلیدی که پردازش هوش مصنوعی را بر روی دستگاه‌های لبه امکان‌پذیر می‌کنند شناسایی کنید.
- مزایا و محدودیت‌های پیاده‌سازی‌های EdgeAI را بشناسید.
- دانش خود در مورد EdgeAI را در سناریوها و موارد استفاده واقعی به کار ببرید.

## درک پارادایم سنتی محاسبات هوش مصنوعی

به طور سنتی، برنامه‌های هوش مصنوعی مولد برای اجرای مدل‌های زبان بزرگ (LLMs) به زیرساخت‌های محاسباتی با عملکرد بالا متکی هستند. سازمان‌ها معمولاً این مدل‌ها را در خوشه‌های GPU در محیط‌های ابری مستقر کرده و از طریق رابط‌های API به قابلیت‌های آن‌ها دسترسی پیدا می‌کنند.

این مدل متمرکز برای بسیاری از برنامه‌ها به خوبی کار می‌کند، اما محدودیت‌های ذاتی در سناریوهای محاسبات لبه دارد. رویکرد معمول شامل ارسال پرسش‌های کاربران به سرورهای راه دور، پردازش آن‌ها با استفاده از سخت‌افزار قدرتمند و بازگرداندن نتایج از طریق اینترنت است. در حالی که این روش دسترسی به مدل‌های پیشرفته را فراهم می‌کند، وابستگی به اتصال اینترنت ایجاد می‌کند، نگرانی‌هایی در مورد تأخیر به وجود می‌آورد و مسائل مربوط به حریم خصوصی را زمانی که داده‌های حساس باید به سرورهای خارجی ارسال شوند، مطرح می‌کند.

برخی مفاهیم اصلی که هنگام کار با پارادایم‌های سنتی محاسبات هوش مصنوعی باید درک شوند عبارتند از:

- **☁️ پردازش مبتنی بر ابر**: مدل‌های هوش مصنوعی بر روی زیرساخت‌های سرور قدرتمند با منابع محاسباتی بالا اجرا می‌شوند.
- **🔌 دسترسی مبتنی بر API**: برنامه‌ها از طریق تماس‌های API راه دور به قابلیت‌های هوش مصنوعی دسترسی پیدا می‌کنند، نه پردازش محلی.
- **🎛️ مدیریت مدل متمرکز**: مدل‌ها به صورت متمرکز نگهداری و به‌روزرسانی می‌شوند، که ثبات را تضمین می‌کند اما نیاز به اتصال شبکه دارد.
- **📈 مقیاس‌پذیری منابع**: زیرساخت ابری می‌تواند به صورت پویا برای مدیریت نیازهای محاسباتی متغیر مقیاس‌پذیر باشد.

## چالش محاسبات لبه

دستگاه‌های لبه مانند لپ‌تاپ‌ها، تلفن‌های همراه و دستگاه‌های اینترنت اشیا (IoT) مانند Raspberry Pi و NVIDIA Orin Nano محدودیت‌های محاسباتی خاصی دارند. این دستگاه‌ها معمولاً قدرت پردازش، حافظه و منابع انرژی کمتری نسبت به زیرساخت‌های مراکز داده دارند.

اجرای مدل‌های LLM سنتی بر روی چنین دستگاه‌هایی به دلیل این محدودیت‌های سخت‌افزاری همیشه چالش‌برانگیز بوده است. با این حال، نیاز به پردازش هوش مصنوعی لبه در سناریوهای مختلف به طور فزاینده‌ای اهمیت پیدا کرده است. به عنوان مثال، در شرایطی که اتصال به اینترنت غیرقابل اعتماد یا غیرممکن است، مانند سایت‌های صنعتی دورافتاده، وسایل نقلیه در حال حرکت یا مناطقی با پوشش شبکه ضعیف. علاوه بر این، برنامه‌هایی که نیاز به استانداردهای امنیتی بالا دارند، مانند دستگاه‌های پزشکی، سیستم‌های مالی یا برنامه‌های دولتی، ممکن است نیاز به پردازش داده‌های حساس به صورت محلی داشته باشند تا حریم خصوصی و الزامات انطباق را حفظ کنند.

### محدودیت‌های کلیدی محاسبات لبه

محیط‌های محاسبات لبه با چندین محدودیت اساسی مواجه هستند که راه‌حل‌های سنتی هوش مصنوعی مبتنی بر ابر با آن‌ها روبرو نمی‌شوند:

- **قدرت پردازش محدود**: دستگاه‌های لبه معمولاً تعداد هسته‌های CPU کمتر و سرعت کلاک پایین‌تری نسبت به سخت‌افزارهای سرور دارند.
- **محدودیت‌های حافظه**: ظرفیت RAM و فضای ذخیره‌سازی موجود در دستگاه‌های لبه به طور قابل توجهی کاهش یافته است.
- **محدودیت‌های انرژی**: دستگاه‌های باتری‌دار باید عملکرد را با مصرف انرژی برای عملیات طولانی‌مدت متعادل کنند.
- **مدیریت حرارتی**: فرم‌های فشرده قابلیت‌های خنک‌کننده را محدود می‌کنند و بر عملکرد پایدار تحت بار تأثیر می‌گذارند.

## EdgeAI چیست؟

### مفهوم: تعریف Edge AI

Edge AI به استقرار و اجرای الگوریتم‌های هوش مصنوعی مستقیماً بر روی دستگاه‌های لبه اشاره دارد—سخت‌افزار فیزیکی که در "لبه" شبکه، نزدیک به جایی که داده تولید و جمع‌آوری می‌شود، قرار دارد. این دستگاه‌ها شامل تلفن‌های هوشمند، حسگرهای IoT، دوربین‌های هوشمند، وسایل نقلیه خودران، دستگاه‌های پوشیدنی و تجهیزات صنعتی هستند. برخلاف سیستم‌های هوش مصنوعی سنتی که برای پردازش به سرورهای ابری متکی هستند، Edge AI هوش را مستقیماً به منبع داده می‌آورد.

در اصل، Edge AI به معنای غیرمتمرکز کردن پردازش هوش مصنوعی است، انتقال آن از مراکز داده متمرکز و توزیع آن در شبکه گسترده‌ای از دستگاه‌هایی که اکوسیستم دیجیتال ما را تشکیل می‌دهند. این یک تغییر اساسی در نحوه طراحی و استقرار سیستم‌های هوش مصنوعی است.

ستون‌های مفهومی کلیدی Edge AI شامل موارد زیر است:

- **پردازش نزدیک**: محاسبات به صورت فیزیکی نزدیک به جایی که داده‌ها منشأ می‌گیرند انجام می‌شود.
- **هوش غیرمتمرکز**: قابلیت‌های تصمیم‌گیری در میان دستگاه‌های متعدد توزیع می‌شود.
- **حاکمیت داده‌ها**: اطلاعات تحت کنترل محلی باقی می‌مانند و اغلب هرگز دستگاه را ترک نمی‌کنند.
- **عملکرد خودمختار**: دستگاه‌ها می‌توانند بدون نیاز به اتصال دائمی به صورت هوشمند عمل کنند.
- **هوش جاسازی‌شده**: هوش به یک قابلیت ذاتی دستگاه‌های روزمره تبدیل می‌شود.

### تصویری از معماری Edge AI

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI یک تغییر اساسی در نحوه استفاده از هوش مصنوعی ایجاد کرده است، به طوری که قابلیت‌های هوش مصنوعی را مستقیماً به دستگاه‌های لبه منتقل می‌کند و دیگر تنها به پردازش مبتنی بر ابر متکی نیست. این رویکرد امکان اجرای مدل‌های هوش مصنوعی به صورت محلی بر روی دستگاه‌هایی با منابع محاسباتی محدود را فراهم می‌کند و قابلیت‌های استنتاج در زمان واقعی را بدون نیاز به اتصال دائمی به اینترنت ارائه می‌دهد.

EdgeAI شامل فناوری‌ها و تکنیک‌های مختلفی است که برای کارآمدتر کردن مدل‌های هوش مصنوعی و مناسب‌سازی آن‌ها برای استقرار بر روی دستگاه‌های با منابع محدود طراحی شده‌اند. هدف این است که عملکرد قابل قبولی را حفظ کرده و در عین حال نیازهای محاسباتی و حافظه‌ای مدل‌های هوش مصنوعی را به طور قابل توجهی کاهش دهد.

بیایید به رویکردهای اساسی که امکان پیاده‌سازی EdgeAI را در انواع مختلف دستگاه‌ها و موارد استفاده فراهم می‌کنند، نگاهی بیندازیم.

### اصول اصلی EdgeAI

EdgeAI بر اساس چندین اصل بنیادی ساخته شده است که آن را از هوش مصنوعی سنتی مبتنی بر ابر متمایز می‌کند:

- **پردازش محلی**: استنتاج هوش مصنوعی مستقیماً بر روی دستگاه لبه انجام می‌شود و نیازی به اتصال خارجی ندارد.
- **بهینه‌سازی منابع**: مدل‌ها به طور خاص برای محدودیت‌های سخت‌افزاری دستگاه‌های هدف بهینه‌سازی شده‌اند.
- **عملکرد در زمان واقعی**: پردازش با حداقل تأخیر برای برنامه‌های حساس به زمان انجام می‌شود.
- **حریم خصوصی به عنوان طراحی**: داده‌های حساس بر روی دستگاه باقی می‌مانند و امنیت و انطباق را افزایش می‌دهند.

## فناوری‌های کلیدی که EdgeAI را امکان‌پذیر می‌کنند

### کوانتیزه‌سازی مدل

یکی از مهم‌ترین تکنیک‌ها در EdgeAI، کوانتیزه‌سازی مدل است. این فرآیند شامل کاهش دقت پارامترهای مدل، معمولاً از اعداد اعشاری ۳۲ بیتی به اعداد صحیح ۸ بیتی یا حتی فرمت‌های دقت پایین‌تر است. در حالی که این کاهش دقت ممکن است نگران‌کننده به نظر برسد، تحقیقات نشان داده‌اند که بسیاری از مدل‌های هوش مصنوعی می‌توانند عملکرد خود را حتی با دقت کاهش‌یافته به طور قابل توجهی حفظ کنند.

کوانتیزه‌سازی با نگاشت دامنه مقادیر اعشاری به مجموعه‌ای کوچک‌تر از مقادیر گسسته کار می‌کند. به عنوان مثال، به جای استفاده از ۳۲ بیت برای نمایش هر پارامتر، کوانتیزه‌سازی ممکن است فقط از ۸ بیت استفاده کند، که منجر به کاهش ۴ برابری در نیازهای حافظه شده و اغلب منجر به زمان‌های استنتاج سریع‌تر می‌شود.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

تکنیک‌های مختلف کوانتیزه‌سازی شامل موارد زیر است:

- **کوانتیزه‌سازی پس از آموزش (PTQ)**: پس از آموزش مدل اعمال می‌شود و نیازی به آموزش مجدد ندارد.
- **آموزش آگاه به کوانتیزه‌سازی (QAT)**: اثرات کوانتیزه‌سازی را در طول آموزش برای دقت بهتر در نظر می‌گیرد.
- **کوانتیزه‌سازی پویا**: وزن‌ها را به int8 کوانتیزه می‌کند اما فعال‌سازی‌ها را به صورت پویا محاسبه می‌کند.
- **کوانتیزه‌سازی ایستا**: تمام پارامترهای کوانتیزه‌سازی را برای وزن‌ها و فعال‌سازی‌ها از پیش محاسبه می‌کند.

برای استقرارهای EdgeAI، انتخاب استراتژی کوانتیزه‌سازی مناسب به معماری خاص مدل، نیازهای عملکرد و قابلیت‌های سخت‌افزاری دستگاه هدف بستگی دارد.

### فشرده‌سازی و بهینه‌سازی مدل

فراتر از کوانتیزه‌سازی، تکنیک‌های فشرده‌سازی مختلفی به کاهش اندازه مدل و نیازهای محاسباتی کمک می‌کنند. این تکنیک‌ها شامل موارد زیر هستند:

**هرس کردن**: این تکنیک اتصالات یا نورون‌های غیرضروری را از شبکه‌های عصبی حذف می‌کند. با شناسایی و حذف پارامترهایی که تأثیر کمی بر عملکرد مدل دارند، هرس کردن می‌تواند اندازه مدل را به طور قابل توجهی کاهش دهد و در عین حال دقت را حفظ کند.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**تقطیر دانش**: این روش شامل آموزش یک مدل "دانش‌آموز" کوچکتر برای تقلید از رفتار یک مدل "معلم" بزرگ‌تر است. مدل دانش‌آموز یاد می‌گیرد که خروجی‌های معلم را تقریب بزند و اغلب با پارامترهای بسیار کمتر عملکرد مشابهی را به دست می‌آورد.

**بهینه‌سازی معماری مدل**: محققان معماری‌های تخصصی طراحی کرده‌اند که به طور خاص برای استقرار در لبه طراحی شده‌اند، مانند MobileNets، EfficientNets و سایر معماری‌های سبک که عملکرد را با کارایی محاسباتی متعادل می‌کنند.

### مدل‌های زبان کوچک (SLMs)

یک روند نوظهور در EdgeAI توسعه مدل‌های زبان کوچک (SLMs) است. این مدل‌ها از ابتدا برای کوچک و کارآمد بودن طراحی شده‌اند، در حالی که همچنان قابلیت‌های معنادار زبان طبیعی را ارائه می‌دهند. SLM‌ها این کار را از طریق انتخاب‌های معماری دقیق، تکنیک‌های آموزش کارآمد و تمرکز بر آموزش در حوزه‌ها یا وظایف خاص انجام می‌دهند.

برخلاف رویکردهای سنتی که شامل فشرده‌سازی مدل‌های بزرگ است، SLM‌ها اغلب با مجموعه داده‌های کوچکتر و معماری‌های بهینه‌سازی شده که به طور خاص برای استقرار در لبه طراحی شده‌اند، آموزش داده می‌شوند. این رویکرد می‌تواند منجر به مدل‌هایی شود که نه تنها کوچک‌تر هستند، بلکه برای موارد استفاده خاص نیز کارآمدتر هستند.

## شتاب‌دهی سخت‌افزاری برای EdgeAI

دستگاه‌های لبه مدرن به طور فزاینده‌ای شامل سخت‌افزار تخصصی طراحی شده برای شتاب‌دهی بارهای کاری هوش مصنوعی هستند:

### واحدهای پردازش عصبی (NPUs)

NPUs پردازنده‌های تخصصی هستند که به طور خاص برای محاسبات شبکه‌های عصبی طراحی شده‌اند. این تراشه‌ها می‌توانند وظایف استنتاج هوش مصنوعی را بسیار کارآمدتر از CPU‌های سنتی انجام دهند، اغلب با مصرف انرژی کمتر. بسیاری از تلفن‌های هوشمند، لپ‌تاپ‌ها و دستگاه‌های IoT مدرن اکنون شامل NPUs هستند تا پردازش هوش مصنوعی در دستگاه را امکان‌پذیر کنند.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

دستگاه‌های دارای NPU شامل موارد زیر هستند:

- **Apple**: تراشه‌های سری A و سری M با Neural Engine
- **Qualcomm**: پردازنده‌های Snapdragon با Hexagon DSP/NPU
- **Samsung**: پردازنده‌های Exynos با NPU
- **Intel**: شتاب‌دهنده‌های Movidius VPUs و Habana Labs
- **Microsoft**: رایانه‌های Windows Copilot+ با NPUs

### 🎮 شتاب‌دهی GPU

در حالی که دستگاه‌های لبه ممکن است GPU‌های قدرتمند موجود در مراکز داده را نداشته باشند، بسیاری از آن‌ها همچنان شامل GPU‌های یکپارچه یا مجزا هستند که می‌توانند بارهای کاری هوش مصنوعی را شتاب دهند. GPU‌های موبایل مدرن و پردازنده‌های گرافیکی یکپارچه می‌توانند بهبود قابل توجهی در عملکرد وظایف استنتاج هوش مصنوعی ارائه دهند.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### بهینه‌سازی CPU

حتی دستگاه‌های فقط دارای CPU می‌توانند از EdgeAI از طریق پیاده‌سازی‌های بهینه‌شده بهره‌مند شوند. CPU‌های مدرن شامل دستورالعمل‌های تخصصی برای بارهای کاری هوش مصنوعی هستند و چارچوب‌های نرم‌افزاری برای به حداکثر رساندن عملکرد CPU برای استنتاج هوش مصنوعی توسعه یافته‌اند.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

برای مهندسان نرم‌افزار که با EdgeAI کار می‌کنند، درک نحوه استفاده از این گزینه‌های شتاب‌دهی سخت‌افزاری برای بهینه‌سازی عملکرد استنتاج و بهره‌وری انرژی در دستگاه‌های هدف بسیار مهم است.

## مزایای EdgeAI

### حریم خصوصی و امنیت

یکی از مهم‌ترین مزایای EdgeAI افزایش حریم خصوصی و امنیت است. با پردازش داده‌ها به صورت محلی بر روی دستگاه، اطلاعات حساس هرگز از کنترل کاربر خارج نمی‌شود. این امر به ویژه برای برنامه‌هایی که با داده‌های شخصی، اطلاعات پزشکی یا داده‌های محرمانه تجاری سروکار دارند، اهمیت دارد.

### کاهش تأخیر
EdgeAI نیاز به ارسال داده‌ها به سرورهای راه دور برای پردازش را از بین می‌برد و به طور قابل توجهی تأخیر را کاهش می‌دهد. این امر برای برنامه‌های زمان واقعی مانند وسایل نقلیه خودران، اتوماسیون صنعتی یا برنامه‌های تعاملی که نیاز به پاسخ‌های فوری دارند، بسیار مهم است.

### قابلیت آفلاین

EdgeAI قابلیت‌های هوش مصنوعی را حتی زمانی که اتصال به اینترنت در دسترس نیست، امکان‌پذیر می‌کند. این ویژگی برای برنامه‌هایی در مکان‌های دورافتاده، در طول سفر یا در شرایطی که قابلیت اطمینان شبکه مورد توجه است، ارزشمند است.

### صرفه‌جویی در هزینه

با کاهش وابستگی به خدمات هوش مصنوعی مبتنی بر ابر، EdgeAI می‌تواند به کاهش هزینه‌های عملیاتی کمک کند، به ویژه برای برنامه‌هایی با حجم استفاده بالا. سازمان‌ها می‌توانند از هزینه‌های مداوم API اجتناب کرده و نیازهای پهنای باند را کاهش دهند.

### مقیاس‌پذیری

EdgeAI بار محاسباتی را در میان دستگاه‌های لبه توزیع می‌کند، به جای اینکه آن را در مراکز داده متمرکز کند. این می‌تواند به کاهش هزینه‌های زیرساخت و بهبود مقیاس‌پذیری کلی سیستم کمک کند.

## کاربردهای EdgeAI

### دستگاه‌های هوشمند و IoT

EdgeAI بسیاری از ویژگی‌های دستگاه‌های هوشمند را تأمین می‌کند، از دستیارهای صوتی که می‌توانند دستورات را به صورت محلی پردازش کنند تا دوربین‌های هوشمند که می‌توانند اشیاء و افراد را بدون ارسال ویدئو به ابر شناسایی کنند. دستگاه‌های IoT از EdgeAI برای نگهداری پیش‌بینی‌کننده، نظارت بر محیط و تصمیم‌گیری خودکار استفاده می‌کنند.

### برنامه‌های موبایل

تلفن‌های هوشمند و تبلت‌ها از EdgeAI برای ویژگی‌های مختلفی از جمله بهبود عکس، ترجمه زمان واقعی، واقعیت افزوده و توصیه‌های شخصی استفاده می‌کنند. این برنامه‌ها از مزایای تأخیر کم و مزایای حریم خصوصی پردازش محلی بهره‌مند می‌شوند.

### کاربردهای صنعتی

محیط‌های تولید و
- [02: کاربردهای EdgeAI](02.RealWorldCaseStudies.md)

---

**سلب مسئولیت**:  
این سند با استفاده از سرویس ترجمه هوش مصنوعی [Co-op Translator](https://github.com/Azure/co-op-translator) ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را حفظ کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادرستی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما مسئولیتی در قبال سوء تفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.