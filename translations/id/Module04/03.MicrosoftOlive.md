<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:42:19+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "id"
}
-->
# Bagian 3: Microsoft Olive Optimization Suite

## Daftar Isi
1. [Pendahuluan](../../../Module04)
2. [Apa itu Microsoft Olive?](../../../Module04)
3. [Instalasi](../../../Module04)
4. [Panduan Memulai Cepat](../../../Module04)
5. [Contoh: Mengonversi Qwen3 ke ONNX INT4](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Repositori Resep Olive](../../../Module04)
8. [Praktik Terbaik](../../../Module04)
9. [Pemecahan Masalah](../../../Module04)
10. [Sumber Daya Tambahan](../../../Module04)

## Pendahuluan

Microsoft Olive adalah toolkit optimasi model yang kuat dan mudah digunakan, yang dirancang untuk perangkat keras tertentu. Olive mempermudah proses optimasi model pembelajaran mesin untuk diterapkan pada berbagai platform perangkat keras. Baik Anda menargetkan CPU, GPU, atau akselerator AI khusus, Olive membantu Anda mencapai kinerja optimal sambil mempertahankan akurasi model.

## Apa itu Microsoft Olive?

Olive adalah alat optimasi model yang mudah digunakan dan dirancang untuk perangkat keras tertentu, yang menggabungkan teknik-teknik terdepan di industri dalam kompresi model, optimasi, dan kompilasi. Olive bekerja dengan ONNX Runtime sebagai solusi optimasi inferensi E2E.

### Fitur Utama

- **Optimasi Perangkat Keras**: Secara otomatis memilih teknik optimasi terbaik untuk perangkat keras target Anda
- **40+ Komponen Optimasi Bawaan**: Meliputi kompresi model, kuantisasi, optimasi grafik, dan lainnya
- **Antarmuka CLI yang Mudah**: Perintah sederhana untuk tugas optimasi umum
- **Dukungan Multi-Kerangka**: Bekerja dengan PyTorch, model Hugging Face, dan ONNX
- **Dukungan Model Populer**: Olive dapat secara otomatis mengoptimalkan arsitektur model populer seperti Llama, Phi, Qwen, Gemma, dll secara langsung

### Manfaat

- **Pengurangan Waktu Pengembangan**: Tidak perlu bereksperimen secara manual dengan berbagai teknik optimasi
- **Peningkatan Kinerja**: Peningkatan kecepatan yang signifikan (hingga 6x dalam beberapa kasus)
- **Penerapan Lintas Platform**: Model yang dioptimalkan dapat bekerja di berbagai perangkat keras dan sistem operasi
- **Akurasi Terjaga**: Optimasi mempertahankan kualitas model sambil meningkatkan kinerja

## Instalasi

### Prasyarat

- Python 3.8 atau lebih tinggi
- Pengelola paket pip
- Lingkungan virtual (disarankan)

### Instalasi Dasar

Buat dan aktifkan lingkungan virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instal Olive dengan fitur optimasi otomatis:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependensi Opsional

Olive menawarkan berbagai dependensi opsional untuk fitur tambahan:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifikasi Instalasi

```bash
olive --help
```

Jika berhasil, Anda akan melihat pesan bantuan CLI Olive.

## Panduan Memulai Cepat

### Optimasi Pertama Anda

Mari kita optimalkan model bahasa kecil menggunakan fitur optimasi otomatis Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Apa yang Dilakukan Perintah Ini

Proses optimasi melibatkan: mendapatkan model dari cache lokal, menangkap Grafik ONNX dan menyimpan bobot dalam file data ONNX, mengoptimalkan Grafik ONNX, dan mengkuantisasi model ke int4 menggunakan metode RTN.

### Penjelasan Parameter Perintah

- `--model_name_or_path`: Identifikasi model Hugging Face atau jalur lokal
- `--output_path`: Direktori tempat model yang dioptimalkan akan disimpan
- `--device`: Perangkat target (cpu, gpu)
- `--provider`: Penyedia eksekusi (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gunakan ONNX Runtime Generate AI untuk inferensi
- `--precision`: Presisi kuantisasi (int4, int8, fp16)
- `--log_level`: Tingkat detail log (0=minimal, 1=rinci)

## Contoh: Mengonversi Qwen3 ke ONNX INT4

Berdasarkan contoh Hugging Face yang disediakan di [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), berikut cara mengoptimalkan model Qwen3:

### Langkah 1: Unduh Model (Opsional)

Untuk meminimalkan waktu unduh, cache hanya file yang penting:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Langkah 2: Optimalkan Model Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Langkah 3: Uji Model yang Dioptimalkan

Buat skrip Python sederhana untuk menguji model yang dioptimalkan:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktur Output

Setelah optimasi, direktori output Anda akan berisi:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Penggunaan Lanjutan

### File Konfigurasi

Untuk alur kerja optimasi yang lebih kompleks, Anda dapat menggunakan file konfigurasi JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Jalankan dengan konfigurasi:

```bash
olive run --config config.json
```

### Optimasi GPU

Untuk optimasi GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Untuk DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning dengan Olive

Olive juga mendukung fine-tuning model:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Praktik Terbaik

### 1. Pemilihan Model
- Mulailah dengan model yang lebih kecil untuk pengujian (misalnya, parameter 0.5B-7B)
- Pastikan arsitektur model target Anda didukung oleh Olive

### 2. Pertimbangan Perangkat Keras
- Sesuaikan target optimasi Anda dengan perangkat keras penerapan
- Gunakan optimasi GPU jika Anda memiliki perangkat keras yang kompatibel dengan CUDA
- Pertimbangkan DirectML untuk mesin Windows dengan grafis terintegrasi

### 3. Pemilihan Presisi
- **INT4**: Kompresi maksimum, sedikit kehilangan akurasi
- **INT8**: Keseimbangan ukuran dan akurasi yang baik
- **FP16**: Kehilangan akurasi minimal, pengurangan ukuran sedang

### 4. Pengujian dan Validasi
- Selalu uji model yang dioptimalkan dengan kasus penggunaan spesifik Anda
- Bandingkan metrik kinerja (latensi, throughput, akurasi)
- Gunakan data input yang representatif untuk evaluasi

### 5. Optimasi Iteratif
- Mulailah dengan optimasi otomatis untuk hasil cepat
- Gunakan file konfigurasi untuk kontrol yang lebih rinci
- Bereksperimenlah dengan berbagai langkah optimasi

## Pemecahan Masalah

### Masalah Umum

#### 1. Masalah Instalasi
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Masalah CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Masalah Memori
- Gunakan ukuran batch yang lebih kecil selama optimasi
- Coba kuantisasi dengan presisi lebih tinggi terlebih dahulu (int8 daripada int4)
- Pastikan ruang disk yang cukup untuk cache model

#### 4. Kesalahan Memuat Model
- Verifikasi jalur model dan izin akses
- Periksa apakah model memerlukan `trust_remote_code=True`
- Pastikan semua file model yang diperlukan telah diunduh

### Mendapatkan Bantuan

- **Dokumentasi**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Contoh**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repositori Resep Olive

### Pengantar Resep Olive

Repositori [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) melengkapi toolkit utama Olive dengan menyediakan koleksi lengkap resep optimasi siap pakai untuk model AI populer. Repositori ini berfungsi sebagai referensi praktis baik untuk mengoptimalkan model yang tersedia secara publik maupun membuat alur kerja optimasi untuk model milik sendiri.

### Fitur Utama

- **100+ Resep Siap Pakai**: Konfigurasi optimasi siap pakai untuk model populer
- **Dukungan Multi-Arsitektur**: Meliputi model transformer, model vision, dan arsitektur multimodal
- **Optimasi Khusus Perangkat Keras**: Resep yang disesuaikan untuk CPU, GPU, dan akselerator khusus
- **Keluarga Model Populer**: Termasuk Phi, Llama, Qwen, Gemma, Mistral, dan banyak lagi

### Keluarga Model yang Didukung

Repositori ini mencakup resep optimasi untuk:

#### Model Bahasa
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, seri Qwen2.5 (0.5B hingga 14B)
- **Google Gemma**: Berbagai konfigurasi model Gemma
- **Mistral AI**: Seri Mistral-7B
- **DeepSeek**: Model seri R1-Distill

#### Model Vision dan Multimodal
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Model CLIP**: Berbagai konfigurasi CLIP-ViT
- **ResNet**: Optimasi ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Model Khusus
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Varian dasar dan multibahasa
- **Sentence Transformers**: all-MiniLM-L6-v2

### Menggunakan Resep Olive

#### Metode 1: Clone Resep Tertentu

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metode 2: Gunakan Resep sebagai Template

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Struktur Resep

Setiap direktori resep biasanya berisi:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Contoh: Menggunakan Resep Phi-4-mini

Mari gunakan resep Phi-4-mini sebagai contoh:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

File konfigurasi biasanya mencakup:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Menyesuaikan Resep

#### Memodifikasi Perangkat Keras Target

Untuk mengubah perangkat keras target, perbarui bagian `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Menyesuaikan Parameter Optimasi

Modifikasi bagian `passes` untuk tingkat optimasi yang berbeda:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Membuat Resep Anda Sendiri

1. **Mulai dengan Model Serupa**: Temukan resep untuk model dengan arsitektur serupa
2. **Perbarui Konfigurasi Model**: Ubah nama/jalur model dalam konfigurasi
3. **Sesuaikan Parameter**: Modifikasi parameter optimasi sesuai kebutuhan
4. **Uji dan Validasi**: Jalankan optimasi dan validasi hasilnya
5. **Kontribusi Kembali**: Pertimbangkan untuk menyumbangkan resep Anda ke repositori

### Manfaat Menggunakan Resep

#### 1. **Konfigurasi Terbukti**
- Pengaturan optimasi yang telah diuji untuk model tertentu
- Menghindari trial-and-error dalam menemukan parameter optimal

#### 2. **Penyetelan Khusus Perangkat Keras**
- Dioptimalkan sebelumnya untuk berbagai penyedia eksekusi
- Konfigurasi siap pakai untuk target CPU, GPU, dan NPU

#### 3. **Cakupan Komprehensif**
- Mendukung model open-source paling populer
- Pembaruan rutin dengan rilis model baru

#### 4. **Kontribusi Komunitas**
- Pengembangan kolaboratif dengan komunitas AI
- Berbagi pengetahuan dan praktik terbaik

### Berkontribusi pada Resep Olive

Jika Anda telah mengoptimalkan model yang belum tercakup dalam repositori:

1. **Fork Repositori**: Buat fork repositori olive-recipes Anda sendiri
2. **Buat Direktori Resep**: Tambahkan direktori baru untuk model Anda
3. **Sertakan Konfigurasi**: Tambahkan olive_config.json dan file pendukung
4. **Dokumentasikan Penggunaan**: Berikan README yang jelas dengan instruksi
5. **Kirim Pull Request**: Berkontribusi kembali ke komunitas

### Benchmark Kinerja

Banyak resep menyertakan benchmark kinerja yang menunjukkan:
- **Peningkatan Latensi**: Biasanya 2-6x lebih cepat dibandingkan baseline
- **Pengurangan Memori**: Pengurangan penggunaan memori 50-75% dengan kuantisasi
- **Preservasi Akurasi**: Akurasi tetap terjaga 95-99%

### Integrasi dengan AI Toolkit

Resep bekerja dengan mulus dengan:
- **VS Code AI Toolkit**: Integrasi langsung untuk optimasi model
- **Azure Machine Learning**: Alur kerja optimasi berbasis cloud
- **ONNX Runtime**: Penerapan inferensi yang dioptimalkan

## Sumber Daya Tambahan

### Tautan Resmi
- **Repositori GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repositori Resep Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Dokumentasi ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Contoh Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Contoh Komunitas
- **Jupyter Notebooks**: Tersedia di repositori GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Ekstensi VS Code**: AI Toolkit untuk VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Post**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Alat Terkait
- **ONNX Runtime**: Mesin inferensi berkinerja tinggi — https://onnxruntime.ai/
- **Hugging Face Transformers**: Sumber banyak model yang kompatibel — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Alur kerja optimasi berbasis cloud — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Langkah Selanjutnya

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk memberikan hasil yang akurat, harap diketahui bahwa terjemahan otomatis dapat mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang timbul dari penggunaan terjemahan ini.