<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:56:56+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "id"
}
-->
# Bagian 7: Qualcomm QNN (Qualcomm Neural Network) Optimization Suite

## Daftar Isi
1. [Pendahuluan](../../../Module04)
2. [Apa itu Qualcomm QNN?](../../../Module04)
3. [Instalasi](../../../Module04)
4. [Panduan Memulai Cepat](../../../Module04)
5. [Contoh: Mengonversi dan Mengoptimalkan Model dengan QNN](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Praktik Terbaik](../../../Module04)
8. [Pemecahan Masalah](../../../Module04)
9. [Sumber Daya Tambahan](../../../Module04)

## Pendahuluan

Qualcomm QNN (Qualcomm Neural Network) adalah kerangka kerja inferensi AI yang komprehensif, dirancang untuk memaksimalkan potensi penuh akselerator perangkat keras AI Qualcomm, termasuk Hexagon NPU, Adreno GPU, dan Kryo CPU. Baik Anda menargetkan perangkat seluler, platform komputasi edge, atau sistem otomotif, QNN menyediakan kemampuan inferensi yang dioptimalkan yang memanfaatkan unit pemrosesan AI khusus Qualcomm untuk kinerja maksimal dan efisiensi energi.

## Apa itu Qualcomm QNN?

Qualcomm QNN adalah kerangka kerja inferensi AI terpadu yang memungkinkan pengembang untuk menerapkan model AI secara efisien di seluruh arsitektur komputasi heterogen Qualcomm. Kerangka ini menyediakan antarmuka pemrograman terpadu untuk mengakses Hexagon NPU (Neural Processing Unit), Adreno GPU, dan Kryo CPU, secara otomatis memilih unit pemrosesan optimal untuk berbagai lapisan model dan operasi.

### Fitur Utama

- **Komputasi Heterogen**: Akses terpadu ke NPU, GPU, dan CPU dengan distribusi beban kerja otomatis
- **Optimasi Berbasis Perangkat Keras**: Optimasi khusus untuk platform Snapdragon Qualcomm
- **Dukungan Kuantisasi**: Teknik kuantisasi canggih INT8, INT16, dan presisi campuran
- **Alat Konversi Model**: Dukungan langsung untuk model TensorFlow, PyTorch, ONNX, dan Caffe
- **Optimasi AI Edge**: Dirancang khusus untuk skenario penerapan seluler dan edge dengan fokus pada efisiensi daya

### Manfaat

- **Kinerja Maksimal**: Memanfaatkan perangkat keras AI khusus untuk peningkatan kinerja hingga 15x
- **Efisiensi Daya**: Dioptimalkan untuk perangkat seluler dan bertenaga baterai dengan manajemen daya cerdas
- **Latensi Rendah**: Inferensi yang dipercepat perangkat keras dengan overhead minimal untuk aplikasi waktu nyata
- **Penerapan yang Skalabel**: Dari smartphone hingga platform otomotif di seluruh ekosistem Qualcomm
- **Siap Produksi**: Kerangka kerja yang telah teruji digunakan di jutaan perangkat yang telah diterapkan

## Instalasi

### Prasyarat

- Qualcomm QNN SDK (memerlukan pendaftaran dengan Qualcomm)
- Python 3.7 atau lebih tinggi
- Perangkat keras Qualcomm yang kompatibel atau simulator
- Android NDK (untuk penerapan seluler)
- Lingkungan pengembangan Linux atau Windows

### Pengaturan QNN SDK

1. **Daftar dan Unduh**: Kunjungi Qualcomm Developer Network untuk mendaftar dan mengunduh QNN SDK
2. **Ekstrak SDK**: Ekstrak QNN SDK ke direktori pengembangan Anda
3. **Atur Variabel Lingkungan**: Konfigurasikan jalur untuk alat dan pustaka QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Pengaturan Lingkungan Python

Buat dan aktifkan lingkungan virtual:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Instal paket Python yang diperlukan:

```bash
pip install numpy tensorflow torch onnx
```

### Verifikasi Instalasi

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Jika berhasil, Anda akan melihat informasi bantuan untuk setiap alat QNN.

## Panduan Memulai Cepat

### Konversi Model Pertama Anda

Mari kita konversi model PyTorch sederhana untuk dijalankan di perangkat keras Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Konversi ONNX ke Format QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Hasilkan Pustaka Model QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Apa yang Dilakukan Proses Ini

Alur kerja optimasi melibatkan: mengonversi model asli ke format ONNX, menerjemahkan ONNX ke representasi antara QNN, menerapkan optimasi khusus perangkat keras, dan menghasilkan pustaka model yang dikompilasi untuk penerapan.

### Penjelasan Parameter Utama

- `--input_network`: File model ONNX sumber
- `--output_path`: File sumber C++ yang dihasilkan
- `--input_dim`: Dimensi tensor input untuk optimasi
- `--quantization_overrides`: Konfigurasi kuantisasi khusus
- `-t x86_64-linux-clang`: Arsitektur target dan compiler

## Contoh: Mengonversi dan Mengoptimalkan Model dengan QNN

### Langkah 1: Konversi Model Lanjutan dengan Kuantisasi

Berikut cara menerapkan kuantisasi khusus selama konversi:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konversi dengan kuantisasi khusus:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Langkah 2: Optimasi Multi-Backend

Konfigurasikan untuk eksekusi heterogen di NPU, GPU, dan CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Langkah 3: Buat Binary Konteks untuk Penerapan

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Langkah 4: Inferensi dengan Runtime QNN

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Struktur Output

Setelah optimasi, direktori penerapan Anda akan berisi:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Penggunaan Lanjutan

### Konfigurasi Backend Khusus

Konfigurasikan optimasi backend tertentu:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Kuantisasi Dinamis

Terapkan kuantisasi saat runtime untuk akurasi yang lebih baik:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profiling Kinerja

Pantau kinerja di berbagai backend:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Pemilihan Backend Otomatis

Implementasikan pemilihan backend cerdas berdasarkan karakteristik model:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Praktik Terbaik

### 1. Optimasi Arsitektur Model
- **Layer Fusion**: Gabungkan operasi seperti Conv+BatchNorm+ReLU untuk pemanfaatan NPU yang lebih baik
- **Depth-wise Separable Convolutions**: Gunakan ini daripada konvolusi standar untuk penerapan seluler
- **Desain Ramah Kuantisasi**: Gunakan aktivasi ReLU dan hindari operasi yang sulit untuk dikuantisasi

### 2. Strategi Kuantisasi
- **Kuantisasi Pasca-Pelatihan**: Mulailah dengan ini untuk penerapan cepat
- **Dataset Kalibrasi**: Gunakan data representatif yang mencakup semua variasi input
- **Presisi Campuran**: Gunakan INT8 untuk sebagian besar lapisan, pertahankan lapisan penting dalam presisi lebih tinggi

### 3. Panduan Pemilihan Backend
- **NPU (HTP)**: Terbaik untuk beban kerja CNN, model yang dikuantisasi, dan aplikasi yang sensitif terhadap daya
- **GPU**: Optimal untuk operasi intensif komputasi, model yang lebih besar, dan presisi FP16
- **CPU**: Cadangan untuk operasi yang tidak didukung dan debugging

### 4. Optimasi Kinerja
- **Ukuran Batch**: Gunakan ukuran batch 1 untuk aplikasi waktu nyata, batch lebih besar untuk throughput
- **Pra-pemrosesan Input**: Minimalkan overhead penyalinan dan konversi data
- **Penggunaan Ulang Konteks**: Pra-kompilasi konteks untuk menghindari overhead kompilasi runtime

### 5. Manajemen Memori
- **Alokasi Tensor**: Gunakan alokasi statis jika memungkinkan untuk menghindari overhead runtime
- **Pool Memori**: Implementasikan pool memori khusus untuk tensor yang sering dialokasikan
- **Penggunaan Ulang Buffer**: Gunakan kembali buffer input/output di antara panggilan inferensi

### 6. Optimasi Daya
- **Mode Kinerja**: Gunakan mode kinerja yang sesuai berdasarkan batasan termal
- **Dynamic Frequency Scaling**: Biarkan sistem menskalakan frekuensi berdasarkan beban kerja
- **Manajemen Status Idle**: Lepaskan sumber daya dengan benar saat tidak digunakan

## Pemecahan Masalah

### Masalah Umum

#### 1. Masalah Instalasi SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Kesalahan Konversi Model
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Masalah Kuantisasi
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Masalah Kinerja
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Masalah Memori
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Kompatibilitas Backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Debugging Kinerja

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Mendapatkan Bantuan

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Dokumentasi QNN**: Tersedia dalam paket SDK
- **Forum Komunitas**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Dukungan Teknis**: Melalui portal pengembang Qualcomm

## Sumber Daya Tambahan

### Tautan Resmi
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Platform Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portal Pengembang**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Sumber Belajar
- **Panduan Memulai**: Tersedia dalam dokumentasi QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Panduan Optimasi**: Dokumentasi SDK mencakup panduan optimasi yang komprehensif
- **Tutorial Video**: [Saluran YouTube Qualcomm Developer Network](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Alat Integrasi
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Model yang telah dioptimalkan untuk perangkat keras Qualcomm
- **Android Neural Networks API**: Integrasi dengan Android NNAPI
- **TensorFlow Lite Delegate**: Delegasi Qualcomm untuk TFLite

### Benchmark Kinerja
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Penelitian AI Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Contoh Komunitas
- **Aplikasi Contoh**: Tersedia dalam direktori contoh QNN SDK
- **Repositori GitHub**: Contoh dan alat yang disumbangkan oleh komunitas
- **Blog Teknis**: [Blog Pengembang Qualcomm](https://developer.qualcomm.com/blog)

### Alat Terkait
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Teknik kuantisasi dan kompresi lanjutan
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Untuk perbandingan dan penerapan cadangan
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Mesin inferensi lintas platform

### Spesifikasi Perangkat Keras
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Platform Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Langkah Selanjutnya

Lanjutkan perjalanan AI Edge Anda dengan menjelajahi [Modul 5: SLMOps dan Penerapan Produksi](../Module05/README.md) untuk mempelajari aspek operasional dari manajemen siklus hidup Small Language Model.

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk memberikan hasil yang akurat, harap diperhatikan bahwa terjemahan otomatis dapat mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa penerjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau interpretasi yang keliru yang timbul dari penggunaan terjemahan ini.