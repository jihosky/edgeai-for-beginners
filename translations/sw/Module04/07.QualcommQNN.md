<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:58:01+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "sw"
}
-->
# Sehemu ya 7: Qualcomm QNN (Qualcomm Neural Network) Optimization Suite

## Jedwali la Yaliyomo
1. [Utangulizi](../../../Module04)
2. [Qualcomm QNN ni nini?](../../../Module04)
3. [Usakinishaji](../../../Module04)
4. [Mwongozo wa Kuanza Haraka](../../../Module04)
5. [Mfano: Kubadilisha na Kuboresha Miundo kwa QNN](../../../Module04)
6. [Matumizi ya Juu](../../../Module04)
7. [Mbinu Bora](../../../Module04)
8. [Kutatua Matatizo](../../../Module04)
9. [Rasilimali za Ziada](../../../Module04)

## Utangulizi

Qualcomm QNN (Qualcomm Neural Network) ni mfumo wa kina wa AI inference ulioundwa ili kufungua uwezo kamili wa viongeza kasi vya vifaa vya AI vya Qualcomm, ikiwa ni pamoja na Hexagon NPU, Adreno GPU, na Kryo CPU. Iwe unalenga vifaa vya rununu, majukwaa ya kompyuta ya ukingoni, au mifumo ya magari, QNN hutoa uwezo wa inference ulioboreshwa unaotumia vitengo maalum vya usindikaji wa AI vya Qualcomm kwa utendaji wa juu na ufanisi wa nishati.

## Qualcomm QNN ni nini?

Qualcomm QNN ni mfumo wa AI inference uliounganishwa unaowezesha watengenezaji kupeleka miundo ya AI kwa ufanisi katika usanifu wa kompyuta mseto wa Qualcomm. Inatoa kiolesura cha programu kilichounganishwa kwa ufikiaji wa Hexagon NPU (Neural Processing Unit), Adreno GPU, na Kryo CPU, ikichagua kiotomatiki kitengo bora cha usindikaji kwa tabaka na operesheni tofauti za modeli.

### Vipengele Muhimu

- **Kompyuta Mseto**: Ufikiaji uliounganishwa kwa NPU, GPU, na CPU na usambazaji wa mzigo wa kazi kiotomatiki
- **Uboreshaji Unaotambua Vifaa**: Uboreshaji maalum kwa majukwaa ya Qualcomm Snapdragon
- **Msaada wa Quantization**: Mbinu za hali ya juu za quantization za INT8, INT16, na mchanganyiko wa usahihi
- **Zana za Kubadilisha Modeli**: Msaada wa moja kwa moja kwa miundo ya TensorFlow, PyTorch, ONNX, na Caffe
- **Imeboreshwa kwa AI ya Ukingoni**: Imeundwa mahsusi kwa hali za kupelekwa kwa rununu na ukingoni kwa kuzingatia ufanisi wa nishati

### Faida

- **Utendaji wa Juu Zaidi**: Tumia vifaa maalum vya AI kwa maboresho ya utendaji hadi mara 15
- **Ufanisi wa Nishati**: Imeboreshwa kwa vifaa vya rununu na vinavyotumia betri na usimamizi wa nishati wa akili
- **Latency ya Chini**: Inference inayoharakishwa na vifaa na mzigo mdogo kwa programu za wakati halisi
- **Upelekaji Unaoweza Kupimika**: Kutoka simu za mkononi hadi majukwaa ya magari katika mfumo wa Qualcomm
- **Tayari kwa Uzalishaji**: Mfumo uliothibitishwa na kutumika katika mamilioni ya vifaa vilivyowekwa

## Usakinishaji

### Mahitaji ya Awali

- Qualcomm QNN SDK (inahitaji usajili na Qualcomm)
- Python 3.7 au zaidi
- Vifaa vinavyolingana vya Qualcomm au simulator
- Android NDK (kwa upelekaji wa rununu)
- Mazingira ya maendeleo ya Linux au Windows

### Usanidi wa QNN SDK

1. **Jisajili na Pakua**: Tembelea Mtandao wa Watengenezaji wa Qualcomm ili kujisajili na kupakua QNN SDK
2. **Fungua SDK**: Fungua QNN SDK kwenye saraka yako ya maendeleo
3. **Sanidi Vigezo vya Mazingira**: Sanidi njia za zana na maktaba za QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Usanidi wa Mazingira ya Python

Unda na uwashe mazingira ya kawaida:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Sakinisha vifurushi vinavyohitajika vya Python:

```bash
pip install numpy tensorflow torch onnx
```

### Thibitisha Usakinishaji

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Ikiwa imefanikiwa, unapaswa kuona maelezo ya msaada kwa kila zana ya QNN.

## Mwongozo wa Kuanza Haraka

### Kubadilisha Modeli Yako ya Kwanza

Hebu tubadilishe modeli rahisi ya PyTorch ili ifanye kazi kwenye vifaa vya Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Badilisha ONNX kuwa Muundo wa QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Tengeneza Maktaba ya Modeli ya QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Mchakato Huu Unafanya Nini

Mfumo wa uboreshaji unahusisha: kubadilisha modeli asili kuwa muundo wa ONNX, kutafsiri ONNX kuwa uwakilishi wa kati wa QNN, kutumia uboreshaji maalum wa vifaa, na kutengeneza maktaba ya modeli iliyosanikishwa kwa upelekaji.

### Vigezo Muhimu Vilivyoelezwa

- `--input_network`: Faili ya modeli ya ONNX chanzo
- `--output_path`: Faili ya chanzo ya C++ iliyotengenezwa
- `--input_dim`: Vipimo vya tensor ya ingizo kwa uboreshaji
- `--quantization_overrides`: Usanidi maalum wa quantization
- `-t x86_64-linux-clang`: Usanifu lengwa na mkusanyaji

## Mfano: Kubadilisha na Kuboresha Miundo kwa QNN

### Hatua ya 1: Kubadilisha Modeli kwa Quantization ya Juu

Hivi ndivyo unavyotumia quantization maalum wakati wa kubadilisha:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Badilisha kwa quantization maalum:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Hatua ya 2: Uboreshaji wa Backend Mseto

Sanidi kwa utekelezaji mseto katika NPU, GPU, na CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Hatua ya 3: Unda Binary ya Muktadha kwa Upelekaji

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Hatua ya 4: Inference na QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Muundo wa Matokeo

Baada ya uboreshaji, saraka yako ya upelekaji itakuwa na:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Matumizi ya Juu

### Usanidi Maalum wa Backend

Sanidi uboreshaji maalum wa backend:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Quantization ya Dynamic

Tumia quantization wakati wa kukimbia kwa usahihi bora:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Ufuatiliaji wa Utendaji

Fuatilia utendaji katika backend tofauti:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Uchaguzi wa Backend Kiotomatiki

Tekeleza uchaguzi wa backend wa akili kulingana na sifa za modeli:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Mbinu Bora

### 1. Uboreshaji wa Usanifu wa Modeli
- **Fusion ya Tabaka**: Unganisha operesheni kama Conv+BatchNorm+ReLU kwa matumizi bora ya NPU
- **Convolutions Zinazotenganishwa kwa Kina**: Pendelea hizi badala ya convolutions za kawaida kwa upelekaji wa rununu
- **Miundo Rafiki kwa Quantization**: Tumia uanzishaji wa ReLU na epuka operesheni ambazo hazifanyi vizuri kwa quantization

### 2. Mkakati wa Quantization
- **Quantization Baada ya Mafunzo**: Anza na hii kwa upelekaji wa haraka
- **Dataset ya Kalibrasi**: Tumia data inayowakilisha inayofunika tofauti zote za ingizo
- **Usahihi Mchanganyiko**: Tumia INT8 kwa tabaka nyingi, weka tabaka muhimu katika usahihi wa juu

### 3. Miongozo ya Uchaguzi wa Backend
- **NPU (HTP)**: Bora kwa mzigo wa CNN, miundo iliyokadiriwa, na programu nyeti kwa nishati
- **GPU**: Bora kwa operesheni zinazohitaji hesabu nyingi, miundo mikubwa, na usahihi wa FP16
- **CPU**: Njia mbadala kwa operesheni zisizosaidiwa na utatuzi wa matatizo

### 4. Uboreshaji wa Utendaji
- **Ukubwa wa Kundi**: Tumia ukubwa wa kundi 1 kwa programu za wakati halisi, makundi makubwa kwa throughput
- **Usindikaji wa Awali wa Ingizo**: Punguza nakala za data na mzigo wa ubadilishaji
- **Matumizi ya Muktadha**: Sanidi muktadha kabla ili kuepuka mzigo wa kusanidi wakati wa kukimbia

### 5. Usimamizi wa Kumbukumbu
- **Ugawaji wa Tensor**: Tumia ugawaji tuli inapowezekana ili kuepuka mzigo wa wakati wa kukimbia
- **Mabwawa ya Kumbukumbu**: Tekeleza mabwawa maalum ya kumbukumbu kwa tensors zinazotengwa mara kwa mara
- **Matumizi ya Buffer**: Tumia tena buffer za ingizo/mazao katika miito ya inference

### 6. Uboreshaji wa Nishati
- **Njia za Utendaji**: Tumia njia za utendaji zinazofaa kulingana na vikwazo vya joto
- **Upunguzaji wa Marudio ya Dynamic**: Ruhusu mfumo kupunguza marudio kulingana na mzigo wa kazi
- **Usimamizi wa Hali ya Kupumzika**: Achilia rasilimali ipasavyo wakati hazitumiki

## Kutatua Matatizo

### Masuala ya Kawaida

#### 1. Matatizo ya Usakinishaji wa SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Makosa ya Kubadilisha Modeli
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Masuala ya Quantization
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Masuala ya Utendaji
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Masuala ya Kumbukumbu
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Utangamano wa Backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Ufuatiliaji wa Utendaji

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Kupata Msaada

- **Mtandao wa Watengenezaji wa Qualcomm**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Nyaraka za QNN**: Zinapatikana katika kifurushi cha SDK
- **Majukwaa ya Jamii**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Msaada wa Kiufundi**: Kupitia lango la watengenezaji wa Qualcomm

## Rasilimali za Ziada

### Viungo Rasmi
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Majukwaa ya Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Lango la Watengenezaji**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Rasilimali za Kujifunza
- **Mwongozo wa Kuanza**: Unapatikana katika nyaraka za QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Mwongozo wa Uboreshaji**: Nyaraka za SDK zinajumuisha miongozo ya kina ya uboreshaji
- **Video za Mafunzo**: [Kituo cha YouTube cha Watengenezaji wa Qualcomm](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Zana za Ujumuishaji
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Miundo iliyoboreshwa kwa vifaa vya Qualcomm
- **API ya Neural Networks ya Android**: Ujumuishaji na Android NNAPI
- **TensorFlow Lite Delegate**: Mwakilishi wa Qualcomm kwa TFLite

### Viwango vya Utendaji
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Utafiti wa AI wa Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Mifano ya Jamii
- **Programu za Mfano**: Zinapatikana katika saraka ya mifano ya QNN SDK
- **Hifadhi za GitHub**: Mifano na zana zilizochangiwa na jamii
- **Blogu za Kiufundi**: [Blogu ya Watengenezaji wa Qualcomm](https://developer.qualcomm.com/blog)

### Zana Zinazohusiana
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Mbinu za hali ya juu za quantization na compression
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Kwa kulinganisha na upelekaji wa mbadala
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Injini ya inference ya majukwaa mbalimbali

### Maelezo ya Vifaa
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Majukwaa ya Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Nini kinachofuata

Endelea na safari yako ya AI ya Ukingoni kwa kuchunguza [Moduli ya 5: SLMOps na Upelekaji wa Uzalishaji](../Module05/README.md) ili kujifunza kuhusu vipengele vya kiutendaji vya usimamizi wa mzunguko wa maisha wa Small Language Model.

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya tafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kwa usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya asili inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.