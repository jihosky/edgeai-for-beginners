<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T14:00:07+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sw"
}
-->
# Sehemu ya 3: Microsoft Olive Optimization Suite

## Jedwali la Maudhui
1. [Utangulizi](../../../Module04)
2. [Microsoft Olive ni nini?](../../../Module04)
3. [Usakinishaji](../../../Module04)
4. [Mwongozo wa Kuanza Haraka](../../../Module04)
5. [Mfano: Kubadilisha Qwen3 kuwa ONNX INT4](../../../Module04)
6. [Matumizi ya Juu](../../../Module04)
7. [Hifadhi ya Mapishi ya Olive](../../../Module04)
8. [Mbinu Bora](../../../Module04)
9. [Kutatua Tatizo](../../../Module04)
10. [Rasilimali za Ziada](../../../Module04)

## Utangulizi

Microsoft Olive ni zana yenye nguvu na rahisi kutumia ya kuboresha mifano ya kujifunza kwa mashine kwa kuzingatia vifaa, inayorahisisha mchakato wa kuboresha mifano kwa ajili ya matumizi kwenye majukwaa tofauti ya vifaa. Iwe unalenga CPU, GPU, au viongeza kasi vya AI maalum, Olive hukusaidia kufanikisha utendaji bora huku ukihifadhi usahihi wa mfano.

## Microsoft Olive ni nini?

Olive ni zana rahisi kutumia ya kuboresha mifano kwa kuzingatia vifaa, inayojumuisha mbinu bora za sekta katika ukandamizaji wa mifano, uboreshaji, na uundaji. Inafanya kazi na ONNX Runtime kama suluhisho la mwisho hadi mwisho la uboreshaji wa inferensi.

### Vipengele Muhimu

- **Uboreshaji wa Kifaa**: Huchagua kiotomatiki mbinu bora za uboreshaji kwa vifaa unavyolenga
- **Vipengele 40+ vya Uboreshaji Vilivyojengwa Ndani**: Inashughulikia ukandamizaji wa mifano, upunguzaji, uboreshaji wa grafu, na zaidi
- **Kiolesura Rahisi cha CLI**: Amri rahisi kwa kazi za kawaida za uboreshaji
- **Msaada wa Mfumo Mbalimbali**: Inafanya kazi na PyTorch, mifano ya Hugging Face, na ONNX
- **Msaada wa Mifano Maarufu**: Olive inaweza kuboresha kiotomatiki usanifu wa mifano maarufu kama Llama, Phi, Qwen, Gemma, nk bila matatizo

### Faida

- **Kupunguza Muda wa Maendeleo**: Hakuna haja ya kujaribu mbinu tofauti za uboreshaji kwa mikono
- **Faida za Utendaji**: Uboreshaji mkubwa wa kasi (hadi mara 6 katika baadhi ya matukio)
- **Matumizi ya Majukwaa Mbalimbali**: Mifano iliyoboreshwa inafanya kazi kwenye vifaa na mifumo ya uendeshaji tofauti
- **Usahihi Uliodumishwa**: Uboreshaji unahifadhi ubora wa mfano huku ukiboresha utendaji

## Usakinishaji

### Mahitaji ya Awali

- Python 3.8 au zaidi
- Meneja wa kifurushi cha pip
- Mazingira ya kawaida (inapendekezwa)

### Usakinishaji wa Msingi

Unda na uwashe mazingira ya kawaida:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Sakinisha Olive na vipengele vya uboreshaji kiotomatiki:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Mahitaji ya Hiari

Olive inatoa mahitaji ya hiari kwa vipengele vya ziada:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Thibitisha Usakinishaji

```bash
olive --help
```

Ikiwa imefanikiwa, unapaswa kuona ujumbe wa msaada wa Olive CLI.

## Mwongozo wa Kuanza Haraka

### Uboreshaji Wako wa Kwanza

Hebu tuboreshe mfano mdogo wa lugha kwa kutumia kipengele cha uboreshaji kiotomatiki cha Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Kile Amri Hii Inafanya

Mchakato wa uboreshaji unahusisha: kupata mfano kutoka kwenye hifadhi ya ndani, kunasa Grafu ya ONNX na kuhifadhi uzito katika faili ya data ya ONNX, kuboresha Grafu ya ONNX, na kupunguza mfano hadi int4 kwa kutumia njia ya RTN.

### Maelezo ya Vigezo vya Amri

- `--model_name_or_path`: Kitambulisho cha mfano wa Hugging Face au njia ya ndani
- `--output_path`: Saraka ambapo mfano ulioboreshwa utaokolewa
- `--device`: Kifaa kinacholengwa (cpu, gpu)
- `--provider`: Mtoa huduma wa utekelezaji (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Tumia ONNX Runtime Generate AI kwa inferensi
- `--precision`: Usahihi wa upunguzaji (int4, int8, fp16)
- `--log_level`: Ukali wa uandishi wa kumbukumbu (0=minimal, 1=verbose)

## Mfano: Kubadilisha Qwen3 kuwa ONNX INT4

Kulingana na mfano uliotolewa wa Hugging Face katika [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), hivi ndivyo unavyoweza kuboresha mfano wa Qwen3:

### Hatua ya 1: Pakua Mfano (Hiari)

Ili kupunguza muda wa kupakua, hifadhi faili muhimu tu:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Hatua ya 2: Boresha Mfano wa Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hatua ya 3: Jaribu Mfano Ulioboreshwa

Unda hati rahisi ya Python ili kujaribu mfano wako ulioboreshwa:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Muundo wa Matokeo

Baada ya uboreshaji, saraka yako ya matokeo itakuwa na:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Matumizi ya Juu

### Faili za Usanidi

Kwa mchakato wa uboreshaji changamano zaidi, unaweza kutumia faili za usanidi za JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Endesha na usanidi:

```bash
olive run --config config.json
```

### Uboreshaji wa GPU

Kwa uboreshaji wa GPU ya CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Kwa DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Kuboresha na Olive

Olive pia inasaidia kuboresha mifano:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Mbinu Bora

### 1. Uchaguzi wa Mfano
- Anza na mifano midogo kwa majaribio (mfano, vigezo 0.5B-7B)
- Hakikisha usanifu wa mfano unaolengwa unasaidiwa na Olive

### 2. Mazingatio ya Vifaa
- Linganisha lengo lako la uboreshaji na vifaa vya matumizi
- Tumia uboreshaji wa GPU ikiwa una vifaa vinavyooana na CUDA
- Fikiria DirectML kwa mashine za Windows zenye picha zilizounganishwa

### 3. Uchaguzi wa Usahihi
- **INT4**: Ukandamizaji wa juu zaidi, upotevu mdogo wa usahihi
- **INT8**: Uwiano mzuri wa ukubwa na usahihi
- **FP16**: Upotevu mdogo wa usahihi, upunguzaji wa wastani wa ukubwa

### 4. Upimaji na Uthibitishaji
- Daima jaribu mifano iliyoboreshwa na matumizi yako maalum
- Linganisha vipimo vya utendaji (latency, throughput, usahihi)
- Tumia data ya pembejeo inayowakilisha kwa tathmini

### 5. Uboreshaji wa Mara kwa Mara
- Anza na uboreshaji kiotomatiki kwa matokeo ya haraka
- Tumia faili za usanidi kwa udhibiti wa kina
- Jaribu njia tofauti za uboreshaji

## Kutatua Tatizo

### Masuala ya Kawaida

#### 1. Matatizo ya Usakinishaji
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Masuala ya CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Masuala ya Kumbukumbu
- Tumia ukubwa mdogo wa kundi wakati wa uboreshaji
- Jaribu upunguzaji wa usahihi wa juu kwanza (int8 badala ya int4)
- Hakikisha nafasi ya diski ya kutosha kwa hifadhi ya mfano

#### 4. Hitilafu za Kupakia Mfano
- Thibitisha njia ya mfano na ruhusa za ufikiaji
- Angalia ikiwa mfano unahitaji `trust_remote_code=True`
- Hakikisha faili zote muhimu za mfano zimepakuliwa

### Kupata Msaada

- **Nyaraka**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Masuala ya GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Mifano**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Hifadhi ya Mapishi ya Olive

### Utangulizi wa Mapishi ya Olive

Hifadhi ya [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) inakamilisha zana kuu ya Olive kwa kutoa mkusanyiko wa kina wa mapishi ya uboreshaji yanayoweza kutumika kwa mifano maarufu ya AI. Hifadhi hii inatumika kama rejeleo la vitendo kwa kuboresha mifano inayopatikana hadharani na kuunda mchakato wa uboreshaji kwa mifano ya wamiliki.

### Vipengele Muhimu

- **Mapishi 100+ Yaliyotengenezwa Tayari**: Usanidi wa uboreshaji unaoweza kutumika kwa mifano maarufu
- **Msaada wa Usanifu Mbalimbali**: Inashughulikia mifano ya transformer, mifano ya maono, na usanifu wa multimodal
- **Uboreshaji Maalum wa Vifaa**: Mapishi yaliyotengenezwa kwa CPU, GPU, na viongeza kasi maalum
- **Familia za Mifano Maarufu**: Inajumuisha Phi, Llama, Qwen, Gemma, Mistral, na zaidi

### Familia za Mifano Zinazosaidiwa

Hifadhi inajumuisha mapishi ya uboreshaji kwa:

#### Mifano ya Lugha
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B hadi 14B)
- **Google Gemma**: Usanidi mbalimbali wa Gemma
- **Mistral AI**: Mistral-7B series
- **DeepSeek**: R1-Distill series models

#### Mifano ya Maono na Multimodal
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Mifano ya CLIP**: Usanidi mbalimbali wa CLIP-ViT
- **ResNet**: Uboreshaji wa ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Mifano Maalum
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Toleo la msingi na la lugha nyingi
- **Sentence Transformers**: all-MiniLM-L6-v2

### Kutumia Mapishi ya Olive

#### Njia ya 1: Nakili Mapishi Mahususi

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Njia ya 2: Tumia Mapishi kama Kiolezo

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Muundo wa Mapishi

Kila saraka ya mapishi kwa kawaida ina:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Mfano: Kutumia Mapishi ya Phi-4-mini

Hebu tutumie mfano wa Phi-4-mini:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Faili ya usanidi kwa kawaida inajumuisha:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Kubadilisha Mapishi

#### Kubadilisha Vifaa Vinavyolengwa

Ili kubadilisha vifaa vinavyolengwa, sasisha sehemu ya `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Kurekebisha Vigezo vya Uboreshaji

Badilisha sehemu ya `passes` kwa viwango tofauti vya uboreshaji:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Kuunda Mapishi Yako

1. **Anza na Mfano Sawa**: Tafuta mapishi ya mfano wenye usanifu sawa
2. **Sasisha Usanidi wa Mfano**: Badilisha jina/njia ya mfano katika usanidi
3. **Rekebisha Vigezo**: Badilisha vigezo vya uboreshaji kama inavyohitajika
4. **Jaribu na Thibitisha**: Endesha uboreshaji na thibitisha matokeo
5. **Changia Nyuma**: Fikiria kuchangia mapishi yako kwenye hifadhi

### Faida za Kutumia Mapishi

#### 1. **Usanidi Uliothibitishwa**
- Mipangilio ya uboreshaji iliyojaribiwa kwa mifano maalum
- Kuepuka majaribio na makosa katika kutafuta vigezo bora

#### 2. **Urekebishaji Maalum wa Vifaa**
- Uboreshaji wa awali kwa watoa huduma tofauti wa utekelezaji
- Usanidi unaoweza kutumika kwa CPU, GPU, na malengo ya NPU

#### 3. **Ushirikiano wa Kina**
- Inasaidia mifano maarufu ya chanzo huria
- Sasisho za mara kwa mara na matoleo mapya ya mifano

#### 4. **Michango ya Jamii**
- Maendeleo ya pamoja na jamii ya AI
- Maarifa ya pamoja na mbinu bora

### Kuchangia kwa Mapishi ya Olive

Ikiwa umeboresha mfano ambao haujashughulikiwa katika hifadhi:

1. **Nakili Hifadhi**: Unda nakala yako ya olive-recipes
2. **Unda Saraka ya Mapishi**: Ongeza saraka mpya kwa mfano wako
3. **Jumuisha Usanidi**: Ongeza olive_config.json na faili za kusaidia
4. **Andika Maelekezo**: Toa README wazi na maelekezo
5. **Tuma Ombi la Kuvuta**: Changia nyuma kwa jamii

### Vipimo vya Utendaji

Mapishi mengi yanajumuisha vipimo vya utendaji vinavyoonyesha:
- **Uboreshaji wa Latency**: Kasi ya kawaida mara 2-6 zaidi ya msingi
- **Upunguzaji wa Kumbukumbu**: Upunguzaji wa matumizi ya kumbukumbu kwa 50-75% na upunguzaji
- **Uhifadhi wa Usahihi**: Uhifadhi wa usahihi wa 95-99%

### Muunganisho na Zana za AI

Mapishi hufanya kazi bila matatizo na:
- **VS Code AI Toolkit**: Muunganisho wa moja kwa moja kwa uboreshaji wa mifano
- **Azure Machine Learning**: Michakato ya uboreshaji inayotegemea wingu
- **ONNX Runtime**: Utekelezaji wa inferensi ulioboreshwa

## Rasilimali za Ziada

### Viungo Rasmi
- **Hifadhi ya GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Hifadhi ya Mapishi ya Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Nyaraka za ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Mfano wa Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Mifano ya Jamii
- **Notebooks za Jupyter**: Zinapatikana katika hifadhi ya GitHub ya Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Kiendelezi cha VS Code**: Muhtasari wa AI Toolkit kwa VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Machapisho ya Blogu**: Blogu ya Microsoft Open Source — https://opensource.microsoft.com/blog/

### Zana Zinazohusiana
- **ONNX Runtime**: Injini ya inferensi yenye utendaji wa juu — https://onnxruntime.ai/
- **Hugging Face Transformers**: Chanzo cha mifano mingi inayooana — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Michakato ya uboreshaji inayotegemea wingu — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Nini kinachofuata

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma ya kutafsiri ya AI [Co-op Translator](https://github.com/Azure/co-op-translator). Ingawa tunajitahidi kwa usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo cha mamlaka. Kwa taarifa muhimu, tafsiri ya kitaalamu ya binadamu inapendekezwa. Hatutawajibika kwa kutoelewana au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.