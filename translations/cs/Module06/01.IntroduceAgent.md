<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T14:14:21+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "cs"
}
-->
# AI agenti a malé jazykové modely: Komplexní průvodce

## Úvod

V tomto tutoriálu se podíváme na AI agenty a malé jazykové modely (SLM) a jejich pokročilé implementační strategie pro prostředí edge computingu. Probereme základní koncepty agentní AI, optimalizační techniky SLM, praktické strategie nasazení pro zařízení s omezenými zdroji a Microsoft Agent Framework pro vytváření produkčně připravených agentních systémů.

Oblast umělé inteligence zažívá v roce 2025 paradigmatický posun. Zatímco rok 2023 byl rokem chatbotů a rok 2024 přinesl boom kopilotů, rok 2025 patří AI agentům — inteligentním systémům, které myslí, plánují, používají nástroje a vykonávají úkoly s minimálním lidským vstupem, stále více poháněné efektivními malými jazykovými modely. Microsoft Agent Framework se stává předním řešením pro vytváření těchto inteligentních systémů s offline schopnostmi na edge zařízeních.

## Cíle učení

Na konci tohoto tutoriálu budete schopni:

- 🤖 Porozumět základním konceptům AI agentů a agentních systémů
- 🔬 Identifikovat výhody malých jazykových modelů oproti velkým jazykovým modelům v agentních aplikacích
- 🚀 Naučit se pokročilé strategie nasazení SLM pro prostředí edge computingu
- 📱 Implementovat praktické agenty poháněné SLM pro reálné aplikace
- 🏗️ Vytvořit produkčně připravené agenty pomocí Microsoft Agent Framework
- 🌐 Nasadit offline agenty na edge zařízeních s integrací lokálních LLM a SLM
- 🔧 Integrovat Microsoft Agent Framework s Foundry Local pro nasazení na edge

## Porozumění AI agentům: Základy a klasifikace

### Definice a základní koncepty

Umělý inteligentní agent označuje systém nebo program, který je schopen autonomně vykonávat úkoly jménem uživatele nebo jiného systému tím, že navrhuje svůj pracovní postup a využívá dostupné nástroje. Na rozdíl od tradiční AI, která pouze odpovídá na vaše otázky, agent může jednat nezávisle na dosažení cílů.

### Rámec klasifikace agentů

Porozumění hranicím agentů pomáhá při výběru vhodných typů agentů pro různé scénáře výpočetní techniky:

- **🔬 Jednoduché reflexní agenty**: Systémy založené na pravidlech, které reagují na okamžité vnímání (termostaty, základní automatizace)
- **📱 Agenti založení na modelu**: Systémy, které udržují vnitřní stav a paměť (robotické vysavače, navigační systémy)
- **⚖️ Agenti založení na cílech**: Systémy, které plánují a provádějí sekvence k dosažení cílů (plánovače tras, plánovače úkolů)
- **🧠 Učící se agenti**: Adaptivní systémy, které se zlepšují v průběhu času (doporučovací systémy, personalizovaní asistenti)

### Klíčové výhody AI agentů

AI agenti nabízejí několik základních výhod, které je činí ideálními pro aplikace edge computingu:

**Operační autonomie**: Agenti poskytují nezávislé provádění úkolů bez neustálého lidského dohledu, což je ideální pro aplikace v reálném čase. Vyžadují minimální dohled při zachování adaptivního chování, což umožňuje nasazení na zařízeních s omezenými zdroji a snižuje provozní náklady.

**Flexibilita nasazení**: Tyto systémy umožňují AI schopnosti na zařízení bez požadavků na připojení k internetu, zvyšují soukromí a bezpečnost prostřednictvím lokálního zpracování, mohou být přizpůsobeny pro aplikace specifické pro danou oblast a jsou vhodné pro různá prostředí edge computingu.

**Nákladová efektivita**: Agentní systémy nabízejí nákladově efektivní nasazení ve srovnání s cloudovými řešeními, s nižšími provozními náklady a nižšími požadavky na šířku pásma pro aplikace na edge.

## Pokročilé strategie malých jazykových modelů

### Základy SLM (Small Language Model)

Malý jazykový model (SLM) je jazykový model, který se vejde na běžné spotřebitelské elektronické zařízení a provádí inference s latencí dostatečně nízkou, aby byl praktický při obsluze agentních požadavků jednoho uživatele. Prakticky vzato, SLM jsou obvykle modely s méně než 10 miliardami parametrů.

**Funkce objevování formátů**: SLM nabízejí pokročilou podporu pro různé úrovně kvantizace, kompatibilitu napříč platformami, optimalizaci výkonu v reálném čase a schopnosti nasazení na edge. Uživatelé mohou využívat zvýšené soukromí prostřednictvím lokálního zpracování a podporu WebGPU pro nasazení v prohlížeči.

**Sbírky úrovní kvantizace**: Populární formáty SLM zahrnují Q4_K_M pro vyváženou kompresi v mobilních aplikacích, Q5_K_S sérii pro nasazení zaměřené na kvalitu na edge, Q8_0 pro téměř původní přesnost na výkonných edge zařízeních a experimentální formáty jako Q2_K pro scénáře s ultra nízkými zdroji.

### GGUF (General GGML Universal Format) pro nasazení SLM

GGUF slouží jako primární formát pro nasazení kvantizovaných SLM na CPU a edge zařízeních, speciálně optimalizovaný pro agentní aplikace:

**Agentně optimalizované funkce**: Formát poskytuje komplexní zdroje pro konverzi a nasazení SLM s rozšířenou podporou pro volání nástrojů, generování strukturovaných výstupů a vícenásobné konverzace. Kompatibilita napříč platformami zajišťuje konzistentní chování agentů na různých edge zařízeních.

**Optimalizace výkonu**: GGUF umožňuje efektivní využití paměti pro pracovní postupy agentů, podporuje dynamické načítání modelů pro systémy s více agenty a poskytuje optimalizovanou inference pro interakce agentů v reálném čase.

### Edge-optimalizované rámce SLM

#### Optimalizace Llama.cpp pro agenty

Llama.cpp poskytuje špičkové kvantizační techniky speciálně optimalizované pro nasazení agentních SLM:

**Agentně specifická kvantizace**: Rámec podporuje Q4_0 (optimální pro mobilní nasazení agentů s 75% redukcí velikosti), Q5_1 (vyvážená kvalita-komprese pro inference agentů na edge) a Q8_0 (téměř původní kvalita pro produkční agentní systémy). Pokročilé formáty umožňují ultra-komprimované agenty pro extrémní edge scénáře.

**Výhody implementace**: Inference optimalizovaná pro CPU s akcelerací SIMD poskytuje paměťově efektivní provádění agentů. Kompatibilita napříč platformami na architekturách x86, ARM a Apple Silicon umožňuje univerzální schopnosti nasazení agentů.

#### Apple MLX Framework pro SLM agenty

Apple MLX poskytuje nativní optimalizaci speciálně navrženou pro agenty poháněné SLM na zařízeních Apple Silicon:

**Optimalizace agentů na Apple Silicon**: Rámec využívá sjednocenou paměťovou architekturu s integrací Metal Performance Shaders, automatickou smíšenou přesnost pro inference agentů a optimalizovanou šířku paměťového pásma pro systémy s více agenty. Agenti SLM vykazují výjimečný výkon na čipech řady M.

**Vývojové funkce**: Podpora API pro Python a Swift s optimalizacemi specifickými pro agenty, automatická diferenciace pro učení agentů a bezproblémová integrace s vývojovými nástroji Apple poskytují komplexní prostředí pro vývoj agentů.

#### ONNX Runtime pro agenty SLM napříč platformami

ONNX Runtime poskytuje univerzální inference engine, který umožňuje agentům SLM běžet konzistentně na různých hardwarových platformách a operačních systémech:

**Univerzální nasazení**: ONNX Runtime zajišťuje konzistentní chování agentů SLM napříč platformami Windows, Linux, macOS, iOS a Android. Tato kompatibilita napříč platformami umožňuje vývojářům psát jednou a nasadit všude, což výrazně snižuje náklady na vývoj a údržbu pro aplikace na více platformách.

**Možnosti hardwarové akcelerace**: Rámec poskytuje optimalizované poskytovatele exekuce pro různé hardwarové konfigurace včetně CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) a specializovaných akcelerátorů (Intel VPU, Qualcomm NPU). Agenti SLM mohou automaticky využívat nejlepší dostupný hardware bez změn kódu.

**Funkce připravené pro produkci**: ONNX Runtime nabízí funkce na podnikové úrovni nezbytné pro nasazení agentů v produkci, včetně optimalizace grafů pro rychlejší inference, správy paměti pro prostředí s omezenými zdroji a komplexních nástrojů pro profilování výkonu. Rámec podporuje API pro Python i C++ pro flexibilní integraci.

## SLM vs LLM v agentních systémech: Pokročilé srovnání

### Výhody SLM v agentních aplikacích

**Operační efektivita**: SLM poskytují 10-30× snížení nákladů ve srovnání s LLM pro agentní úkoly, umožňují agentní odpovědi v reálném čase ve velkém měřítku. Nabízejí rychlejší časy inference díky snížené výpočetní složitosti, což je činí ideálními pro interaktivní agentní aplikace.

**Schopnosti nasazení na edge**: SLM umožňují provádění agentů na zařízení bez závislosti na internetu, zvýšené soukromí prostřednictvím lokálního zpracování a přizpůsobení pro aplikace specifické pro danou oblast vhodné pro různá prostředí edge computingu.

**Optimalizace specifická pro agenty**: SLM vynikají při volání nástrojů, generování strukturovaných výstupů a rutinních pracovních postupech rozhodování, které tvoří 70-80% typických agentních úkolů.

### Kdy použít SLM vs LLM v agentních systémech

**Ideální pro SLM**:
- **Opakující se agentní úkoly**: Zadávání dat, vyplňování formulářů, rutinní API volání
- **Integrace nástrojů**: Dotazy na databáze, operace se soubory, interakce se systémem
- **Strukturované pracovní postupy**: Následování předdefinovaných procesů agentů
- **Agenti specifického oboru**: Zákaznický servis, plánování, základní analýza
- **Lokální zpracování**: Operace agentů citlivé na soukromí

**Lepší pro LLM**:
- **Komplexní uvažování**: Nové řešení problémů, strategické plánování
- **Otevřené konverzace**: Obecný chat, kreativní diskuse
- **Úkoly s širokými znalostmi**: Výzkum vyžadující rozsáhlé obecné znalosti
- **Nové situace**: Řešení zcela nových scénářů agentů

### Hybridní architektura agentů

Optimální přístup kombinuje SLM a LLM v heterogenních agentních systémech:

**Chytrá orchestrace agentů**:
1. **SLM jako primární**: Zpracování 70-80% rutinních agentních úkolů lokálně
2. **LLM podle potřeby**: Směrování složitých dotazů na cloudové větší modely
3. **Specializované SLM**: Různé malé modely pro různé domény agentů
4. **Optimalizace nákladů**: Minimalizace drahých volání LLM prostřednictvím inteligentního směrování

## Strategie nasazení produkčních SLM agentů

### Foundry Local: Edge AI runtime na podnikové úrovni

Foundry Local (https://github.com/microsoft/foundry-local) slouží jako vlajkové řešení Microsoftu pro nasazení malých jazykových modelů v produkčních edge prostředích. Poskytuje kompletní runtime prostředí speciálně navržené pro agenty poháněné SLM s funkcemi na podnikové úrovni a bezproblémovými integračními schopnostmi.

**Základní architektura a funkce**:
- **Kompatibilní API s OpenAI**: Plná kompatibilita s OpenAI SDK a integracemi Agent Framework
- **Automatická optimalizace hardwaru**: Inteligentní výběr variant modelů na základě dostupného hardwaru (CUDA GPU, Qualcomm NPU, CPU)
- **Správa modelů**: Automatické stahování, ukládání do mezipaměti a správa životního cyklu modelů SLM
- **Objevování služeb**: Detekce služeb bez konfigurace pro agentní rámce
- **Optimalizace zdrojů**: Inteligentní správa paměti a energetická účinnost pro nasazení na edge

#### Instalace a nastavení

**Instalace napříč platformami**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Rychlý start pro vývoj agentů**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integrace Agent Framework

**Integrace Foundry Local SDK**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automatický výběr modelů a optimalizace hardwaru**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Produkční vzory nasazení

**Produkční nastavení jednoho agenta**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orchestrace produkce více agentů**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Funkce na podnikové úrovni a monitorování

**Monitorování zdraví a pozorovatelnost**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Správa zdrojů a automatické škálování**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Pokročilá konfigurace a optimalizace

**Vlastní konfigurace modelu**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Kontrolní seznam pro produkční nasazení**:

✅ **Konfigurace služby**:
- Nakonfigurujte vhodné aliasy modelů pro případy použití
- Nastavte limity zdrojů a prahové hodnoty monitorování
- Aktivujte kontroly zdraví a sběr metrik
- Nakonfigurujte automatické restartování a záložní režim

✅ **Nastavení zabezpečení**:
- Aktivujte přístup k API pouze lokálně (bez externího přístupu)
- Nakonfigurujte správu klíčů API
- Nastavte auditní logování interakcí agentů
- Implementujte omezení rychlosti pro produkční použití

✅ **Optimalizace výkonu**:
- Testujte výkon modelu při očekávané zátěži
- Nakonfigurujte vhodné úrovně kvantizace
- Nastavte strategie ukládání modelů do mezipaměti a zahřívání
- Monitorujte vzory využití paměti a CPU

✅ **Testování integrace**:
- Testujte integraci agentního rámce
- Ověřte schopnosti offline provozu
- Testujte scénáře záložního režimu a obnovy
- Validujte pracovní postupy agentů od začátku do konce

### Ollama: Zjednodušené nasazení agentů SLM

### Ollama: Nasazení agentů SLM zaměřené na komunitu

Ollama poskytuje přístup zaměřený na komunitu pro nasazení agentů SLM s důrazem na jednoduchost, rozsáhlý ekosystém modelů a uživatelsky přívětivé pracovní postupy. Zatímco Foundry Local se zaměřuje na funkce na podnikové úrovni, Ollama vyniká v rychlém prototypování, přístupu k modelům od komunity a zjednodušených scénářích nasazení.

**Základní architektura a funkce**:
- **Kompatibilní API s OpenAI**: Plná kompatibilita REST API pro bezproblémovou integraci agentního rámce
- **Rozsáhlá knihovna modelů**: Přístup ke stovkám modelů od komunity i oficiálních modelů
- **
- Testování integrace Microsoft Agent Framework
- Ověření schopností offline provozu
- Testování scénářů selhání a zpracování chyb
- Validace kompletních pracovních postupů agentů

**Srovnání s Foundry Local**:

| Funkce | Foundry Local | Ollama |
|--------|---------------|--------|
| **Cílové použití** | Podniková produkce | Vývoj a komunita |
| **Ekosystém modelů** | Kurátorováno Microsoftem | Rozsáhlá komunita |
| **Optimalizace hardwaru** | Automatická (CUDA/NPU/CPU) | Manuální konfigurace |
| **Podnikové funkce** | Vestavěné monitorování, bezpečnost | Nástroje komunity |
| **Složitost nasazení** | Jednoduché (winget install) | Jednoduché (curl install) |
| **Kompatibilita API** | OpenAI + rozšíření | Standard OpenAI |
| **Podpora** | Oficiální podpora Microsoftu | Řízeno komunitou |
| **Nejlepší pro** | Produkční agenti | Prototypování, výzkum |

**Kdy zvolit Ollama**:
- **Vývoj a prototypování**: Rychlé experimentování s různými modely
- **Komunitní modely**: Přístup k nejnovějším modelům od komunity
- **Vzdělávací využití**: Učení a výuka vývoje AI agentů
- **Výzkumné projekty**: Akademický výzkum vyžadující přístup k různým modelům
- **Vlastní modely**: Vytváření a testování vlastních modelů s jemným doladěním

### VLLM: Vysoce výkonná inference SLM agentů

VLLM (Inference velmi velkých jazykových modelů) poskytuje vysoce výkonný, paměťově efektivní inference engine, optimalizovaný pro produkční nasazení SLM ve velkém měřítku. Zatímco Foundry Local se zaměřuje na snadné použití a Ollama na komunitní modely, VLLM vyniká v scénářích vyžadujících maximální propustnost a efektivní využití zdrojů.

**Základní architektura a funkce**:
- **PagedAttention**: Revoluční správa paměti pro efektivní výpočet pozornosti
- **Dynamické dávkování**: Inteligentní dávkování požadavků pro optimální propustnost
- **Optimalizace GPU**: Pokročilé CUDA jádra a podpora paralelismu tensorů
- **Kompatibilita s OpenAI**: Plná kompatibilita API pro bezproblémovou integraci
- **Spekulativní dekódování**: Pokročilé techniky zrychlení inference
- **Podpora kvantizace**: Kvantizace INT4, INT8 a FP16 pro efektivní využití paměti

#### Instalace a nastavení

**Možnosti instalace**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**Rychlý start pro vývoj agentů**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integrace Agent Framework

**VLLM s Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**Nastavení více agentů s vysokou propustností**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### Produkční vzory nasazení

**Podniková služba VLLM**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### Podnikové funkce a monitorování

**Pokročilé monitorování výkonu VLLM**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### Pokročilá konfigurace a optimalizace

**Šablony konfigurace produkčního VLLM**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**Kontrolní seznam pro produkční nasazení VLLM**:

✅ **Optimalizace hardwaru**:
- Konfigurace paralelismu tensorů pro nastavení více GPU
- Aktivace kvantizace (AWQ/GPTQ) pro efektivní využití paměti
- Nastavení optimálního využití paměti GPU (85-95 %)
- Konfigurace vhodných velikostí dávky pro propustnost

✅ **Ladění výkonu**:
- Aktivace prefixového ukládání do mezipaměti pro opakované dotazy
- Konfigurace segmentovaného předvyplnění pro dlouhé sekvence
- Nastavení spekulativního dekódování pro rychlejší inference
- Optimalizace max_num_seqs na základě hardwaru

✅ **Produkční funkce**:
- Nastavení monitorování stavu a sběru metrik
- Konfigurace automatického restartu a přepnutí při selhání
- Implementace front požadavků a vyvažování zátěže
- Nastavení komplexního logování a upozornění

✅ **Bezpečnost a spolehlivost**:
- Konfigurace pravidel firewallu a kontrol přístupu
- Nastavení omezení rychlosti API a autentizace
- Implementace plynulého vypnutí a vyčištění
- Konfigurace zálohování a obnovy po havárii

✅ **Testování integrace**:
- Testování integrace Microsoft Agent Framework
- Validace scénářů s vysokou propustností
- Testování postupů při selhání a obnově
- Benchmarking výkonu při zátěži

**Srovnání s jinými řešeními**:

| Funkce | VLLM | Foundry Local | Ollama |
|--------|------|---------------|--------|
| **Cílové použití** | Produkce s vysokou propustností | Snadné použití v podniku | Vývoj a komunita |
| **Výkon** | Maximální propustnost | Vyvážený | Dobrý |
| **Efektivita paměti** | Optimalizace PagedAttention | Automatická optimalizace | Standardní |
| **Složitost nastavení** | Vysoká (mnoho parametrů) | Nízká (automatická) | Nízká (jednoduchá) |
| **Škálovatelnost** | Výborná (tensor/pipeline paralelismus) | Dobrá | Omezená |
| **Kvantizace** | Pokročilá (AWQ, GPTQ, FP8) | Automatická | Standardní GGUF |
| **Podnikové funkce** | Nutná vlastní implementace | Vestavěné | Nástroje komunity |
| **Nejlepší pro** | Produkční agenti ve velkém měřítku | Podniková produkce | Vývoj |

**Kdy zvolit VLLM**:
- **Požadavky na vysokou propustnost**: Zpracování stovek požadavků za sekundu
- **Nasazení ve velkém měřítku**: Multi-GPU, multi-node nasazení
- **Kritický výkon**: Odezvy pod sekundu ve velkém měřítku
- **Pokročilá optimalizace**: Potřeba vlastní kvantizace a dávkování
- **Efektivita zdrojů**: Maximální využití drahého GPU hardwaru

## Reálné aplikace SLM agentů

### SLM agenti pro zákaznický servis
- **Schopnosti SLM**: Vyhledávání účtů, resetování hesel, kontrola stavu objednávek
- **Úspory nákladů**: 10x snížení nákladů na inference ve srovnání s LLM agenty
- **Výkon**: Rychlejší odezvy s konzistentní kvalitou pro rutinní dotazy

### SLM agenti pro obchodní procesy
- **Agenti pro zpracování faktur**: Extrakce dat, validace informací, směrování k schválení
- **Agenti pro správu e-mailů**: Kategorizace, prioritizace, automatické návrhy odpovědí
- **Agenti pro plánování**: Koordinace schůzek, správa kalendářů, zasílání připomínek

### Osobní digitální asistenti SLM
- **Agenti pro správu úkolů**: Vytváření, aktualizace, organizace seznamů úkolů
- **Agenti pro sběr informací**: Výzkum témat, lokální shrnutí zjištění
- **Agenti pro komunikaci**: Návrhy e-mailů, zpráv, příspěvků na sociální sítě

### SLM agenti pro obchodování a finance
- **Agenti pro sledování trhu**: Sledování cen, identifikace trendů v reálném čase
- **Agenti pro generování zpráv**: Automatické vytváření denních/týdenních přehledů
- **Agenti pro hodnocení rizik**: Posouzení portfoliových pozic pomocí lokálních dat

### SLM agenti pro podporu zdravotnictví
- **Agenti pro plánování pacientů**: Koordinace schůzek, automatické připomínky
- **Agenti pro dokumentaci**: Lokální generování lékařských souhrnů, zpráv
- **Agenti pro správu receptů**: Sledování doplnění, kontrola interakcí

## Microsoft Agent Framework: Vývoj produkčních agentů

### Přehled a architektura

Microsoft Agent Framework poskytuje komplexní, podnikové řešení pro vytváření, nasazení a správu AI agentů, kteří mohou fungovat jak v cloudu, tak v offline prostředí na okraji sítě. Framework je navržen tak, aby bezproblémově pracoval s malými jazykovými modely a scénáři edge computingu, což ho činí ideálním pro nasazení citlivá na soukromí a omezené zdroje.

**Základní komponenty frameworku**:
- **Agent Runtime**: Lehký výkonný prostředí optimalizované pro edge zařízení
- **Systém integrace nástrojů**: Rozšiřitelná architektura pluginů pro připojení externích služeb a API
- **Správa stavu**: Trvalá paměť agenta a správa kontextu mezi relacemi
- **Bezpečnostní vrstva**: Vestavěné bezpečnostní kontroly pro podnikové nasazení
- **Orchestrační engine**: Koordinace více agentů a správa pracovních postupů

### Klíčové funkce pro nasazení na okraji

**Offline-First Architektura**: Microsoft Agent Framework je navržen s principy offline-first, což umožňuje agentům efektivně fungovat bez neustálého připojení k internetu. To zahrnuje lokální inference modelů, uložené znalostní báze, offline provádění nástrojů a plynulé degradace při nedostupnosti cloudových služeb.

**Optimalizace zdrojů**: Framework poskytuje inteligentní správu zdrojů s automatickou optimalizací paměti pro SLM, vyvažování zátěže CPU/GPU pro edge zařízení, adaptivní výběr modelů na základě dostupných zdrojů a energeticky efektivní vzory inference pro mobilní nasazení.

**Bezpečnost a soukromí**: Podnikové bezpečnostní funkce zahrnují lokální zpracování dat pro zachování soukromí, šifrované komunikační kanály agentů, řízení přístupu na základě rolí pro schopnosti agentů a auditní logování pro požadavky na shodu.

### Integrace s Foundry Local

Microsoft Agent Framework se bezproblémově integruje s Foundry Local a poskytuje kompletní řešení pro edge AI:

**Automatické vyhledávání modelů**: Framework automaticky detekuje a připojuje se k instancím Foundry Local, vyhledává dostupné SLM modely a vybírá optimální modely na základě požadavků agentů a schopností hardwaru.

**Dynamické načítání modelů**: Agenti mohou dynamicky načítat různé SLM modely pro specifické úkoly, což umožňuje systémy více modelů, kde různé modely zpracovávají různé typy požadavků, a automatické přepnutí mezi modely na základě dostupnosti a výkonu.

**Optimalizace výkonu**: Integrované mechanismy ukládání do mezipaměti snižují časy načítání modelů, pooling připojení optimalizuje API volání na Foundry Local a inteligentní dávkování zlepšuje propustnost pro více požadavků agentů.

### Vytváření agentů s Microsoft Agent Framework

#### Definice a konfigurace agenta

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integrace nástrojů pro scénáře na okraji

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orchestrace více agentů

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### Pokročilé vzory nasazení na okraji

#### Hierarchická architektura agentů

**Lokální klastery agentů**: Nasazení více specializovaných SLM agentů na edge zařízení, každý optimalizovaný pro specifické úkoly. Použití lehkých modelů jako Qwen2.5-0.5B pro jednoduché směrování a plánování, středních modelů jako Phi-4-Mini pro zákaznický servis a dokumentaci, a větších modelů pro složité uvažování, pokud to zdroje umožňují.

**Koordinace edge-to-cloud**: Implementace inteligentních eskalačních vzorů, kde lokální agenti zpracovávají rutinní úkoly, cloudoví agenti poskytují složité uvažování, pokud to připojení umožňuje, a plynulé předávání mezi edge a cloudovým zpracováním zachovává kontinuitu.

#### Konfigurace nasazení

**Nasazení na jednom zařízení**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**Distribuované nasazení na okraji**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Optimalizace výkonu pro agenty na okraji

#### Strategie výběru modelů

**Přiřazení modelů na základě úkolů**: Microsoft Agent Framework umožňuje inteligentní výběr modelů na základě složitosti úkolu a požadavků:

- **Jednoduché úkoly** (Q&A, směrování): Qwen2.5-0.5B (500MB, <100ms odezva)
- **Středně složité úkoly** (zákaznický servis, plánování): Phi-4-Mini (2.4GB, 200-500ms odezva)
- **Složité úkoly** (technická analýza, plánování): Phi-4 (7GB, 1-3s odezva, pokud to zdroje umožňují)

**Dynamické přepínání modelů**: Agenti mohou přepínat mezi modely na základě aktuálního zatížení systému, hodnocení složitosti úkolu, priorit uživatele a dostupných hardwarových zdrojů.

#### Správa paměti a zdrojů

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### Podnikové vzory integrace

#### Bezpečnost a shoda

**Lokální zpracování dat**: Veškeré zpracování agentů probíhá lokálně, což zajišťuje, že citlivá data nikdy neopustí edge zařízení. To zahrnuje ochranu informací o zákaznících, shodu s HIPAA pro zdravotnické agenty, bezpečnost finančních dat pro bankovní agenty a shodu s GDPR pro evropská nasazení.

**Kontrola přístupu**: Oprávnění na základě rolí kontrolují, ke kterým nástrojům mají agenti přístup, autentizace uživatelů pro interakce s agenty a auditní stopy pro všechny akce a rozhodnutí agentů.

#### Monitorování a pozorovatelnost

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### Reálné příklady implementace

#### Systém agentů pro maloobchod na okraji

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Agent pro podporu zdravotnictví

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### Nejlepší postupy pro Microsoft Agent Framework

#### Pokyny pro vývoj

1. **Začněte jednoduše**: Začněte scénáři s jedním agentem před budováním komplexních systémů více agentů
2. **Správná velikost modelu**: Vyberte nejmenší model, který splňuje vaše požadavky na přesnost
3. **Návrh nástrojů**: Vytvářejte zaměřené, jednoúčelové nástroje místo složitých multifunkčních nástrojů
4. **Zpracování chyb**: Implementujte plynulou degradaci pro offline scénáře a selhání modelů
5. **Testování**: Testujte agenty důkladně v offline podmínkách a prostředích s omezenými zdroji

#### Nejlepší postupy pro nasazení

1. **Postupné zavádění**: Nasazujte nejprve malým skupinám uživatelů, pečlivě sledujte metriky výkonu
2. **Monitorování zdrojů**: Nastavte upozornění na limity paměti, CPU a doby odezvy
3. **Strategie zálohování**: Vždy mějte záložní plány pro selhání modelů nebo vyčerpání zdrojů
4. **Bezpečnost na prvním místě**: Implementujte bezpečnostní kontroly od začátku, ne jako dodatečné opatření
5. **Dokumentace**: Udržujte jasnou dokumentaci schopností a omezení agentů

### Budoucí plány a integrace
**Výběr frameworku pro nasazení agentů**: Vyberte optimalizační frameworky podle cílového hardwaru a požadavků agentů. Použijte Llama.cpp pro nasazení agentů optimalizovaných pro CPU, Apple MLX pro aplikace agentů na Apple Silicon a ONNX pro kompatibilitu agentů napříč platformami.

## Praktická konverze SLM agentů a příklady použití

### Scénáře nasazení agentů v reálném světě

**Mobilní aplikace agentů**: Formáty Q4_K vynikají v aplikacích pro chytré telefony díky minimálnímu využití paměti, zatímco Q8_0 poskytuje vyvážený výkon pro systémy agentů na tabletech. Formáty Q5_K nabízejí špičkovou kvalitu pro mobilní produktivní agenty.

**Agentové výpočty na desktopu a na okraji sítě**: Q5_K zajišťuje optimální výkon pro aplikace agentů na desktopu, Q8_0 poskytuje vysoce kvalitní inferenci pro pracovní stanice a Q4_K umožňuje efektivní zpracování na zařízeních na okraji sítě.

**Výzkumní a experimentální agenti**: Pokročilé kvantizační formáty umožňují zkoumání ultra-nízké přesnosti inferencí agentů pro akademický výzkum a aplikace proof-of-concept vyžadující extrémní omezení zdrojů.

### Výkonnostní benchmarky SLM agentů

**Rychlost inferencí agentů**: Q4_K dosahuje nejrychlejších odezev agentů na mobilních CPU, Q5_K poskytuje vyvážený poměr rychlosti a kvality pro obecné aplikace agentů, Q8_0 nabízí špičkovou kvalitu pro složité úkoly agentů a experimentální formáty zajišťují maximální propustnost pro specializovaný hardware agentů.

**Požadavky na paměť agentů**: Úrovně kvantizace pro agenty se pohybují od Q2_K (méně než 500 MB pro malé modely agentů) po Q8_0 (přibližně 50 % původní velikosti), přičemž experimentální konfigurace dosahují maximální komprese pro prostředí s omezenými zdroji.

## Výzvy a úvahy pro SLM agenty

### Kompromisy výkonu v systémech agentů

Nasazení SLM agentů vyžaduje pečlivé zvážení kompromisů mezi velikostí modelu, rychlostí odezvy agentů a kvalitou výstupu. Zatímco Q4_K nabízí výjimečnou rychlost a efektivitu pro mobilní agenty, Q8_0 poskytuje špičkovou kvalitu pro složité úkoly agentů. Q5_K představuje střední cestu vhodnou pro většinu obecných aplikací agentů.

### Kompatibilita hardwaru pro SLM agenty

Různá zařízení na okraji sítě mají různé schopnosti pro nasazení SLM agentů. Q4_K běží efektivně na základních procesorech pro jednoduché agenty, Q5_K vyžaduje střední výpočetní zdroje pro vyvážený výkon agentů a Q8_0 těží z vysoce výkonného hardwaru pro pokročilé schopnosti agentů.

### Bezpečnost a ochrana soukromí v systémech SLM agentů

Zatímco SLM agenti umožňují lokální zpracování pro zvýšenou ochranu soukromí, je nutné implementovat správná bezpečnostní opatření k ochraně modelů agentů a dat v prostředích na okraji sítě. To je obzvláště důležité při nasazení vysoce přesných formátů agentů v podnikových prostředích nebo komprimovaných formátů agentů v aplikacích, které zpracovávají citlivá data.

## Budoucí trendy ve vývoji SLM agentů

Prostředí SLM agentů se neustále vyvíjí díky pokrokům v kompresních technikách, metodách optimalizace a strategiích nasazení na okraji sítě. Budoucí vývoj zahrnuje efektivnější algoritmy kvantizace pro modely agentů, vylepšené metody komprese pro pracovní postupy agentů a lepší integraci s hardwarovými akcelerátory na okraji sítě pro zpracování agentů.

**Predikce trhu pro SLM agenty**: Podle nedávného výzkumu by automatizace poháněná agenty mohla do roku 2027 eliminovat 40–60 % opakujících se kognitivních úkolů v podnikových pracovních postupech, přičemž SLM budou vést tuto transformaci díky své nákladové efektivitě a flexibilitě nasazení.

**Technologické trendy v SLM agentech**:
- **Specializovaní SLM agenti**: Modely zaměřené na konkrétní úkoly agentů a odvětví
- **Výpočty agentů na okraji sítě**: Vylepšené schopnosti agentů na zařízení s lepší ochranou soukromí a sníženou latencí
- **Orchestrace agentů**: Lepší koordinace mezi více SLM agenty s dynamickým směrováním a vyvažováním zátěže
- **Demokratizace**: Flexibilita SLM umožňuje širší zapojení do vývoje agentů napříč organizacemi

## Začínáme s SLM agenty

### Krok 1: Nastavení prostředí Microsoft Agent Framework

**Instalace závislostí**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicializace Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Krok 2: Výběr SLM pro aplikace agentů
Oblíbené možnosti pro Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Skvělý pro obecné úkoly agentů s vyváženým výkonem
- **Qwen2.5-0.5B (0.5B)**: Ultra-efektivní pro jednoduché agenty zaměřené na směrování a klasifikaci
- **Qwen2.5-Coder-0.5B (0.5B)**: Specializovaný na úkoly agentů související s kódem
- **Phi-4 (7B)**: Pokročilé uvažování pro složité scénáře na okraji sítě, pokud to zdroje umožňují

### Krok 3: Vytvoření prvního agenta s Microsoft Agent Framework

**Základní nastavení agenta**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Krok 4: Definování rozsahu a požadavků agenta
Začněte s cílenými, dobře definovanými aplikacemi agentů pomocí Microsoft Agent Framework:
- **Agenti pro jeden obor**: Zákaznický servis NEBO plánování NEBO výzkum
- **Jasné cíle agenta**: Specifické, měřitelné cíle pro výkon agenta
- **Omezená integrace nástrojů**: Maximálně 3–5 nástrojů pro počáteční nasazení agenta
- **Definované hranice agenta**: Jasné cesty eskalace pro složité scénáře
- **Design zaměřený na okraj sítě**: Prioritizace offline funkcionality a lokálního zpracování

### Krok 5: Implementace nasazení na okraji sítě s Microsoft Agent Framework

**Konfigurace zdrojů**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Nasazení bezpečnostních opatření pro agenty na okraji sítě**:
- **Lokální validace vstupů**: Kontrola požadavků bez závislosti na cloudu
- **Offline filtrování výstupů**: Zajištění kvality odpovědí lokálně
- **Bezpečnostní kontroly na okraji sítě**: Implementace bezpečnostních opatření bez nutnosti připojení k internetu
- **Lokální monitorování**: Sledování výkonu a označování problémů pomocí telemetrie na okraji sítě

### Krok 6: Měření a optimalizace výkonu agentů na okraji sítě
- **Míra dokončení úkolů agenta**: Sledování úspěšnosti v offline scénářích
- **Časy odezvy agenta**: Zajištění odezvy pod jednu sekundu pro nasazení na okraji sítě
- **Využití zdrojů**: Sledování paměti, CPU a spotřeby baterie na zařízeních na okraji sítě
- **Nákladová efektivita**: Porovnání nákladů na nasazení na okraji sítě s alternativami založenými na cloudu
- **Spolehlivost offline**: Měření výkonu agenta během výpadků sítě

## Klíčové poznatky pro implementaci SLM agentů

1. **SLM jsou dostatečné pro agenty**: Pro většinu úkolů agentů malé modely fungují stejně dobře jako velké, přičemž nabízejí významné výhody
2. **Nákladová efektivita agentů**: 10–30x levnější provoz SLM agentů, což je činí ekonomicky životaschopnými pro široké nasazení
3. **Specializace funguje pro agenty**: Jemně doladěné SLM často překonávají obecné LLM v konkrétních aplikacích agentů
4. **Hybridní architektura agentů**: Používejte SLM pro rutinní úkoly agentů, LLM pro složité uvažování, když je to nutné
5. **Microsoft Agent Framework umožňuje produkční nasazení**: Poskytuje nástroje na podnikové úrovni pro vytváření, nasazení a správu agentů na okraji sítě
6. **Principy designu zaměřené na okraj sítě**: Agenti schopní offline zpracování s lokálním zpracováním zajišťují ochranu soukromí a spolehlivost
7. **Integrace Foundry Local**: Bezproblémové propojení mezi Microsoft Agent Framework a lokální inferencí modelů
8. **Budoucnost patří SLM agentům**: Malé jazykové modely s produkčními frameworky jsou budoucností agentické AI, umožňující demokratizované a efektivní nasazení agentů

## Odkazy a další čtení

### Základní výzkumné práce a publikace

#### AI agenti a agentické systémy
- **"Language Agents as Optimizable Graphs"** (2024) - Základní výzkum architektury agentů a optimalizačních strategií
  - Autoři: Wenyue Hua, Lishan Yang, et al.
  - Odkaz: https://arxiv.org/abs/2402.16823
  - Klíčové poznatky: Návrh agentů založený na grafech a optimalizační strategie

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Autoři: Zhiheng Xi, Wenxiang Chen, et al.
  - Odkaz: https://arxiv.org/abs/2309.07864
  - Klíčové poznatky: Komplexní přehled schopností a aplikací agentů založených na LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Autoři: Theodore Sumers, Shunyu Yao, et al.
  - Odkaz: https://arxiv.org/abs/2309.02427
  - Klíčové poznatky: Kognitivní rámce pro návrh inteligentních agentů

#### Malé jazykové modely a optimalizace
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Autoři: Microsoft Research Team
  - Odkaz: https://arxiv.org/abs/2404.14219
  - Klíčové poznatky: Principy návrhu SLM a strategie mobilního nasazení

- **"Qwen2.5 Technical Report"** (2024)
  - Autoři: Alibaba Cloud Team
  - Odkaz: https://arxiv.org/abs/2407.10671
  - Klíčové poznatky: Pokročilé techniky tréninku SLM a optimalizace výkonu

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Autoři: Peiyuan Zhang, Guangtao Zeng, et al.
  - Odkaz: https://arxiv.org/abs/2401.02385
  - Klíčové poznatky: Ultra-kompaktní návrh modelu a efektivita tréninku

### Oficiální dokumentace a frameworky

#### Microsoft Agent Framework
- **Oficiální dokumentace**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub Repository**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Primární repozitář**: https://github.com/microsoft/foundry-local
- **Dokumentace**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Hlavní repozitář**: https://github.com/vllm-project/vllm
- **Dokumentace**: https://docs.vllm.ai/


#### Ollama
- **Oficiální web**: https://ollama.ai/
- **GitHub Repository**: https://github.com/ollama/ollama

### Frameworky pro optimalizaci modelů

#### Llama.cpp
- **Repozitář**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentace**: https://microsoft.github.io/Olive/
- **GitHub Repository**: https://github.com/microsoft/Olive

#### OpenVINO
- **Oficiální stránka**: https://docs.openvino.ai/

#### Apple MLX
- **Repozitář**: https://github.com/ml-explore/mlx

### Průmyslové zprávy a analýzy trhu

#### Výzkum trhu AI agentů
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Odkaz: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Klíčové poznatky: Trendy trhu a vzorce adopce v podnicích

#### Technické benchmarky

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Odkaz: https://mlcommons.org/en/inference-edge/
  - Klíčové poznatky: Standardizované metriky výkonu pro nasazení na okraji sítě

### Standardy a specifikace

#### Formáty modelů a standardy
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Formát modelu napříč platformami pro interoperabilitu
- **GGUF Specification**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantizovaný formát modelu pro inferenci na CPU
- **OpenAI API Specification**: https://platform.openai.com/docs/api-reference
  - Standardní formát API pro integraci jazykových modelů

#### Bezpečnost a shoda
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: Rámec pro AI systémy a bezpečnost
- **IEEE Standards for AI**: https://standards.ieee.org/industry-connections/ai/

Posun směrem k agentům poháněným SLM představuje zásadní změnu v přístupu k nasazení AI. Microsoft Agent Framework, kombinovaný s lokálními platformami a efektivními malými jazykovými modely, poskytuje kompletní řešení pro vytváření produkčně připravených agentů, kteří efektivně fungují v prostředích na okraji sítě. Zaměřením na efektivitu, specializaci a praktickou užitečnost činí tento technologický stack AI agenty dostupnějšími, cenově výhodnějšími a efektivnějšími pro aplikace v reálném světě napříč všemi odvětvími a prostředími výpočtů na okraji sítě.

Jak postupujeme do roku 2025, kombinace stále schopnějších malých modelů, sofistikovaných frameworků pro agenty, jako je Microsoft Agent Framework, a robustních platforem pro nasazení na okraji sítě odemkne nové možnosti pro autonomní systémy, které mohou efektivně fungovat na zařízeních na okraji sítě při zachování soukromí, snížení nákladů a poskytování výjimečných uživatelských zkušeností.

**Další kroky pro implementaci**:
1. **Prozkoumejte volání funkcí**: Naučte se, jak SLM zpracovávají integraci nástrojů a strukturované výstupy
2. **Ovládněte Model Context Protocol (MCP)**: Pochopte pokročilé komunikační vzory agentů
3. **Vytvořte produkční agenty**: Použij

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby AI pro překlady [Co-op Translator](https://github.com/Azure/co-op-translator). I když se snažíme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho rodném jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.