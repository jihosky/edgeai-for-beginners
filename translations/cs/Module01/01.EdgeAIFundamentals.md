<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T10:04:32+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "cs"
}
-->
# Sekce 1: Základy EdgeAI

EdgeAI představuje zásadní změnu v nasazení umělé inteligence, přinášející schopnosti AI přímo na koncová zařízení, místo spoléhání se pouze na zpracování v cloudu. Je důležité pochopit, jak EdgeAI umožňuje lokální zpracování AI na zařízeních s omezenými zdroji, přičemž zachovává přiměřený výkon a řeší výzvy jako soukromí, latence a offline schopnosti.

## Úvod

V této lekci se budeme zabývat EdgeAI a jejími základními koncepty. Probereme tradiční paradigma výpočetní AI, výzvy spojené s edge computingem, klíčové technologie umožňující EdgeAI a praktické aplikace napříč různými odvětvími.

## Cíle učení

Na konci této lekce budete schopni:

- Porozumět rozdílu mezi tradičním přístupem AI založeným na cloudu a EdgeAI.
- Identifikovat klíčové technologie umožňující zpracování AI na koncových zařízeních.
- Rozpoznat výhody a omezení implementací EdgeAI.
- Aplikovat znalosti EdgeAI na reálné scénáře a případy použití.

## Porozumění tradičnímu paradigmatu výpočetní AI

Tradičně se generativní AI aplikace spoléhají na vysoce výkonnou výpočetní infrastrukturu, aby efektivně provozovaly velké jazykové modely (LLMs). Organizace obvykle nasazují tyto modely na GPU clusterech v cloudových prostředích a přistupují k jejich schopnostem prostřednictvím API rozhraní.

Tento centralizovaný model funguje dobře pro mnoho aplikací, ale má inherentní omezení v případě scénářů edge computingu. Tradiční přístup zahrnuje odesílání uživatelských dotazů na vzdálené servery, jejich zpracování pomocí výkonného hardwaru a vracení výsledků přes internet. Zatímco tato metoda poskytuje přístup k nejmodernějším modelům, vytváří závislosti na internetovém připojení, přináší obavy z latence a vyvolává otázky ohledně soukromí při přenosu citlivých dat na externí servery.

Existují některé základní koncepty, které je třeba pochopit při práci s tradičními paradigmaty výpočetní AI, konkrétně:

- **☁️ Zpracování v cloudu**: AI modely běží na výkonné serverové infrastruktuře s vysokými výpočetními zdroji.
- **🔌 Přístup přes API**: Aplikace přistupují k schopnostem AI prostřednictvím vzdálených API volání místo lokálního zpracování.
- **🎛️ Centralizovaná správa modelů**: Modely jsou udržovány a aktualizovány centrálně, což zajišťuje konzistenci, ale vyžaduje síťové připojení.
- **📈 Škálovatelnost zdrojů**: Cloudová infrastruktura se může dynamicky přizpůsobit různým výpočetním požadavkům.

## Výzvy edge computingu

Koncová zařízení, jako jsou notebooky, mobilní telefony a zařízení Internetu věcí (IoT), například Raspberry Pi a NVIDIA Orin Nano, představují jedinečná výpočetní omezení. Tato zařízení obvykle mají omezený výpočetní výkon, paměť a energetické zdroje ve srovnání s datacentrovou infrastrukturou.

Provozování tradičních LLMs na těchto zařízeních bylo historicky náročné kvůli těmto hardwarovým omezením. Nicméně potřeba zpracování AI na koncových zařízeních se stává stále důležitější v různých situacích. Zvažte situace, kdy je internetové připojení nespolehlivé nebo nedostupné, například na vzdálených průmyslových lokalitách, v dopravních prostředcích nebo v oblastech se špatným pokrytím sítě. Navíc aplikace vyžadující vysoké bezpečnostní standardy, jako jsou lékařská zařízení, finanční systémy nebo vládní aplikace, mohou potřebovat zpracovávat citlivá data lokálně, aby zachovaly soukromí a splnily požadavky na soulad.

### Klíčová omezení edge computingu

Prostředí edge computingu čelí několika základním omezením, se kterými se tradiční cloudová řešení AI nesetkávají:

- **Omezený výpočetní výkon**: Koncová zařízení obvykle mají méně jader CPU a nižší taktovací frekvence ve srovnání s hardwarem na úrovni serveru.
- **Paměťová omezení**: Dostupná RAM a kapacita úložiště jsou na koncových zařízeních výrazně sníženy.
- **Energetická omezení**: Zařízení napájená bateriemi musí vyvažovat výkon s energetickou spotřebou pro dlouhodobý provoz.
- **Tepelné řízení**: Kompaktní formáty omezují schopnosti chlazení, což ovlivňuje udržitelný výkon při zátěži.

## Co je EdgeAI?

### Koncept: Definice Edge AI

Edge AI označuje nasazení a provádění algoritmů umělé inteligence přímo na koncových zařízeních – fyzickém hardwaru, který existuje na "okraji" sítě, blízko místa, kde jsou data generována a sbírána. Tato zařízení zahrnují chytré telefony, IoT senzory, chytré kamery, autonomní vozidla, nositelná zařízení a průmyslové vybavení. Na rozdíl od tradičních AI systémů, které se spoléhají na cloudové servery pro zpracování, Edge AI přináší inteligenci přímo ke zdroji dat.

V jádru jde u Edge AI o decentralizaci zpracování AI, jeho přesun od centralizovaných datových center a rozdělení napříč rozsáhlou sítí zařízení, která tvoří náš digitální ekosystém. To představuje zásadní architektonický posun v tom, jak jsou AI systémy navrhovány a nasazovány.

Klíčové konceptuální pilíře Edge AI zahrnují:

- **Zpracování v blízkosti**: Výpočty probíhají fyzicky blízko místa, kde data vznikají.
- **Decentralizovaná inteligence**: Schopnosti rozhodování jsou rozděleny mezi více zařízení.
- **Suverenita dat**: Informace zůstávají pod lokální kontrolou, často nikdy neopouštějí zařízení.
- **Autonomní provoz**: Zařízení mohou fungovat inteligentně bez nutnosti neustálého připojení.
- **Vestavěná AI**: Inteligence se stává nedílnou schopností běžných zařízení.

### Vizualizace architektury Edge AI

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI představuje zásadní změnu v nasazení umělé inteligence, přinášející schopnosti AI přímo na koncová zařízení místo spoléhání se pouze na zpracování v cloudu. Tento přístup umožňuje AI modelům běžet lokálně na zařízeních s omezenými výpočetními zdroji, poskytovat schopnosti inferencí v reálném čase bez nutnosti neustálého internetového připojení.

EdgeAI zahrnuje různé technologie a techniky navržené tak, aby AI modely byly efektivnější a vhodné pro nasazení na zařízeních s omezenými zdroji. Cílem je zachovat přiměřený výkon při výrazném snížení výpočetních a paměťových požadavků AI modelů.

Podívejme se na základní přístupy, které umožňují implementace EdgeAI napříč různými typy zařízení a případy použití.

### Základní principy EdgeAI

EdgeAI je postaveno na několika základních principech, které jej odlišují od tradiční AI založené na cloudu:

- **Lokální zpracování**: Inferenční AI probíhá přímo na koncovém zařízení bez nutnosti externího připojení.
- **Optimalizace zdrojů**: Modely jsou optimalizovány specificky pro hardwarová omezení cílových zařízení.
- **Výkon v reálném čase**: Zpracování probíhá s minimální latencí pro aplikace citlivé na čas.
- **Soukromí jako základ**: Citlivá data zůstávají na zařízení, což zvyšuje bezpečnost a soulad.

## Klíčové technologie umožňující EdgeAI

### Kvantizace modelů

Jednou z nejdůležitějších technik v EdgeAI je kvantizace modelů. Tento proces zahrnuje snížení přesnosti parametrů modelu, obvykle z 32bitových čísel s plovoucí desetinnou čárkou na 8bitová celá čísla nebo dokonce formáty s nižší přesností. Ačkoli by se toto snížení přesnosti mohlo zdát znepokojující, výzkum ukázal, že mnoho AI modelů si může zachovat svůj výkon i při výrazně snížené přesnosti.

Kvantizace funguje tak, že mapuje rozsah hodnot s plovoucí desetinnou čárkou na menší množinu diskrétních hodnot. Například místo použití 32 bitů k reprezentaci každého parametru může kvantizace použít pouze 8 bitů, což vede ke čtyřnásobnému snížení paměťových požadavků a často k rychlejším časům inferencí.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Různé techniky kvantizace zahrnují:

- **Post-Training Quantization (PTQ)**: Aplikováno po trénování modelu bez nutnosti opětovného trénování.
- **Quantization-Aware Training (QAT)**: Zahrnuje efekty kvantizace během trénování pro lepší přesnost.
- **Dynamická kvantizace**: Kvantizuje váhy na int8, ale aktivace počítá dynamicky.
- **Statická kvantizace**: Předem vypočítává všechny parametry kvantizace pro váhy i aktivace.

Pro nasazení EdgeAI je výběr vhodné strategie kvantizace závislý na konkrétní architektuře modelu, požadavcích na výkon a hardwarových schopnostech cílového zařízení.

### Komprese a optimalizace modelů

Kromě kvantizace pomáhají různé techniky komprese snížit velikost modelu a výpočetní požadavky. Patří sem:

**Pruning**: Tato technika odstraňuje nepotřebné spojení nebo neurony z neuronových sítí. Identifikací a eliminací parametrů, které málo přispívají k výkonu modelu, může pruning výrazně snížit velikost modelu při zachování přesnosti.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Knowledge Distillation**: Tento přístup zahrnuje trénování menšího "studentského" modelu, aby napodoboval chování většího "učitelského" modelu. Studentský model se učí přibližovat výstupy učitele, často dosahuje podobného výkonu s výrazně menším počtem parametrů.

**Optimalizace architektury modelu**: Výzkumníci vyvinuli specializované architektury navržené specificky pro nasazení na koncových zařízeních, jako jsou MobileNets, EfficientNets a další lehké architektury, které vyvažují výkon s výpočetní efektivitou.

### Malé jazykové modely (SLMs)

Vznikajícím trendem v EdgeAI je vývoj malých jazykových modelů (SLMs). Tyto modely jsou navrženy od základu tak, aby byly kompaktní a efektivní, přičemž stále poskytují smysluplné schopnosti přirozeného jazyka. SLMs toho dosahují prostřednictvím pečlivých architektonických rozhodnutí, efektivních tréninkových technik a zaměřeného tréninku na specifické domény nebo úkoly.

Na rozdíl od tradičních přístupů, které zahrnují kompresi velkých modelů, jsou SLMs často trénovány na menších datových sadách a optimalizovaných architekturách specificky navržených pro nasazení na koncových zařízeních. Tento přístup může vést k modelům, které jsou nejen menší, ale také efektivnější pro specifické případy použití.

## Hardwarová akcelerace pro EdgeAI

Moderní koncová zařízení stále častěji zahrnují specializovaný hardware navržený k akceleraci AI úloh:

### Neuronové procesorové jednotky (NPUs)

NPUs jsou specializované procesory navržené specificky pro výpočty neuronových sítí. Tyto čipy mohou provádět úlohy inferencí AI mnohem efektivněji než tradiční CPU, často s nižší spotřebou energie. Mnoho moderních chytrých telefonů, notebooků a IoT zařízení nyní zahrnuje NPUs, aby umožnilo zpracování AI přímo na zařízení.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Zařízení s NPUs zahrnují:

- **Apple**: Čipy řady A a M s Neural Engine
- **Qualcomm**: Procesory Snapdragon s Hexagon DSP/NPU
- **Samsung**: Procesory Exynos s NPU
- **Intel**: Movidius VPUs a akcelerátory Habana Labs
- **Microsoft**: Windows Copilot+ PC s NPUs

### 🎮 Akcelerace GPU

Zatímco koncová zařízení nemusí mít výkonné GPU nalezené v datových centrech, mnoho z nich stále zahrnuje integrované nebo diskrétní GPU, které mohou akcelerovat AI úlohy. Moderní mobilní GPU a integrované grafické procesory mohou poskytnout významné zlepšení výkonu pro úlohy inferencí AI.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### Optimalizace CPU

Dokonce i zařízení pouze s CPU mohou těžit z EdgeAI prostřednictvím optimalizovaných implementací. Moderní CPU zahrnují specializované instrukce pro AI úlohy a byly vyvinuty softwarové rámce, které maximalizují výkon CPU pro inferenci AI.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Pro softwarové inženýry pracující s EdgeAI je klíčové pochopit, jak využít tyto možnosti hardwarové akcelerace pro optimalizaci výkonu inferencí a energetické účinnosti na cílových zařízeních.

## Výhody EdgeAI

### Soukromí a bezpečnost

Jednou z nejvýznamnějších výhod EdgeAI je zvýšené soukromí a bezpečnost. Zpracováním dat lokálně na zařízení citlivé informace nikdy neopustí kontrolu uživatele. To je obzvláště důležité pro aplikace, které pracují s osobními údaji, lékařskými informacemi nebo důvěrnými obchodními daty.

### Snížená latence

EdgeAI eliminuje potřebu odesílat data na vzdálené servery ke zpracování, což výrazně snižuje latenci. To je klíčové pro aplikace v reálném čase, jako jsou autonomní vozidla, průmyslová automatizace nebo interaktivní aplikace, kde jsou vyžadovány okamžité reakce.

### Offline schopnosti

EdgeAI umožňuje funkčnost AI i v případě, že internetové připojení není dostupné. To je cenné pro aplikace na vzdálených místech, během cestování nebo v situacích, kdy je spolehlivost sítě problematická.

### Nákladová efektivita

Snížením závislosti na cloudových službách AI může EdgeAI pomoci snížit provozní náklady, zejména u aplikací s vysokým objemem využití. Organizace se mohou vyhnout průběžným nákladům na API a snížit požadavky na šířku pásma.

### Škálovatelnost

EdgeAI rozděluje výpočetní zátěž mezi koncová zařízení místo jejího centralizování v datových centrech. To může pomoci snížit náklady na infrastrukturu a zlepšit celkovou škálovatelnost systému.

## Aplikace EdgeAI

### Chytrá zařízení a IoT

EdgeAI pohání mnoho funkcí chytrých zařízení, od hlasových asistentů, kteří mohou zpracovávat příkazy lokálně, po chytré kamery, které dokážou identifikovat objekty a osoby bez odesílání videa do cloudu. IoT zařízení využívají EdgeAI pro prediktivní údržbu, monitorování prostředí a automatizované rozhodování.

### Mobilní aplikace

Chytré telefony a tablety využívají EdgeAI pro různé funkce, včetně vylepšení fotografií, překladu v reálném čase, rozšířené reality a personalizovaných doporučení. Tyto aplikace těží z nízké latence a výhod soukromí lokální
- [02: EdgeAI Aplikace](02.RealWorldCaseStudies.md)

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby AI pro překlady [Co-op Translator](https://github.com/Azure/co-op-translator). Ačkoli se snažíme o přesnost, mějte prosím na paměti, že automatizované překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.