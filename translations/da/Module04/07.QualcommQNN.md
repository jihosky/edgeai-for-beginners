<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:54:35+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "da"
}
-->
# Afsnit 7: Qualcomm QNN (Qualcomm Neural Network) Optimeringssuite

## Indholdsfortegnelse
1. [Introduktion](../../../Module04)
2. [Hvad er Qualcomm QNN?](../../../Module04)
3. [Installation](../../../Module04)
4. [Hurtig startguide](../../../Module04)
5. [Eksempel: Konvertering og optimering af modeller med QNN](../../../Module04)
6. [Avanceret brug](../../../Module04)
7. [Bedste praksis](../../../Module04)
8. [Fejlfinding](../../../Module04)
9. [Yderligere ressourcer](../../../Module04)

## Introduktion

Qualcomm QNN (Qualcomm Neural Network) er en omfattende AI-inferensramme designet til at udnytte det fulde potentiale af Qualcomms AI-hardwareacceleratorer, herunder Hexagon NPU, Adreno GPU og Kryo CPU. Uanset om du arbejder med mobile enheder, edge computing-platforme eller bilsystemer, tilbyder QNN optimerede inferensmuligheder, der udnytter Qualcomms specialiserede AI-processeringsenheder for maksimal ydeevne og energieffektivitet.

## Hvad er Qualcomm QNN?

Qualcomm QNN er en samlet AI-inferensramme, der gør det muligt for udviklere effektivt at implementere AI-modeller på tværs af Qualcomms heterogene computearkitektur. Den tilbyder en samlet programmeringsgrænseflade til adgang til Hexagon NPU (Neural Processing Unit), Adreno GPU og Kryo CPU og vælger automatisk den optimale processeringsenhed til forskellige modellag og operationer.

### Nøglefunktioner

- **Heterogen computing**: Samlet adgang til NPU, GPU og CPU med automatisk arbejdsfordeling
- **Hardware-bevidst optimering**: Specialiserede optimeringer til Qualcomm Snapdragon-platforme
- **Kvantisering**: Avancerede INT8-, INT16- og blandet præcision kvantiseringsteknikker
- **Modelkonverteringsværktøjer**: Direkte understøttelse af TensorFlow-, PyTorch-, ONNX- og Caffe-modeller
- **Optimeret til Edge AI**: Specifikt designet til mobile og edge-implementeringsscenarier med fokus på energieffektivitet

### Fordele

- **Maksimal ydeevne**: Udnyt specialiseret AI-hardware for op til 15x ydeevneforbedringer
- **Energieffektivitet**: Optimeret til mobile og batteridrevne enheder med intelligent strømstyring
- **Lav latenstid**: Hardwareaccelereret inferens med minimal forsinkelse til realtidsapplikationer
- **Skalerbar implementering**: Fra smartphones til bilplatforme på tværs af Qualcomms økosystem
- **Produktionsklar**: En battle-testet ramme, der bruges i millioner af implementerede enheder

## Installation

### Forudsætninger

- Qualcomm QNN SDK (kræver registrering hos Qualcomm)
- Python 3.7 eller nyere
- Kompatibel Qualcomm-hardware eller simulator
- Android NDK (til mobilimplementering)
- Linux- eller Windows-udviklingsmiljø

### Opsætning af QNN SDK

1. **Registrer og download**: Besøg Qualcomm Developer Network for at registrere og downloade QNN SDK
2. **Udpak SDK**: Pak QNN SDK ud til din udviklingsmappe
3. **Konfigurer miljøvariabler**: Indstil stier til QNN-værktøjer og biblioteker

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Opsætning af Python-miljø

Opret og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Installer nødvendige Python-pakker:

```bash
pip install numpy tensorflow torch onnx
```

### Verificer installationen

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Hvis det lykkes, bør du se hjælpeinformation for hvert QNN-værktøj.

## Hurtig startguide

### Din første modelkonvertering

Lad os konvertere en simpel PyTorch-model til at køre på Qualcomm-hardware:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Konverter ONNX til QNN-format

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generer QNN-modulbibliotek

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Hvad gør denne proces?

Optimeringsarbejdsgangen involverer: konvertering af den oprindelige model til ONNX-format, oversættelse af ONNX til QNNs mellemliggende repræsentation, anvendelse af hardware-specifikke optimeringer og generering af et kompileret modelbibliotek til implementering.

### Forklaring af nøgleparametre

- `--input_network`: Kildens ONNX-modelfil
- `--output_path`: Genereret C++ kildefil
- `--input_dim`: Input tensor-dimensioner til optimering
- `--quantization_overrides`: Brugerdefineret kvantiseringskonfiguration
- `-t x86_64-linux-clang`: Målarkitektur og compiler

## Eksempel: Konvertering og optimering af modeller med QNN

### Trin 1: Avanceret modelkonvertering med kvantisering

Sådan anvender du brugerdefineret kvantisering under konvertering:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konverter med brugerdefineret kvantisering:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Trin 2: Multi-backend optimering

Konfigurer til heterogen eksekvering på tværs af NPU, GPU og CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Trin 3: Opret kontekst-binær til implementering

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Trin 4: Inferens med QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Outputstruktur

Efter optimering vil din implementeringsmappe indeholde:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Avanceret brug

### Brugerdefineret backend-konfiguration

Konfigurer specifikke backend-optimeringer:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dynamisk kvantisering

Anvend kvantisering under kørsel for bedre nøjagtighed:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Ydeevneprofilering

Overvåg ydeevne på tværs af forskellige backends:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatisk backend-valg

Implementer intelligent backend-valg baseret på modelkarakteristika:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Bedste praksis

### 1. Optimering af modelarkitektur
- **Lagfusion**: Kombiner operationer som Conv+BatchNorm+ReLU for bedre NPU-udnyttelse
- **Depth-wise separable konvolutioner**: Foretræk disse frem for standardkonvolutioner til mobilimplementering
- **Kvantisering-venlige designs**: Brug ReLU-aktiveringer og undgå operationer, der ikke kvantiserer godt

### 2. Kvantiseringsstrategi
- **Post-træningskvantisering**: Start med dette for hurtig implementering
- **Kalibreringsdatasæt**: Brug repræsentative data, der dækker alle inputvariationer
- **Blandet præcision**: Brug INT8 til de fleste lag, hold kritiske lag i højere præcision

### 3. Retningslinjer for backend-valg
- **NPU (HTP)**: Bedst til CNN-arbejdsbelastninger, kvantiserede modeller og strømfølsomme applikationer
- **GPU**: Optimal til beregningsintensive operationer, større modeller og FP16-præcision
- **CPU**: Fallback til ikke-understøttede operationer og debugging

### 4. Ydeevneoptimering
- **Batchstørrelse**: Brug batchstørrelse 1 til realtidsapplikationer, større batches til gennemløb
- **Inputforbehandling**: Minimer datakopiering og konverteringsomkostninger
- **Kontekstgenbrug**: Forudkompiler kontekster for at undgå kompilering under kørsel

### 5. Hukommelsesstyring
- **Tensorallokering**: Brug statisk allokering, når det er muligt, for at undgå overhead under kørsel
- **Hukommelsespuljer**: Implementer brugerdefinerede hukommelsespuljer til ofte allokerede tensorer
- **Buffergenbrug**: Genbrug input/output-buffere på tværs af inferenskald

### 6. Strømoptimering
- **Ydeevnetilstande**: Brug passende ydeevnetilstande baseret på termiske begrænsninger
- **Dynamisk frekvensskalering**: Tillad systemet at skalere frekvens baseret på arbejdsbelastning
- **Håndtering af inaktiv tilstand**: Frigør ressourcer korrekt, når de ikke er i brug

## Fejlfinding

### Almindelige problemer

#### 1. Problemer med SDK-installation
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Fejl ved modelkonvertering
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Kvantiseringsproblemer
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Ydeevneproblemer
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Hukommelsesproblemer
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Backend-kompatibilitet
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Ydeevnefejlfinding

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Få hjælp

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN-dokumentation**: Tilgængelig i SDK-pakken
- **Community-fora**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Teknisk support**: Via Qualcomm udviklerportal

## Yderligere ressourcer

### Officielle links
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon-platforme**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Udviklerportal**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Læringsressourcer
- **Kom godt i gang-guide**: Tilgængelig i QNN SDK-dokumentationen
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimeringsguide**: SDK-dokumentationen inkluderer omfattende optimeringsvejledninger
- **Videotutorials**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integration Tools
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Forudoptimerede modeller til Qualcomm-hardware
- **Android Neural Networks API**: Integration med Android NNAPI
- **TensorFlow Lite Delegate**: Qualcomm-delegat til TFLite

### Ydeevne benchmarks
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Community-eksempler
- **Eksempelapplikationer**: Tilgængelige i QNN SDK-eksempelmappen
- **GitHub-repositorier**: Community-bidragede eksempler og værktøjer
- **Tekniske blogs**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Relaterede værktøjer
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Avancerede kvantiserings- og kompressionsteknikker
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Til sammenligning og fallback-implementering
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Cross-platform inferensmotor

### Hardware-specifikationer
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon-platforme**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Hvad er næste skridt

Fortsæt din Edge AI-rejse ved at udforske [Modul 5: SLMOps og produktionsimplementering](../Module05/README.md) for at lære om operationelle aspekter af Small Language Model-livscyklusstyring.

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi er ikke ansvarlige for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.