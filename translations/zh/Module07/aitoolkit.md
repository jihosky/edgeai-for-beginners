<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "efb0e70d6e87d0795f4d381c3bc99074",
  "translation_date": "2025-10-21T06:53:32+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "zh"
}
-->
# AI工具包适用于Visual Studio Code - 边缘AI开发指南

## 简介

欢迎使用AI工具包的全面指南，帮助您在Visual Studio Code中进行边缘AI开发。随着人工智能从集中式云计算转向分布式边缘设备，开发者需要强大的集成工具来应对边缘部署的独特挑战——从资源限制到离线操作需求。

AI工具包为Visual Studio Code提供了一个完整的开发环境，专门用于构建、测试和优化能够高效运行在边缘设备上的AI应用程序。无论您是为物联网传感器、移动设备、嵌入式系统还是边缘服务器开发，这款工具包都能在熟悉的VS Code环境中简化您的整个开发工作流程。

本指南将带您了解如何在边缘AI项目中利用AI工具包的核心概念、工具和最佳实践，从初始模型选择到生产部署。

## 概述

AI工具包是一个强大的扩展，能够简化智能体开发和AI应用程序创建。该工具包提供了全面的功能，用于探索、评估和部署来自多种提供商的AI模型，包括Anthropic、OpenAI、GitHub、Google，同时支持使用ONNX和Ollama进行本地模型执行。

AI工具包的独特之处在于其对整个AI开发生命周期的全面覆盖。与传统的AI开发工具仅关注单一方面不同，AI工具包提供了一个集成环境，涵盖了模型发现、实验、智能体开发、评估和部署——全部在熟悉的VS Code环境中完成。

该平台专为快速原型设计和生产部署而设计，具有提示生成、快速入门、无缝MCP（模型上下文协议）工具集成以及广泛的评估功能等特点。对于边缘AI开发，这意味着您可以高效地开发、测试和优化适用于边缘部署场景的AI应用程序，同时保持完整的开发工作流程。

## 学习目标

通过本指南，您将能够：

### 核心能力
- **安装和配置** AI工具包以支持边缘AI开发工作流程
- **导航和使用** AI工具包界面，包括模型目录、游乐场和智能体构建器
- **选择和评估**适合边缘部署的AI模型，基于性能和资源限制
- **转换和优化**模型，使用ONNX格式和量化技术以适配边缘设备

### 边缘AI开发技能
- **设计和实现**使用集成开发环境的边缘AI应用程序
- **在边缘条件下进行模型测试**，使用本地推理和资源监控
- **创建和定制**针对边缘部署场景优化的AI智能体
- **使用与边缘计算相关的指标**（延迟、内存使用、准确性）评估模型性能

### 优化和部署
- **应用量化和剪枝技术**，在保持性能的同时减少模型大小
- **优化模型**以适配特定的边缘硬件平台，包括CPU、GPU和NPU加速
- **实施边缘AI开发的最佳实践**，包括资源管理和回退策略
- **为边缘设备的生产部署**准备模型和应用程序

### 高级边缘AI概念
- **与边缘AI框架集成**，包括ONNX Runtime、Windows ML和TensorFlow Lite
- **实现多模型架构**和边缘环境中的联邦学习场景
- **解决常见的边缘AI问题**，包括内存限制、推理速度和硬件兼容性
- **设计监控和日志记录策略**，用于生产中的边缘AI应用程序

### 实际应用
- **构建端到端的边缘AI解决方案**，从模型选择到部署
- **展示在边缘特定开发工作流程和优化技术方面的熟练程度**
- **将学到的概念应用于实际的边缘AI用例**，包括物联网、移动和嵌入式应用
- **评估和比较**不同的边缘AI部署策略及其权衡

## 边缘AI开发的关键功能

### 1. 模型目录和发现
- **多提供商支持**：浏览并访问来自Anthropic、OpenAI、GitHub、Google等提供商的AI模型
- **本地模型集成**：简化ONNX和Ollama模型的发现以适配边缘部署
- **GitHub模型**：与GitHub的模型托管直接集成，简化访问
- **模型比较**：并排比较模型以找到适合边缘设备限制的最佳平衡

### 2. 交互式游乐场
- **交互式测试环境**：在受控环境中快速实验模型功能
- **多模态支持**：使用图像、文本和其他典型边缘场景输入进行测试
- **实时实验**：即时反馈模型响应和性能
- **参数优化**：针对边缘部署需求微调模型参数

### 3. 提示（智能体）构建器
- **自然语言生成**：使用自然语言描述生成初始提示
- **迭代优化**：根据模型响应和性能改进提示
- **任务分解**：通过提示链和结构化输出分解复杂任务
- **变量支持**：在提示中使用变量实现动态智能体行为
- **生产代码生成**：生成生产就绪代码以快速开发应用程序

### 4. 批量运行和评估
- **多模型测试**：同时对选定模型执行多个提示
- **高效规模化测试**：高效测试各种输入和配置
- **自定义测试用例**：运行智能体以验证功能的测试用例
- **性能比较**：比较不同模型和配置的结果

### 5. 使用数据集进行模型评估
- **标准指标**：使用内置评估器测试AI模型（F1分数、相关性、相似性、一致性）
- **自定义评估器**：为特定用例创建自己的评估指标
- **数据集集成**：使用全面的数据集测试模型
- **性能测量**：量化模型性能以进行边缘部署决策

### 6. 微调功能
- **模型定制**：根据特定用例和领域定制模型
- **专业化适配**：适配模型以满足专业领域和需求
- **边缘优化**：专门针对边缘部署限制微调模型
- **领域特定训练**：创建适合特定边缘用例的模型

### 7. MCP工具集成
- **外部工具连接**：通过模型上下文协议服务器连接智能体到外部工具
- **现实世界操作**：使智能体能够查询数据库、访问API或执行自定义逻辑
- **现有MCP服务器**：使用命令（stdio）或HTTP（服务器发送事件）协议的工具
- **自定义MCP开发**：在智能体构建器中构建和测试新的MCP服务器

### 8. 智能体开发和测试
- **函数调用支持**：使智能体能够动态调用外部函数
- **实时集成测试**：通过实时运行和工具使用测试集成
- **智能体版本控制**：智能体的版本控制及评估结果的比较功能
- **调试和追踪**：智能体开发的本地追踪和调试功能

## 边缘AI开发工作流程

### 阶段1：模型发现和选择
1. **探索模型目录**：使用模型目录寻找适合边缘部署的模型
2. **比较性能**：根据大小、准确性和推理速度评估模型
3. **本地测试**：使用Ollama或ONNX模型在边缘部署前进行本地测试
4. **评估资源需求**：确定目标边缘设备的内存和计算需求

### 阶段2：模型优化
1. **转换为ONNX**：将选定模型转换为ONNX格式以适配边缘兼容性
2. **应用量化**：通过INT8或INT4量化减少模型大小
3. **硬件优化**：针对目标边缘硬件（ARM、x86、专用加速器）进行优化
4. **性能验证**：验证优化后的模型是否保持可接受的准确性

### 阶段3：应用开发
1. **智能体设计**：使用智能体构建器创建边缘优化的AI智能体
2. **提示工程**：开发适合较小边缘模型的提示
3. **集成测试**：在模拟边缘条件下测试智能体
4. **代码生成**：生成适合边缘部署的生产代码

### 阶段4：评估和测试
1. **批量评估**：测试多种配置以找到最佳边缘设置
2. **性能分析**：分析推理速度、内存使用和准确性
3. **边缘模拟**：在类似目标边缘部署环境的条件下测试
4. **压力测试**：在各种负载条件下评估性能

### 阶段5：部署准备
1. **最终优化**：根据测试结果进行最终优化
2. **部署打包**：打包模型和代码以进行边缘部署
3. **文档编制**：记录部署需求和配置
4. **监控设置**：为边缘部署准备监控和日志记录

## 边缘AI开发的目标受众

### 边缘AI开发者
- 构建AI驱动边缘设备和物联网解决方案的应用开发者
- 将AI功能集成到资源受限设备中的嵌入式系统开发者
- 为智能手机和平板电脑创建设备端AI应用的移动开发者

### 边缘AI工程师
- 优化模型以进行边缘部署并管理推理管道的AI工程师
- 在分布式边缘基础设施上部署和管理AI模型的DevOps工程师
- 优化AI工作负载以适配边缘硬件限制的性能工程师

### 研究人员和教育工作者
- 开发高效模型和算法以适配边缘计算的AI研究人员
- 教授边缘AI概念并演示优化技术的教育工作者
- 学习边缘AI部署中的挑战和解决方案的学生

## 边缘AI用例

### 智能物联网设备
- **实时图像识别**：在物联网摄像头和传感器上部署计算机视觉模型
- **语音处理**：在智能音箱上实现语音识别和自然语言处理
- **预测性维护**：在工业边缘设备上运行异常检测模型
- **环境监测**：部署传感器数据分析模型以进行环境应用

### 移动和嵌入式应用
- **设备端翻译**：实现离线工作的语言翻译模型
- **增强现实**：为AR应用部署实时对象识别和跟踪
- **健康监测**：在可穿戴设备和医疗设备上运行健康分析模型
- **自主系统**：为无人机、机器人和车辆实现决策模型

### 边缘计算基础设施
- **边缘数据中心**：在边缘数据中心部署AI模型以支持低延迟应用
- **CDN集成**：将AI处理能力集成到内容分发网络中
- **5G边缘**：利用5G边缘计算支持AI驱动的应用
- **雾计算**：在雾计算环境中实现AI处理

## 安装和设置

### 扩展安装
直接从Visual Studio Code市场安装AI工具包扩展：

**扩展ID**：`ms-windows-ai-studio.windows-ai-studio`

**安装方法**：
1. **VS Code市场**：在扩展视图中搜索“AI Toolkit”
2. **命令行**：`code --install-extension ms-windows-ai-studio.windows-ai-studio`
3. **直接安装**：从[VS Code市场](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)下载

### 边缘AI开发的前提条件
- **Visual Studio Code**：建议使用最新版本
- **Python环境**：Python 3.8+，安装所需的AI库
- **ONNX Runtime**（可选）：用于ONNX模型推理
- **Ollama**（可选）：用于本地模型服务
- **硬件加速工具**：CUDA、OpenVINO或平台特定加速器

### 初始配置
1. **扩展激活**：打开VS Code并验证AI工具包是否出现在活动栏中
2. **模型提供商设置**：配置访问GitHub、OpenAI、Anthropic或其他模型提供商
3. **本地环境**：设置Python环境并安装所需的包
4. **硬件加速**：如果可用，配置GPU/NPU加速
5. **MCP集成**：根据需要设置模型上下文协议服务器

### 初次设置检查表
- [ ] AI工具包扩展已安装并激活
- [ ] 模型目录可访问且模型可发现
- [ ] 游乐场功能正常，用于模型测试
- [ ] 智能体构建器可访问，用于提示开发
- [ ] 本地开发环境已配置
- [ ] 硬件加速（如果可用）已正确配置

## 使用AI工具包入门

### 快速入门指南

我们建议从GitHub托管的模型开始，以获得最简化的体验：

1. **安装**：按照[安装指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)设置您的设备上的AI工具包
2. **模型发现**：在扩展树视图中选择**CATALOG > Models**以探索可用模型
3. **GitHub模型**：从GitHub托管的模型开始，以实现最佳集成
4. **游乐场测试**：从任何模型卡中选择**Try in Playground**以开始实验模型功能

### 边缘AI开发的分步指南

#### 第一步：模型探索和选择
1. 在VS Code活动栏中打开AI工具包视图
2. 浏览模型目录以寻找适合边缘部署的模型
3. 根据您的边缘需求按提供商（GitHub、ONNX、Ollama）筛选
4. 使用**Try in Playground**立即测试模型功能

#### 第二步：智能体开发
1. 使用**提示（智能体）构建器**创建边缘优化的AI智能体
2. 使用自然语言描述生成初始提示
3. 根据模型响应迭代并优化提示
4. 集成MCP工具以增强代理功能

#### 第三步：测试与评估
1. 使用**批量运行**测试多个提示在选定模型上的表现
2. 使用测试用例运行代理以验证功能
3. 使用内置或自定义指标评估准确性和性能
4. 比较不同模型和配置

#### 第四步：微调与优化
1. 为特定边缘用例定制模型
2. 应用领域特定的微调
3. 针对边缘部署限制进行优化
4. 对不同的代理配置进行版本管理和比较

#### 第五步：部署准备
1. 使用Agent Builder生成生产级代码
2. 设置MCP服务器连接以支持生产环境
3. 为边缘设备准备部署包
4. 配置监控和评估指标

## AI工具包示例

试用我们的示例  
[AI工具包示例](https://github.com/Azure-Samples/AI_Toolkit_Samples)旨在帮助开发者和研究人员有效地探索和实施AI解决方案。

我们的示例包括：

示例代码：预构建的示例，用于演示AI功能，例如训练、部署或将模型集成到应用程序中。  
文档：指南和教程，帮助用户了解AI工具包的功能及其使用方法。  

前提条件

- Visual Studio Code  
- Visual Studio Code的AI工具包  
- GitHub细粒度个人访问令牌（PAT）  
- Foundry Local  

## 边缘AI开发最佳实践

### 模型选择
- **尺寸限制**：选择适合目标设备内存限制的模型  
- **推理速度**：优先选择推理速度快的模型以支持实时应用  
- **准确性权衡**：在模型准确性与资源限制之间找到平衡  
- **格式兼容性**：优先选择ONNX或硬件优化格式以支持边缘部署  

### 优化技术
- **量化**：使用INT8或INT4量化以减少模型大小并提高速度  
- **剪枝**：移除不必要的模型参数以降低计算需求  
- **知识蒸馏**：创建较小的模型，同时保持较大模型的性能  
- **硬件加速**：在可用时利用NPU、GPU或专用加速器  

### 开发工作流
- **迭代测试**：在开发过程中频繁在类似边缘的条件下测试  
- **性能监控**：持续监控资源使用和推理速度  
- **版本控制**：跟踪模型版本和优化设置  
- **文档记录**：记录所有优化决策和性能权衡  

### 部署注意事项
- **资源监控**：在生产环境中监控内存、CPU和功耗使用  
- **回退策略**：为模型故障实施回退机制  
- **更新机制**：规划模型更新和版本管理  
- **安全性**：为边缘AI应用实施适当的安全措施  

## 与边缘AI框架的集成

### ONNX Runtime
- **跨平台部署**：在不同的边缘平台上部署ONNX模型  
- **硬件优化**：利用ONNX Runtime的硬件特定优化  
- **移动支持**：使用ONNX Runtime Mobile支持智能手机和平板应用  
- **物联网集成**：通过ONNX Runtime的轻量级分发在物联网设备上部署  

### Windows ML
- **Windows设备**：优化用于基于Windows的边缘设备和PC  
- **NPU加速**：利用Windows设备上的神经处理单元  
- **DirectML**：在Windows平台上使用DirectML进行GPU加速  
- **UWP集成**：与通用Windows平台应用集成  

### TensorFlow Lite
- **移动优化**：在移动和嵌入式设备上部署TensorFlow Lite模型  
- **硬件代理**：使用专用硬件代理进行加速  
- **微控制器**：使用TensorFlow Lite Micro在微控制器上部署  
- **跨平台支持**：在Android、iOS和嵌入式Linux系统上部署  

### Azure IoT Edge
- **云-边缘混合**：结合云端训练与边缘推理  
- **模块部署**：将AI模型作为IoT Edge模块部署  
- **设备管理**：远程管理边缘设备和模型更新  
- **遥测**：收集边缘部署的性能数据和模型指标  

## 高级边缘AI场景

### 多模型部署
- **模型集成**：部署多个模型以提高准确性或冗余性  
- **A/B测试**：在边缘设备上同时测试不同模型  
- **动态选择**：根据当前设备条件选择模型  
- **资源共享**：优化多个部署模型的资源使用  

### 联邦学习
- **分布式训练**：在多个边缘设备上训练模型  
- **隐私保护**：保持训练数据本地化，同时共享模型改进  
- **协作学习**：使设备能够从集体经验中学习  
- **边缘-云协调**：协调边缘设备与云基础设施之间的学习  

### 实时处理
- **流处理**：在边缘设备上处理连续数据流  
- **低延迟推理**：优化以实现最小推理延迟  
- **批处理**：在边缘设备上高效处理数据批次  
- **自适应处理**：根据当前设备能力调整处理方式  

## 边缘AI开发故障排除

### 常见问题
- **内存限制**：模型过大，无法适应目标设备内存  
- **推理速度**：模型推理速度过慢，无法满足实时需求  
- **准确性下降**：优化导致模型准确性不可接受地降低  
- **硬件兼容性**：模型与目标硬件不兼容  

### 调试策略
- **性能分析**：使用AI工具包的追踪功能识别瓶颈  
- **资源监控**：在开发过程中监控内存和CPU使用情况  
- **增量测试**：逐步测试优化以隔离问题  
- **硬件模拟**：使用开发工具模拟目标硬件  

### 优化解决方案
- **进一步量化**：应用更激进的量化技术  
- **模型架构**：考虑不同的模型架构以优化边缘性能  
- **预处理优化**：优化数据预处理以适应边缘限制  
- **推理优化**：使用硬件特定的推理优化  

## 资源与下一步

### 官方文档
- [AI工具包开发者文档](https://aka.ms/AIToolkit/doc)  
- [安装与设置指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)  
- [VS Code智能应用文档](https://code.visualstudio.com/docs/intelligentapps)  
- [模型上下文协议（MCP）文档](https://modelcontextprotocol.io/)  

### 社区与支持
- [AI工具包GitHub仓库](https://github.com/microsoft/vscode-ai-toolkit)  
- [GitHub问题与功能请求](https://aka.ms/AIToolkit/feedback)  
- [Azure AI Foundry Discord社区](https://aka.ms/azureaifoundry/discord)  
- [VS Code扩展市场](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)  

### 技术资源
- [ONNX Runtime文档](https://onnxruntime.ai/)  
- [Ollama文档](https://ollama.ai/)  
- [Windows ML文档](https://docs.microsoft.com/en-us/windows/ai/)  
- [Azure AI Foundry文档](https://learn.microsoft.com/en-us/azure/ai-foundry/)  

### 学习路径
- [边缘AI基础课程](../Module01/README.md)  
- [小型语言模型指南](../Module02/README.md)  
- [边缘部署策略](../Module03/README.md)  
- [Windows边缘AI开发](./windowdeveloper.md)  

### 其他资源
- **仓库统计**：1.8k+星标，150+分叉，18+贡献者  
- **许可证**：MIT许可证  
- **安全性**：适用微软安全政策  
- **遥测**：遵守VS Code遥测设置  

## 结论

Visual Studio Code的AI工具包是一个现代AI开发的综合平台，提供了特别适合边缘AI应用的流线型代理开发能力。其广泛的模型目录支持Anthropic、OpenAI、GitHub和Google等提供商，并通过ONNX和Ollama实现本地执行，为多样化的边缘部署场景提供了所需的灵活性。

该工具包的优势在于其集成方法——从Playground中的模型发现和实验，到Prompt Builder的复杂代理开发、全面的评估能力以及无缝的MCP工具集成。对于边缘AI开发者来说，这意味着在边缘部署之前快速原型设计和测试AI代理，并能够快速迭代和优化以适应资源受限的环境。

边缘AI开发的关键优势包括：
- **快速实验**：在边缘部署之前快速测试模型和代理  
- **多提供商灵活性**：从多个来源访问模型以找到最佳边缘解决方案  
- **本地开发**：使用ONNX和Ollama进行离线和隐私保护开发  
- **生产准备**：生成生产级代码并通过MCP集成外部工具  
- **全面评估**：使用内置和自定义指标验证边缘AI性能  

随着AI不断向边缘部署场景发展，VS Code的AI工具包提供了所需的开发环境和工作流，支持在资源受限环境中构建、测试和优化智能应用。无论您是在开发物联网解决方案、移动AI应用还是嵌入式智能系统，该工具包的全面功能集和集成工作流都支持整个边缘AI开发生命周期。

凭借持续的开发和活跃的社区（1.8k+ GitHub星标），AI工具包始终处于AI开发工具的前沿，不断发展以满足现代AI开发者在边缘部署场景中的需求。

[下一步 Foundry Local](./foundrylocal.md)

---

**免责声明**：  
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于重要信息，建议使用专业人工翻译。我们对因使用此翻译而产生的任何误解或误读不承担责任。