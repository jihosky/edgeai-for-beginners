<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T11:11:09+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "zh"
}
-->
# 第三章：Microsoft Olive 优化套件

## 目录
1. [简介](../../../Module04)
2. [什么是 Microsoft Olive？](../../../Module04)
3. [安装](../../../Module04)
4. [快速入门指南](../../../Module04)
5. [示例：将 Qwen3 转换为 ONNX INT4](../../../Module04)
6. [高级用法](../../../Module04)
7. [Olive 配方库](../../../Module04)
8. [最佳实践](../../../Module04)
9. [故障排除](../../../Module04)
10. [其他资源](../../../Module04)

## 简介

Microsoft Olive 是一个强大且易于使用的硬件感知模型优化工具包，简化了针对不同硬件平台部署机器学习模型的优化过程。无论目标是 CPU、GPU 还是专用 AI 加速器，Olive 都能帮助您在保持模型准确性的同时实现最佳性能。

## 什么是 Microsoft Olive？

Olive 是一个易于使用的硬件感知模型优化工具，集成了行业领先的模型压缩、优化和编译技术。它与 ONNX Runtime 配合使用，提供端到端的推理优化解决方案。

### 主要功能

- **硬件感知优化**：自动选择适合目标硬件的最佳优化技术
- **40+ 内置优化组件**：涵盖模型压缩、量化、图优化等
- **简单的 CLI 接口**：通过简单命令完成常见优化任务
- **多框架支持**：支持 PyTorch、Hugging Face 模型和 ONNX
- **流行模型支持**：Olive 可自动优化流行的模型架构，如 Llama、Phi、Qwen、Gemma 等

### 优势

- **减少开发时间**：无需手动尝试不同的优化技术
- **性能提升**：显著的速度提升（某些情况下可达 6 倍）
- **跨平台部署**：优化后的模型可在不同硬件和操作系统上运行
- **保持准确性**：优化在提升性能的同时保持模型质量

## 安装

### 前提条件

- Python 3.8 或更高版本
- pip 包管理器
- 虚拟环境（推荐）

### 基本安装

创建并激活虚拟环境：

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
安装带自动优化功能的 Olive：

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  

### 可选依赖项

Olive 提供了多种可选依赖项以支持额外功能：

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  

### 验证安装

```bash
olive --help
```
  
如果成功，您应该会看到 Olive CLI 的帮助信息。

## 快速入门指南

### 第一次优化

让我们使用 Olive 的自动优化功能优化一个小型语言模型：

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### 此命令的作用

优化过程包括：从本地缓存获取模型，捕获 ONNX 图并将权重存储在 ONNX 数据文件中，优化 ONNX 图，并使用 RTN 方法将模型量化为 int4。

### 命令参数说明

- `--model_name_or_path`：Hugging Face 模型标识符或本地路径
- `--output_path`：保存优化模型的目录
- `--device`：目标设备（cpu, gpu）
- `--provider`：执行提供程序（CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider）
- `--use_ort_genai`：使用 ONNX Runtime Generate AI 进行推理
- `--precision`：量化精度（int4, int8, fp16）
- `--log_level`：日志详细程度（0=最少，1=详细）

## 示例：将 Qwen3 转换为 ONNX INT4

基于 Hugging Face 提供的示例 [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)，以下是优化 Qwen3 模型的步骤：

### 第一步：下载模型（可选）

为了减少下载时间，仅缓存必要文件：

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  

### 第二步：优化 Qwen3 模型

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### 第三步：测试优化后的模型

创建一个简单的 Python 脚本来测试优化后的模型：

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  

### 输出结构

优化完成后，输出目录将包含：

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  

## 高级用法

### 配置文件

对于更复杂的优化工作流，可以使用 JSON 配置文件：

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
使用配置文件运行：

```bash
olive run --config config.json
```
  

### GPU 优化

针对 CUDA GPU 的优化：

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
针对 DirectML（Windows）的优化：

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  

### 使用 Olive 进行微调

Olive 还支持模型微调：

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  

## 最佳实践

### 1. 模型选择
- 从较小的模型开始测试（例如，0.5B-7B 参数）
- 确保目标模型架构受 Olive 支持

### 2. 硬件考虑
- 将优化目标与部署硬件匹配
- 如果有 CUDA 兼容硬件，请使用 GPU 优化
- 对于 Windows 机器的集成显卡，可考虑使用 DirectML

### 3. 精度选择
- **INT4**：最大压缩，轻微准确性损失
- **INT8**：尺寸与准确性的良好平衡
- **FP16**：最小准确性损失，中等尺寸缩减

### 4. 测试与验证
- 始终使用您的具体用例测试优化后的模型
- 比较性能指标（延迟、吞吐量、准确性）
- 使用具有代表性的输入数据进行评估

### 5. 迭代优化
- 从自动优化开始快速获得结果
- 使用配置文件进行精细控制
- 尝试不同的优化步骤

## 故障排除

### 常见问题

#### 1. 安装问题
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  

#### 2. CUDA/GPU 问题
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  

#### 3. 内存问题
- 在优化过程中使用较小的批量大小
- 尝试先使用较高精度的量化（int8 而不是 int4）
- 确保有足够的磁盘空间用于模型缓存

#### 4. 模型加载错误
- 验证模型路径和访问权限
- 检查模型是否需要 `trust_remote_code=True`
- 确保所有必要的模型文件已下载

### 获取帮助

- **文档**：[microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub 问题**：[github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **示例**：[microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive 配方库

### Olive 配方简介

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) 仓库补充了主 Olive 工具包，提供了全面的流行 AI 模型优化配方集合。该仓库既是优化公开模型的实用参考，也可用于创建专有模型的优化工作流。

### 主要功能

- **100+ 预构建配方**：流行模型的现成优化配置
- **多架构支持**：涵盖 Transformer 模型、视觉模型和多模态架构
- **硬件特定优化**：针对 CPU、GPU 和专用加速器的配方
- **流行模型系列**：包括 Phi、Llama、Qwen、Gemma、Mistral 等

### 支持的模型系列

该仓库包含以下模型的优化配方：

#### 语言模型
- **Microsoft Phi**：Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**：Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**：Qwen1.5-7B, Qwen2-7B, Qwen2.5 系列（0.5B 至 14B）
- **Google Gemma**：各种 Gemma 模型配置
- **Mistral AI**：Mistral-7B 系列
- **DeepSeek**：R1-Distill 系列模型

#### 视觉和多模态模型
- **Stable Diffusion**：v1.4, XL-base-1.0
- **CLIP 模型**：各种 CLIP-ViT 配置
- **ResNet**：ResNet-50 优化
- **视觉 Transformer**：ViT-base-patch16-224

#### 专用模型
- **Whisper**：OpenAI Whisper-large-v3
- **BERT**：基础和多语言变体
- **句子 Transformer**：all-MiniLM-L6-v2

### 使用 Olive 配方

#### 方法一：克隆特定配方

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```
  

#### 方法二：将配方用作模板

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```
  

### 配方结构

每个配方目录通常包含：

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```
  

### 示例：使用 Phi-4-mini 配方

以下是使用 Phi-4-mini 配方的示例：

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```
  
配置文件通常包括：

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```
  

### 自定义配方

#### 修改目标硬件

要更改目标硬件，请更新 `systems` 部分：

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```
  

#### 调整优化参数

修改 `passes` 部分以实现不同的优化级别：

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```
  

### 创建自己的配方

1. **从类似模型开始**：找到与目标模型架构相似的配方
2. **更新模型配置**：在配置中更改模型名称/路径
3. **调整参数**：根据需要修改优化参数
4. **测试与验证**：运行优化并验证结果
5. **贡献回馈**：考虑将您的配方贡献到仓库

### 使用配方的优势

#### 1. **验证的配置**
- 针对特定模型测试过的优化设置
- 避免在寻找最佳参数时的反复试验

#### 2. **硬件特定调优**
- 针对不同执行提供程序的预优化
- 针对 CPU、GPU 和 NPU 目标的现成配置

#### 3. **全面覆盖**
- 支持最流行的开源模型
- 随新模型发布定期更新

#### 4. **社区贡献**
- 与 AI 社区协作开发
- 共享知识和最佳实践

### 向 Olive 配方库贡献

如果您优化了仓库中未涵盖的模型：

1. **Fork 仓库**：创建您自己的 olive-recipes 分支
2. **创建配方目录**：为您的模型添加新目录
3. **包含配置**：添加 olive_config.json 和支持文件
4. **文档使用方法**：提供清晰的 README 说明
5. **提交 Pull Request**：回馈社区

### 性能基准

许多配方包括性能基准，显示：
- **延迟改进**：通常比基线快 2-6 倍
- **内存减少**：通过量化减少 50-75% 的内存使用
- **准确性保留**：保持 95-99% 的准确性

### 与 AI 工具包集成

这些配方可无缝集成到以下工具中：
- **VS Code AI 工具包**：直接集成模型优化
- **Azure 机器学习**：基于云的优化工作流
- **ONNX Runtime**：优化的推理部署

## 其他资源

### 官方链接
- **GitHub 仓库**：[github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive 配方库**：[github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime 文档**：[onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face 示例**：[huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### 社区示例
- **Jupyter Notebooks**：可在 Olive GitHub 仓库中找到 — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code 扩展**：VS Code AI 工具包概述 — https://learn.microsoft.com/azure/ai-toolkit/overview
- **博客文章**：Microsoft 开源博客 — https://opensource.microsoft.com/blog/

### 相关工具
- **ONNX Runtime**：高性能推理引擎 — https://onnxruntime.ai/
- **Hugging Face Transformers**：许多兼容模型的来源 — https://huggingface.co/docs/transformers/index
- **Azure 机器学习**：基于云的优化工作流 — https://learn.microsoft.com/azure/machine-learning/

## ➡️ 下一步

- [04: OpenVINO 工具包优化套件](./04.openvino.md)

---

**免责声明**：  
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于重要信息，建议使用专业人工翻译。我们对因使用此翻译而产生的任何误解或误读不承担责任。