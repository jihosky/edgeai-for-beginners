<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:27:50+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "pa"
}
-->
# ਸੈਕਸ਼ਨ 2: ਸਥਾਨਕ ਵਾਤਾਵਰਣ ਵਿੱਚ ਤੈਨਾਤੀ - ਗੋਪਨੀਯਤਾ-ਪਹਿਲਾਂ ਹੱਲ

ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲ (SLMs) ਦੀ ਸਥਾਨਕ ਤੈਨਾਤੀ ਗੋਪਨੀਯਤਾ-ਸੰਰਕਸ਼ਣ ਅਤੇ ਲਾਗਤ-ਪ੍ਰਭਾਵੀ AI ਹੱਲਾਂ ਵੱਲ ਇੱਕ ਨਵਾਂ ਰੁਝਾਨ ਹੈ। ਇਹ ਵਿਸਤ੍ਰਿਤ ਗਾਈਡ ਦੋ ਸ਼ਕਤੀਸ਼ਾਲੀ ਫਰੇਮਵਰਕ—Ollama ਅਤੇ Microsoft Foundry Local—ਦੀ ਜਾਂਚ ਕਰਦੀ ਹੈ ਜੋ ਵਿਕਾਸਕਾਰਾਂ ਨੂੰ SLMs ਦੀ ਪੂਰੀ ਸਮਰਥਾ ਨੂੰ ਵਰਤਣ ਦੇ ਯੋਗ ਬਣਾਉਂਦੀ ਹੈ ਜਦੋਂ ਕਿ ਉਹਨਾਂ ਦੇ ਤੈਨਾਤੀ ਵਾਤਾਵਰਣ 'ਤੇ ਪੂਰਾ ਨਿਯੰਤਰਣ ਬਣਾਈ ਰੱਖਦੀ ਹੈ।

## ਪਰਿਚਯ

ਇਸ ਪਾਠ ਵਿੱਚ, ਅਸੀਂ ਸਥਾਨਕ ਵਾਤਾਵਰਣ ਵਿੱਚ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲਾਂ ਦੀ ਤੈਨਾਤੀ ਲਈ ਉੱਚ-ਸਤਰੀ ਤਕਨੀਕਾਂ ਦੀ ਜਾਂਚ ਕਰਾਂਗੇ। ਅਸੀਂ ਸਥਾਨਕ AI ਤੈਨਾਤੀ ਦੇ ਮੂਲ ਸੰਕਲਪਾਂ ਨੂੰ ਕਵਰ ਕਰਾਂਗੇ, ਦੋ ਪ੍ਰਮੁੱਖ ਪਲੇਟਫਾਰਮਾਂ (Ollama ਅਤੇ Microsoft Foundry Local) ਦੀ ਜਾਂਚ ਕਰਾਂਗੇ, ਅਤੇ ਉਤਪਾਦਨ-ਤਿਆਰ ਹੱਲਾਂ ਲਈ ਵਿਹੰਗਮ ਕਾਰਜਨੁਮਾਂ ਦੀ ਸਲਾਹ ਦਿਆਂਗੇ।

## ਸਿੱਖਣ ਦੇ ਉਦੇਸ਼

ਇਸ ਪਾਠ ਦੇ ਅੰਤ ਤੱਕ, ਤੁਸੀਂ ਸਮਰਥ ਹੋਵੋਗੇ:

- ਸਥਾਨਕ SLM ਤੈਨਾਤੀ ਫਰੇਮਵਰਕਾਂ ਦੀ ਆਰਕੀਟੈਕਚਰ ਅਤੇ ਫਾਇਦਿਆਂ ਨੂੰ ਸਮਝਣਾ।
- Ollama ਅਤੇ Microsoft Foundry Local ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਉਤਪਾਦਨ-ਤਿਆਰ ਤੈਨਾਤੀਆਂ ਨੂੰ ਲਾਗੂ ਕਰਨਾ।
- ਵਿਸ਼ੇਸ਼ ਜ਼ਰੂਰਤਾਂ ਅਤੇ ਪਾਬੰਦੀਆਂ ਦੇ ਆਧਾਰ 'ਤੇ ਉਚਿਤ ਪਲੇਟਫਾਰਮ ਦੀ ਚੋਣ ਅਤੇ ਤੁਲਨਾ ਕਰਨਾ।
- ਪ੍ਰਦਰਸ਼ਨ, ਸੁਰੱਖਿਆ ਅਤੇ ਸਕੇਲਬਿਲਟੀ ਲਈ ਸਥਾਨਕ ਤੈਨਾਤੀਆਂ ਨੂੰ ਅਨੁਕੂਲ ਬਣਾਉਣਾ।

## ਸਥਾਨਕ SLM ਤੈਨਾਤੀ ਆਰਕੀਟੈਕਚਰ ਨੂੰ ਸਮਝਣਾ

ਸਥਾਨਕ SLM ਤੈਨਾਤੀ ਕਲਾਉਡ-ਨਿਰਭਰ AI ਸੇਵਾਵਾਂ ਤੋਂ ਸਥਾਨਕ, ਗੋਪਨੀਯਤਾ-ਸੰਰਕਸ਼ਣ ਹੱਲਾਂ ਵੱਲ ਇੱਕ ਮੂਲ ਤਬਦੀਲੀ ਦਾ ਪ੍ਰਤੀਕ ਹੈ। ਇਹ ਪਹੁੰਚ ਸੰਗਠਨਾਂ ਨੂੰ ਆਪਣੀ AI ਬੁਨਿਆਦ ਨੂੰ ਪੂਰੀ ਤਰ੍ਹਾਂ ਨਿਯੰਤਰਿਤ ਕਰਨ ਦੇ ਯੋਗ ਬਣਾਉਂਦੀ ਹੈ ਜਦੋਂ ਕਿ ਡਾਟਾ ਸਾਰਵਭੌਮਤਾ ਅਤੇ ਕਾਰਜਕਾਰੀ ਸੁਤੰਤਰਤਾ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ।

### ਤੈਨਾਤੀ ਫਰੇਮਵਰਕ ਵਰਗੀਕਰਨ

ਵੱਖ-ਵੱਖ ਤੈਨਾਤੀ ਪਹੁੰਚਾਂ ਨੂੰ ਸਮਝਣਾ ਵਿਸ਼ੇਸ਼ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ ਲਈ ਸਹੀ ਰਣਨੀਤੀ ਦੀ ਚੋਣ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ:

- **ਵਿਕਾਸ-ਕੇਂਦਰਿਤ**: ਪ੍ਰਯੋਗ ਅਤੇ ਪ੍ਰੋਟੋਟਾਈਪਿੰਗ ਲਈ ਸਧਾਰਨ ਸੈਟਅੱਪ
- **ਇੰਟਰਪ੍ਰਾਈਜ਼-ਗ੍ਰੇਡ**: ਇੰਟਰਪ੍ਰਾਈਜ਼ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਸਮਰਥਾ ਨਾਲ ਉਤਪਾਦਨ-ਤਿਆਰ ਹੱਲ  
- **ਕਰਾਸ-ਪਲੇਟਫਾਰਮ**: ਵੱਖ-ਵੱਖ ਓਪਰੇਟਿੰਗ ਸਿਸਟਮ ਅਤੇ ਹਾਰਡਵੇਅਰ 'ਤੇ ਯੂਨੀਵਰਸਲ ਅਨੁਕੂਲਤਾ

### ਸਥਾਨਕ SLM ਤੈਨਾਤੀ ਦੇ ਮੁੱਖ ਫਾਇਦੇ

ਸਥਾਨਕ SLM ਤੈਨਾਤੀ ਕਈ ਮੁੱਖ ਫਾਇਦੇ ਪੇਸ਼ ਕਰਦੀ ਹੈ ਜੋ ਇਸਨੂੰ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਅਤੇ ਗੋਪਨੀਯਤਾ-ਸੰਵੇਦਨਸ਼ੀਲ ਐਪਲੀਕੇਸ਼ਨਾਂ ਲਈ ਆਦਰਸ਼ ਬਣਾਉਂਦੇ ਹਨ:

**ਗੋਪਨੀਯਤਾ ਅਤੇ ਸੁਰੱਖਿਆ**: ਸਥਾਨਕ ਪ੍ਰੋਸੈਸਿੰਗ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ ਕਿ ਸੰਵੇਦਨਸ਼ੀਲ ਡਾਟਾ ਕਦੇ ਵੀ ਸੰਗਠਨ ਦੇ ਬੁਨਿਆਦ ਤੋਂ ਬਾਹਰ ਨਹੀਂ ਜਾਂਦਾ, GDPR, HIPAA ਅਤੇ ਹੋਰ ਨਿਯਮਾਂ ਦੀ ਪਾਲਣਾ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ। ਗੁਪਤ ਵਾਤਾਵਰਣਾਂ ਲਈ ਏਅਰ-ਗੈਪਡ ਤੈਨਾਤੀਆਂ ਸੰਭਵ ਹਨ, ਜਦੋਂ ਕਿ ਪੂਰੇ ਆਡਿਟ ਟ੍ਰੇਲ ਸੁਰੱਖਿਆ ਦੀ ਨਿਗਰਾਨੀ ਬਣਾਈ ਰੱਖਦੇ ਹਨ।

**ਲਾਗਤ ਦੀ ਪ੍ਰਭਾਵਸ਼ੀਲਤਾ**: ਪ੍ਰਤੀ-ਟੋਕਨ ਕੀਮਤ ਮਾਡਲਾਂ ਨੂੰ ਹਟਾਉਣ ਨਾਲ ਕਾਰਜਕਾਰੀ ਲਾਗਤਾਂ ਵਿੱਚ ਕਾਫੀ ਕਮੀ ਆਉਂਦੀ ਹੈ। ਘੱਟ ਬੈਂਡਵਿਡਥ ਦੀਆਂ ਜ਼ਰੂਰਤਾਂ ਅਤੇ ਘੱਟ ਕਲਾਉਡ ਨਿਰਭਰਤਾ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਬਜਟਿੰਗ ਲਈ ਪੂਰੇ ਲਾਗਤ ਦੇ ਢਾਂਚੇ ਪ੍ਰਦਾਨ ਕਰਦੇ ਹਨ।

**ਪ੍ਰਦਰਸ਼ਨ ਅਤੇ ਭਰੋਸੇਯੋਗਤਾ**: ਨੈੱਟਵਰਕ ਲੈਟੈਂਸੀ ਤੋਂ ਬਿਨਾਂ ਤੇਜ਼ ਇੰਫਰੈਂਸ ਸਮਾਂ ਰੀਅਲ-ਟਾਈਮ ਐਪਲੀਕੇਸ਼ਨਾਂ ਨੂੰ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ। ਇੰਟਰਨੈਟ ਕਨੈਕਟਿਵਿਟੀ ਤੋਂ ਬਿਨਾਂ ਲਗਾਤਾਰ ਕਾਰਜਕਾਰੀ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ, ਜਦੋਂ ਕਿ ਸਥਾਨਕ ਸਰੋਤਾਂ ਦੀ ਅਨੁਕੂਲਤਾ ਸਥਿਰ ਪ੍ਰਦਰਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦੀ ਹੈ।

## Ollama: ਯੂਨੀਵਰਸਲ ਸਥਾਨਕ ਤੈਨਾਤੀ ਪਲੇਟਫਾਰਮ

### ਮੁੱਖ ਆਰਕੀਟੈਕਚਰ ਅਤੇ ਦਰਸ਼ਨ

Ollama ਨੂੰ ਇੱਕ ਯੂਨੀਵਰਸਲ, ਡਿਵੈਲਪਰ-ਫ੍ਰੈਂਡਲੀ ਪਲੇਟਫਾਰਮ ਵਜੋਂ ਡਿਜ਼ਾਈਨ ਕੀਤਾ ਗਿਆ ਹੈ ਜੋ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਸੰਰਚਨਾਵਾਂ ਅਤੇ ਓਪਰੇਟਿੰਗ ਸਿਸਟਮਾਂ ਵਿੱਚ ਸਥਾਨਕ LLM ਤੈਨਾਤੀ ਨੂੰ ਲੋਕਤੰਤਰਤਮਕ ਬਣਾਉਂਦਾ ਹੈ।

**ਤਕਨੀਕੀ ਅਧਾਰ**: ਮਜ਼ਬੂਤ llama.cpp ਫਰੇਮਵਰਕ 'ਤੇ ਬਣਿਆ, Ollama ਕੁਸ਼ਲ GGUF ਮਾਡਲ ਫਾਰਮੈਟ ਦੀ ਵਰਤੋਂ ਕਰਦਾ ਹੈ ਜੋ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ। ਕਰਾਸ-ਪਲੇਟਫਾਰਮ ਅਨੁਕੂਲਤਾ Windows, macOS, ਅਤੇ Linux ਵਾਤਾਵਰਣਾਂ ਵਿੱਚ ਸਥਿਰ ਵਿਹਾਰ ਯਕੀਨੀ ਬਣਾਉਂਦੀ ਹੈ, ਜਦੋਂ ਕਿ ਬੁੱਧੀਮਾਨ ਸਰੋਤ ਪ੍ਰਬੰਧਨ CPU, GPU, ਅਤੇ ਮੈਮੋਰੀ ਦੀ ਵਰਤੋਂ ਨੂੰ ਅਨੁਕੂਲ ਬਣਾਉਂਦਾ ਹੈ।

**ਡਿਜ਼ਾਈਨ ਦਰਸ਼ਨ**: Ollama ਸਧਾਰਨਤਾ ਨੂੰ ਤਰਜੀਹ ਦਿੰਦਾ ਹੈ ਬਿਨਾਂ ਕਾਰਜਸ਼ੀਲਤਾ ਨੂੰ ਘਟਾਏ, ਤੁਰੰਤ ਉਤਪਾਦਕਤਾ ਲਈ ਜ਼ੀਰੋ-ਕੰਫਿਗਰੇਸ਼ਨ ਤੈਨਾਤੀ ਪੇਸ਼ ਕਰਦਾ ਹੈ। ਪਲੇਟਫਾਰਮ ਵਿਆਪਕ ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਬਣਾਈ ਰੱਖਦਾ ਹੈ ਜਦੋਂ ਕਿ ਵੱਖ-ਵੱਖ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰਾਂ ਵਿੱਚ ਸਥਿਰ APIs ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

### ਉੱਚ-ਸਤਰੀ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਅਤੇ ਸਮਰਥਾ

**ਮਾਡਲ ਪ੍ਰਬੰਧਨ ਸ਼੍ਰੇਸ਼ਠਤਾ**: Ollama ਆਟੋਮੈਟਿਕ ਪੁਲਿੰਗ, ਕੈਸ਼ਿੰਗ, ਅਤੇ ਵਰਜਨਿੰਗ ਨਾਲ ਵਿਸਤ੍ਰਿਤ ਮਾਡਲ ਲਾਈਫਸਾਈਕਲ ਪ੍ਰਬੰਧਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਪਲੇਟਫਾਰਮ Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral, ਅਤੇ ਵਿਸ਼ੇਸ਼ ਐਮਬੈਡਿੰਗ ਮਾਡਲਾਂ ਸਮੇਤ ਇੱਕ ਵਿਸਤ੍ਰਿਤ ਮਾਡਲ ਪਰਿਸਰ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ।

**ਮਾਡਲਫਾਈਲਾਂ ਰਾਹੀਂ ਅਨੁਕੂਲਤਾ**: ਉੱਚ-ਸਤਰੀ ਉਪਭੋਗਤਾ ਵਿਸ਼ੇਸ਼ ਪੈਰਾਮੀਟਰਾਂ, ਸਿਸਟਮ ਪ੍ਰੋਮਟਾਂ, ਅਤੇ ਵਿਹਾਰ ਵਿੱਚ ਤਬਦੀਲੀਆਂ ਨਾਲ ਕਸਟਮ ਮਾਡਲ ਸੰਰਚਨਾਵਾਂ ਬਣਾਉਣ ਦੇ ਯੋਗ ਹਨ। ਇਹ ਖੇਤਰ-ਵਿਸ਼ੇਸ਼ ਅਨੁਕੂਲਤਾ ਅਤੇ ਵਿਸ਼ੇਸ਼ ਐਪਲੀਕੇਸ਼ਨ ਜ਼ਰੂਰਤਾਂ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

**ਪ੍ਰਦਰਸ਼ਨ ਅਨੁਕੂਲਤਾ**: Ollama ਆਟੋਮੈਟਿਕ ਤੌਰ 'ਤੇ ਉਪਲਬਧ ਹਾਰਡਵੇਅਰ ਐਕਸਲੇਰੇਸ਼ਨ ਦੀ ਪਛਾਣ ਅਤੇ ਵਰਤੋਂ ਕਰਦਾ ਹੈ ਜਿਸ ਵਿੱਚ NVIDIA CUDA, Apple Metal, ਅਤੇ OpenCL ਸ਼ਾਮਲ ਹਨ। ਬੁੱਧੀਮਾਨ ਮੈਮੋਰੀ ਪ੍ਰਬੰਧਨ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਸੰਰਚਨਾਵਾਂ ਵਿੱਚ ਵਧੀਆ ਸਰੋਤ ਵਰਤੋਂ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

### ਉਤਪਾਦਨ ਕਾਰਜਨੁਮਾਂ ਦੀ ਰਣਨੀਤੀ

**ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਸੈਟਅੱਪ**: Ollama ਵੱਖ-ਵੱਖ ਪਲੇਟਫਾਰਮਾਂ ਵਿੱਚ ਸਥਾਨਕ ਇੰਸਟਾਲਰਾਂ, ਪੈਕੇਜ ਮੈਨੇਜਰਾਂ (WinGet, Homebrew, APT), ਅਤੇ Docker ਕੰਟੇਨਰਾਂ ਰਾਹੀਂ ਸਧਾਰਨ ਇੰਸਟਾਲੇਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**ਮੁੱਢਲੇ ਕਮਾਂਡ ਅਤੇ ਕਾਰਵਾਈਆਂ**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**ਉੱਚ-ਸਤਰੀ ਸੰਰਚਨਾ**: ਮਾਡਲਫਾਈਲਾਂ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਜ਼ਰੂਰਤਾਂ ਲਈ ਵਿਸ਼ੇਸ਼ ਅਨੁਕੂਲਤਾ ਯਕੀਨੀ ਬਣਾਉਂਦੀਆਂ ਹਨ:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### ਡਿਵੈਲਪਰ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਉਦਾਹਰਨ

**Python API ਇੰਟੀਗ੍ਰੇਸ਼ਨ**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript ਇੰਟੀਗ੍ਰੇਸ਼ਨ (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API ਦੀ ਵਰਤੋਂ cURL ਨਾਲ**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### ਪ੍ਰਦਰਸ਼ਨ ਟਿਊਨਿੰਗ ਅਤੇ ਅਨੁਕੂਲਤਾ

**ਮੈਮੋਰੀ ਅਤੇ ਥ੍ਰੈਡ ਸੰਰਚਨਾ**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਲਈ ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ ਚੋਣ**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: ਇੰਟਰਪ੍ਰਾਈਜ਼ ਐਜ AI ਪਲੇਟਫਾਰਮ

### ਇੰਟਰਪ੍ਰਾਈਜ਼-ਗ੍ਰੇਡ ਆਰਕੀਟੈਕਚਰ

Microsoft Foundry Local ਇੱਕ ਵਿਸਤ੍ਰਿਤ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਹੱਲ ਹੈ ਜੋ Microsoft ਪਰਿਸਰ ਵਿੱਚ ਡੂੰਘੀ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਨਾਲ ਉਤਪਾਦਨ ਐਜ AI ਤੈਨਾਤੀ ਲਈ ਖਾਸ ਤੌਰ 'ਤੇ ਡਿਜ਼ਾਈਨ ਕੀਤਾ ਗਿਆ ਹੈ।

**ONNX-ਅਧਾਰਿਤ ਅਧਾਰ**: ਉਦਯੋਗ-ਮਿਆਰੀ ONNX Runtime 'ਤੇ ਬਣਿਆ, Foundry Local ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਆਰਕੀਟੈਕਚਰਾਂ ਵਿੱਚ ਅਨੁਕੂਲ ਪ੍ਰਦਰਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਪਲੇਟਫਾਰਮ Windows ML ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਨੂੰ ਵਰਤਦਾ ਹੈ ਜੋ ਸਥਾਨਕ Windows ਅਨੁਕੂਲਤਾ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ ਜਦੋਂ ਕਿ ਕਰਾਸ-ਪਲੇਟਫਾਰਮ ਅਨੁਕੂਲਤਾ ਬਣਾਈ ਰੱਖਦਾ ਹੈ।

**ਹਾਰਡਵੇਅਰ ਐਕਸਲੇਰੇਸ਼ਨ ਸ਼੍ਰੇਸ਼ਠਤਾ**: Foundry Local CPUs, GPUs, ਅਤੇ NPUs ਵਿੱਚ ਬੁੱਧੀਮਾਨ ਹਾਰਡਵੇਅਰ ਪਛਾਣ ਅਤੇ ਅਨੁਕੂਲਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਹਾਰਡਵੇਅਰ ਵਿਕਰੇਤਾ (AMD, Intel, NVIDIA, Qualcomm) ਨਾਲ ਡੂੰਘੇ ਸਹਿਯੋਗ ਵਪਾਰਕ ਹਾਰਡਵੇਅਰ ਸੰਰਚਨਾਵਾਂ 'ਤੇ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

### ਉੱਚ-ਸਤਰੀ ਡਿਵੈਲਪਰ ਅਨੁਭਵ

**ਮਲਟੀ-ਇੰਟਰਫੇਸ ਪਹੁੰਚ**: Foundry Local ਵਿਸ਼ੇਸ਼ CLI, ਬਹੁ-ਭਾਸ਼ਾ SDKs (Python, NodeJS), ਅਤੇ RESTful APIs ਨਾਲ ਵਿਸ਼ੇਸ਼ ਵਿਕਾਸ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

**Visual Studio ਇੰਟੀਗ੍ਰੇਸ਼ਨ**: ਪਲੇਟਫਾਰਮ VS Code ਲਈ AI Toolkit ਨਾਲ ਸਹੀ ਤੌਰ 'ਤੇ ਇੰਟੀਗ੍ਰੇਟ ਹੁੰਦਾ ਹੈ, ਵਿਕਾਸ ਦੇ ਵਾਤਾਵਰਣ ਵਿੱਚ ਮਾਡਲ ਰੂਪਾਂਤਰਨ, ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ, ਅਤੇ ਅਨੁਕੂਲਤਾ ਟੂਲ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਇਹ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਵਿਕਾਸ ਵਰਕਫਲੋਜ਼ ਨੂੰ ਤੇਜ਼ ਕਰਦਾ ਹੈ ਅਤੇ ਤੈਨਾਤੀ ਦੀ ਜਟਿਲਤਾ ਨੂੰ ਘਟਾਉਂਦਾ ਹੈ।

**ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਪਾਈਪਲਾਈਨ**: Microsoft Olive ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਗ੍ਰਾਫ ਅਨੁਕੂਲਤਾ ਅਤੇ ਹਾਰਡਵੇਅਰ-ਵਿਸ਼ੇਸ਼ ਟਿਊਨਿੰਗ ਸਮੇਤ ਵਿਸ਼ੇਸ਼ ਮਾਡਲ ਅਨੁਕੂਲਤਾ ਵਰਕਫਲੋਜ਼ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ। Azure ML ਰਾਹੀਂ ਕਲਾਉਡ-ਅਧਾਰਿਤ ਰੂਪਾਂਤਰਨ ਸਮਰਥਾ ਵੱਡੇ ਮਾਡਲਾਂ ਲਈ ਸਕੇਲਬਲ ਅਨੁਕੂਲਤਾ ਪ੍ਰਦਾਨ ਕਰਦੀ ਹੈ।

### ਉਤਪਾਦਨ ਕਾਰਜਨੁਮਾਂ ਦੀ ਰਣਨੀਤੀ

**ਇੰਸਟਾਲੇਸ਼ਨ ਅਤੇ ਸੰਰਚਨਾ**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**ਮਾਡਲ ਪ੍ਰਬੰਧਨ ਕਾਰਵਾਈਆਂ**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**ਉੱਚ-ਸਤਰੀ ਤੈਨਾਤੀ ਸੰਰਚਨਾ**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### ਇੰਟਰਪ੍ਰਾਈਜ਼ ਪਰਿਸਰ ਇੰਟੀਗ੍ਰੇਸ਼ਨ

**ਸੁਰੱਖਿਆ ਅਤੇ ਪਾਲਣਾ**: Foundry Local ਇੰਟਰਪ੍ਰਾਈਜ਼-ਗ੍ਰੇਡ ਸੁਰੱਖਿਆ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਜਿਸ ਵਿੱਚ ਰੋਲ-ਅਧਾਰਿਤ ਪਹੁੰਚ ਨਿਯੰਤਰਣ, ਆਡਿਟ ਲੌਗਿੰਗ, ਪਾਲਣਾ ਰਿਪੋਰਟਿੰਗ, ਅਤੇ ਇੰਕ੍ਰਿਪਟਡ ਮਾਡਲ ਸਟੋਰੇਜ ਸ਼ਾਮਲ ਹਨ। Microsoft ਸੁਰੱਖਿਆ ਬੁਨਿਆਦ ਨਾਲ ਇੰਟੀਗ੍ਰੇਸ਼ਨ ਇੰਟਰਪ੍ਰਾਈਜ਼ ਸੁਰੱਖਿਆ ਨੀਤੀਆਂ ਦੀ ਪਾਲਣਾ ਯਕੀਨੀ ਬਣਾਉਂਦਾ ਹੈ।

**ਅੰਦਰੂਨੀ AI ਸੇਵਾਵਾਂ**: ਪਲੇਟਫਾਰਮ Phi Silica ਲਈ ਸਥਾਨਕ ਭਾਸ਼ਾ ਪ੍ਰੋਸੈਸਿੰਗ, AI Imaging ਲਈ ਚਿੱਤਰ ਸੁਧਾਰ ਅਤੇ ਵਿਸ਼ਲੇਸ਼ਣ, ਅਤੇ ਆਮ ਇੰਟਰਪ੍ਰਾਈਜ਼ AI ਕਾਰਜਾਂ ਲਈ ਵਿਸ਼ੇਸ਼ APIs ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ।

## Ollama ਅਤੇ Foundry Local ਦੀ ਤੁਲਨਾ

### ਤਕਨੀਕੀ ਆਰਕੀਟੈਕਚਰ ਦੀ ਤੁਲਨਾ

| **ਪਹਲੂ** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **ਮਾਡਲ ਫਾਰਮੈਟ** | GGUF (via llama.cpp) | ONNX (via ONNX Runtime) |
| **ਪਲੇਟਫਾਰਮ ਫੋਕਸ** | ਯੂਨੀਵਰਸਲ ਕਰਾਸ-ਪਲੇਟਫਾਰਮ | Windows/ਇੰਟਰਪ੍ਰਾਈਜ਼ ਅਨੁਕੂਲਤਾ |
| **ਹਾਰਡਵੇਅਰ ਇੰਟੀਗ੍ਰੇਸ਼ਨ** | ਜਨਰਲ GPU/CPU ਸਮਰਥਨ | ਡੂੰਘੀ Windows ML, NPU ਸਮਰਥਨ |
| **ਅਨੁਕੂਲਤਾ** | llama.cpp ਕੁਆਂਟਾਈਜ਼ੇਸ਼ਨ | Microsoft Olive + ONNX Runtime |
| **ਇੰਟਰਪ੍ਰਾਈਜ਼ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ** | ਕਮਿਊਨਿਟੀ-ਚਲਾਇਆ | ਇੰਟਰਪ੍ਰਾਈਜ਼-ਗ੍ਰੇਡ SLAs ਨਾਲ |

### ਪ੍ਰਦਰਸ਼ਨ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

**Ollama ਪ੍ਰਦਰਸ਼ਨ ਤਾਕਤਾਂ**:
- llama.cpp ਅਨੁਕੂਲਤਾ ਰਾਹੀਂ ਸ਼ਾਨਦਾਰ CPU ਪ੍ਰਦਰਸ਼ਨ
- ਵੱਖ-ਵੱਖ ਪਲੇਟਫਾਰਮਾਂ ਅਤੇ ਹਾਰਡਵੇਅਰ 'ਤੇ ਸਥਿਰ ਵਿਹਾਰ
- ਬੁੱਧੀਮਾਨ ਮਾਡਲ ਲੋਡਿੰਗ ਨਾਲ ਕੁਸ਼ਲ ਮੈਮੋਰੀ ਵਰਤੋਂ
- ਵਿਕਾਸ ਅਤੇ ਟੈਸਟਿੰਗ ਦ੍ਰਿਸ਼ਾਂ ਲਈ ਤੇਜ਼ ਕੋਲਡ-ਸਟਾਰਟ ਸਮਾਂ

**Foundry Local ਪ੍ਰਦਰਸ਼ਨ ਫਾਇਦੇ**:
- ਆਧੁਨਿਕ Windows ਹਾਰਡਵੇਅਰ

---

**ਅਸਵੀਕਰਤੀ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।