<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T15:08:27+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "my"
}
-->
# အပိုင်း ၃ : Microsoft Olive Optimization Suite

## အကြောင်းအရာများ
1. [မိတ်ဆက်](../../../Module04)
2. [Microsoft Olive ဆိုတာဘာလဲ?](../../../Module04)
3. [တပ်ဆင်ခြင်း](../../../Module04)
4. [လွယ်ကူသော စတင်အသုံးပြုနည်းလမ်းညွှန်](../../../Module04)
5. [ဥပမာ: Qwen3 ကို ONNX INT4 သို့ ပြောင်းလဲခြင်း](../../../Module04)
6. [အဆင့်မြင့်အသုံးပြုမှု](../../../Module04)
7. [Olive Recipes Repository](../../../Module04)
8. [အကောင်းဆုံး လုပ်ဆောင်နည်းများ](../../../Module04)
9. [ပြဿနာများကို ဖြေရှင်းခြင်း](../../../Module04)
10. [အပိုဆောင်း အရင်းအမြစ်များ](../../../Module04)

## မိတ်ဆက်

Microsoft Olive သည် hardware-aware model optimization toolkit အားလုံးကို လွယ်ကူစွာ အသုံးပြုနိုင်သော အင်အားကြီး tool တစ်ခုဖြစ်ပြီး machine learning model များကို အမျိုးမျိုးသော hardware platform များတွင် အသုံးပြုရန်အတွက် optimization လုပ်ဆောင်ခြင်းကို လွယ်ကူစေပါသည်။ CPU, GPU, သို့မဟုတ် AI accelerators အထူးပြု hardware များကို ရည်ရွယ်ထားပါက Olive သည် model accuracy ကို ထိန်းသိမ်းထားပြီး အကောင်းဆုံး performance ကို ရရှိစေပါသည်။

## Microsoft Olive ဆိုတာဘာလဲ?

Olive သည် hardware-aware model optimization tool တစ်ခုဖြစ်ပြီး model compression, optimization, compilation စသည်တို့အတွက် စက်မှုလုပ်ငန်းအဆင့် techniques များကို ပေါင်းစပ်ထားသည်။ ONNX Runtime နှင့်အတူ E2E inference optimization solution အဖြစ် အလုပ်လုပ်ပါသည်။

### အဓိက အင်္ဂါရပ်များ

- **Hardware-Aware Optimization**: သင့်ရည်ရွယ်ထားသော hardware အတွက် အကောင်းဆုံး optimization techniques ကို အလိုအလျောက် ရွေးချယ်ပေးသည်။
- **40+ Built-in Optimization Components**: Model compression, quantization, graph optimization စသည်တို့ကို အကျုံးဝင်သည်။
- **Easy CLI Interface**: Optimization လုပ်ဆောင်မှုများအတွက် လွယ်ကူသော command များ
- **Multi-Framework Support**: PyTorch, Hugging Face models, ONNX နှင့်အတူ အလုပ်လုပ်နိုင်သည်။
- **Popular Model Support**: Llama, Phi, Qwen, Gemma စသည်တို့ကဲ့သို့သော model architecture များကို အလိုအလျောက် optimize လုပ်နိုင်သည်။

### အကျိုးကျေးဇူးများ

- **ဖွံ့ဖြိုးတိုးတက်မှုအချိန် လျှော့ချခြင်း**: အမျိုးမျိုးသော optimization techniques များကို လက်တွေ့စမ်းသပ်ရန် မလိုအပ်တော့ပါ။
- **Performance တိုးတက်မှု**: အချို့သောအခြေအနေများတွင် ၆ ဆအထိ အမြန်နှုန်းတိုးတက်မှု
- **Cross-Platform Deployment**: Optimized models များကို hardware နှင့် operating systems များအနှံ့ အသုံးပြုနိုင်သည်။
- **Accuracy ထိန်းသိမ်းထားခြင်း**: Optimization များသည် model quality ကို ထိန်းသိမ်းထားပြီး performance ကို တိုးတက်စေသည်။

## တပ်ဆင်ခြင်း

### လိုအပ်ချက်များ

- Python 3.8 သို့မဟုတ် အထက်
- pip package manager
- Virtual environment (အကြံပြုသည်)

### အခြေခံ တပ်ဆင်ခြင်း

Virtual environment တစ်ခုကို ဖန်တီးပြီး အလုပ်လုပ်ပါ:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Auto-optimization features ဖြင့် Olive ကို တပ်ဆင်ပါ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### အပိုလိုအပ်ချက်များ

Olive သည် အပို features များအတွက် optional dependencies များကို ပေးထားသည်:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### တပ်ဆင်မှုကို အတည်ပြုပါ

```bash
olive --help
```

အောင်မြင်ပါက Olive CLI help message ကို မြင်ရပါမည်။

## လွယ်ကူသော စတင်အသုံးပြုနည်းလမ်းညွှန်

### သင့်ပထမဆုံး Optimization

Olive ၏ auto-optimization feature ကို အသုံးပြု၍ language model သေးငယ်တစ်ခုကို optimize လုပ်ပါ:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Command သည် ဘာလုပ်ဆောင်သနည်း

Optimization လုပ်ဆောင်မှုတွင် model ကို local cache မှရယူခြင်း၊ ONNX Graph ကို capture လုပ်ပြီး ONNX data file တွင် weight များကို သိမ်းဆည်းခြင်း၊ ONNX Graph ကို optimize လုပ်ခြင်း၊ RTN နည်းလမ်းဖြင့် model ကို int4 သို့ quantize လုပ်ခြင်းတို့ ပါဝင်သည်။

### Command Parameters ရှင်းလင်းချက်

- `--model_name_or_path`: Hugging Face model identifier သို့မဟုတ် local path
- `--output_path`: Optimized model ကို သိမ်းဆည်းမည့် directory
- `--device`: ရည်ရွယ်ထားသော device (cpu, gpu)
- `--provider`: Execution provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI ကို inference အတွက် အသုံးပြုပါ
- `--precision`: Quantization precision (int4, int8, fp16)
- `--log_level`: Logging verbosity (0=minimal, 1=verbose)

## ဥပမာ: Qwen3 ကို ONNX INT4 သို့ ပြောင်းလဲခြင်း

Hugging Face မှပေးထားသော [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) ဥပမာအရ Qwen3 model ကို optimize လုပ်နည်း:

### အဆင့် ၁: Model ကို Download လုပ်ပါ (Optional)

Download အချိန်ကို လျှော့ချရန် အရေးကြီးသော file များကိုသာ cache လုပ်ပါ:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### အဆင့် ၂: Qwen3 Model ကို Optimize လုပ်ပါ

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### အဆင့် ၃: Optimized Model ကို စမ်းသပ်ပါ

Optimized model ကို စမ်းသပ်ရန် Python script ရေးပါ:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Output Structure

Optimization ပြီးနောက် output directory တွင် ပါဝင်မည့်အရာများ:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## အဆင့်မြင့်အသုံးပြုမှု

### Configuration Files

ပိုမိုရှုပ်ထွေးသော optimization workflows များအတွက် JSON configuration files ကို အသုံးပြုနိုင်သည်:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Configuration ဖြင့် run လုပ်ပါ:

```bash
olive run --config config.json
```

### GPU Optimization

CUDA GPU optimization အတွက်:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) အတွက်:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive ဖြင့် Fine-tuning

Olive သည် model များကို fine-tuning လုပ်နိုင်သည်:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## အကောင်းဆုံး လုပ်ဆောင်နည်းများ

### 1. Model ရွေးချယ်ခြင်း
- စမ်းသပ်ရန် model သေးငယ်များ (ဥပမာ 0.5B-7B parameters) ဖြင့် စတင်ပါ။
- Olive မှာ target model architecture ကို support လုပ်နိုင်ကြောင်း သေချာပါစေ။

### 2. Hardware ကို စဉ်းစားပါ
- Optimization target ကို deployment hardware နှင့် ကိုက်ညီစေပါ။
- CUDA-compatible hardware ရှိပါက GPU optimization ကို အသုံးပြုပါ။
- Windows machine များတွင် integrated graphics ရှိပါက DirectML ကို စဉ်းစားပါ။

### 3. Precision ရွေးချယ်ခြင်း
- **INT4**: အများဆုံး compression, accuracy အနည်းငယ်ဆုံး ဆုံးရှုံးမှု
- **INT8**: အရွယ်အစားနှင့် accuracy အကြား အကောင်းဆုံး balance
- **FP16**: Accuracy ဆုံးရှုံးမှု အနည်းငယ်, အရွယ်အစား လျှော့ချမှု အလယ်အလတ်

### 4. စမ်းသပ်ခြင်းနှင့် အတည်ပြုခြင်း
- Optimized models များကို သင့်ရည်ရွယ်ထားသော use cases များနှင့်အတူ စမ်းသပ်ပါ။
- Performance metrics (latency, throughput, accuracy) ကို နှိုင်းယှဉ်ပါ။
- အကဲဖြတ်ရန် representative input data ကို အသုံးပြုပါ။

### 5. Iterative Optimization
- အမြန်ရလဒ်များအတွက် auto-optimization ဖြင့် စတင်ပါ။
- Fine-grained control အတွက် configuration files ကို အသုံးပြုပါ။
- Optimization passes များကို အမျိုးမျိုး စမ်းသပ်ပါ။

## ပြဿနာများကို ဖြေရှင်းခြင်း

### အများဆုံး ကြုံရသော ပြဿနာများ

#### 1. တပ်ဆင်မှု ပြဿနာများ
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU ပြဿနာများ
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. မှတ်ဉာဏ် ပြဿနာများ
- Optimization လုပ်စဉ်တွင် batch size သေးငယ်များကို အသုံးပြုပါ။
- အရင်ဆုံး precision မြင့်မားသော quantization (int8) ကို စမ်းသပ်ပါ။
- Model caching အတွက် disk space လုံလောက်စေရန် သေချာပါစေ။

#### 4. Model Loading အမှားများ
- Model path နှင့် access permissions ကို စစ်ဆေးပါ။
- Model သည် `trust_remote_code=True` လိုအပ်ကြောင်း စစ်ဆေးပါ။
- Model files အားလုံးကို download လုပ်ထားကြောင်း သေချာပါစေ။

### အကူအညီရယူခြင်း

- **Documentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Examples**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes Repository

### Olive Recipes မိတ်ဆက်

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) repository သည် Olive toolkit ကို ဖြည့်စွက်ပေးပြီး AI models များအတွက် optimization recipes များကို အကျုံးဝင်စေသည်။ Public models များကို optimize လုပ်ခြင်းနှင့် proprietary models များအတွက် optimization workflows ဖန်တီးခြင်းတို့အတွက် အသုံးဝင်သော လမ်းညွှန်အဖြစ် တည်ဆောက်ထားသည်။

### အဓိက အင်္ဂါရပ်များ

- **100+ Pre-built Recipes**: Popular models များအတွက် optimization configurations
- **Multi-Architecture Support**: Transformer models, vision models, multimodal architectures အကျုံးဝင်သည်။
- **Hardware-Specific Optimizations**: CPU, GPU, AI accelerators အတွက် recipes
- **Popular Model Families**: Phi, Llama, Qwen, Gemma, Mistral စသည်တို့ ပါဝင်သည်။

### Supported Model Families

Repository တွင် optimization recipes ပါဝင်သော model များ:

#### Language Models
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B to 14B)
- **Google Gemma**: Gemma model configurations များ
- **Mistral AI**: Mistral-7B series
- **DeepSeek**: R1-Distill series models

#### Vision နှင့် Multimodal Models
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: CLIP-ViT configurations များ
- **ResNet**: ResNet-50 optimizations
- **Vision Transformers**: ViT-base-patch16-224

#### Specialized Models
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Base နှင့် multilingual variants
- **Sentence Transformers**: all-MiniLM-L6-v2

### Olive Recipes အသုံးပြုခြင်း

#### နည်းလမ်း ၁: Recipe တစ်ခုကို Clone လုပ်ပါ

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### နည်းလမ်း ၂: Template အဖြစ် Recipe ကို အသုံးပြုပါ

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Recipe Structure

Recipe directory တစ်ခုတွင် အများအားဖြင့် ပါဝင်မည့်အရာများ:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### ဥပမာ: Phi-4-mini Recipe အသုံးပြုခြင်း

Phi-4-mini recipe ကို ဥပမာအဖြစ် အသုံးပြုပါ:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Configuration file တွင် အများအားဖြင့် ပါဝင်မည့်အရာများ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Recipes ကို Customize လုပ်ခြင်း

#### Target Hardware ကို ပြောင်းလဲခြင်း

`systems` section ကို update လုပ်ပါ:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimization Parameters ကို ပြင်ဆင်ခြင်း

`passes` section ကို ပြောင်းလဲပါ:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### သင့်ကိုယ်ပိုင် Recipe ဖန်တီးခြင်း

1. **တူညီသော Model ဖြင့် စတင်ပါ**: Architecture တူသော model recipe ကို ရှာပါ။
2. **Model Configuration ကို Update လုပ်ပါ**: Configuration တွင် model name/path ကို ပြောင်းပါ။
3. **Parameters ကို ပြင်ဆင်ပါ**: Optimization parameters ကို လိုအပ်သလို ပြင်ပါ။
4. **စမ်းသပ်ပြီး အတည်ပြုပါ**: Optimization ကို run လုပ်ပြီး ရလဒ်များကို အတည်ပြုပါ။
5. **Community သို့ ပြန်လည်ပေးအပ်ပါ**: သင့် recipe ကို repository သို့ ပေးအပ်ပါ။

### Recipes အသုံးပြုခြင်း၏ အကျိုးကျေးဇူးများ

#### 1. **အတည်ပြုထားသော Configurations**
- Model-specific optimization settings များကို စမ်းသပ်ပြီးသား
- Optimal parameters ရှာဖွေရန် trial-and-error မလိုအပ်တော့ပါ။

#### 2. **Hardware-Specific Tuning**
- Execution providers များအတွက် pre-optimized
- CPU, GPU, NPU targets အတွက် configurations အသင့်ရှိသည်။

#### 3. **ကျယ်ကျယ်ပြန့်ပြန့် Coverage**
- Popular open-source models များကို support လုပ်သည်။
- Model အသစ်များ release လုပ်သည်နှင့်အတူ regular updates ရရှိသည်။

#### 4. **Community Contributions**
- AI community နှင့် ပူးပေါင်းဖွံ့ဖြိုးတိုးတက်မှု
- Shared knowledge နှင့် best practices

### Olive Recipes သို့ ပေးအပ်ခြင်း

သင့် optimized model သည် repository တွင် မပါဝင်ပါက:

1. **Repository ကို Fork လုပ်ပါ**: Olive-recipes ကို သင့်အတွက် fork လုပ်ပါ။
2. **Recipe Directory ဖန်တီးပါ**: Model အတွက် directory အသစ်တစ်ခု ထည့်ပါ။
3. **Configuration ထည့်ပါ**: olive_config.json နှင့် support files များ ထည့်ပါ။
4. **အသုံးပြုနည်းကို Documentation လုပ်ပါ**: README file တွင် အသုံးပြုနည်းကို ရှင်းလင်းရေးသားပါ။
5. **Pull Request တင်ပါ**: Community သို့ ပြန်လည်ပေးအပ်ပါ။

### Performance Benchmarks

Recipes များတွင် performance benchmarks ပါဝင်ပြီး:

- **Latency Improvements**: Baseline ထက် ၂-၆ ဆအထိ အမြန်နှုန်းတိုးတက်မှု
- **Memory Reduction**: Quantization ဖြင့် memory usage ကို ၅၀-၇၅% လျှော့ချနိုင်သည်။
- **Accuracy Retention**: Accuracy ကို ၉၅-၉၉% ထိန်းသိမ်းထားသည်။

### AI Toolkit နှင့် ပေါင်းစည်းမှု

Recipes များသည် အောက်ပါများနှင့် အဆင်ပြေစွာ အလုပ်လုပ်သည်:
- **VS Code AI Toolkit**: Model optimization အတွက် direct integration
- **Azure Machine Learning**: Cloud-based optimization workflows
- **ONNX Runtime**: Optimized inference deployment

## အပိုဆောင်း အရင်းအမြစ်များ

### တရားဝင် Links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community Examples
- **Jupyter Notebooks**: Olive GitHub repository တွင် ရရှိနိုင်သည် — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit for VS Code overview — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Posts**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### ဆက်စပ် Tools
- **ONNX

---

**အကြောင်းကြားချက်**:  
ဤစာရွက်စာတမ်းကို AI ဘာသာပြန်ဝန်ဆောင်မှု [Co-op Translator](https://github.com/Azure/co-op-translator) ကို အသုံးပြု၍ ဘာသာပြန်ထားပါသည်။ ကျွန်ုပ်တို့သည် တိကျမှုအတွက် ကြိုးစားနေသော်လည်း အလိုအလျောက် ဘာသာပြန်မှုများတွင် အမှားများ သို့မဟုတ် မတိကျမှုများ ပါဝင်နိုင်သည်ကို သတိပြုပါ။ မူရင်းဘာသာစကားဖြင့် ရေးသားထားသော စာရွက်စာတမ်းကို အာဏာတရ အရင်းအမြစ်အဖြစ် သတ်မှတ်သင့်ပါသည်။ အရေးကြီးသော အချက်အလက်များအတွက် လူက ဘာသာပြန်မှုကို အသုံးပြုရန် အကြံပြုပါသည်။ ဤဘာသာပြန်မှုကို အသုံးပြုခြင်းမှ ဖြစ်ပေါ်လာသော အလွဲအမှားများ သို့မဟုတ် အနားလွဲမှုများအတွက် ကျွန်ုပ်တို့သည် တာဝန်မယူပါ။