<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T09:16:38+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "hk"
}
-->
# 第一章：EdgeAI 基礎知識

EdgeAI 代表人工智能部署的一個新範式，將 AI 能力直接帶到邊緣設備，而不僅僅依賴於基於雲端的處理。了解 EdgeAI 如何在資源有限的設備上進行本地 AI 處理，同時保持合理的性能並解決隱私、延遲和離線功能等挑戰是非常重要的。

## 簡介

在本課程中，我們將探討 EdgeAI 及其基本概念。我們將涵蓋傳統 AI 計算範式、邊緣計算的挑戰、支持 EdgeAI 的關鍵技術，以及在各行業中的實際應用。

## 學習目標

完成本課程後，您將能夠：

- 理解傳統基於雲端的 AI 方法與 EdgeAI 方法的區別。
- 識別支持邊緣設備上 AI 處理的關鍵技術。
- 認識 EdgeAI 實施的優勢和局限性。
- 將 EdgeAI 的知識應用於真實場景和使用案例。

## 理解傳統 AI 計算範式

傳統上，生成式 AI 應用依賴於高性能計算基礎設施來有效運行大型語言模型（LLMs）。組織通常將這些模型部署在雲端環境中的 GPU 集群上，通過 API 接口訪問其功能。

這種集中式模型對許多應用來說效果良好，但在邊緣計算場景中存在固有的局限性。傳統方法涉及將用戶查詢發送到遠程服務器，使用強大的硬件進行處理，並通過互聯網返回結果。雖然此方法提供了最先進的模型訪問，但它對互聯網連接產生依賴，引入了延遲問題，並在需要將敏感數據傳輸到外部服務器時引發隱私問題。

在使用傳統 AI 計算範式時，我們需要理解一些核心概念：

- **☁️ 基於雲端的處理**：AI 模型在具有高計算資源的強大服務器基礎設施上運行。
- **🔌 基於 API 的訪問**：應用通過遠程 API 調用訪問 AI 功能，而非本地處理。
- **🎛️ 集中式模型管理**：模型集中維護和更新，確保一致性但需要網絡連接。
- **📈 資源可擴展性**：雲基礎設施可以動態擴展以應對不同的計算需求。

## 邊緣計算的挑戰

邊緣設備如筆記本電腦、手機以及物聯網（IoT）設備（如 Raspberry Pi 和 NVIDIA Orin Nano）具有獨特的計算限制。與數據中心基礎設施相比，這些設備通常具有有限的處理能力、內存和能源資源。

由於這些硬件限制，傳統 LLMs 在此類設備上運行一直具有挑戰性。然而，在各種場景中，邊緣 AI 處理的需求變得越來越重要。考慮以下情況：互聯網連接不可靠或不可用，例如偏遠的工業地點、行駛中的車輛或網絡覆蓋不佳的地區。此外，要求高安全標準的應用（如醫療設備、金融系統或政府應用）可能需要本地處理敏感數據以保持隱私和合規性。

### 邊緣計算的主要限制

邊緣計算環境面臨一些傳統基於雲端的 AI 解決方案所不遇到的基本限制：

- **有限的處理能力**：邊緣設備通常具有較少的 CPU 核心和較低的時鐘速度，與服務器級硬件相比。
- **內存限制**：邊緣設備上的可用 RAM 和存儲容量顯著減少。
- **能源限制**：電池供電的設備必須在性能和能源消耗之間取得平衡，以延長運行時間。
- **熱管理**：緊湊的外形限制了冷卻能力，影響了負載下的持續性能。

## 什麼是 EdgeAI？

### 概念：EdgeAI 定義

EdgeAI 是指人工智能算法直接部署和執行於邊緣設備——即網絡“邊緣”處的物理硬件，靠近數據生成和收集的地方。這些設備包括智能手機、IoT 傳感器、智能攝像頭、自動駕駛車輛、可穿戴設備和工業設備。與依賴雲服務器進行處理的傳統 AI 系統不同，EdgeAI 將智能直接帶到數據源。

EdgeAI 的核心是去中心化 AI 處理，將其從集中式數據中心移動到構成我們數字生態系統的廣泛設備網絡中分佈。這代表了 AI 系統設計和部署方式的根本架構轉變。

EdgeAI 的關鍵概念支柱包括：

- **就近處理**：計算在數據來源附近物理發生
- **分散式智能**：決策能力分佈於多個設備
- **數據主權**：信息保持在本地控制之下，通常不離開設備
- **自主運行**：設備可以在不需要持續連接的情況下智能運行
- **嵌入式 AI**：智能成為日常設備的內在能力

### EdgeAI 架構可視化

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI 代表人工智能部署的一個新範式，將 AI 能力直接帶到邊緣設備，而不僅僅依賴於基於雲端的處理。此方法使 AI 模型能夠在具有有限計算資源的設備上本地運行，提供實時推理能力而無需持續的互聯網連接。

EdgeAI 涵蓋了各種技術和方法，旨在使 AI 模型更高效並適合在資源有限的設備上部署。目標是在顯著降低 AI 模型的計算和內存需求的同時保持合理的性能。

讓我們來看看支持 EdgeAI 在不同設備類型和使用案例中實施的基本方法。

### EdgeAI 核心原則

EdgeAI 建立在幾個區別於傳統基於雲端 AI 的基礎原則之上：

- **本地處理**：AI 推理直接在邊緣設備上進行，無需外部連接。
- **資源優化**：模型專門針對目標設備的硬件限制進行優化。
- **實時性能**：處理以最小延遲進行，適用於時間敏感的應用。
- **隱私設計**：敏感數據保持在設備上，增強安全性和合規性。

## 支持 EdgeAI 的關鍵技術

### 模型量化

EdgeAI 中最重要的技術之一是模型量化。此過程涉及將模型參數的精度從 32 位浮點數降低到 8 位整數甚至更低的精度格式。雖然精度的降低可能看起來令人擔憂，但研究表明，許多 AI 模型即使在精度顯著降低的情況下仍能保持其性能。

量化通過將浮點值的範圍映射到一組較小的離散值來工作。例如，量化可能僅使用 8 位來表示每個參數，而不是使用 32 位，從而減少 4 倍的內存需求，並通常導致更快的推理時間。

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

不同的量化技術包括：

- **後訓練量化（PTQ）**：在模型訓練後應用，無需重新訓練
- **量化感知訓練（QAT）**：在訓練期間納入量化效果以提高準確性
- **動態量化**：將權重量化為 int8，但動態計算激活
- **靜態量化**：預先計算權重和激活的所有量化參數

對於 EdgeAI 部署，選擇適當的量化策略取決於特定的模型架構、性能需求以及目標設備的硬件能力。

### 模型壓縮和優化

除了量化之外，各種壓縮技術有助於減少模型大小和計算需求，包括：

**剪枝**：此技術移除神經網絡中不必要的連接或神經元。通過識別並消除對模型性能貢獻較小的參數，剪枝可以顯著減少模型大小，同時保持準確性。

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**知識蒸餾**：此方法涉及訓練一個較小的“學生”模型來模仿較大的“教師”模型的行為。學生模型學習近似教師的輸出，通常以顯著較少的參數實現類似的性能。

**模型架構優化**：研究人員開發了專門設計用於邊緣部署的架構，例如 MobileNets、EfficientNets 和其他輕量級架構，平衡性能與計算效率。

### 小型語言模型（SLMs）

EdgeAI 的一個新興趨勢是小型語言模型（SLMs）的開發。這些模型從零開始設計，既緊湊又高效，同時仍提供有意義的自然語言功能。SLMs 通過精心的架構選擇、高效的訓練技術以及專注於特定領域或任務的訓練來實現。

與傳統方法壓縮大型模型不同，SLMs 通常使用較小的數據集和專門設計的架構進行訓練，專門針對邊緣部署。此方法可以產生不僅更小且更高效的模型，適用於特定使用案例。

## EdgeAI 的硬件加速

現代邊緣設備越來越多地包括專門設計用於加速 AI 工作負載的硬件：

### 神經處理單元（NPUs）

NPUs 是專門設計用於神經網絡計算的處理器。這些芯片可以比傳統 CPU 更高效地執行 AI 推理任務，通常具有更低的功耗。許多現代智能手機、筆記本電腦和 IoT 設備現在都包括 NPUs，以支持設備上的 AI 處理。

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

具有 NPUs 的設備包括：

- **Apple**：配備 Neural Engine 的 A 系列和 M 系列芯片
- **Qualcomm**：配備 Hexagon DSP/NPU 的 Snapdragon 處理器
- **Samsung**：配備 NPU 的 Exynos 處理器
- **Intel**：Movidius VPUs 和 Habana Labs 加速器
- **Microsoft**：配備 NPUs 的 Windows Copilot+ PC

### 🎮 GPU 加速

雖然邊緣設備可能沒有數據中心中的強大 GPU，但許多仍包括集成或獨立 GPU，可以加速 AI 工作負載。現代移動 GPU 和集成圖形處理器可以為 AI 推理任務提供顯著的性能提升。

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU 優化

即使是僅配備 CPU 的設備也可以通過優化實現 EdgeAI。現代 CPU 包括專門的 AI 工作負載指令，並且已開發出軟件框架以最大化 CPU 在 AI 推理中的性能。

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

對於從事 EdgeAI 的軟件工程師來說，了解如何利用這些硬件加速選項對於優化目標設備上的推理性能和能源效率至關重要。

## EdgeAI 的優勢

### 隱私和安全

EdgeAI 的最大優勢之一是增強的隱私和安全性。通過在設備上本地處理數據，敏感信息永遠不會離開用戶的控制範圍。這對於處理個人數據、醫療信息或機密商業數據的應用尤為重要。

### 減少延遲

EdgeAI 消除了將數據發送到遠程服務器進行處理的需求，顯著減少了延遲。這對於需要即時響應的應用（如自動駕駛車輛、工業自動化或交互式應用）至關重要。

### 離線功能

EdgeAI 即使在互聯網連接不可用時也能實現 AI 功能。這對於偏遠地區、旅行期間或網絡可靠性成問題的情況下的應用非常有價值。

### 成本效益

通過減少對基於雲端的 AI 服務的依賴，EdgeAI 可以幫助降低運營成本，特別是對於使用量大的應用。組織可以避免持續的 API 成本並減少帶寬需求。

### 可擴展性

EdgeAI 將計算負載分佈到邊緣設備上，而不是集中在數據中心。這可以幫助降低基礎設施成本並提高整體系統的可擴展性。

## EdgeAI 的應用

### 智能設備和物聯網

EdgeAI 為許多智能設備功能提供支持，從可以本地處理命令的語音助手到可以識別物體和人物的智能攝像頭，而無需將視頻發送到雲端。物聯網設備使用 EdgeAI 進行預測性維護、環境監測和自動化決策。

### 移動應用

智能手機和平板電腦使用 EdgeAI 提供各種功能，包括照片增強、實時翻譯、增強現實和個性化推薦。這些應用受益於本地處理的低延遲和隱私優勢。

### 工業應用

製造和工業環境使用 EdgeAI 進行質量控制、預測性維護和流程優化。這些應用通常需要實時處理，並可能在連接有限的環境中運行。

### 醫療保健

醫療設備和醫療應用使用 EdgeAI 進行患者監測、診斷輔助和治療建議。本地處理的隱私和安全性優勢在醫療應用中特別重要。

## 挑戰和局限性

### 性能取捨

EdgeAI 通常涉及模型大小、計算效率和性能之間的取捨。雖然量化和剪枝等技術可以顯著減少資源需求，但它們也可能影響模型的準確性或能力。

### 開發複雜性

開發 EdgeAI 應用需要專業知識和工具。開發人員必須了解優化技術、硬件能力和部署限制，這可能增加開發的複雜性。

### 硬件限制

儘管邊緣硬件取得了進步，但與數據中心基礎設施相比，這些設備仍然存在顯著限制。並非所有 AI 應用都能有效部署在邊緣設備上，有些可能需要混合方法。

### 模型更新和維護

更新部署在邊緣設備上的 AI 模型可能具有挑戰性，特別是對於連接或存儲容量有限的設備。組織必須制定模型版本管理、更新和維護的策略。

## EdgeAI 的未來

EdgeAI 領域正在快速發展，硬件、軟件和技術方面的持續進步。未來趨勢包括更多專門的邊緣 AI 芯片、改進的優化技術以及更好的 EdgeAI 開發和部署工具。

隨著 5G 網絡的普及，我們可能會看到結合邊緣處理和雲端能力的混合方法，實現更複雜的 AI 應用，同時保持本地處理的優勢。

EdgeAI 代表了一種更分散、更高效、更注重隱私的 AI 系統的根本轉變。隨著技術的不斷成熟，我們可以預期 EdgeAI 在支持各種應用和設備的 AI 能力方面變得越來越重要。

通過 EdgeAI 的 AI 民主化開啟了創新的新可能性，使開發者能夠創建在多樣化環境中可靠運行的 AI 驅動應用，同時尊重用戶隱私並提供響應迅速的實時體驗。理解 EdgeAI 對於任何從事 AI 技術的人來說越來越重要，因為它代表了 AI 在我們日常生活中部署和體驗的未來。

## ➡️ 下一步
- [02: EdgeAI 應用](02.RealWorldCaseStudies.md)

---

**免責聲明**：  
此文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要信息，建議使用專業人工翻譯。我們不對因使用此翻譯而引起的任何誤解或誤釋承擔責任。