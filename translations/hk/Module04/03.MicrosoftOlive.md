<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T11:19:06+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "hk"
}
-->
# 第三章：Microsoft Olive 優化套件

## 目錄
1. [簡介](../../../Module04)
2. [什麼是 Microsoft Olive？](../../../Module04)
3. [安裝](../../../Module04)
4. [快速入門指南](../../../Module04)
5. [範例：將 Qwen3 轉換為 ONNX INT4](../../../Module04)
6. [進階使用](../../../Module04)
7. [Olive 配方庫](../../../Module04)
8. [最佳實踐](../../../Module04)
9. [故障排除](../../../Module04)
10. [其他資源](../../../Module04)

## 簡介

Microsoft Olive 是一款功能強大且易於使用的硬件感知模型優化工具包，簡化了針對不同硬件平台部署機器學習模型的優化過程。無論您是針對 CPU、GPU 還是專用 AI 加速器，Olive 都能幫助您在保持模型準確性的同時實現最佳性能。

## 什麼是 Microsoft Olive？

Olive 是一款易於使用的硬件感知模型優化工具，結合了行業領先的模型壓縮、優化和編譯技術。它與 ONNX Runtime 配合使用，提供端到端的推理優化解決方案。

### 主要功能

- **硬件感知優化**：自動選擇最適合目標硬件的優化技術
- **40+ 內建優化組件**：涵蓋模型壓縮、量化、圖形優化等
- **簡易 CLI 介面**：使用簡單命令完成常見優化任務
- **多框架支持**：支持 PyTorch、Hugging Face 模型和 ONNX
- **流行模型支持**：Olive 可自動優化流行的模型架構，例如 Llama、Phi、Qwen、Gemma 等

### 優勢

- **縮短開發時間**：無需手動嘗試不同的優化技術
- **性能提升**：顯著的速度改進（某些情況下可達 6 倍）
- **跨平台部署**：優化後的模型可在不同硬件和操作系統上運行
- **保持準確性**：優化在提升性能的同時保留模型質量

## 安裝

### 先決條件

- Python 3.8 或更高版本
- pip 套件管理器
- 建議使用虛擬環境

### 基本安裝

建立並啟用虛擬環境：

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

安裝具有自動優化功能的 Olive：

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### 可選依賴項

Olive 提供多種可選依賴項以支持額外功能：

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### 驗證安裝

```bash
olive --help
```

如果成功，您應該能看到 Olive CLI 的幫助信息。

## 快速入門指南

### 第一次優化

讓我們使用 Olive 的自動優化功能來優化一個小型語言模型：

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 此命令的作用

優化過程包括：從本地緩存獲取模型，捕獲 ONNX 圖形並將權重存儲在 ONNX 數據文件中，優化 ONNX 圖形，並使用 RTN 方法將模型量化為 int4。

### 命令參數解析

- `--model_name_or_path`：Hugging Face 模型標識符或本地路徑
- `--output_path`：保存優化模型的目錄
- `--device`：目標設備（cpu, gpu）
- `--provider`：執行提供者（CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider）
- `--use_ort_genai`：使用 ONNX Runtime Generate AI 進行推理
- `--precision`：量化精度（int4, int8, fp16）
- `--log_level`：日誌詳細程度（0=最少，1=詳細）

## 範例：將 Qwen3 轉換為 ONNX INT4

根據 Hugging Face 提供的範例 [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)，以下是如何優化 Qwen3 模型的步驟：

### 步驟 1：下載模型（可選）

為了減少下載時間，只緩存必要文件：

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### 步驟 2：優化 Qwen3 模型

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 步驟 3：測試優化後的模型

創建一個簡單的 Python 腳本來測試您的優化模型：

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### 輸出結構

優化完成後，您的輸出目錄將包含：

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## 進階使用

### 配置文件

對於更複雜的優化工作流程，您可以使用 JSON 配置文件：

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

使用配置文件運行：

```bash
olive run --config config.json
```

### GPU 優化

針對 CUDA GPU 的優化：

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

針對 DirectML（Windows）：

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 使用 Olive 進行微調

Olive 也支持模型微調：

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## 最佳實踐

### 1. 模型選擇
- 從較小的模型開始測試（例如 0.5B-7B 參數）
- 確保您的目標模型架構受 Olive 支持

### 2. 硬件考量
- 將優化目標與部署硬件匹配
- 如果有 CUDA 兼容硬件，使用 GPU 優化
- 對於 Windows 機器，考慮使用 DirectML

### 3. 精度選擇
- **INT4**：最大壓縮，輕微的準確性損失
- **INT8**：大小和準確性的良好平衡
- **FP16**：最小的準確性損失，中等大小減少

### 4. 測試與驗證
- 始終使用您的特定用例測試優化模型
- 比較性能指標（延遲、吞吐量、準確性）
- 使用代表性輸入數據進行評估

### 5. 迭代優化
- 從自動優化開始以快速獲得結果
- 使用配置文件進行精細控制
- 嘗試不同的優化步驟

## 故障排除

### 常見問題

#### 1. 安裝問題
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU 問題
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. 記憶體問題
- 在優化過程中使用較小的批量大小
- 首先嘗試精度較高的量化（int8 而非 int4）
- 確保有足夠的磁盤空間用於模型緩存

#### 4. 模型加載錯誤
- 驗證模型路徑和訪問權限
- 檢查模型是否需要 `trust_remote_code=True`
- 確保所有必要的模型文件已下載

### 獲取幫助

- **文檔**：[microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub 問題**：[github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **範例**：[microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive 配方庫

### Olive 配方簡介

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) 配方庫補充了主要的 Olive 工具包，提供了全面的流行 AI 模型優化配方集合。該配方庫既是優化公開模型的實用參考，也可用於創建專有模型的優化工作流程。

### 主要功能

- **100+ 預建配方**：流行模型的即用型優化配置
- **多架構支持**：涵蓋 Transformer 模型、視覺模型和多模態架構
- **硬件特定優化**：針對 CPU、GPU 和專用加速器的配方
- **流行模型系列**：包括 Phi、Llama、Qwen、Gemma、Mistral 等

### 支持的模型系列

配方庫包含以下模型的優化配方：

#### 語言模型
- **Microsoft Phi**：Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**：Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**：Qwen1.5-7B, Qwen2-7B, Qwen2.5 系列（0.5B 至 14B）
- **Google Gemma**：各種 Gemma 模型配置
- **Mistral AI**：Mistral-7B 系列
- **DeepSeek**：R1-Distill 系列模型

#### 視覺和多模態模型
- **Stable Diffusion**：v1.4, XL-base-1.0
- **CLIP 模型**：各種 CLIP-ViT 配置
- **ResNet**：ResNet-50 優化
- **視覺 Transformer**：ViT-base-patch16-224

#### 專用模型
- **Whisper**：OpenAI Whisper-large-v3
- **BERT**：基礎和多語言版本
- **句子 Transformer**：all-MiniLM-L6-v2

### 使用 Olive 配方

#### 方法 1：克隆特定配方

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### 方法 2：使用配方作為模板

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### 配方結構

每個配方目錄通常包含：

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### 範例：使用 Phi-4-mini 配方

以下是使用 Phi-4-mini 配方的範例：

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

配置文件通常包括：

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### 自定義配方

#### 修改目標硬件

要更改目標硬件，更新 `systems` 部分：

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### 調整優化參數

修改 `passes` 部分以設置不同的優化級別：

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### 創建自己的配方

1. **從相似模型開始**：找到與目標模型架構相似的配方
2. **更新模型配置**：更改配置中的模型名稱/路徑
3. **調整參數**：根據需要修改優化參數
4. **測試與驗證**：運行優化並驗證結果
5. **回饋貢獻**：考慮將您的配方貢獻到配方庫

### 使用配方的優勢

#### 1. **經過驗證的配置**
- 特定模型的測試優化設置
- 避免在尋找最佳參數時的反覆試驗

#### 2. **硬件特定調整**
- 預先針對不同執行提供者進行優化
- 即用型配置適用於 CPU、GPU 和 NPU 目標

#### 3. **全面覆蓋**
- 支持最流行的開源模型
- 隨著新模型的發布定期更新

#### 4. **社群貢獻**
- 與 AI 社群合作開發
- 分享知識和最佳實踐

### 貢獻至 Olive 配方庫

如果您已優化了配方庫中未涵蓋的模型：

1. **Fork 配方庫**：創建自己的 olive-recipes 分支
2. **創建配方目錄**：為您的模型新增目錄
3. **包含配置**：添加 olive_config.json 和支持文件
4. **使用文檔**：提供清晰的 README 說明
5. **提交 Pull Request**：回饋社群

### 性能基準

許多配方包含性能基準，顯示：
- **延遲改進**：通常比基線快 2-6 倍
- **記憶體減少**：通過量化減少 50-75% 的記憶體使用
- **準確性保留**：保持 95-99% 的準確性

### 與 AI 工具包集成

配方可無縫集成至：
- **VS Code AI 工具包**：直接集成模型優化
- **Azure 機器學習**：基於雲的優化工作流程
- **ONNX Runtime**：優化推理部署

## 其他資源

### 官方連結
- **GitHub 儲存庫**：[github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive 配方庫**：[github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime 文檔**：[onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face 範例**：[huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### 社群範例
- **Jupyter 筆記本**：可在 Olive GitHub 儲存庫中找到 — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code 擴展**：VS Code AI 工具包概述 — https://learn.microsoft.com/azure/ai-toolkit/overview
- **部落格文章**：Microsoft 開源部落格 — https://opensource.microsoft.com/blog/

### 相關工具
- **ONNX Runtime**：高性能推理引擎 — https://onnxruntime.ai/
- **Hugging Face Transformers**：許多兼容模型的來源 — https://huggingface.co/docs/transformers/index
- **Azure 機器學習**：基於雲的優化工作流程 — https://learn.microsoft.com/azure/machine-learning/

## ➡️ 下一步

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**免責聲明**：  
此文件已使用人工智能翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，但請注意，自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。