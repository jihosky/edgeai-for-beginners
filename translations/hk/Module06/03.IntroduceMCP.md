<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T11:18:34+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "hk"
}
-->
# 第三章 - 模型上下文協議 (MCP) 整合

## MCP (模型上下文協議) 簡介

模型上下文協議 (MCP) 是一個開源標準，用於將人工智能應用程式連接到外部系統。透過 MCP，像 Claude 或 ChatGPT 這樣的人工智能應用程式可以連接到數據來源（例如本地文件、數據庫）、工具（例如搜索引擎、計算器）以及工作流程（例如專門的提示），使它們能夠獲取關鍵信息並執行任務。

可以將 MCP 想像成人工智能應用程式的 **USB-C 接口**。就像 USB-C 提供了一種標準化的方式來連接電子設備，MCP 提供了一種標準化的方式來連接人工智能應用程式與外部系統。

### MCP 能實現什麼？

MCP 為人工智能應用程式解鎖了強大的功能：

- **個性化 AI 助手**：代理可以訪問您的 Google 日曆和 Notion，成為更個性化的 AI 助手
- **高級代碼生成**：Claude Code 可以根據 Figma 設計生成整個網頁應用程式
- **企業數據整合**：企業聊天機器人可以連接到組織內的多個數據庫，幫助用戶通過聊天分析數據
- **創意工作流程**：AI 模型可以在 Blender 上創建 3D 設計並使用 3D 打印機打印出來
- **實時信息訪問**：連接到外部數據來源以獲取最新信息
- **複雜的多步操作**：結合多個工具和系統執行複雜的工作流程

### MCP 為什麼重要？

MCP 為整個生態系統帶來了多方面的好處：

**對開發者而言**：MCP 減少了構建或整合人工智能應用程式或代理的開發時間和複雜性。

**對人工智能應用程式而言**：MCP 提供了訪問數據來源、工具和應用程式的生態系統，提升了功能並改善了最終用戶體驗。

**對最終用戶而言**：MCP 使人工智能應用程式或代理更具能力，能夠在必要時訪問您的數據並代表您採取行動。

## MCP 中的小型語言模型 (SLMs)

小型語言模型代表了一種高效的人工智能部署方式，具有以下幾個優勢：

### SLMs 的優勢
- **資源效率**：較低的計算需求
- **響應速度更快**：減少實時應用程式的延遲  
- **成本效益**：基礎設施需求最小
- **隱私**：可以在本地運行，無需數據傳輸
- **定制化**：更容易針對特定領域進行微調

### 為什麼 SLMs 與 MCP 配合良好

SLMs 與 MCP 配合使用，形成了一種強大的組合，模型的推理能力通過外部工具得到增強，彌補了其參數數量較少的不足，從而提升了功能。

## Python MCP SDK 概述

Python MCP SDK 為構建支持 MCP 的應用程式提供了基礎。該 SDK 包括：

- **客戶端庫**：用於連接 MCP 伺服器
- **伺服器框架**：用於創建自定義 MCP 伺服器
- **協議處理器**：用於管理通信
- **工具整合**：用於執行外部功能

## 實際應用：Phi-4 MCP 客戶端

讓我們探索一個使用 Microsoft 的 Phi-4 小型模型整合 MCP 功能的實際應用。

### MCP 架構概述

MCP 遵循 **客戶端-伺服器架構**，其中 MCP 主機（如 Claude Code 或 Claude Desktop 等人工智能應用程式）與一個或多個 MCP 伺服器建立連接。MCP 主機通過為每個 MCP 伺服器創建一個 MCP 客戶端來完成此操作。

#### 主要參與者

- **MCP 主機**：協調和管理一個或多個 MCP 客戶端的人工智能應用程式
- **MCP 客戶端**：維持與 MCP 伺服器的連接，並從 MCP 伺服器獲取上下文供 MCP 主機使用
- **MCP 伺服器**：提供上下文給 MCP 客戶端的程式

#### 雙層架構

MCP 包括兩個不同的層：

**數據層**：定義基於 JSON-RPC 的客戶端-伺服器通信協議，包括：
- 生命周期管理（連接初始化、功能協商）
- 核心原語（工具、資源、提示）
- 客戶端功能（採樣、引導、日誌記錄）
- 實用功能（通知、進度跟蹤）

**傳輸層**：定義通信機制和通道：
- **STDIO 傳輸**：使用標準輸入/輸出流進行本地進程通信（性能最佳，無網絡開銷）
- **可流式 HTTP 傳輸**：使用 HTTP POST 和可選的伺服器推送事件進行遠程伺服器通信（支持標準 HTTP 認證）

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### MCP 核心原語

MCP 定義了原語，指定了可以與人工智能應用程式共享的上下文信息類型以及可以執行的操作範圍。

#### 伺服器原語

MCP 定義了伺服器可以公開的三個核心原語：

**工具**：人工智能應用程式可以調用的可執行功能
- 示例：文件操作、API 調用、數據庫查詢
- 方法：`tools/list`，`tools/call`
- 支持動態發現和執行

**資源**：為人工智能應用程式提供上下文信息的數據來源
- 示例：文件內容、數據庫記錄、API 響應
- 方法：`resources/list`，`resources/read`
- 允許訪問結構化數據

**提示**：幫助結構化與語言模型交互的可重用模板
- 示例：系統提示、少量示例
- 方法：`prompts/list`，`prompts/get`
- 標準化人工智能交互模式

#### 客戶端原語

MCP 也定義了客戶端可以公開的原語，以實現更豐富的交互：

**採樣**：允許伺服器從客戶端的人工智能應用程式請求語言模型完成
- 方法：`sampling/complete`
- 使伺服器的開發與模型無關
- 提供對主機語言模型的訪問

**引導**：允許伺服器向用戶請求額外信息
- 方法：`elicitation/request`
- 支持用戶交互和確認
- 支持動態信息收集

**日誌記錄**：使伺服器能夠向客戶端發送日誌消息
- 用於調試和監控
- 提供伺服器操作的可見性

### MCP 協議生命周期

#### 初始化和功能協商

MCP 是一種有狀態的協議，需要進行生命周期管理。初始化過程有以下幾個關鍵目的：

1. **協議版本協商**：確保客戶端和伺服器使用兼容的協議版本（例如 "2025-06-18"）
2. **功能發現**：每一方聲明支持的功能和原語
3. **身份交換**：提供身份和版本信息

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### 工具發現和執行

初始化後，客戶端可以發現並執行工具：

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### 實時通知

MCP 支持實時通知以進行動態更新：

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## 入門指南：逐步指導

### 第一步：環境設置

安裝所需的依賴項：
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### 第二步：基本配置

設置您的環境變量：
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### 第三步：運行您的第一個 MCP 客戶端

**基本 Ollama 設置：**
```bash
python ghmodel_mcp_demo.py
```

**使用 vLLM 後端：**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**伺服器推送事件連接：**
```bash
python ghmodel_mcp_demo.py --run sse
```

**自定義 MCP 伺服器：**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### 第四步：程式化使用

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 高級功能

### 多後端支持

該實現支持 Ollama 和 vLLM 後端，您可以根據需求進行選擇：

- **Ollama**：更適合本地開發和測試
- **vLLM**：針對生產和高吞吐量場景進行優化

### 靈活的連接協議

支持兩種連接模式：

**STDIO 模式**：直接進程通信
- 延遲更低
- 適合本地工具
- 設置簡單

**SSE 模式**：基於 HTTP 的流式通信
- 支持網絡
- 更適合分佈式系統
- 實時更新

### 工具整合能力

系統可以整合多種工具：
- 網頁自動化（Playwright）
- 文件操作
- API 交互
- 系統命令
- 自定義功能

## 錯誤處理和最佳實踐

### 全面的錯誤管理

該實現包括針對以下情況的強大錯誤處理：

**連接錯誤：**
- MCP 伺服器故障
- 網絡超時
- 連接問題

**工具執行錯誤：**
- 缺少工具
- 參數驗證
- 執行失敗

**響應處理錯誤：**
- JSON 解析問題
- 格式不一致
- LLM 響應異常

### 最佳實踐

1. **資源管理**：使用異步上下文管理器
2. **錯誤處理**：實施全面的 try-catch 塊
3. **日誌記錄**：啟用適當的日誌級別
4. **安全性**：驗證輸入並清理輸出
5. **性能**：使用連接池和緩存

## 實際應用

### 網頁自動化
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### 數據處理
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API 整合
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## 性能優化

### 記憶體管理
- 高效的消息歷史記錄處理
- 適當的資源清理
- 連接池

### 網絡優化
- 異步 HTTP 操作
- 可配置的超時
- 優雅的錯誤恢復

### 並發處理
- 非阻塞 I/O
- 工具的並行執行
- 高效的異步模式

## 安全考量

### 數據保護
- 安全的 API 密鑰管理
- 輸入驗證
- 輸出清理

### 網絡安全
- 支持 HTTPS
- 默認本地端點
- 安全令牌處理

### 執行安全性
- 工具篩選
- 沙盒環境
- 審計日誌記錄

## MCP 生態系統與開發

### MCP 項目範圍

模型上下文協議生態系統包括以下幾個關鍵組件：

- **[MCP 規範](https://modelcontextprotocol.io/specification/latest)**：官方規範，概述客戶端和伺服器的實施要求
- **[MCP SDKs](https://modelcontextprotocol.io/docs/sdk)**：實現 MCP 的不同編程語言 SDK
- **MCP 開發工具**：用於開發 MCP 伺服器和客戶端的工具，包括 [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[MCP 參考伺服器實現](https://github.com/modelcontextprotocol/servers)**：MCP 伺服器的參考實現

### 開始 MCP 開發

要開始使用 MCP 進行開發：

**構建伺服器**：[創建 MCP 伺服器](https://modelcontextprotocol.io/docs/develop/build-server)，以公開您的數據和工具

**構建客戶端**：[開發應用程式](https://modelcontextprotocol.io/docs/develop/build-client)，以連接 MCP 伺服器

**學習概念**：[了解核心概念](https://modelcontextprotocol.io/docs/learn/architecture) 和 MCP 的架構

## 結論

與 MCP 整合的小型語言模型代表了人工智能應用程式開發的一次範式轉變。通過將小型模型的高效性與外部工具的強大功能相結合，開發者可以創建既資源高效又功能強大的智能系統。

模型上下文協議提供了一種標準化的方式來將人工智能應用程式連接到外部系統，就像 USB-C 為電子設備提供通用連接標準一樣。這種標準化使得：

- **無縫整合**：將人工智能模型連接到多樣化的數據來源和工具
- **生態系統增長**：一次構建，可在多個人工智能應用程式中使用
- **功能增強**：通過外部功能擴展 SLMs
- **實時更新**：支持動態、響應式人工智能應用程式

關鍵要點：
- MCP 是一個開放標準，橋接人工智能應用程式與外部系統
- 該協議支持工具、資源和提示作為核心原語
- 實時通知支持動態、響應式應用程式
- 正確的生命周期管理和錯誤處理對於生產使用至關重要
- 生態系統提供了全面的 SDK 和開發工具

## 參考資料與進一步閱讀

### 官方 MCP 文檔

- **[模型上下文協議官方網站](https://modelcontextprotocol.io/)** - 完整文檔和規範
- **[MCP 入門指南](https://modelcontextprotocol.io/docs/getting-started/intro)** - 介紹和核心概念
- **[MCP 架構概述](https://modelcontextprotocol.io/docs/learn/architecture)** - 詳細技術架構
- **[MCP 規範](https://modelcontextprotocol.io/specification/latest)** - 官方協議規範
- **[MCP SDKs 文檔](https://modelcontextprotocol.io/docs/sdk)** - 特定編程語言的 SDK 指南

### 開發資源

- **[MCP 初學者指南](https://aka.ms/mcp-for-beginners)** - 模型上下文協議的全面初學者指南
- **[MCP GitHub 組織](https://github.com/modelcontextprotocol)** - 官方倉庫和示例
- **[MCP 伺服器倉庫](https://github.com/modelcontextprotocol/servers)** - 參考伺服器實現
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - 開發和調試工具
- **[構建 MCP 伺服器指南](https://modelcontextprotocol.io/docs/develop/build-server)** - 伺服器開發教程
- **[構建 MCP 客戶端指南](https://modelcontextprotocol.io/docs/develop/build-client)** - 客戶端開發教程

### 小型語言模型與邊緣 AI

- **[Microsoft Phi 模型](https://aka.ms/phicookbook)** - Phi 模型系列
- **[Foundry Local 文檔](https://github.com/microsoft/Foundry-Local)** - Microsoft 的邊緣 AI 運行時
- **[Ollama 文件](https://ollama.ai/docs)** - 本地 LLM 部署平台  
- **[vLLM 文件](https://docs.vllm.ai/)** - 高效能 LLM 服務  

### 技術標準與協議  

- **[JSON-RPC 2.0 規範](https://www.jsonrpc.org/)** - MCP 使用的底層 RPC 協議  
- **[JSON Schema](https://json-schema.org/)** - MCP 工具的架構定義標準  
- **[OpenAPI 規範](https://swagger.io/specification/)** - API 文件標準  
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - 即時更新的網絡標準  

### AI代理開發  

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - 適合生產環境的代理開發框架  
- **[LangChain 文件](https://docs.langchain.com/)** - 代理與工具整合框架  
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - 微軟的 AI 編排 SDK  

### 行業報告與研究  

- **[Anthropic 的模型上下文協議公告](https://www.anthropic.com/news/model-context-protocol)** - MCP 的原始介紹  
- **[小型語言模型調查](https://arxiv.org/abs/2410.20011)** - 關於 SLM 研究的學術調查  
- **[邊緣 AI 市場分析](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - 行業趨勢與預測  
- **[AI代理開發最佳實踐](https://arxiv.org/abs/2309.02427)** - 關於代理架構的研究  

此部分為構建您自己的基於 SLM 的 MCP 應用程序提供基礎，開啟自動化、數據處理及智能系統整合的可能性。  

## ➡️ 下一步  

- [模組 7. 邊緣 AI 範例](../Module07/README.md)  

---

**免責聲明**：  
此文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於重要信息，建議使用專業人工翻譯。我們不對因使用此翻譯而引起的任何誤解或誤釋承擔責任。