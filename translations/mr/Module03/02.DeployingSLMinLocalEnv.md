<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:24:13+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "mr"
}
-->
# विभाग 2: स्थानिक वातावरणात तैनात करणे - गोपनीयतेला प्राधान्य देणारे उपाय

लहान भाषा मॉडेल्स (SLMs) चे स्थानिक तैनात करणे गोपनीयता जपणाऱ्या, खर्च-प्रभावी AI उपायांकडे एक मोठा बदल दर्शवते. ही सविस्तर मार्गदर्शिका दोन शक्तिशाली फ्रेमवर्क्स—Ollama आणि Microsoft Foundry Local—चा अभ्यास करते, जे विकसकांना SLMs चा पूर्ण क्षमतेने उपयोग करताना त्यांच्या तैनातीच्या वातावरणावर पूर्ण नियंत्रण ठेवण्यास सक्षम करतात.

## परिचय

या धड्यात, आपण स्थानिक वातावरणात लहान भाषा मॉडेल्स तैनात करण्याच्या प्रगत रणनीतींचा अभ्यास करू. आपण स्थानिक AI तैनातीची मूलभूत संकल्पना कव्हर करू, दोन प्रमुख प्लॅटफॉर्म्स (Ollama आणि Microsoft Foundry Local) चा अभ्यास करू आणि उत्पादन-तयार उपायांसाठी व्यावहारिक अंमलबजावणी मार्गदर्शन प्रदान करू.

## शिकण्याची उद्दिष्टे

या धड्याच्या शेवटी, आपण हे करू शकाल:

- स्थानिक SLM तैनाती फ्रेमवर्क्सची रचना आणि फायदे समजून घेणे.
- Ollama आणि Microsoft Foundry Local वापरून उत्पादन-तयार तैनाती अंमलात आणणे.
- विशिष्ट आवश्यकता आणि मर्यादांनुसार योग्य प्लॅटफॉर्मची तुलना आणि निवड करणे.
- कार्यक्षमता, सुरक्षा आणि स्केलेबिलिटीसाठी स्थानिक तैनातींचा ऑप्टिमायझेशन करणे.

## स्थानिक SLM तैनाती आर्किटेक्चर समजून घेणे

स्थानिक SLM तैनाती ही क्लाउड-आधारित AI सेवांपासून ऑन-प्रिमायसेस, गोपनीयता जपणाऱ्या उपायांकडे एक मूलभूत बदल दर्शवते. या दृष्टिकोनामुळे संस्थांना त्यांच्या AI पायाभूत सुविधांवर पूर्ण नियंत्रण ठेवता येते, डेटा सार्वभौमत्व आणि ऑपरेशनल स्वातंत्र्य सुनिश्चित होते.

### तैनाती फ्रेमवर्क वर्गीकरण

वेगवेगळ्या तैनाती दृष्टिकोन समजून घेणे विशिष्ट उपयोग प्रकरणांसाठी योग्य रणनीती निवडण्यास मदत करते:

- **विकसन-केंद्रित**: प्रयोग आणि प्रोटोटायपिंगसाठी सुलभ सेटअप
- **एंटरप्राइझ-ग्रेड**: एंटरप्राइझ एकत्रीकरण क्षमता असलेले उत्पादन-तयार उपाय  
- **क्रॉस-प्लॅटफॉर्म**: विविध ऑपरेटिंग सिस्टम्स आणि हार्डवेअरमध्ये सार्वत्रिक सुसंगतता

### स्थानिक SLM तैनातीचे मुख्य फायदे

स्थानिक SLM तैनाती अनेक मूलभूत फायदे देते जी एंटरप्राइझ आणि गोपनीयता-संवेदनशील अनुप्रयोगांसाठी आदर्श बनवते:

**गोपनीयता आणि सुरक्षा**: स्थानिक प्रक्रिया सुनिश्चित करते की संवेदनशील डेटा कधीही संस्थेच्या पायाभूत सुविधांबाहेर जात नाही, GDPR, HIPAA आणि इतर नियामक आवश्यकता पूर्ण करण्यास सक्षम करते. वर्गीकृत वातावरणासाठी एअर-गॅप्ड तैनाती शक्य आहेत, तर संपूर्ण ऑडिट ट्रेल्स सुरक्षा देखरेख राखतात.

**खर्च प्रभावीता**: प्रति-टोकन किंमती मॉडेल्स काढून टाकल्याने ऑपरेशनल खर्च लक्षणीयरीत्या कमी होतो. कमी बँडविड्थ आवश्यकता आणि कमी क्लाउड अवलंबित्व एंटरप्राइझ बजेटिंगसाठी अंदाजे खर्च संरचना प्रदान करते.

**कार्यक्षमता आणि विश्वसनीयता**: नेटवर्क लेटन्सीशिवाय जलद अनुमान वेळा रिअल-टाइम अनुप्रयोग सक्षम करतात. ऑफलाइन कार्यक्षमता इंटरनेट कनेक्टिव्हिटीच्या बाबतीत सतत ऑपरेशन सुनिश्चित करते, तर स्थानिक संसाधन ऑप्टिमायझेशन सातत्यपूर्ण कार्यक्षमता प्रदान करते.

## Ollama: सार्वत्रिक स्थानिक तैनाती प्लॅटफॉर्म

### मुख्य आर्किटेक्चर आणि तत्त्वज्ञान

Ollama हे एक सार्वत्रिक, विकसक-अनुकूल प्लॅटफॉर्म म्हणून तयार केले गेले आहे जे विविध हार्डवेअर कॉन्फिगरेशन्स आणि ऑपरेटिंग सिस्टम्समध्ये स्थानिक LLM तैनाती लोकशाही बनवते.

**तांत्रिक पाया**: मजबूत llama.cpp फ्रेमवर्कवर आधारित, Ollama कार्यक्षम GGUF मॉडेल स्वरूपाचा उपयोग करते. क्रॉस-प्लॅटफॉर्म सुसंगतता Windows, macOS आणि Linux वातावरणांमध्ये सातत्यपूर्ण वर्तन सुनिश्चित करते, तर बुद्धिमान संसाधन व्यवस्थापन CPU, GPU आणि मेमरीचा उपयोग ऑप्टिमाइझ करते.

**डिझाइन तत्त्वज्ञान**: Ollama कार्यक्षमतेचा त्याग न करता साधेपणाला प्राधान्य देते, त्वरित उत्पादकतेसाठी शून्य-कॉन्फिगरेशन तैनाती ऑफर करते. प्लॅटफॉर्म विस्तृत मॉडेल सुसंगतता राखतो आणि विविध मॉडेल आर्किटेक्चरमध्ये सातत्यपूर्ण API प्रदान करतो.

### प्रगत वैशिष्ट्ये आणि क्षमता

**मॉडेल व्यवस्थापन उत्कृष्टता**: Ollama स्वयंचलित पुलिंग, कॅशिंग आणि आवृत्तीकरणासह व्यापक मॉडेल जीवनचक्र व्यवस्थापन प्रदान करते. प्लॅटफॉर्म Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral आणि विशेष एम्बेडिंग मॉडेल्ससह विस्तृत मॉडेल इकोसिस्टमला समर्थन देते.

**मॉडेलफाइल्सद्वारे सानुकूलन**: प्रगत वापरकर्ते विशिष्ट पॅरामीटर्स, सिस्टम प्रॉम्प्ट्स आणि वर्तन सुधारणा असलेल्या सानुकूल मॉडेल कॉन्फिगरेशन्स तयार करू शकतात. हे डोमेन-विशिष्ट ऑप्टिमायझेशन आणि विशेष अनुप्रयोग आवश्यकता सक्षम करते.

**कार्यक्षमता ऑप्टिमायझेशन**: Ollama स्वयंचलितपणे उपलब्ध हार्डवेअर प्रवेगकांचा शोध घेते आणि वापर करते ज्यामध्ये NVIDIA CUDA, Apple Metal आणि OpenCL समाविष्ट आहे. बुद्धिमान मेमरी व्यवस्थापन विविध हार्डवेअर कॉन्फिगरेशन्समध्ये इष्टतम संसाधन उपयोग सुनिश्चित करते.

### उत्पादन अंमलबजावणी रणनीती

**स्थापना आणि सेटअप**: Ollama विविध प्लॅटफॉर्म्समध्ये स्थानिक इंस्टॉलर्स, पॅकेज मॅनेजर्स (WinGet, Homebrew, APT) आणि कंटेनराइज्ड तैनातीसाठी Docker कंटेनर्सद्वारे सुलभ स्थापना प्रदान करते.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**महत्त्वाचे आदेश आणि ऑपरेशन्स**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**प्रगत कॉन्फिगरेशन**: एंटरप्राइझ आवश्यकताांसाठी मॉडेलफाइल्स सखोल सानुकूलन सक्षम करतात:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### विकसक एकत्रीकरण उदाहरणे

**Python API एकत्रीकरण**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript एकत्रीकरण (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API वापर cURL सह**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### कार्यक्षमता ट्यूनिंग आणि ऑप्टिमायझेशन

**मेमरी आणि थ्रेड कॉन्फिगरेशन**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**विविध हार्डवेअरसाठी क्वांटायझेशन निवड**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: एंटरप्राइझ एज AI प्लॅटफॉर्म

### एंटरप्राइझ-ग्रेड आर्किटेक्चर

Microsoft Foundry Local हे उत्पादन एज AI तैनातीसाठी विशेषतः डिझाइन केलेले एक व्यापक एंटरप्राइझ उपाय आहे ज्यामध्ये Microsoft इकोसिस्टममध्ये सखोल एकत्रीकरण आहे.

**ONNX-आधारित पाया**: उद्योग-मानक ONNX Runtime वर आधारित, Foundry Local विविध हार्डवेअर आर्किटेक्चर्समध्ये ऑप्टिमाइझ केलेली कार्यक्षमता प्रदान करते. प्लॅटफॉर्म Windows ML एकत्रीकरणाचा उपयोग Windows साठी स्थानिक ऑप्टिमायझेशनसाठी करतो, तर क्रॉस-प्लॅटफॉर्म सुसंगतता राखतो.

**हार्डवेअर प्रवेगक उत्कृष्टता**: Foundry Local CPU, GPU आणि NPU मध्ये बुद्धिमान हार्डवेअर शोध आणि ऑप्टिमायझेशन वैशिष्ट्यीकृत करते. हार्डवेअर विक्रेत्यांसोबत (AMD, Intel, NVIDIA, Qualcomm) सखोल सहकार्य एंटरप्राइझ हार्डवेअर कॉन्फिगरेशन्सवर इष्टतम कार्यक्षमता सुनिश्चित करते.

### प्रगत विकसक अनुभव

**मल्टी-इंटरफेस प्रवेश**: Foundry Local शक्तिशाली CLI, मल्टी-भाषा SDKs (Python, NodeJS) आणि OpenAI सुसंगत RESTful APIs सह व्यापक विकास इंटरफेस प्रदान करते, जे सहज स्थलांतर सक्षम करतात.

**Visual Studio एकत्रीकरण**: प्लॅटफॉर्म AI Toolkit साठी VS Code सह सहजपणे एकत्रित होते, विकास वातावरणात मॉडेल रूपांतरण, क्वांटायझेशन आणि ऑप्टिमायझेशन साधने प्रदान करते. हे एकत्रीकरण विकास कार्यप्रवाहांना गती देते आणि तैनातीची गुंतागुंत कमी करते.

**मॉडेल ऑप्टिमायझेशन पाइपलाइन**: Microsoft Olive एकत्रीकरण डायनॅमिक क्वांटायझेशन, ग्राफ ऑप्टिमायझेशन आणि हार्डवेअर-विशिष्ट ट्यूनिंगसह परिष्कृत मॉडेल ऑप्टिमायझेशन कार्यप्रवाह सक्षम करते. Azure ML द्वारे क्लाउड-आधारित रूपांतरण क्षमता मोठ्या मॉडेल्ससाठी स्केलेबल ऑप्टिमायझेशन प्रदान करते.

### उत्पादन अंमलबजावणी रणनीती

**स्थापना आणि कॉन्फिगरेशन**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**मॉडेल व्यवस्थापन ऑपरेशन्स**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**प्रगत तैनाती कॉन्फिगरेशन**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### एंटरप्राइझ इकोसिस्टम एकत्रीकरण

**सुरक्षा आणि अनुपालन**: Foundry Local एंटरप्राइझ-ग्रेड सुरक्षा वैशिष्ट्ये प्रदान करते ज्यामध्ये भूमिका-आधारित प्रवेश नियंत्रण, ऑडिट लॉगिंग, अनुपालन अहवाल आणि एन्क्रिप्टेड मॉडेल स्टोरेज समाविष्ट आहे. Microsoft सुरक्षा पायाभूत सुविधांसह एकत्रीकरण एंटरप्राइझ सुरक्षा धोरणांचे पालन सुनिश्चित करते.

**अंतर्निहित AI सेवा**: प्लॅटफॉर्म स्थानिक भाषा प्रक्रिया, प्रतिमा सुधारणा आणि विश्लेषणासाठी AI Imaging, आणि सामान्य एंटरप्राइझ AI कार्यांसाठी विशेष API समाविष्ट असलेल्या Phi Silica सारख्या तयार-टू-यूज AI क्षमता प्रदान करते.

## तुलनात्मक विश्लेषण: Ollama vs Foundry Local

### तांत्रिक आर्किटेक्चर तुलना

| **पहलू** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **मॉडेल स्वरूप** | GGUF (llama.cpp द्वारे) | ONNX (ONNX Runtime द्वारे) |
| **प्लॅटफॉर्म फोकस** | सार्वत्रिक क्रॉस-प्लॅटफॉर्म | Windows/एंटरप्राइझ ऑप्टिमायझेशन |
| **हार्डवेअर एकत्रीकरण** | सामान्य GPU/CPU समर्थन | Windows ML, NPU समर्थन |
| **ऑप्टिमायझेशन** | llama.cpp क्वांटायझेशन | Microsoft Olive + ONNX Runtime |
| **एंटरप्राइझ वैशिष्ट्ये** | समुदाय-चालित | एंटरप्राइझ-ग्रेड SLAs सह |

### कार्यक्षमता वैशिष्ट्ये

**Ollama कार्यक्षमता ताकद**:
- llama.cpp ऑप्टिमायझेशनद्वारे अपवादात्मक CPU कार्यक्षमता
- विविध प्लॅटफॉर्म्स आणि हार्डवेअरमध्ये सातत्यपूर्ण वर्तन
- बुद्धिमान मॉडेल लोडिंगसह कार्यक्षम मेमरी उपयोग
- विकास आणि चाचणी परिस्थितीसाठी जलद कोल्ड-स्टार्ट वेळा

**Foundry Local कार्यक्षमता फायदे**:
- आधुनिक Windows हार्डवेअरवर उत्कृष्ट NPU उपयोग
- विक्रेता भागीदारीद्वारे GPU प्रवेगक ऑप्टिमायझेशन
- एंटरप्राइझ-ग्रेड कार्यक्षमता निरीक्षण आणि ऑप्टिमायझेशन
- उत्पादन वातावरणासाठी स्केलेबल तैनाती क्षमता

### विकास अनुभव विश्लेषण

**Ollama विकसक अनुभव**:
- त्वरित उत्पादकतेसाठी किमान सेटअप आवश्यकता
- सर्व ऑपरेशन्ससाठी अंतर्ज्ञानी कमांड-लाइन इंटरफेस
- विस्तृत समुदाय समर्थन आणि दस्तऐवजीकरण
- मॉडेलफाइल्सद्वारे लवचिक सानुकूलन

**Foundry Local विकसक अनुभव**:
- Visual Studio इकोसिस्टमसह व्यापक IDE एकत्रीकरण
- टीम सहयोग वैशिष्ट्यांसह एंटरप्राइझ विकास कार्यप्रवाह
- Microsoft समर्थनासह व्यावसायिक समर्थन चॅनेल
- प्रगत डीबगिंग आणि ऑप्टिमायझेशन साधने

### उपयोग प्रकरण ऑप्टिमायझेशन

**Ollama निवडा जेव्हा**:
- क्रॉस-प्लॅटफॉर्म अनुप्रयोग विकसित करणे ज्यासाठी सातत्यपूर्ण वर्तन आवश्यक आहे
- ओपन-सोर्स पारदर्शकता आणि समुदाय योगदानांना प्राधान्य देणे
- मर्यादित संसाधने किंवा बजेट मर्यादा असताना काम करणे
- प्रायोगिक किंवा संशोधन-केंद्रित अनुप्रयोग तयार करणे
- विविध आर्किटेक्चर्समध्ये विस्तृत मॉडेल सुसंगतता आवश्यक आहे

**Foundry Local निवडा जेव्हा**:
- कडक कार्यक्षमता आवश्यकता असलेल्या एंटरप्राइझ अनुप्रयोग तैनात करणे
- Windows-विशिष्ट हार्डवेअर ऑप्टिमायझेशन (NPU, Windows ML) चा लाभ घेणे
- एंटरप्राइझ समर्थन, SLAs आणि अनुपालन वैशिष्ट्ये आवश्यक आहेत
- Microsoft इकोसिस्टम एकत्रीकरणासह उत्पादन अनुप्रयोग तयार करणे
- प्रगत ऑप्टिमायझेशन साधने आणि व्यावसायिक विकास कार्यप्रवाह आवश्यक आहेत

## प्रगत तैनाती रणनीती

### कंटेनराइज्ड तैनाती नमुने

**Ollama कंटेनरायझेशन**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local एंटरप्राइझ तैनाती**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### कार्यक्षमता ऑप्टिमायझेशन तंत्र

**Ollama ऑप्टिमायझेशन रणनीती**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local ऑप्टिमायझेशन**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## सुरक्षा आणि अनुपालन विचार

### एंटरप्राइझ सुरक्षा अंमलबजावणी

**Ollama सुरक्षा सर्वोत्तम पद्धती**:
- फायरवॉल नियम आणि VPN प्रवेशासह नेटवर्क अलगाव
- रिव्हर्स प्रॉक्सी एकत्रीकरणाद्वारे प्रमाणीकरण
- मॉडेल अखंडता पडताळणी आणि सुरक्षित मॉडेल वितरण
- API प्रवेश आणि मॉडेल ऑपरेशन्ससाठी ऑडिट लॉगिंग

**Foundry Local एंटरप्राइझ सुरक्षा**:
- Active Directory एकत्रीकरणासह अंगभूत भूमिका-आधारित प्रवेश नियंत्रण
- अनुपालन अहवालासह व्यापक ऑडिट ट्रेल्स
- एन्क्रिप्टेड मॉडेल स्टोरेज आणि सुरक्षित मॉडेल तैनाती
- Microsoft सुरक्षा पायाभूत सुविधांसह एकत्रीकरण

### अनुपालन आणि नियामक आवश्यकता

दोन्ही प्लॅटफॉर्म्स स्थानिक प्रक्रिया सुनिश्चित करून नियामक अनुपालनास समर्थन देतात:
- डेटा निवासी नियंत्रण
- नियामक अहवाल आवश्यकताांसाठी ऑडिट लॉगिंग
- संवेदनशील डेटा हाताळण्यासाठी प्रवेश नियंत्रण
- डेटा संरक्षणासाठी विश्रांती आणि ट्रान्झिटमध्ये एन्क्रिप्शन

## उत्पादन तैनातीसाठी सर्वोत्तम पद्धती

### निरीक्षण आणि निरीक्षण क्षमता

**मॉनिटर करावयाचे मुख्य मेट्रिक्स**:
- मॉडेल अनुमान विलंबता आणि थ्रूपुट
- संसाधन उपयोग (CPU, GPU, मेमरी)
- API प्रतिसाद वेळा आणि त्रुटी दर
- मॉडेल अचूकता आणि कार्यक्षमता विचलन

**मॉनिटरिंग अंमलबजावणी**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### सतत एकत्रीकरण आणि तैनाती

**CI/CD पाइपलाइन एकत्रीकरण**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## भविष्यातील ट्रेंड्स आणि विचार

### उदयोन्मुख तंत्रज्ञान

स्थानिक SLM तैनाती लँडस्केप अनेक प्रमुख ट्रेंड्ससह विकसित होत आहे:

**प्रगत मॉडेल आर्किटेक्चर्स**: सुधारित कार्यक्षमता आणि क्षमता गुणोत्तरांसह पुढील पिढीचे SLMs उदयास येत आहेत, ज्यामध्ये डायनॅमिक स्केलिंगसाठी मिश्रण-ऑफ-एक्सपर्ट्स मॉडेल्स आणि एज तैनातीसाठी विशेष आर्किटेक्चर्स समाविष्ट आहेत.

**हार्डवेअर एकत्रीकरण**: विशेष AI हार्डवेअर, NPUs, कस्टम सिलिकॉन आणि एज कंप्युटिंग प्रवेगकांसह सखोल एकत्रीकरण सुधारित कार्यक्षमता क्षमता प्रदान करेल.

**इकोसिस्टम उत्क्रांती**: तैनाती प्लॅटफॉर्म्समध्ये मानकीकरण प्रयत्न आणि विविध फ्रेमवर्क

---

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात ठेवा की स्वयंचलित भाषांतरे त्रुटी किंवा अचूकतेच्या अभावाने युक्त असू शकतात. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून उद्भवलेल्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.