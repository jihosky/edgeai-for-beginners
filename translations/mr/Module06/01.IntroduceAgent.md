<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T11:52:47+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "mr"
}
-->
# рдПрдЖрдп рдПрдЬрдВрдЯреНрд╕ рдЖрдгрд┐ рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕: рдПрдХ рд╡реНрдпрд╛рдкрдХ рдорд╛рд░реНрдЧрджрд░реНрд╢рдХ

## рдкрд░рд┐рдЪрдп

рдпрд╛ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓рдордзреНрдпреЗ, рдЖрдкрдг рдПрдЖрдп рдПрдЬрдВрдЯреНрд╕ рдЖрдгрд┐ рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕ (SLMs) рдпрд╛рдВрдЪрд╛ рдЕрднреНрдпрд╛рд╕ рдХрд░рдгрд╛рд░ рдЖрд╣реЛрдд рдЖрдгрд┐ рдПрдЬ рдХрдВрдкреНрдпреБрдЯрд┐рдВрдЧ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рддреНрдпрд╛рдВрдЪреА рдкреНрд░рдЧрдд рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рд░рдгрдиреАрддреА рд╢реЛрдзрдгрд╛рд░ рдЖрд╣реЛрдд. рдЖрдкрдг рдПрдЬрдВрдЯрд┐рдХ рдПрдЖрдпрдЪреЗ рдореВрд▓рднреВрдд рд╕рдВрдХрд▓реНрдкрдирд╛, SLM рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рддрдВрддреНрд░, рд╕рдВрд╕рд╛рдзрди-рдЖрдзрд╛рд░рд┐рдд рдЙрдкрдХрд░рдгрд╛рдВрд╕рд╛рдареА рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рддреИрдирд╛рддреА рд░рдгрдиреАрддреА рдЖрдгрд┐ рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рдПрдЬрдВрдЯ рдкреНрд░рдгрд╛рд▓реА рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА Microsoft Agent Framework рдпрд╛рд╡рд░ рдЪрд░реНрдЪрд╛ рдХрд░реВ.

рдХреГрддреНрд░рд┐рдо рдмреБрджреНрдзрд┐рдорддреНрддреЗрдЪреЗ рдХреНрд╖реЗрддреНрд░ 2025 рдордзреНрдпреЗ рдПрдХ рдореЛрдард╛ рдмрджрд▓ рдЕрдиреБрднрд╡рдд рдЖрд╣реЗ. 2023 рд╣реЗ рдЪреЕрдЯрдмреЙрдЯреНрд╕рдЪреЗ рд╡рд░реНрд╖ рд╣реЛрддреЗ рдЖрдгрд┐ 2024 рдордзреНрдпреЗ рдХреЛ-рдкрд╛рдпрд▓рдЯреНрд╕рдЪрд╛ рдЙрджрдп рдЭрд╛рд▓рд╛, рддрд░ 2025 рд╣реЗ рдПрдЖрдп рдПрдЬрдВрдЯреНрд╕рдЪреЗ рд╡рд░реНрд╖ рдЖрд╣реЗ тАФ рдмреБрджреНрдзрд┐рдорд╛рди рдкреНрд░рдгрд╛рд▓реА рдЬреЗ рд╡рд┐рдЪрд╛рд░ рдХрд░рддрд╛рдд, рдХрд╛рд░рдг рд╕рд╛рдВрдЧрддрд╛рдд, рдпреЛрдЬрдирд╛ рдЖрдЦрддрд╛рдд, рд╕рд╛рдзрдиреЗ рд╡рд╛рдкрд░рддрд╛рдд рдЖрдгрд┐ рдорд╛рдирд╡реА рд╣рд╕реНрддрдХреНрд╖реЗрдкрд╛рд╢рд┐рд╡рд╛рдп рдХрд╛рд░реНрдпреЗ рдкреВрд░реНрдг рдХрд░рддрд╛рдд, рдЬреЗ рдЕрдзрд┐рдХрд╛рдзрд┐рдХ рдХрд╛рд░реНрдпрдХреНрд╖рдо рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕рджреНрд╡рд╛рд░реЗ рд╕рдорд░реНрдерд┐рдд рдЖрд╣реЗрдд. Microsoft Agent Framework рд╣реЗ рдСрдлрд▓рд╛рдЗрди рдПрдЬ-рдЖрдзрд╛рд░рд┐рдд рдХреНрд╖рдорддрд╛ рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рдпрд╛ рдмреБрджреНрдзрд┐рдорд╛рди рдкреНрд░рдгрд╛рд▓реА рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдПрдХ рдкреНрд░рдореБрдЦ рд╕рдорд╛рдзрд╛рди рдореНрд╣рдгреВрди рдЙрджрдпрд╛рд╕ рдЖрд▓реЗ рдЖрд╣реЗ.

## рд╢рд┐рдХрдгреНрдпрд╛рдЪреЗ рдЙрджреНрджрд┐рд╖реНрдЯ

рдпрд╛ рдЯреНрдпреВрдЯреЛрд░рд┐рдпрд▓рдЪреНрдпрд╛ рд╢реЗрд╡рдЯреА, рдЖрдкрдг рдЦрд╛рд▓реАрд▓ рдЧреЛрд╖реНрдЯреА рдХрд░рдгреНрдпрд╛рдд рд╕рдХреНрд╖рдо рдЕрд╕рд╛рд▓:

- ЁЯдЦ рдПрдЖрдп рдПрдЬрдВрдЯреНрд╕ рдЖрдгрд┐ рдПрдЬрдВрдЯрд┐рдХ рдкреНрд░рдгрд╛рд▓реАрдВрдЪреНрдпрд╛ рдореВрд▓рднреВрдд рд╕рдВрдХрд▓реНрдкрдирд╛ рд╕рдордЬреВрди рдШреЗрдгреЗ
- ЁЯФм рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕рдЪреНрдпрд╛ рдореЛрдареНрдпрд╛ рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕рд╡рд░реАрд▓ рдлрд╛рдпрджреЗ рдУрд│рдЦрдгреЗ
- ЁЯЪА рдПрдЬ рдХрдВрдкреНрдпреБрдЯрд┐рдВрдЧ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдкреНрд░рдЧрдд SLM рддреИрдирд╛рддреА рд░рдгрдиреАрддреА рд╢рд┐рдХрдгреЗ
- ЁЯУ▒ рд╡рд╛рд╕реНрддрд╡рд┐рдХ-рдЬрдЧрд╛рддреАрд▓ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ SLM-рд╕рдорд░реНрдерд┐рдд рдПрдЬрдВрдЯреНрд╕ рдЕрдВрдорд▓рд╛рдд рдЖрдгрдгреЗ
- ЁЯПЧя╕П Microsoft Agent Framework рд╡рд╛рдкрд░реВрди рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рдПрдЬрдВрдЯреНрд╕ рддрдпрд╛рд░ рдХрд░рдгреЗ
- ЁЯМР рд╕реНрдерд╛рдирд┐рдХ LLM рдЖрдгрд┐ SLM рдПрдХрддреНрд░реАрдХрд░рдгрд╛рд╕рд╣ рдСрдлрд▓рд╛рдЗрди рдПрдЬ-рдЖрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯреНрд╕ рддреИрдирд╛рдд рдХрд░рдгреЗ
- ЁЯФз Microsoft Agent Framework рд▓рд╛ Foundry Local рд╕рд╣ рдПрдЬ рддреИрдирд╛рддреАрд╕рд╛рдареА рдПрдХрддреНрд░рд┐рдд рдХрд░рдгреЗ

## рдПрдЖрдп рдПрдЬрдВрдЯреНрд╕ рд╕рдордЬреВрди рдШреЗрдгреЗ: рдкрд╛рдпрд╛ рдЖрдгрд┐ рд╡рд░реНрдЧреАрдХрд░рдг

### рдкрд░рд┐рднрд╛рд╖рд╛ рдЖрдгрд┐ рдореБрдЦреНрдп рд╕рдВрдХрд▓реНрдкрдирд╛

рдХреГрддреНрд░рд┐рдо рдмреБрджреНрдзрд┐рдорддреНрддрд╛ (рдПрдЖрдп) рдПрдЬрдВрдЯ рдореНрд╣рдгрдЬреЗ рдПрдХ рдкреНрд░рдгрд╛рд▓реА рдХрд┐рдВрд╡рд╛ рдкреНрд░реЛрдЧреНрд░рд╛рдо рдЬреЛ рд╡рд╛рдкрд░рдХрд░реНрддреНрдпрд╛рдЪреНрдпрд╛ рдХрд┐рдВрд╡рд╛ рджреБрд╕рд▒реНрдпрд╛ рдкреНрд░рдгрд╛рд▓реАрдЪреНрдпрд╛ рд╡рддреАрдиреЗ рд╕реНрд╡рд╛рдпрддреНрддрдкрдгреЗ рдХрд╛рд░реНрдпреЗ рдкрд╛рд░ рдкрд╛рдбрдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдЕрд╕рддреЛ, рддреНрдпрд╛рдЪрд╛ рдХрд╛рд░реНрдпрдкреНрд░рд╡рд╛рд╣ рдбрд┐рдЭрд╛рдЗрди рдХрд░реВрди рдЖрдгрд┐ рдЙрдкрд▓рдмреНрдз рд╕рд╛рдзрдиреЗ рд╡рд╛рдкрд░реВрди. рдкрд╛рд░рдВрдкрд░рд┐рдХ рдПрдЖрдпрдЪреНрдпрд╛ рд╡рд┐рдкрд░реАрдд, рдЬреЛ рдлрдХреНрдд рддреБрдордЪреНрдпрд╛ рдкреНрд░рд╢реНрдирд╛рдВрдирд╛ рдЙрддреНрддрд░ рджреЗрддреЛ, рдПрдЬрдВрдЯ рд╕реНрд╡рддрдВрддреНрд░рдкрдгреЗ рдХрд╛рд░реНрдп рдХрд░реВ рд╢рдХрддреЛ рдЖрдгрд┐ рдЙрджреНрджрд┐рд╖реНрдЯреЗ рд╕рд╛рдзреНрдп рдХрд░реВ рд╢рдХрддреЛ.

### рдПрдЬрдВрдЯ рд╡рд░реНрдЧреАрдХрд░рдг рдлреНрд░реЗрдорд╡рд░реНрдХ

рдПрдЬрдВрдЯрдЪреНрдпрд╛ рдорд░реНрдпрд╛рджрд╛ рд╕рдордЬреВрди рдШреЗрдгреЗ рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рд╕рдВрдЧрдгрдХреАрдп рдкрд░рд┐рд╕реНрдерд┐рддреАрдВрд╕рд╛рдареА рдпреЛрдЧреНрдп рдПрдЬрдВрдЯ рдкреНрд░рдХрд╛рд░ рдирд┐рд╡рдбрдгреНрдпрд╛рдд рдорджрдд рдХрд░рддреЗ:

- **ЁЯФм рд╕рд╛рдзреЗ рд░рд┐рдлреНрд▓реЗрдХреНрд╕ рдПрдЬрдВрдЯреНрд╕**: рдирд┐рдпрдо-рдЖрдзрд╛рд░рд┐рдд рдкреНрд░рдгрд╛рд▓реА рдЬреЗ рддреНрд╡рд░рд┐рдд рдЖрдХрд▓рдирд╛рдВрдирд╛ рдкреНрд░рддрд┐рд╕рд╛рдж рджреЗрддрд╛рдд (рдерд░реНрдореЛрд╕реНрдЯреЕрдЯреНрд╕, рдореВрд▓рднреВрдд рдСрдЯреЛрдореЗрд╢рди)
- **ЁЯУ▒ рдореЙрдбреЗрд▓-рдЖрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯреНрд╕**: рдЕрдВрддрд░реНрдЧрдд рд╕реНрдерд┐рддреА рдЖрдгрд┐ рдореЗрдорд░реА рд░рд╛рдЦрдгрд╛рд▒реНрдпрд╛ рдкреНрд░рдгрд╛рд▓реА (рд░реЛрдмреЛрдЯ рд╡реНрд╣реЕрдХреНрдпреБрдореНрд╕, рдиреЗрд╡реНрд╣рд┐рдЧреЗрд╢рди рдкреНрд░рдгрд╛рд▓реА)
- **тЪЦя╕П рдЙрджреНрджрд┐рд╖реНрдЯ-рдЖрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯреНрд╕**: рдЙрджреНрджрд┐рд╖реНрдЯреЗ рд╕рд╛рдзреНрдп рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдХреНрд░рдорд╛рдВрдХ рдпреЛрдЬрдирд╛ рдЖрдЦрдгрд╛рд▒реНрдпрд╛ рдЖрдгрд┐ рдЕрдВрдорд▓рд╛рдд рдЖрдгрдгрд╛рд▒реНрдпрд╛ рдкреНрд░рдгрд╛рд▓реА (рдорд╛рд░реНрдЧ рдирд┐рдпреЛрдЬрдХ, рдХрд╛рд░реНрдп рд╡реЗрд│рд╛рдкрддреНрд░рдХ)
- **ЁЯза рд╢рд┐рдХрдгрд╛рд░реЗ рдПрдЬрдВрдЯреНрд╕**: рд╡реЗрд│реЛрд╡реЗрд│реА рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╕реБрдзрд╛рд░рдгрд╛рд▒реНрдпрд╛ рдЕрдиреБрдХреВрд▓ рдкреНрд░рдгрд╛рд▓реА (рд╢рд┐рдлрд╛рд░рд╕ рдкреНрд░рдгрд╛рд▓реА, рд╡реИрдпрдХреНрддрд┐рдХ рд╕рд╣рд╛рдпреНрдпрдХ)

### рдПрдЖрдп рдПрдЬрдВрдЯреНрд╕рдЪреЗ рдореБрдЦреНрдп рдлрд╛рдпрджреЗ

рдПрдЖрдп рдПрдЬрдВрдЯреНрд╕ рдХрд╛рд╣реА рдореВрд▓рднреВрдд рдлрд╛рдпрджреЗ рджреЗрддрд╛рдд рдЬреЗ рддреНрдпрд╛рдВрдирд╛ рдПрдЬ рдХрдВрдкреНрдпреБрдЯрд┐рдВрдЧ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЖрджрд░реНрд╢ рдмрдирд╡рддрд╛рдд:

**рдСрдкрд░реЗрд╢рдирд▓ рд╕реНрд╡рд╛рдпрддреНрддрддрд╛**: рдПрдЬрдВрдЯреНрд╕ рд╕рддрдд рдорд╛рдирд╡реА рджреЗрдЦрд░реЗрдЦреАрд╢рд┐рд╡рд╛рдп рд╕реНрд╡рддрдВрддреНрд░ рдХрд╛рд░реНрдп рдЕрдВрдорд▓рд╛рдд рдЖрдгрддрд╛рдд, рдЬреНрдпрд╛рдореБрд│реЗ рддреЗ рд░рд┐рдЕрд▓-рдЯрд╛рдЗрдо рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЖрджрд░реНрд╢ рдмрдирддрд╛рдд. рддреЗ рдХрдореА рджреЗрдЦрд░реЗрдЦреАрдЪреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдЕрд╕рддрд╛рдирд╛ рдЕрдиреБрдХреВрд▓ рд╡рд░реНрддрди рд░рд╛рдЦрддрд╛рдд, рд╕рдВрд╕рд╛рдзрди-рдЖрдзрд╛рд░рд┐рдд рдЙрдкрдХрд░рдгрд╛рдВрд╡рд░ рдХрдореА рдСрдкрд░реЗрд╢рдирд▓ рдУрд╡реНрд╣рд░рд╣реЗрдбрд╕рд╣ рддреИрдирд╛рдд рдХрд░рдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд.

**рддреИрдирд╛рддреАрдЪреА рд▓рд╡рдЪрд┐рдХрддрд╛**: рдпрд╛ рдкреНрд░рдгрд╛рд▓реАрдВрдордзреНрдпреЗ рдЗрдВрдЯрд░рдиреЗрдЯ рдХрдиреЗрдХреНрдЯрд┐рд╡реНрд╣рд┐рдЯреАрдЪреНрдпрд╛ рдЧрд░рдЬреЗрд╢рд┐рд╡рд╛рдп рдСрди-рдбрд┐рд╡реНрд╣рд╛рдЗрд╕ рдПрдЖрдп рдХреНрд╖рдорддрд╛ рд╕рдХреНрд╖рдо рдЖрд╣реЗрдд, рд╕реНрдерд╛рдирд┐рдХ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдХрд░реВрди рдЧреЛрдкрдиреАрдпрддрд╛ рдЖрдгрд┐ рд╕реБрд░рдХреНрд╖рд╛ рд╡рд╛рдврд╡рддрд╛рдд, рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рд╕рд╛рдиреБрдХреВрд▓рд┐рдд рдХреЗрд▓реНрдпрд╛ рдЬрд╛рдК рд╢рдХрддрд╛рдд рдЖрдгрд┐ рд╡рд┐рд╡рд┐рдз рдПрдЬ рдХрдВрдкреНрдпреБрдЯрд┐рдВрдЧ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдпреЛрдЧреНрдп рдЖрд╣реЗрдд.

**рдЦрд░реНрдЪ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛**: рдПрдЬрдВрдЯ рдкреНрд░рдгрд╛рд▓реА рдХреНрд▓рд╛рдЙрдб-рдЖрдзрд╛рд░рд┐рдд рдЙрдкрд╛рдпрд╛рдВрд╢реА рддреБрд▓рдирд╛ рдХрд░рддрд╛ рдпреЗрдгреНрдпрд╛рдЬреЛрдЧреНрдпрд╛ рдЦрд░реНрдЪрд╛рдд рддреИрдирд╛рдд рдХрд░рдгреНрдпрд╛рдЪреА рдСрдлрд░ рджреЗрддрд╛рдд, рдХрдореА рдСрдкрд░реЗрд╢рдирд▓ рдЦрд░реНрдЪ рдЖрдгрд┐ рдПрдЬ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдХрдореА рдмрдБрдбрд╡рд┐рдбреНрде рдЖрд╡рд╢реНрдпрдХрддрд╛.

## рдкреНрд░рдЧрдд рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рд░рдгрдиреАрддреА

### SLM (рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓) рдореВрд▓рднреВрдд рдЧреЛрд╖реНрдЯреА

рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ (SLM) рдореНрд╣рдгрдЬреЗ рдПрдХ рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рдЬреЗ рд╕рд╛рдорд╛рдиреНрдп рдЧреНрд░рд╛рд╣рдХ рдЗрд▓реЗрдХреНрдЯреНрд░реЙрдирд┐рдХ рдЙрдкрдХрд░рдгрд╛рд╡рд░ рдмрд╕реВ рд╢рдХрддреЗ рдЖрдгрд┐ рдПрдХрд╛ рд╡рд╛рдкрд░рдХрд░реНрддреНрдпрд╛рдЪреНрдпрд╛ рдПрдЬрдВрдЯрд┐рдХ рд╡рд┐рдирдВрддреНрдпрд╛рдВрдирд╛ рд╕реЗрд╡рд╛ рджреЗрддрд╛рдирд╛ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХрдкрдгреЗ рдХрдореА рд╡рд┐рд▓рдВрдмрд╛рд╕рд╣ рдЕрдиреБрдорд╛рди рдХрд░реВ рд╢рдХрддреЗ. рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХрджреГрд╖реНрдЯреНрдпрд╛, SLMs рд╕рд╣рд╕рд╛ 10 рдЕрдмреНрдЬ рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕рдкреЗрдХреНрд╖рд╛ рдХрдореА рдореЙрдбреЗрд▓реНрд╕ рдЕрд╕рддрд╛рдд.

**рдлреЙрд░реНрдореЕрдЯ рдбрд┐рд╕реНрдХрд╡рд░реА рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ**: SLMs рд╡рд┐рд╡рд┐рдз рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рд╕реНрддрд░, рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рд╕реБрд╕рдВрдЧрддрддрд╛, рд░рд┐рдЕрд▓-рдЯрд╛рдЗрдо рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдЖрдгрд┐ рдПрдЬ рддреИрдирд╛рддреА рдХреНрд╖рдорддрд╛ рдпрд╛рд╕рд╛рдареА рдкреНрд░рдЧрдд рд╕рдорд░реНрдерди рджреЗрддрд╛рдд. рд╡рд╛рдкрд░рдХрд░реНрддреЗ рд╕реНрдерд╛рдирд┐рдХ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдЖрдгрд┐ рдмреНрд░рд╛рдЙрдЭрд░-рдЖрдзрд╛рд░рд┐рдд рддреИрдирд╛рддреАрд╕рд╛рдареА WebGPU рд╕рдорд░реНрдердирд╛рджреНрд╡рд╛рд░реЗ рд╡рд╛рдврд▓реЗрд▓реА рдЧреЛрдкрдиреАрдпрддрд╛ рдорд┐рд│рд╡реВ рд╢рдХрддрд╛рдд.

**рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рд╕реНрддрд░ рд╕рдВрдЧреНрд░рд╣**: рд▓реЛрдХрдкреНрд░рд┐рдп SLM рдлреЙрд░реНрдореЕрдЯреНрд╕рдордзреНрдпреЗ рдореЛрдмрд╛рдЗрд▓ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдордзреНрдпреЗ рд╕рдВрддреБрд▓рд┐рдд рд╕рдВрдХреНрд╖реЗрдкрдгрд╛рд╕рд╛рдареА Q4_K_M, рдЧреБрдгрд╡рддреНрддреЗ-рдХреЗрдВрджреНрд░рд┐рдд рдПрдЬ рддреИрдирд╛рддреАрд╕рд╛рдареА Q5_K_S рдорд╛рд▓рд┐рдХрд╛, рд╢рдХреНрддрд┐рд╢рд╛рд▓реА рдПрдЬ рдЙрдкрдХрд░рдгрд╛рдВрд╡рд░ рдЬрд╡рд│рдЬрд╡рд│ рдореВрд│ рдЕрдЪреВрдХрддреЗрд╕рд╛рдареА Q8_0 рдЖрдгрд┐ рдЕрд▓реНрдЯреНрд░рд╛-рд▓реЛ рд╕рдВрд╕рд╛рдзрди рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА Q2_K рд╕рд╛рд░рдЦреНрдпрд╛ рдкреНрд░рд╛рдпреЛрдЧрд┐рдХ рдлреЙрд░реНрдореЕрдЯреНрд╕рдЪрд╛ рд╕рдорд╛рд╡реЗрд╢ рдЖрд╣реЗ.

### GGUF (рд╕рд╛рдорд╛рдиреНрдп GGML рдпреБрдирд┐рд╡реНрд╣рд░реНрд╕рд▓ рдлреЙрд░реНрдореЕрдЯ) SLM рддреИрдирд╛рддреАрд╕рд╛рдареА

GGUF рдПрдЬрдВрдЯрд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рд╡рд┐рд╢реЗрд╖рддрдГ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реНрдпрд╛ CPU рдЖрдгрд┐ рдПрдЬ рдЙрдкрдХрд░рдгрд╛рдВрд╡рд░ рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭрдб SLMs рддреИрдирд╛рдд рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдкреНрд░рд╛рдердорд┐рдХ рдлреЙрд░реНрдореЕрдЯ рдореНрд╣рдгреВрди рдХрд╛рдо рдХрд░рддреЗ:

**рдПрдЬрдВрдЯ-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реА рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ**: рдлреЙрд░реНрдореЕрдЯ рдЯреВрд▓ рдХреЙрд▓рд┐рдВрдЧ, рд╕рдВрд░рдЪрд┐рдд рдЖрдЙрдЯрдкреБрдЯ рдЬрдирд░реЗрд╢рди рдЖрдгрд┐ рдорд▓реНрдЯреА-рдЯрд░реНрди рд╕рдВрднрд╛рд╖рдгрд╛рдВрд╕рд╛рдареА рдкреНрд░рдЧрдд рд╕рдорд░реНрдердирд╛рд╕рд╣ SLM рд░реВрдкрд╛рдВрддрд░рдг рдЖрдгрд┐ рддреИрдирд╛рддреАрд╕рд╛рдареА рд╡реНрдпрд╛рдкрдХ рд╕рдВрд╕рд╛рдзрдиреЗ рдкреНрд░рджрд╛рди рдХрд░рддреЗ. рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рд╕реБрд╕рдВрдЧрддрддрд╛ рд╡рд┐рд╡рд┐рдз рдПрдЬ рдЙрдкрдХрд░рдгрд╛рдВрд╡рд░ рд╕реБрд╕рдВрдЧрдд рдПрдЬрдВрдЯ рд╡рд░реНрддрди рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рддреЗ.

**рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: GGUF рдПрдЬрдВрдЯ рд╡рд░реНрдХрдлреНрд▓реЛ рд╕рд╛рдареА рдХрд╛рд░реНрдпрдХреНрд╖рдо рдореЗрдорд░реА рд╡рд╛рдкрд░ рд╕рдХреНрд╖рдо рдХрд░рддреЗ, рдорд▓реНрдЯреА-рдПрдЬрдВрдЯ рдкреНрд░рдгрд╛рд▓реАрдВрд╕рд╛рдареА рдбрд╛рдпрдиреЕрдорд┐рдХ рдореЙрдбреЗрд▓ рд▓реЛрдбрд┐рдВрдЧрд▓рд╛ рд╕рдорд░реНрдерди рджреЗрддреЗ рдЖрдгрд┐ рд░рд┐рдЕрд▓-рдЯрд╛рдЗрдо рдПрдЬрдВрдЯ рд╕рдВрд╡рд╛рджрд╛рдВрд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реЗ рдЕрдиреБрдорд╛рди рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

### рдПрдЬ-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реЗ SLM рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕

#### Llama.cpp рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди

Llama.cpp рдПрдЬрдВрдЯрд┐рдХ SLM рддреИрдирд╛рддреАрд╕рд╛рдареА рд╡рд┐рд╢реЗрд╖рддрдГ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реНрдпрд╛ рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рддрдВрддреНрд░ рдкреНрд░рджрд╛рди рдХрд░рддреЗ:

**рдПрдЬрдВрдЯ-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди**: рдлреНрд░реЗрдорд╡рд░реНрдХ Q4_0 (рдореЛрдмрд╛рдЗрд▓ рдПрдЬрдВрдЯ рддреИрдирд╛рддреАрд╕рд╛рдареА 75% рдЖрдХрд╛рд░ рдХрдореА рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдЖрджрд░реНрд╢), Q5_1 (рдПрдЬ рдЕрдиреБрдорд╛рди рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рд╕рдВрддреБрд▓рд┐рдд рдЧреБрдгрд╡рддреНрддрд╛-рд╕рдВрдХреНрд╖реЗрдкрдг) рдЖрдгрд┐ Q8_0 (рдЙрддреНрдкрд╛рджрди рдПрдЬрдВрдЯ рдкреНрд░рдгрд╛рд▓реАрдВрд╕рд╛рдареА рдЬрд╡рд│рдЬрд╡рд│ рдореВрд│ рдЧреБрдгрд╡рддреНрддрд╛) рд▓рд╛ рд╕рдорд░реНрдерди рджреЗрддреЗ. рдкреНрд░рдЧрдд рдлреЙрд░реНрдореЕрдЯреНрд╕ рдЕрд▓реНрдЯреНрд░рд╛-рдХрдВрдкреНрд░реЗрд╕реНрдб рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рдЕрддреНрдпрдВрдд рдПрдЬ рдкрд░рд┐рд╕реНрдерд┐рддреА рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд.

**рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рдлрд╛рдпрджреЗ**: SIMD рдкреНрд░рд╡реЗрдЧрд╛рд╕рд╣ CPU-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реЗ рдЕрдиреБрдорд╛рди рдореЗрдорд░реА-рдХрд╛рд░реНрдпрдХреНрд╖рдо рдПрдЬрдВрдЯ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рдкреНрд░рджрд╛рди рдХрд░рддреЗ. x86, ARM рдЖрдгрд┐ Apple Silicon рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░рдордзреНрдпреЗ рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рд╕реБрд╕рдВрдЧрддрддрд╛ рд╕рд╛рд░реНрд╡рддреНрд░рд┐рдХ рдПрдЬрдВрдЯ рддреИрдирд╛рддреА рдХреНрд╖рдорддрд╛ рд╕рдХреНрд╖рдо рдХрд░рддреЗ.

#### Apple MLX рдлреНрд░реЗрдорд╡рд░реНрдХ SLM рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА

Apple MLX Apple Silicon рдЙрдкрдХрд░рдгрд╛рдВрд╡рд░ SLM-рд╕рдорд░реНрдерд┐рдд рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рд╡рд┐рд╢реЗрд╖рддрдГ рдбрд┐рдЭрд╛рдЗрди рдХреЗрд▓реЗрд▓реА рдореВрд│ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдкреНрд░рджрд╛рди рдХрд░рддреЗ:

**Apple Silicon рдПрдЬрдВрдЯ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдлреНрд░реЗрдорд╡рд░реНрдХ рдПрдХрддреНрд░рд┐рдд рдореЗрдорд░реА рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдореЗрдЯрд▓ рдкрд░рдлреЙрд░реНрдордиреНрд╕ рд╢реЗрдбрд░реНрд╕ рдПрдХрддреНрд░реАрдХрд░рдгрд╛рд╕рд╣, рдПрдЬрдВрдЯ рдЕрдиреБрдорд╛рдирд╛рд╕рд╛рдареА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдорд┐рд╢реНрд░рд┐рдд рдЕрдЪреВрдХрддрд╛ рдЖрдгрд┐ рдорд▓реНрдЯреА-рдПрдЬрдВрдЯ рдкреНрд░рдгрд╛рд▓реАрдВрд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реА рдореЗрдорд░реА рдмрдБрдбрд╡рд┐рдбреНрде рд╡рд╛рдкрд░рддреЗ. M-рд╕рд┐рд░реАрдЬ рдЪрд┐рдкреНрд╕рд╡рд░ SLM рдПрдЬрдВрдЯреНрд╕ рдЕрдкрд╡рд╛рджрд╛рддреНрдордХ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рджрд░реНрд╢рд╡рддрд╛рдд.

**рд╡рд┐рдХрд╛рд╕ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ**: рдПрдЬрдВрдЯ-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рдирд╕рд╣ Python рдЖрдгрд┐ Swift API рд╕рдорд░реНрдерди, рдПрдЬрдВрдЯ рд╢рд┐рдХреНрд╖рдгрд╛рд╕рд╛рдареА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рднрд┐рдиреНрдирддрд╛ рдЖрдгрд┐ Apple рд╡рд┐рдХрд╛рд╕ рд╕рд╛рдзрдирд╛рдВрд╕рд╣ рдЕрдЦрдВрдб рдПрдХрддреНрд░реАрдХрд░рдг рд╡реНрдпрд╛рдкрдХ рдПрдЬрдВрдЯ рд╡рд┐рдХрд╛рд╕ рд╡рд╛рддрд╛рд╡рд░рдг рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

#### ONNX Runtime рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо SLM рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА

ONNX Runtime рдПрдХ рд╕рд╛рд░реНрд╡рддреНрд░рд┐рдХ рдЕрдиреБрдорд╛рди рдЗрдВрдЬрд┐рди рдкреНрд░рджрд╛рди рдХрд░рддреЗ рдЬреЗ SLM рдПрдЬрдВрдЯреНрд╕рдирд╛ рд╡рд┐рд╡рд┐рдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рдЖрдгрд┐ рдСрдкрд░реЗрдЯрд┐рдВрдЧ рд╕рд┐рд╕реНрдЯрдорд╡рд░ рд╕рд╛рддрддреНрдпрд╛рдиреЗ рдЪрд╛рд▓рд╡рдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдХрд░рддреЗ:

**рд╕рд╛рд░реНрд╡рддреНрд░рд┐рдХ рддреИрдирд╛рддреА**: ONNX Runtime Windows, Linux, macOS, iOS рдЖрдгрд┐ Android рдкреНрд▓реЕрдЯрдлреЙрд░реНрдорд╡рд░ SLM рдПрдЬрдВрдЯ рд╡рд░реНрддрдирд╛рдЪреА рд╕рд╛рддрддреНрдп рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рддреЗ. рд╣реЗ рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рд╕реБрд╕рдВрдЧрддрддрд╛ рд╡рд┐рдХрд╕рдХрд╛рдВрдирд╛ рдПрдХрджрд╛рдЪ рд▓рд┐рд╣рд┐рдгреНрдпрд╛рд╕ рдЖрдгрд┐ рд╕рд░реНрд╡рддреНрд░ рддреИрдирд╛рдд рдХрд░рдгреНрдпрд╛рд╕ рд╕рдХреНрд╖рдо рдХрд░рддреЗ, рдорд▓реНрдЯреА-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рд╡рд┐рдХрд╛рд╕ рдЖрдгрд┐ рджреЗрдЦрднрд╛рд▓ рдУрд╡реНрд╣рд░рд╣реЗрдб рд▓рдХреНрд╖рдгреАрдпрд░реАрддреНрдпрд╛ рдХрдореА рдХрд░рддреЗ.

**рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдкреНрд░рд╡реЗрдЧ рдкрд░реНрдпрд╛рдп**: рдлреНрд░реЗрдорд╡рд░реНрдХ рд╡рд┐рд╡рд┐рдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рдирд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реЗ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рдкреНрд░рджрд╛рддреЗ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рдЬреНрдпрд╛рдордзреНрдпреЗ CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) рдЖрдгрд┐ рд╡рд┐рд╢реЗрд╖ рдкреНрд░рд╡реЗрдЧрдХ (Intel VPU, Qualcomm NPU) рдпрд╛рдВрдЪрд╛ рд╕рдорд╛рд╡реЗрд╢ рдЖрд╣реЗ. SLM рдПрдЬрдВрдЯреНрд╕ рдХреЛрдб рдмрджрд▓рд╛рдВрд╢рд┐рд╡рд╛рдп рдЙрдкрд▓рдмреНрдз рд╕рд░реНрд╡реЛрддреНрддрдо рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рдЪрд╛ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рд▓рд╛рдн рдШреЗрдК рд╢рдХрддрд╛рдд.

**рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ**: ONNX Runtime рдЙрддреНрдкрд╛рджрди рдПрдЬрдВрдЯ рддреИрдирд╛рддреАрд╕рд╛рдареА рдЖрд╡рд╢реНрдпрдХ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ-рдЧреНрд░реЗрдб рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ рдкреНрд░рджрд╛рди рдХрд░рддреЗ рдЬреНрдпрд╛рдордзреНрдпреЗ рдЬрд▓рдж рдЕрдиреБрдорд╛рдирд╛рд╕рд╛рдареА рдЧреНрд░рд╛рдл рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди, рд╕рдВрд╕рд╛рдзрди-рдЖрдзрд╛рд░рд┐рдд рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдореЗрдорд░реА рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╡рд┐рд╢реНрд▓реЗрд╖рдгрд╛рд╕рд╛рдареА рд╡реНрдпрд╛рдкрдХ рдкреНрд░реЛрдлрд╛рдЗрд▓рд┐рдВрдЧ рд╕рд╛рдзрдиреЗ рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдЖрд╣реЗрдд. рдлреНрд░реЗрдорд╡рд░реНрдХ Python рдЖрдгрд┐ C++ APIs рджреЛрдиреНрд╣реАрд▓рд╛ рд╕рдорд░реНрдерди рджреЗрддреЗ рдЬреЗ рд▓рд╡рдЪрд┐рдХ рдПрдХрддреНрд░реАрдХрд░рдг рд╕рдХреНрд╖рдо рдХрд░рддреЗ.

## SLM рд╡рд┐рд░реБрджреНрдз LLM рдПрдЬрдВрдЯрд┐рдХ рдкреНрд░рдгрд╛рд▓реАрдВрдордзреНрдпреЗ: рдкреНрд░рдЧрдд рддреБрд▓рдирд╛

### рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдордзреНрдпреЗ SLM рдлрд╛рдпрджреЗ

**рдСрдкрд░реЗрд╢рдирд▓ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛**: SLMs рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА LLMs рдЪреНрдпрд╛ рддреБрд▓рдиреЗрдд 10-30├Ч рдЦрд░реНрдЪ рдХрдореА рдХрд░рддрд╛рдд, рдЬреНрдпрд╛рдореБрд│реЗ рдореЛрдареНрдпрд╛ рдкреНрд░рдорд╛рдгрд╛рд╡рд░ рд░рд┐рдЕрд▓-рдЯрд╛рдЗрдо рдПрдЬрдВрдЯрд┐рдХ рдкреНрд░рддрд┐рд╕рд╛рдж рд╕рдХреНрд╖рдо рд╣реЛрддреЛ. рддреЗ рдХрдореА рд╕рдВрдЧрдгрдХреАрдп рдЬрдЯрд┐рд▓рддреЗрдореБрд│реЗ рдЬрд▓рдж рдЕрдиреБрдорд╛рди рд╡реЗрд│рд╛ рджреЗрддрд╛рдд, рдЬреНрдпрд╛рдореБрд│реЗ рддреЗ рдкрд░рд╕реНрдкрд░ рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЖрджрд░реНрд╢ рдмрдирддрд╛рдд.

**рдПрдЬ рддреИрдирд╛рддреА рдХреНрд╖рдорддрд╛**: SLMs рдЗрдВрдЯрд░рдиреЗрдЯ рдЕрд╡рд▓рдВрдмрд┐рддреНрд╡рд╛рд╢рд┐рд╡рд╛рдп рдСрди-рдбрд┐рд╡реНрд╣рд╛рдЗрд╕ рдПрдЬрдВрдЯ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд, рд╕реНрдерд╛рдирд┐рдХ рдПрдЬрдВрдЯ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдХрд░реВрди рд╡рд╛рдврд▓реЗрд▓реА рдЧреЛрдкрдиреАрдпрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд рдЖрдгрд┐ рд╡рд┐рд╡рд┐рдз рдПрдЬ рдХрдВрдкреНрдпреБрдЯрд┐рдВрдЧ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдпреЛрдЧреНрдп рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рд╕рд╛рдиреБрдХреВрд▓рд┐рдд рдХреЗрд▓реНрдпрд╛ рдЬрд╛рдК рд╢рдХрддрд╛рдд.

**рдПрдЬрдВрдЯ-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: SLMs рдЯреВрд▓ рдХреЙрд▓рд┐рдВрдЧ, рд╕рдВрд░рдЪрд┐рдд рдЖрдЙрдЯрдкреБрдЯ рдЬрдирд░реЗрд╢рди рдЖрдгрд┐ рдирд┐рдпрдорд┐рдд рдирд┐рд░реНрдгрдп рдШреЗрдгреНрдпрд╛рдЪреНрдпрд╛ рд╡рд░реНрдХрдлреНрд▓реЛрдордзреНрдпреЗ рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗрдд рдЬреЗ рд╕рд╛рдорд╛рдиреНрдпрддрдГ 70-80% рд╕рд╛рдорд╛рдиреНрдп рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпреЗ рдмрдирд╡рддрд╛рдд.

### рдПрдЬрдВрдЯ рдкреНрд░рдгрд╛рд▓реАрдВрдордзреНрдпреЗ SLMs рд╡рд┐рд░реБрджреНрдз LLMs рдХрдзреА рд╡рд╛рдкрд░рд╛рдпрдЪреЗ

**SLMs рд╕рд╛рдареА рдпреЛрдЧреНрдп**:
- **рдкреБрдирд░рд╛рд╡реГрддреНрддреА рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпреЗ**: рдбреЗрдЯрд╛ рдПрдВрдЯреНрд░реА, рдлреЙрд░реНрдо рднрд░рдгреЗ, рдирд┐рдпрдорд┐рдд API рдХреЙрд▓реНрд╕
- **рдЯреВрд▓ рдПрдХрддреНрд░реАрдХрд░рдг**: рдбреЗрдЯрд╛рдмреЗрд╕ рдХреНрд╡реЗрд░реА, рдлрд╛рдЗрд▓ рдСрдкрд░реЗрд╢рдиреНрд╕, рдкреНрд░рдгрд╛рд▓реА рд╕рдВрд╡рд╛рдж
- **рд╕рдВрд░рдЪрд┐рдд рд╡рд░реНрдХрдлреНрд▓реЛ**: рдкреВрд░реНрд╡рдирд┐рд░реНрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдЕрдиреБрд╕рд░рдгреЗ
- **рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдПрдЬрдВрдЯреНрд╕**: рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛, рд╡реЗрд│рд╛рдкрддреНрд░рдХ, рдореВрд▓рднреВрдд рд╡рд┐рд╢реНрд▓реЗрд╖рдг
- **рд╕реНрдерд╛рдирд┐рдХ рдкреНрд░рдХреНрд░рд┐рдпрд╛**: рдЧреЛрдкрдиреАрдпрддрд╛-рд╕рдВрд╡реЗрджрдирд╢реАрд▓ рдПрдЬрдВрдЯ рдСрдкрд░реЗрд╢рдиреНрд╕

**LLMs рд╕рд╛рдареА рдЪрд╛рдВрдЧрд▓реЗ**:
- **рдЬрдЯрд┐рд▓ рд╡рд┐рдЪрд╛рд░рд╕рд░рдгреА**: рдирд╡реАрди рд╕рдорд╕реНрдпрд╛ рд╕реЛрдбрд╡рдгреЗ, рдзреЛрд░рдгрд╛рддреНрдордХ рдирд┐рдпреЛрдЬрди
- **рдореБрдХреНрдд-рд╢реЗрд╡рдЯ рд╕рдВрднрд╛рд╖рдгреЗ**: рд╕рд╛рдорд╛рдиреНрдп рдЪреЕрдЯ, рд╕рд░реНрдЬрдирд╢реАрд▓ рдЪрд░реНрдЪрд╛
- **рд╡реНрдпрд╛рдкрдХ рдЬреНрдЮрд╛рди рдХрд╛рд░реНрдпреЗ**: рд╡рд┐рд╕реНрддреГрдд рд╕рд╛рдорд╛рдиреНрдп рдЬреНрдЮрд╛рди рдЖрд╡рд╢реНрдпрдХ рдЕрд╕рд▓реЗрд▓реЗ рд╕рдВрд╢реЛрдзрди
- **рдирд╡реАрди рдкрд░рд┐рд╕реНрдерд┐рддреА**: рдкреВрд░реНрдгрдкрдгреЗ рдирд╡реАрди рдПрдЬрдВрдЯ рдкрд░рд┐рд╕реНрдерд┐рддреА рд╣рд╛рддрд╛рд│рдгреЗ

### рд╣рд╛рдпрдмреНрд░рд┐рдб рдПрдЬрдВрдЯ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░

SLMs рдЖрдгрд┐ LLMs рдЪрд╛ рд╕рдорд╛рд╡реЗрд╢ рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рд╡рд┐рд╖рдо рдПрдЬрдВрдЯрд┐рдХ рдкреНрд░рдгрд╛рд▓реАрдВрдордзреНрдпреЗ рд╕рд░реНрд╡реЛрддреНрддрдо рджреГрд╖реНрдЯрд┐рдХреЛрди:

**рд╕реНрдорд╛рд░реНрдЯ рдПрдЬрдВрдЯ рдСрд░реНрдХреЗрд╕реНрдЯреНрд░реЗрд╢рди**:
1. **SLM рдкреНрд░рд╛рдердорд┐рдХ рдореНрд╣рдгреВрди**: 70-80% рдирд┐рдпрдорд┐рдд рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпреЗ рд╕реНрдерд╛рдирд┐рдХ рдкрд╛рддрд│реАрд╡рд░ рд╣рд╛рддрд╛рд│рд╛
2. **LLM рдЖрд╡рд╢реНрдпрдХ рдЕрд╕рд▓реНрдпрд╛рд╕**: рдХреНрд▓рд╛рдЙрдб-рдЖрдзрд╛рд░рд┐рдд рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓реНрд╕рдХрдбреЗ рдЬрдЯрд┐рд▓ рдХреНрд╡реЗрд░реА рд░реВрдЯ рдХрд░рд╛
3. **рд╡рд┐рд╢реЗрд╖ SLMs**: рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рдПрдЬрдВрдЯ рдбреЛрдореЗрдирд╕рд╛рдареА рд╡реЗрдЧрд╡реЗрдЧрд│реЗ рд▓рд╣рд╛рди рдореЙрдбреЗрд▓реНрд╕
4. **рдЦрд░реНрдЪ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдмреБрджреНрдзрд┐рдорд╛рди рд░реВрдЯрд┐рдВрдЧрджреНрд╡рд╛рд░реЗ рдорд╣рд╛рдЧ LLM рдХреЙрд▓реНрд╕ рдХрдореА рдХрд░рд╛

## рдЙрддреНрдкрд╛рджрди SLM рдПрдЬрдВрдЯ рддреИрдирд╛рддреА рд░рдгрдиреАрддреА

### Foundry Local: рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ-рдЧреНрд░реЗрдб рдПрдЬ рдПрдЖрдп рд░рдирдЯрд╛рдЗрдо

Foundry Local (https://github.com/microsoft/foundry-local) рдЙрддреНрдкрд╛рджрди рдПрдЬ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕ рддреИрдирд╛рдд рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА Microsoft рдЪреЗ рдкреНрд░рдореБрдЦ рд╕рдорд╛рдзрд╛рди рдореНрд╣рдгреВрди рдХрд╛рдо рдХрд░рддреЗ. рд╣реЗ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ-рдЧреНрд░реЗрдб рд╡реИрд╢рд┐рд╖реНрдЯреНрдпрд╛рдВрд╕рд╣ рдЖрдгрд┐ рдЕрдЦрдВрдб рдПрдХрддреНрд░реАрдХрд░рдг рдХреНрд╖рдорддрд╛рдВрд╕рд╣ SLM-рд╕рдорд░реНрдерд┐рдд рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рд╡рд┐рд╢реЗрд╖рддрдГ рдбрд┐рдЭрд╛рдЗрди рдХреЗрд▓реЗрд▓реЗ рд╕рдВрдкреВрд░реНрдг рд░рдирдЯрд╛рдЗрдо рд╡рд╛рддрд╛рд╡рд░рдг рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

**рдореБрдЦреНрдп рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЖрдгрд┐ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ**:
- **OpenAI-рд╕реБрд╕рдВрдЧрдд API**: OpenAI SDK рдЖрдгрд┐ Agent Framework рдПрдХрддреНрд░реАрдХрд░рдгрд╛рдВрд╕рд╣ рдкреВрд░реНрдг рд╕реБрд╕рдВрдЧрддрддрд╛
- **рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдЙрдкрд▓рдмреНрдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ (CUDA GPU, Qualcomm NPU, CPU) рдЖрдзрд╛рд░рд┐рдд рдореЙрдбреЗрд▓ рдкреНрд░рдХрд╛рд░рд╛рдВрдЪреА рдмреБрджреНрдзрд┐рдорд╛рди рдирд┐рд╡рдб
- **рдореЙрдбреЗрд▓ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди**: SLM рдореЙрдбреЗрд▓реНрд╕рдЪреЗ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдбрд╛рдЙрдирд▓реЛрдбрд┐рдВрдЧ, рдХреЕрд╢рд┐рдВрдЧ рдЖрдгрд┐ рдЬреАрд╡рдирдЪрдХреНрд░ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди
- **рд╕реЗрд╡рд╛ рд╢реЛрдз**: рдПрдЬрдВрдЯ рдлреНрд░реЗрдорд╡рд░реНрдХрд╕рд╛рдареА рд╢реВрдиреНрдп-рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рд╕реЗрд╡рд╛ рд╢реЛрдз
- **рд╕рдВрд╕рд╛рдзрди рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдПрдЬ рддреИрдирд╛рддреАрд╕рд╛рдареА рдмреБрджреНрдзрд┐рдорд╛рди рдореЗрдорд░реА рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдЖрдгрд┐ рдКрд░реНрдЬрд╛ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛

#### рд╕реНрдерд╛рдкрдирд╛ рдЖрдгрд┐ рд╕реЗрдЯрдЕрдк

**рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рд╕реНрдерд╛рдкрдирд╛**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**рдПрдЬрдВрдЯ рд╡рд┐рдХрд╛рд╕рд╛рд╕рд╛рдареА рдЬрд▓рдж рдкреНрд░рд╛рд░рдВрдн**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### рдПрдЬрдВрдЯ рдлреНрд░реЗрдорд╡рд░реНрдХ рдПрдХрддреНрд░реАрдХрд░рдг

**Foundry Local SDK рдПрдХрддреНрд░реАрдХрд░рдг**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рдЖрдгрд┐ рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### рдЙрддреНрдкрд╛рджрди рддреИрдирд╛рддреА рдирдореБрдиреЗ

**рд╕рд┐рдВрдЧрд▓-рдПрдЬрдВрдЯ рдЙрддреНрдкрд╛рджрди рд╕реЗрдЯрдЕрдк**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**рдорд▓реНрдЯреА-рдПрдЬрдВрдЯ рдЙрддреНрдкрд╛рджрди рдСрд░реНрдХреЗрд╕реНрдЯреНрд░реЗрд╢рди**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ рдЖрдгрд┐ рдирд┐рд░реАрдХреНрд╖рдг

**рдЖрд░реЛрдЧреНрдп рдирд┐рд░реАрдХреНрд╖рдг рдЖрдгрд┐ рдирд┐рд░реАрдХреНрд╖рдгрдХреНрд╖рдорддрд╛**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**рд╕рдВрд╕рд╛рдзрди рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдЖрдгрд┐ рдСрдЯреЛ-рд╕реНрдХреЗрд▓рд┐рдВрдЧ**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### рдкреНрд░рдЧрдд рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди

**рд╕рд╛рдиреБрдХреВрд▓ рдореЙрдбреЗрд▓ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**рдЙрддреНрдкрд╛рджрди рддреИрдирд╛рддреА рдЪреЗрдХрд▓рд┐рд╕реНрдЯ**:

тЬЕ **рд╕реЗрд╡рд╛ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди**:
- рд╡рд╛рдкрд░ рдкреНрд░рдХрд░рдгрд╛рдВрд╕рд╛рдареА рдпреЛрдЧреНрдп рдореЙрдбреЗрд▓ рдЙрдкрдирд╛рдо рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛
- рд╕рдВрд╕рд╛рдзрди рдорд░реНрдпрд╛рджрд╛ рдЖрдгрд┐ рдирд┐рд░реАрдХреНрд╖рдг рдереНрд░реЗрд╢реЛрд▓реНрдб рд╕реЗрдЯ рдХрд░рд╛
- рдЖрд░реЛрдЧреНрдп рддрдкрд╛рд╕рдгреА рдЖрдгрд┐ рдореЗрдЯреНрд░рд┐рдХреНрд╕ рд╕рдВрдЧреНрд░рд╣ рд╕рдХреНрд╖рдо рдХрд░рд╛
- рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдкреБрдирд░рд╛рд░рдВрдн рдЖрдгрд┐ рдлреЗрд▓рдУрд╡реНрд╣рд░ рдХреЙрдиреНрдл
- Microsoft Agent Framework рдПрдХрддреНрд░реАрдХрд░рдгрд╛рдЪреА рдЪрд╛рдЪрдгреА рдХрд░рд╛
- рдСрдлрд▓рд╛рдЗрди рдСрдкрд░реЗрд╢рди рдХреНрд╖рдорддрд╛ рд╕рддреНрдпрд╛рдкрд┐рдд рдХрд░рд╛
- рдлреЗрд▓рдУрд╡реНрд╣рд░ рдкрд░рд┐рд╕реНрдерд┐рддреА рдЖрдгрд┐ рддреНрд░реБрдЯреА рд╣рд╛рддрд╛рд│рдгреАрдЪреА рдЪрд╛рдЪрдгреА рдХрд░рд╛
- рдПрдВрдб-рдЯреВ-рдПрдВрдб рдПрдЬрдВрдЯ рд╡рд░реНрдХрдлреНрд▓реЛрдЪреА рдкрдбрддрд╛рд│рдгреА рдХрд░рд╛

**Foundry Local рд╕реЛрдмрдд рддреБрд▓рдирд╛**:

| рд╡реИрд╢рд┐рд╖реНрдЯреНрдп | Foundry Local | Ollama |
|-----------|---------------|--------|
| **рд▓рдХреНрд╖реНрдп рд╡рд╛рдкрд░ рдкреНрд░рдХрд░рдг** | рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдЙрддреНрдкрд╛рджрди | рд╡рд┐рдХрд╛рд╕ рдЖрдгрд┐ рд╕рдореБрджрд╛рдп |
| **рдореЙрдбреЗрд▓ рдЗрдХреЛрд╕рд┐рд╕реНрдЯрдо** | Microsoft-рдХреНрдпреБрд░реЗрдЯреЗрдб | рд╡рд┐рд╕реНрддреГрдд рд╕рдореБрджрд╛рдп |
| **рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди** | рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд (CUDA/NPU/CPU) | рдореЕрдиреНрдпреБрдЕрд▓ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди |
| **рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ** | рдЕрдВрдЧрднреВрдд рдореЙрдирд┐рдЯрд░рд┐рдВрдЧ, рд╕реБрд░рдХреНрд╖рд╛ | рд╕рдореБрджрд╛рдп рд╕рд╛рдзрдиреЗ |
| **рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЧреБрдВрддрд╛рдЧреБрдВрдд** | рд╕реЛрдкреЗ (winget install) | рд╕реЛрдкреЗ (curl install) |
| **API рд╕реБрд╕рдВрдЧрддрддрд╛** | OpenAI + рд╡рд┐рд╕реНрддрд╛рд░ | OpenAI рдорд╛рдирдХ |
| **рд╕рдорд░реНрдерди** | Microsoft рдЕрдзрд┐рдХреГрдд | рд╕рдореБрджрд╛рдп-рдЪрд╛рд▓рд┐рдд |
| **рд╕рд░реНрд╡реЛрддреНрддрдо рдХреЛрдгрд╛рд╕рд╛рдареА** | рдЙрддреНрдкрд╛рджрди рдПрдЬрдВрдЯреНрд╕ | рдкреНрд░реЛрдЯреЛрдЯрд╛рдпрдкрд┐рдВрдЧ, рд╕рдВрд╢реЛрдзрди |

**Ollama рдирд┐рд╡рдбрдгреНрдпрд╛рдЪреА рд╡реЗрд│**:
- **рд╡рд┐рдХрд╛рд╕ рдЖрдгрд┐ рдкреНрд░реЛрдЯреЛрдЯрд╛рдпрдкрд┐рдВрдЧ**: рд╡рд┐рд╡рд┐рдз рдореЙрдбреЗрд▓реНрд╕рд╕рд╣ рдЬрд▓рдж рдкреНрд░рдпреЛрдЧ
- **рд╕рдореБрджрд╛рдп рдореЙрдбреЗрд▓реНрд╕**: рдирд╡реАрдирддрдо рд╕рдореБрджрд╛рдп-рдпреЛрдЧрджрд╛рди рдХреЗрд▓реЗрд▓реНрдпрд╛ рдореЙрдбреЗрд▓реНрд╕рдордзреНрдпреЗ рдкреНрд░рд╡реЗрд╢
- **рд╢реИрдХреНрд╖рдгрд┐рдХ рд╡рд╛рдкрд░**: AI рдПрдЬрдВрдЯ рд╡рд┐рдХрд╛рд╕ рд╢рд┐рдХрдгреЗ рдЖрдгрд┐ рд╢рд┐рдХрд╡рдгреЗ
- **рд╕рдВрд╢реЛрдзрди рдкреНрд░рдХрд▓реНрдк**: рд╡рд┐рд╡рд┐рдз рдореЙрдбреЗрд▓реНрд╕рдордзреНрдпреЗ рдкреНрд░рд╡реЗрд╢ рдЖрд╡рд╢реНрдпрдХ рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рд╢реИрдХреНрд╖рдгрд┐рдХ рд╕рдВрд╢реЛрдзрдирд╛рд╕рд╛рдареА
- **рдХрд╕реНрдЯрдо рдореЙрдбреЗрд▓реНрд╕**: рдХрд╕реНрдЯрдо рдлрд╛рдЗрди-рдЯреНрдпреВрди рдХреЗрд▓реЗрд▓реНрдпрд╛ рдореЙрдбреЗрд▓реНрд╕ рддрдпрд╛рд░ рдХрд░рдгреЗ рдЖрдгрд┐ рдЪрд╛рдЪрдгреА рдХрд░рдгреЗ

### VLLM: рдЙрдЪреНрдЪ-рдкреНрд░рджрд░реНрд╢рди SLM рдПрдЬрдВрдЯ рдЗрдирдлрд░рдиреНрд╕

VLLM (Very Large Language Model inference) рдПрдХ рдЙрдЪреНрдЪ-рдереНрд░реВрдкреБрдЯ, рдореЗрдорд░реА-рдХрд╛рд░реНрдпрдХреНрд╖рдо рдЗрдирдлрд░рдиреНрд╕ рдЗрдВрдЬрд┐рди рдкреНрд░рджрд╛рди рдХрд░рддреЗ рдЬреЗ рд╡рд┐рд╢реЗрд╖рддрдГ рдЙрддреНрдкрд╛рджрди SLM рдореЛрдареНрдпрд╛ рдкреНрд░рдорд╛рдгрд╛рд╡рд░ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯрд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗ рдЖрд╣реЗ. рдЬрд┐рдереЗ Foundry Local рд╡рд╛рдкрд░рдгреНрдпрд╛рд╕ рд╕реБрд▓рднрддреЗрд╡рд░ рд▓рдХреНрд╖ рдХреЗрдВрджреНрд░рд┐рдд рдХрд░рддреЗ рдЖрдгрд┐ Ollama рд╕рдореБрджрд╛рдп рдореЙрдбреЗрд▓реНрд╕рд╡рд░ рдЬреЛрд░ рджреЗрддреЗ, рддрд┐рдереЗ VLLM рдЙрдЪреНрдЪ-рдкреНрд░рджрд░реНрд╢рди рдкрд░рд┐рд╕реНрдерд┐рддреАрдд рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗ рдЬреНрдпрд╛рд╕рд╛рдареА рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рдереНрд░реВрдкреБрдЯ рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдо рд╕рдВрд╕рд╛рдзрди рдЙрдкрдпреЛрдЧ рдЖрд╡рд╢реНрдпрдХ рдЖрд╣реЗ.

**рдореБрдЦреНрдп рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЖрдгрд┐ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ**:
- **PagedAttention**: рдХрд╛рд░реНрдпрдХреНрд╖рдо рд▓рдХреНрд╖ рд╕рдВрдЧрдгрдирд╛рд╕рд╛рдареА рдХреНрд░рд╛рдВрддрд┐рдХрд╛рд░реА рдореЗрдорд░реА рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди
- **рдбрд╛рдпрдиреЕрдорд┐рдХ рдмреЕрдЪрд┐рдВрдЧ**: рдЗрд╖реНрдЯрддрдо рдереНрд░реВрдкреБрдЯрд╕рд╛рдареА рдмреБрджреНрдзрд┐рдорд╛рди рд╡рд┐рдирдВрддреА рдмреЕрдЪрд┐рдВрдЧ
- **GPU рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдкреНрд░рдЧрдд CUDA рдХрд░реНрдирд▓реНрд╕ рдЖрдгрд┐ рдЯреЗрдиреНрд╕рд░ рдкреЕрд░рд▓рд▓рд┐рдЭрдо рд╕рдорд░реНрдерди
- **OpenAI рд╕реБрд╕рдВрдЧрддрддрд╛**: рдЕрдЦрдВрдб рдПрдХрддреНрд░реАрдХрд░рдгрд╛рд╕рд╛рдареА рдкреВрд░реНрдг API рд╕реБрд╕рдВрдЧрддрддрд╛
- **рд╕реНрдкреЗрдХреНрдпреБрд▓реЗрдЯрд┐рд╡ рдбрд┐рдХреЛрдбрд┐рдВрдЧ**: рдкреНрд░рдЧрдд рдЗрдирдлрд░рдиреНрд╕ рдЧрддреА рддрдВрддреНрд░рдЬреНрдЮрд╛рди
- **рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рд╕рдорд░реНрдерди**: рдореЗрдорд░реА рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрд╕рд╛рдареА INT4, INT8, рдЖрдгрд┐ FP16 рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди

#### рд╕реНрдерд╛рдкрдирд╛ рдЖрдгрд┐ рд╕реЗрдЯрдЕрдк

**рд╕реНрдерд╛рдкрдирд╛ рдкрд░реНрдпрд╛рдп**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**рдПрдЬрдВрдЯ рд╡рд┐рдХрд╛рд╕рд╛рд╕рд╛рдареА рдЬрд▓рдж рдкреНрд░рд╛рд░рдВрдн**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### рдПрдЬрдВрдЯ рдлреНрд░реЗрдорд╡рд░реНрдХ рдПрдХрддреНрд░реАрдХрд░рдг

**Microsoft Agent Framework рд╕рд╣ VLLM**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**рдЙрдЪреНрдЪ-рдереНрд░реВрдкреБрдЯ рдорд▓реНрдЯреА-рдПрдЬрдВрдЯ рд╕реЗрдЯрдЕрдк**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### рдЙрддреНрдкрд╛рджрди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкреЕрдЯрд░реНрдиреНрд╕

**рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ VLLM рдЙрддреНрдкрд╛рджрди рд╕реЗрд╡рд╛**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ рдЖрдгрд┐ рдореЙрдирд┐рдЯрд░рд┐рдВрдЧ

**рдкреНрд░рдЧрдд VLLM рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдореЙрдирд┐рдЯрд░рд┐рдВрдЧ**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### рдкреНрд░рдЧрдд рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди

**рдЙрддреНрдкрд╛рджрди VLLM рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рдЯреЗрдореНрдкрд▓реЗрдЯреНрд╕**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**VLLM рд╕рд╛рдареА рдЙрддреНрдкрд╛рджрди рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдЪреЗрдХрд▓рд┐рд╕реНрдЯ**:

тЬЕ **рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**:
- рдорд▓реНрдЯреА-GPU рд╕реЗрдЯрдЕрдкрд╕рд╛рдареА рдЯреЗрдиреНрд╕рд░ рдкреЕрд░рд▓рд▓рд┐рдЭрдо рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛
- рдореЗрдорд░реА рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрд╕рд╛рдареА рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди (AWQ/GPTQ) рд╕рдХреНрд╖рдо рдХрд░рд╛
- GPU рдореЗрдорд░реА рдЙрдкрдпреЛрдЧрд╛рд╕рд╛рдареА рдЗрд╖реНрдЯрддрдо рд╕реЗрдЯ рдХрд░рд╛ (85-95%)
- рдереНрд░реВрдкреБрдЯрд╕рд╛рдареА рдпреЛрдЧреНрдп рдмреЕрдЪ рд╕рд╛рдЗрдЬ рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛

тЬЕ **рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдЯреНрдпреВрдирд┐рдВрдЧ**:
- рдкреБрдирд░рд╛рд╡реГрддреНрддреА рд╡рд┐рдирдВрддреНрдпрд╛рдВрд╕рд╛рдареА рдкреНрд░реАрдлрд┐рдХреНрд╕ рдХреЕрд╢рд┐рдВрдЧ рд╕рдХреНрд╖рдо рдХрд░рд╛
- рд▓рд╛рдВрдм рдЕрдиреБрдХреНрд░рдорд╛рдВрд╕рд╛рдареА рдЪрдВрдХреЗрдб рдкреНрд░реАрдлрд┐рд▓ рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛
- рдЬрд▓рдж рдЗрдирдлрд░рдиреНрд╕рд╕рд╛рдареА рд╕реНрдкреЗрдХреНрдпреБрд▓реЗрдЯрд┐рд╡ рдбрд┐рдХреЛрдбрд┐рдВрдЧ рд╕реЗрдЯ рдХрд░рд╛
- рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рд╡рд░ рдЖрдзрд╛рд░рд┐рдд max_num_seqs рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХрд░рд╛

тЬЕ **рдЙрддреНрдкрд╛рджрди рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ**:
- рдЖрд░реЛрдЧреНрдп рдореЙрдирд┐рдЯрд░рд┐рдВрдЧ рдЖрдгрд┐ рдореЗрдЯреНрд░рд┐рдХреНрд╕ рд╕рдВрдЧреНрд░рд╣ рд╕реЗрдЯ рдХрд░рд╛
- рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд░реАрд╕реНрдЯрд╛рд░реНрдЯ рдЖрдгрд┐ рдлреЗрд▓рдУрд╡реНрд╣рд░ рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛
- рд╡рд┐рдирдВрддреА рд░рд╛рдВрдЧрд╛ рдЖрдгрд┐ рд▓реЛрдб рдмреЕрд▓рдиреНрд╕рд┐рдВрдЧ рдЕрдВрдорд▓рд╛рдд рдЖрдгрд╛
- рд╡реНрдпрд╛рдкрдХ рд▓реЙрдЧрд┐рдВрдЧ рдЖрдгрд┐ рдЕрд▓рд░реНрдЯрд┐рдВрдЧ рд╕реЗрдЯ рдХрд░рд╛

тЬЕ **рд╕реБрд░рдХреНрд╖рд╛ рдЖрдгрд┐ рд╡рд┐рд╢реНрд╡рд╛рд╕рд╛рд░реНрд╣рддрд╛**:
- рдлрд╛рдпрд░рд╡реЙрд▓ рдирд┐рдпрдо рдЖрдгрд┐ рдкреНрд░рд╡реЗрд╢ рдирд┐рдпрдВрддреНрд░рдг рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛
- API рджрд░ рдорд░реНрдпрд╛рджрд╛ рдЖрдгрд┐ рдкреНрд░рдорд╛рдгреАрдХрд░рдг рд╕реЗрдЯ рдХрд░рд╛
- рдЧреНрд░реЗрд╕рдлреБрд▓ рд╢рдЯрдбрд╛рдЙрди рдЖрдгрд┐ рдХреНрд▓реАрдирдЕрдк рдЕрдВрдорд▓рд╛рдд рдЖрдгрд╛
- рдмреЕрдХрдЕрдк рдЖрдгрд┐ рдЖрдкрддреНрддреА рдкреБрдирд░реНрдкреНрд░рд╛рдкреНрддреА рдХреЙрдиреНрдлрд┐рдЧрд░ рдХрд░рд╛

тЬЕ **рдПрдХрддреНрд░реАрдХрд░рдг рдЪрд╛рдЪрдгреА**:
- Microsoft Agent Framework рдПрдХрддреНрд░реАрдХрд░рдгрд╛рдЪреА рдЪрд╛рдЪрдгреА рдХрд░рд╛
- рдЙрдЪреНрдЪ-рдереНрд░реВрдкреБрдЯ рдкрд░рд┐рд╕реНрдерд┐рддреА рд╕рддреНрдпрд╛рдкрд┐рдд рдХрд░рд╛
- рдлреЗрд▓рдУрд╡реНрд╣рд░ рдЖрдгрд┐ рдкреБрдирд░реНрдкреНрд░рд╛рдкреНрддреА рдкреНрд░рдХреНрд░рд┐рдпреЗрдЪреА рдЪрд╛рдЪрдгреА рдХрд░рд╛
- рд▓реЛрдб рдЕрдВрддрд░реНрдЧрдд рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдмреЗрдВрдЪрдорд╛рд░реНрдХ рдХрд░рд╛

**рдЗрддрд░ рдЙрдкрд╛рдпрд╛рдВрд╕реЛрдмрдд рддреБрд▓рдирд╛**:

| рд╡реИрд╢рд┐рд╖реНрдЯреНрдп | VLLM | Foundry Local | Ollama |
|-----------|------|---------------|--------|
| **рд▓рдХреНрд╖реНрдп рд╡рд╛рдкрд░ рдкреНрд░рдХрд░рдг** | рдЙрдЪреНрдЪ-рдереНрд░реВрдкреБрдЯ рдЙрддреНрдкрд╛рджрди | рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╕реБрд▓рднрддрд╛ | рд╡рд┐рдХрд╛рд╕ рдЖрдгрд┐ рд╕рдореБрджрд╛рдп |
| **рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛** | рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рдереНрд░реВрдкреБрдЯ | рд╕рдВрддреБрд▓рд┐рдд | рдЪрд╛рдВрдЧрд▓реЗ |
| **рдореЗрдорд░реА рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛** | PagedAttention рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди | рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди | рдорд╛рдирдХ |
| **рд╕реЗрдЯрдЕрдк рдЧреБрдВрддрд╛рдЧреБрдВрдд** | рдЙрдЪреНрдЪ (рдЕрдиреЗрдХ рдкреЕрд░рд╛рдореАрдЯрд░реНрд╕) | рдХрдореА (рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд) | рдХрдореА (рд╕реЛрдкреЗ) |
| **рд╕реНрдХреЗрд▓реЗрдмрд┐рд▓рд┐рдЯреА** | рдЙрддреНрдХреГрд╖реНрдЯ (рдЯреЗрдиреНрд╕рд░/рдкрд╛рдИрдкрд▓рд╛рдЗрди рдкреЕрд░рд▓рд▓) | рдЪрд╛рдВрдЧрд▓реЗ | рдорд░реНрдпрд╛рджрд┐рдд |
| **рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди** | рдкреНрд░рдЧрдд (AWQ, GPTQ, FP8) | рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд | рдорд╛рдирдХ GGUF |
| **рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ** | рдХрд╕реНрдЯрдо рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рдЖрд╡рд╢реНрдпрдХ | рдЕрдВрдЧрднреВрдд | рд╕рдореБрджрд╛рдп рд╕рд╛рдзрдиреЗ |
| **рд╕рд░реНрд╡реЛрддреНрддрдо рдХреЛрдгрд╛рд╕рд╛рдареА** | рдЙрдЪреНрдЪ-рд╕реНрддрд░реАрдп рдЙрддреНрдкрд╛рджрди рдПрдЬрдВрдЯреНрд╕ | рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдЙрддреНрдкрд╛рджрди | рд╡рд┐рдХрд╛рд╕ |

**VLLM рдирд┐рд╡рдбрдгреНрдпрд╛рдЪреА рд╡реЗрд│**:
- **рдЙрдЪреНрдЪ-рдереНрд░реВрдкреБрдЯ рдЖрд╡рд╢реНрдпрдХрддрд╛**: рд╕реЗрдХрдВрджрд╛рд▓рд╛ рд╢реЗрдХрдбреЛ рд╡рд┐рдирдВрддреНрдпрд╛ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдХрд░рдгреЗ
- **рдореЛрдареНрдпрд╛ рдкреНрд░рдорд╛рдгрд╛рд╡рд░ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯреНрд╕**: рдорд▓реНрдЯреА-GPU, рдорд▓реНрдЯреА-рдиреЛрдб рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯреНрд╕
- **рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдорд╣рддреНрддреНрд╡рд╛рдЪреЗ**: рдореЛрдареНрдпрд╛ рдкреНрд░рдорд╛рдгрд╛рд╡рд░ рдЙрдкрдпреЛрдЬрдирд╛рдВрдордзреНрдпреЗ рдЙрдк-рд╕реЗрдХрдВрдж рдкреНрд░рддрд┐рд╕рд╛рдж рд╡реЗрд│рд╛
- **рдкреНрд░рдЧрдд рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдХрд╕реНрдЯрдо рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рдЖрдгрд┐ рдмреЕрдЪрд┐рдВрдЧрдЪреА рдЖрд╡рд╢реНрдпрдХрддрд╛
- **рд╕рдВрд╕рд╛рдзрди рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛**: рдорд╣рд╛рдЧрдбреНрдпрд╛ GPU рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рдЪрд╛ рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рдЙрдкрдпреЛрдЧ

## рд╡рд╛рд╕реНрддрд╡рд┐рдХ-рдЬрдЧрд╛рддреАрд▓ SLM рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ

### рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ SLM рдПрдЬрдВрдЯреНрд╕
- **SLM рдХреНрд╖рдорддрд╛**: рдЦрд╛рддреЗ рд╢реЛрдз, рдкрд╛рд╕рд╡рд░реНрдб рд░реАрд╕реЗрдЯреНрд╕, рдСрд░реНрдбрд░ рд╕реНрдерд┐рддреА рддрдкрд╛рд╕рдгреА
- **рдЦрд░реНрдЪ рдлрд╛рдпрджреЗ**: LLM рдПрдЬрдВрдЯреНрд╕рдЪреНрдпрд╛ рддреБрд▓рдиреЗрдд рдЗрдирдлрд░рдиреНрд╕ рдЦрд░реНрдЪрд╛рдд 10x рдХрдкрд╛рдд
- **рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛**: рдирд┐рдпрдорд┐рдд рдЪреМрдХрд╢реАрд╕рд╛рдареА рд╕реБрд╕рдВрдЧрдд рдЧреБрдгрд╡рддреНрддреЗрд╕рд╣ рдЬрд▓рдж рдкреНрд░рддрд┐рд╕рд╛рдж рд╡реЗрд│рд╛

### рд╡реНрдпрд╡рд╕рд╛рдп рдкреНрд░рдХреНрд░рд┐рдпрд╛ SLM рдПрдЬрдВрдЯреНрд╕
- **рдЪрд▓рди рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдПрдЬрдВрдЯреНрд╕**: рдбреЗрдЯрд╛ рдХрд╛рдврдгреЗ, рдорд╛рд╣рд┐рддреА рд╕рддреНрдпрд╛рдкрд┐рдд рдХрд░рдгреЗ, рдордВрдЬреБрд░реАрд╕рд╛рдареА рд░реВрдЯ рдХрд░рдгреЗ
- **рдИрдореЗрд▓ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдПрдЬрдВрдЯреНрд╕**: рд╢реНрд░реЗрдгреАрдмрджреНрдз рдХрд░рдгреЗ, рдкреНрд░рд╛рдзрд╛рдиреНрдп рджреЗрдгреЗ, рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рдкреНрд░рддрд┐рд╕рд╛рдж рддрдпрд╛рд░ рдХрд░рдгреЗ
- **рд╢реЗрдбреНрдпреБрд▓рд┐рдВрдЧ рдПрдЬрдВрдЯреНрд╕**: рдмреИрдардХрд╛рдВрдЪреЗ рд╕рдордиреНрд╡рдп рдХрд░рдгреЗ, рдХреЕрд▓реЗрдВрдбрд░ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрд┐рдд рдХрд░рдгреЗ, рд╕реНрдорд░рдгрдкрддреНрд░реЗ рдкрд╛рдард╡рдгреЗ

### рд╡реИрдпрдХреНрддрд┐рдХ SLM рдбрд┐рдЬрд┐рдЯрд▓ рд╕рд╣рд╛рдпреНрдпрдХ
- **рдХрд╛рд░реНрдп рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдПрдЬрдВрдЯреНрд╕**: рдХрд╛рд░реНрдпреЗ рддрдпрд╛рд░ рдХрд░рдгреЗ, рдЕрджреНрдпрддрдирд┐рдд рдХрд░рдгреЗ, рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдиреЗ рдЯреВ-рдбреВ рдпрд╛рджреА рдЖрдпреЛрдЬрд┐рдд рдХрд░рдгреЗ
- **рдорд╛рд╣рд┐рддреА рдЧреЛрд│рд╛ рдХрд░рдгрд╛рд░реЗ рдПрдЬрдВрдЯреНрд╕**: рд╡рд┐рд╖рдп рд╕рдВрд╢реЛрдзрди рдХрд░рдгреЗ, рд╕реНрдерд╛рдирд┐рдХрдкрдгреЗ рдирд┐рд╖реНрдХрд░реНрд╖рд╛рдВрдЪреЗ рд╕рд╛рд░рд╛рдВрд╢ рддрдпрд╛рд░ рдХрд░рдгреЗ
- **рд╕рдВрд╡рд╛рдж рдПрдЬрдВрдЯреНрд╕**: рдИрдореЗрд▓, рд╕рдВрджреЗрд╢, рд╕реЛрд╢рд▓ рдореАрдбрд┐рдпрд╛ рдкреЛрд╕реНрдЯреНрд╕ рдЦрд╛рдЬрдЧреАрдкрдгреЗ рддрдпрд╛рд░ рдХрд░рдгреЗ

### рдЯреНрд░реЗрдбрд┐рдВрдЧ рдЖрдгрд┐ рд╡рд┐рддреНрддреАрдп SLM рдПрдЬрдВрдЯреНрд╕
- **рдорд╛рд░реНрдХреЗрдЯ рдореЙрдирд┐рдЯрд░рд┐рдВрдЧ рдПрдЬрдВрдЯреНрд╕**: рдХрд┐рдВрдорддреАрдВрдЪреЗ рдирд┐рд░реАрдХреНрд╖рдг рдХрд░рдгреЗ, рд░рд┐рдЕрд▓-рдЯрд╛рдЗрдордордзреНрдпреЗ рдЯреНрд░реЗрдВрдб рдУрд│рдЦрдгреЗ
- **рдЕрд╣рд╡рд╛рд▓ рдирд┐рд░реНрдорд┐рддреА рдПрдЬрдВрдЯреНрд╕**: рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ рджреИрдирд┐рдХ/рд╕рд╛рдкреНрддрд╛рд╣рд┐рдХ рд╕рд╛рд░рд╛рдВрд╢ рддрдпрд╛рд░ рдХрд░рдгреЗ
- **рдЬреЛрдЦреАрдо рдореВрд▓реНрдпрд╛рдВрдХрди рдПрдЬрдВрдЯреНрд╕**: рд╕реНрдерд╛рдирд┐рдХ рдбреЗрдЯрд╛рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рдкреЛрд░реНрдЯрдлреЛрд▓рд┐рдУ рд╕реНрдерд┐рддреАрдВрдЪреЗ рдореВрд▓реНрдпрд╛рдВрдХрди рдХрд░рдгреЗ

### рдЖрд░реЛрдЧреНрдп рд╕реЗрд╡рд╛ рд╕рдорд░реНрдерди SLM рдПрдЬрдВрдЯреНрд╕
- **рд░реБрдЧреНрдг рд╢реЗрдбреНрдпреБрд▓рд┐рдВрдЧ рдПрдЬрдВрдЯреНрд╕**: рдЕрдкреЙрдЗрдВрдЯрдореЗрдВрдЯреНрд╕ рд╕рдордиреНрд╡рдпрд┐рдд рдХрд░рдгреЗ, рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рд╕реНрдорд░рдгрдкрддреНрд░реЗ рдкрд╛рдард╡рдгреЗ
- **рджрд╕реНрддрдРрд╡рдЬреАрдХрд░рдг рдПрдЬрдВрдЯреНрд╕**: рд╡реИрджреНрдпрдХреАрдп рд╕рд╛рд░рд╛рдВрд╢, рд╕реНрдерд╛рдирд┐рдХрдкрдгреЗ рдЕрд╣рд╡рд╛рд▓ рддрдпрд╛рд░ рдХрд░рдгреЗ
- **рдкреНрд░рд┐рд╕реНрдХреНрд░рд┐рдкреНрд╢рди рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдПрдЬрдВрдЯреНрд╕**: рд░рд┐рдлрд┐рд▓реНрд╕ рдЯреНрд░реЕрдХ рдХрд░рдгреЗ, рдкрд░рд╕реНрдкрд░рд╕рдВрд╡рд╛рдж рдЦрд╛рдЬрдЧреАрдкрдгреЗ рддрдкрд╛рд╕рдгреЗ

## Microsoft Agent Framework: рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рдПрдЬрдВрдЯ рд╡рд┐рдХрд╛рд╕

### рдЖрдврд╛рд╡рд╛ рдЖрдгрд┐ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░

Microsoft Agent Framework рдПрдХ рд╡реНрдпрд╛рдкрдХ, рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ-рдЧреНрд░реЗрдб рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рдкреНрд░рджрд╛рди рдХрд░рддреЗ рдЬреЗ AI рдПрдЬрдВрдЯреНрд╕ рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА, рдбрд┐рдкреНрд▓реЙрдп рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдЖрдгрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрд┐рдд рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдбрд┐рдЭрд╛рдЗрди рдХреЗрд▓реЗрд▓реЗ рдЖрд╣реЗ рдЬреЗ рдХреНрд▓рд╛рдЙрдб рдЖрдгрд┐ рдСрдлрд▓рд╛рдЗрди рдПрдЬ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рдХрд╛рд░реНрдп рдХрд░реВ рд╢рдХрддрд╛рдд. рдлреНрд░реЗрдорд╡рд░реНрдХ рд╡рд┐рд╢реЗрд╖рддрдГ Small Language Models рдЖрдгрд┐ рдПрдЬ рдХрдВрдкреНрдпреБрдЯрд┐рдВрдЧ рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╣ рдЕрдЦрдВрдбрдкрдгреЗ рдХрд╛рд░реНрдп рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдбрд┐рдЭрд╛рдЗрди рдХреЗрд▓реЗ рдЖрд╣реЗ, рдЬреНрдпрд╛рдореБрд│реЗ рдЧреЛрдкрдиреАрдпрддрд╛-рд╕рдВрд╡реЗрджрдирд╢реАрд▓ рдЖрдгрд┐ рд╕рдВрд╕рд╛рдзрди-рдЖрдзрд╛рд░рд┐рдд рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯрд╕рд╛рдареА рд╣реЗ рдЖрджрд░реНрд╢ рдмрдирддреЗ.

**рдореБрдЦреНрдп рдлреНрд░реЗрдорд╡рд░реНрдХ рдШрдЯрдХ**:
- **рдПрдЬрдВрдЯ рд░рдирдЯрд╛рдЗрдо**: рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реЗ рд╣рд▓рдХреЗ рдХрд╛рд░реНрдпрдХрд╛рд░реА рд╡рд╛рддрд╛рд╡рд░рдг
- **рдЯреВрд▓ рдЗрдВрдЯрд┐рдЧреНрд░реЗрд╢рди рд╕рд┐рд╕реНрдЯрдо**: рдмрд╛рд╣реНрдп рд╕реЗрд╡рд╛ рдЖрдгрд┐ API рдХрдиреЗрдХреНрдЯ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рд╡рд┐рд╕реНрддрд╛рд░рдХреНрд╖рдо рдкреНрд▓рдЧрдЗрди рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░
- **рд╕реНрдЯреЗрдЯ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди**: рд╕рддреНрд░рд╛рдВрдордзреНрдпреЗ рд╕рддрдд рдПрдЬрдВрдЯ рдореЗрдорд░реА рдЖрдгрд┐ рд╕рдВрджрд░реНрдн рд╣рд╛рддрд╛рд│рдгреА
- **рд╕реБрд░рдХреНрд╖рд╛ рд╕реНрддрд░**: рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯрд╕рд╛рдареА рдЕрдВрдЧрднреВрдд рд╕реБрд░рдХреНрд╖рд╛ рдирд┐рдпрдВрддреНрд░рдг
- **рдСрд░реНрдХреЗрд╕реНрдЯреНрд░реЗрд╢рди рдЗрдВрдЬрд┐рди**: рдорд▓реНрдЯреА-рдПрдЬрдВрдЯ рд╕рдордиреНрд╡рдп рдЖрдгрд┐ рд╡рд░реНрдХрдлреНрд▓реЛ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди

### рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯрд╕рд╛рдареА рдореБрдЦреНрдп рд╡реИрд╢рд┐рд╖реНрдЯреНрдпреЗ

**рдСрдлрд▓рд╛рдЗрди-рдкреНрд░рдердо рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░**: Microsoft Agent Framework рдСрдлрд▓рд╛рдЗрди-рдкреНрд░рдердо рддрддреНрддреНрд╡рд╛рдВрд╕рд╣ рдбрд┐рдЭрд╛рдЗрди рдХреЗрд▓реЗ рдЖрд╣реЗ, рдЬреНрдпрд╛рдореБрд│реЗ рдПрдЬрдВрдЯреНрд╕ рд╕рддрдд рдЗрдВрдЯрд░рдиреЗрдЯ рдХрдиреЗрдХреНрдЯрд┐рд╡реНрд╣рд┐рдЯреАрд╢рд┐рд╡рд╛рдп рдкреНрд░рднрд╛рд╡реАрдкрдгреЗ рдХрд╛рд░реНрдп рдХрд░реВ рд╢рдХрддрд╛рдд. рдпрд╛рдордзреНрдпреЗ рд╕реНрдерд╛рдирд┐рдХ рдореЙрдбреЗрд▓ рдЗрдирдлрд░рдиреНрд╕, рдХреЕрд╢ рдХреЗрд▓реЗрд▓реЗ рдЬреНрдЮрд╛рди рддрд│, рдСрдлрд▓рд╛рдЗрди рдЯреВрд▓ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреА рдЖрдгрд┐ рдХреНрд▓рд╛рдЙрдб рд╕реЗрд╡рд╛ рдЕрдиреБрдкрд▓рдмреНрдз рдЕрд╕рддрд╛рдирд╛ рдЧреНрд░реЗрд╕рдлреБрд▓ рдбрд┐рдЧреНрд░реЗрдбреЗрд╢рди рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдЖрд╣реЗ.

**рд╕рдВрд╕рд╛рдзрди рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдлреНрд░реЗрдорд╡рд░реНрдХ SLM рд╕рд╛рдареА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдореЗрдорд░реА рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рдирд╕рд╣ рдмреБрджреНрдзрд┐рдорд╛рди рд╕рдВрд╕рд╛рдзрди рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди рдкреНрд░рджрд╛рди рдХрд░рддреЗ, рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рд╕рд╛рдареА CPU/GPU рд▓реЛрдб рдмреЕрд▓рдиреНрд╕рд┐рдВрдЧ, рдЙрдкрд▓рдмреНрдз рд╕рдВрд╕рд╛рдзрдирд╛рдВрд╡рд░ рдЖрдзрд╛рд░рд┐рдд рдЕрдиреБрдХреВрд▓реА рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рдЖрдгрд┐ рдореЛрдмрд╛рдЗрд▓ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯрд╕рд╛рдареА рдКрд░реНрдЬрд╛-рдХрд╛рд░реНрдпрдХреНрд╖рдо рдЗрдирдлрд░рдиреНрд╕ рдкреЕрдЯрд░реНрди.

**рд╕реБрд░рдХреНрд╖рд╛ рдЖрдгрд┐ рдЧреЛрдкрдиреАрдпрддрд╛**: рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ-рдЧреНрд░реЗрдб рд╕реБрд░рдХреНрд╖рд╛ рд╡реИрд╢рд┐рд╖реНрдЯреНрдпрд╛рдВрдордзреНрдпреЗ рдЧреЛрдкрдиреАрдпрддрд╛ рд░рд╛рдЦрдгреНрдпрд╛рд╕рд╛рдареА рд╕реНрдерд╛рдирд┐рдХ рдбреЗрдЯрд╛ рдкреНрд░рдХреНрд░рд┐рдпрд╛, рдПрдиреНрдХреНрд░рд┐рдкреНрдЯреЗрдб рдПрдЬрдВрдЯ рдХрдореНрдпреБрдирд┐рдХреЗрд╢рди рдЪреЕрдиреЗрд▓реНрд╕, рдПрдЬрдВрдЯ рдХреНрд╖рдорддрд╛рдВрд╕рд╛рдареА рднреВрдорд┐рдХрд╛-рдЖрдзрд╛рд░рд┐рдд рдкреНрд░рд╡реЗрд╢ рдирд┐рдпрдВрддреНрд░рдг рдЖрдгрд┐ рдЕрдиреБрдкрд╛рд▓рди рдЖрд╡рд╢реНрдпрдХрддрд╛ рдкреВрд░реНрдг рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдСрдбрд┐рдЯ рд▓реЙрдЧрд┐рдВрдЧ рд╕рдорд╛рд╡рд┐рд╖реНрдЯ рдЖрд╣реЗ.

### Foundry Local рд╕реЛрдмрдд рдПрдХрддреНрд░реАрдХрд░рдг

Microsoft Agent Framework Foundry Local рд╕реЛрдмрдд рдЕрдЦрдВрдбрдкрдгреЗ рдПрдХрддреНрд░рд┐рдд рд╣реЛрддреЗ рдЬреЗрдгреЗрдХрд░реВрди рд╕рдВрдкреВрд░реНрдг рдПрдЬ AI рд╕рдорд╛рдзрд╛рди рдкреНрд░рджрд╛рди рдХрд░рддрд╛ рдпреЗрдИрд▓:

**рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдореЙрдбреЗрд▓ рд╢реЛрдз**: рдлреНрд░реЗрдорд╡рд░реНрдХ рд╕реНрд╡рдпрдВрдЪрд▓рд┐рддрдкрдгреЗ Foundry Local рдЙрджрд╛рд╣рд░рдгреЗ рд╢реЛрдзрддреЛ рдЖрдгрд┐ рдХрдиреЗрдХреНрдЯ рдХрд░рддреЛ, рдЙрдкрд▓рдмреНрдз SLM рдореЙрдбреЗрд▓реНрд╕ рд╢реЛрдзрддреЛ рдЖрдгрд┐ рдПрдЬрдВрдЯрдЪреНрдпрд╛ рдЖрд╡рд╢реНрдпрдХрддрд╛ рдЖрдгрд┐ рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдХреНрд╖рдорддрд╛рдВрд╡рд░ рдЖрдзрд╛рд░рд┐рдд рдЗрд╖реНрдЯрддрдо рдореЙрдбреЗрд▓реНрд╕ рдирд┐рд╡рдбрддреЛ.

**рдбрд╛рдпрдиреЕрдорд┐рдХ рдореЙрдбреЗрд▓ рд▓реЛрдбрд┐рдВрдЧ**: рдПрдЬрдВрдЯреНрд╕ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ SLMs рд▓рд╛ рдбрд╛рдпрдиреЕрдорд┐рдХрдкрдгреЗ рд▓реЛрдб рдХрд░реВ рд╢рдХрддрд╛рдд, рдЬреНрдпрд╛рдореБрд│реЗ рдорд▓реНрдЯреА-рдореЙрдбреЗрд▓ рдПрдЬрдВрдЯ рд╕рд┐рд╕реНрдЯрдо рд╕рдХреНрд╖рдо рд╣реЛрддреЗ рдЬрд┐рдереЗ рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рдкреНрд░рдХрд╛рд░рдЪреНрдпрд╛ рд╡рд┐рдирдВрддреНрдпрд╛рдВрд╕рд╛рдареА рд╡реЗрдЧрд╡реЗрдЧрд│реЗ рдореЙрдбреЗрд▓реНрд╕ рд╣рд╛рддрд╛рд│рддрд╛рдд рдЖрдгрд┐ рдЙрдкрд▓рдмреНрдзрддрд╛ рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдЪреНрдпрд╛ рдЖрдзрд╛рд░реЗ рдореЙрдбреЗрд▓реНрд╕ рджрд░рдореНрдпрд╛рди рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рдлреЗрд▓рдУрд╡реНрд╣рд░ рд╣реЛрддреЛ.

**рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди**: рдПрдХрддреНрд░рд┐рдд рдХреЕрд╢рд┐рдВрдЧ рдпрдВрддреНрд░рдгрд╛ рдореЙрдбреЗрд▓ рд▓реЛрдбрд┐рдВрдЧ рд╡реЗрд│рд╛ рдХрдореА рдХрд░рддрд╛рдд, рдХрдиреЗрдХреНрд╢рди рдкреВрд▓рд┐рдВрдЧ Foundry Local рд╕рд╛рдареА API рдХреЙрд▓реНрд╕ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХрд░рддреЗ рдЖрдгрд┐ рдмреБрджреНрдзрд┐рдорд╛рди рдмреЕрдЪрд┐рдВрдЧ рдЕрдиреЗрдХ рдПрдЬрдВрдЯ рд╡рд┐рдирдВрддреНрдпрд╛рдВрд╕рд╛рдареА рдереНрд░реВрдкреБрдЯ рд╕реБрдзрд╛рд░рддреЗ.

### Microsoft Agent Framework рд╕рд╣ рдПрдЬрдВрдЯреНрд╕ рддрдпрд╛рд░ рдХрд░рдгреЗ

#### рдПрдЬрдВрдЯ рдкрд░рд┐рднрд╛рд╖рд╛ рдЖрдгрд┐ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### рдПрдЬ рдкрд░рд┐рд╕реНрдерд┐рддреАрд╕рд╛рдареА рдЯреВрд▓ рдЗрдВрдЯрд┐рдЧреНрд░реЗрд╢рди

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### рдорд▓реНрдЯреА-рдПрдЬрдВрдЯ рдСрд░реНрдХреЗрд╕реНрдЯреНрд░реЗрд╢рди

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### рдкреНрд░рдЧрдд рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдкреЕрдЯрд░реНрдиреНрд╕

#### рд╣рд╛рдпрд░рд╛рд░реНрдХрд┐рдХрд▓ рдПрдЬрдВрдЯ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░

**рд╕реНрдерд╛рдирд┐рдХ рдПрдЬрдВрдЯ рдХреНрд▓рд╕реНрдЯрд░реНрд╕**: рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рд╡рд░ рд╡рд┐рд╢рд┐рд╖реНрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХреЗрд▓реЗрд▓реЗ рдЕрдиреЗрдХ рд╡рд┐рд╢реЗрд╖ SLM рдПрдЬрдВрдЯреНрд╕ рдбрд┐рдкреНрд▓реЙрдп рдХрд░рд╛. рд╕реЛрдкреНрдпрд╛ рд░реВрдЯрд┐рдВрдЧ рдЖрдгрд┐ рд╢реЗрдбреНрдпреБрд▓рд┐рдВрдЧрд╕рд╛рдареА Qwen2.5-0.5B рд╕рд╛рд░рдЦреНрдпрд╛ рд╣рд▓рдХреНрдпрд╛ рдореЙрдбреЗрд▓реНрд╕рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░рд╛, рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдЖрдгрд┐ рджрд╕реНрддрдРрд╡рдЬреАрдХрд░рдгрд╛рд╕рд╛рдареА Phi-4-Mini рд╕рд╛рд░рдЦреНрдпрд╛ рдордзреНрдпрдо рдореЙрдбреЗрд▓реНрд╕рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░рд╛ рдЖрдгрд┐ рд╕рдВрд╕рд╛рдзрдиреЗ рдЙрдкрд▓рдмреНрдз рдЕрд╕рддрд╛рдирд╛ рдЬрдЯрд┐рд▓ рд╡рд┐рдЪрд╛рд░рд╛рдВрд╕рд╛рдареА рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓реНрд╕рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░рд╛.

**рдПрдЬ-рдЯреВ-рдХреНрд▓рд╛рдЙрдб рд╕рдордиреНрд╡рдп**: рд╕реНрдерд╛рдирд┐рдХ рдПрдЬрдВрдЯреНрд╕ рдирд┐рдпрдорд┐рдд рдХрд╛рд░реНрдпреЗ рд╣рд╛рддрд╛рд│рддрд╛рдд, рдХреНрд▓рд╛рдЙрдб рдПрдЬрдВрдЯреНрд╕ рдХрдиреЗрдХреНрдЯрд┐рд╡реНрд╣рд┐рдЯреА рдЙрдкрд▓рдмреНрдз рдЕрд╕рддрд╛рдирд╛ рдЬрдЯрд┐рд▓ рд╡рд┐рдЪрд╛рд░ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд рдЖрдгрд┐ рдПрдЬ рдЖрдгрд┐ рдХреНрд▓рд╛рдЙрдб рдкреНрд░рдХреНрд░рд┐рдпреЗрдордзреНрдпреЗ рдЕрдЦрдВрдб рд╣рд╕реНрддрд╛рдВрддрд░рдг рд╕рддрддрддрд╛ рд░рд╛рдЦрддреЗ.

#### рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рдиреНрд╕

**рд╕рд┐рдВрдЧрд▓ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**рд╡рд┐рддрд░рд┐рдд рдПрдЬ рдбрд┐рдкреНрд▓реЙрдпрдореЗрдВрдЯ**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### рдПрдЬ рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди

#### рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рдзреЛрд░рдгреЗ

**рдХрд╛рд░реНрдп-рдЖрдзрд╛рд░рд┐рдд рдореЙрдбреЗрд▓ рдЕрд╕рд╛рдЗрдирдореЗрдВрдЯ**: Microsoft Agent Framework рдХрд╛рд░реНрдпрд╛рдЪреНрдпрд╛ рдЧреБрдВрддрд╛рдЧреБрдВрддреАрдиреБрд╕рд╛рд░ рдЖрдгрд┐ рдЖрд╡рд╢реНрдпрдХрддрд╛ рдпрд╛рд╡рд░ рдЖрдзрд╛рд░рд┐рдд рдмреБрджреНрдзрд┐рдорд╛рди рдореЙрдбреЗрд▓ рдирд┐рд╡рдб рд╕рдХреНрд╖рдо рдХрд░рддреЗ:

- **рд╕реЛрдкреНрдпрд╛ рдХрд╛рд░реНрдпреЗ** (рдкреНрд░рд╢реНрдиреЛрддреНрддрд░, рд░реВрдЯрд┐рдВрдЧ): Qwen2.5-0.5B (500MB, <100ms рдкреНрд░рддрд┐рд╕рд╛рдж)
- **рдордзреНрдпрдо рдХрд╛рд░реНрдпреЗ** (рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛, рд╢реЗрдбреНрдпреБрд▓рд┐рдВрдЧ): Phi-4-Mini (2.4GB, 200-500ms рдкреНрд░рддрд┐рд╕рд╛рдж)
- **рдЬрдЯрд┐рд▓ рдХрд╛рд░реНрдпреЗ** (рддрд╛рдВрддреНрд░рд┐рдХ рд╡рд┐рд╢реНрд▓реЗрд╖рдг, рдирд┐рдпреЛрдЬрди): Phi-4 (7GB, 1-3s рдкреНрд░рддрд┐рд╕рд╛рдж рдЬреЗрд╡реНрд╣рд╛ рд╕рдВрд╕рд╛рдзрдиреЗ рдЙрдкрд▓рдмреНрдз рдЕрд╕рддрд╛рдд)

**рдбрд╛рдпрдиреЕрдорд┐рдХ рдореЙрдбреЗрд▓ рд╕реНрд╡рд┐рдЪрд┐рдВрдЧ**: рдПрдЬрдВрдЯреНрд╕ рд╕рдзреНрдпрд╛рдЪреНрдпрд╛ рд╕рд┐рд╕реНрдЯрдо рд▓реЛрдб, рдХрд╛рд░реНрдпрд╛рдЪреНрдпрд╛ рдЧреБрдВрддрд╛рдЧреБрдВрддреАрдЪреЗ рдореВрд▓реНрдпрд╛рдВрдХрди, рд╡рд╛рдкрд░рдХрд░реНрддрд╛ рдкреНрд░рд╛рдзрд╛рдиреНрдп рдкрд╛рддрд│реА рдЖрдгрд┐ рдЙрдкрд▓рдмреНрдз рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рд╕рдВрд╕рд╛рдзрдирд╛рдВрд╡рд░ рдЖрдзрд╛рд░рд┐рдд рдореЙрдбреЗрд▓реНрд╕ рджрд░рдореНрдпрд╛рди рд╕реНрд╡рд┐рдЪ рдХрд░реВ рд╢рдХрддрд╛рдд.

#### рдореЗрдорд░реА рдЖрдгрд┐ рд╕рдВрд╕рд╛рдзрди рд╡реНрдпрд╡рд╕реНрдерд╛рдкрди

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рдЗрдВрдЯрд┐рдЧреНрд░реЗрд╢рди рдкреЕрдЯрд░реНрдиреНрд╕

#### рд╕реБрд░рдХреНрд╖рд╛ рдЖрдгрд┐ рдЕрдиреБрдкрд╛рд▓рди

**рд╕реНрдерд╛рдирд┐рдХ рдбреЗрдЯрд╛ рдкреНрд░рдХреНрд░рд┐рдпрд╛**: рд╕рд░реНрд╡ рдПрдЬрдВрдЯ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рд╕реНрдерд╛рдирд┐рдХрдкрдгреЗ рд╣реЛрддреЗ, рдЬреНрдпрд╛рдореБрд│реЗ рд╕рдВрд╡реЗрджрдирд╢реАрд▓ рдбреЗрдЯрд╛ рдХрдзреАрд╣реА рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕ рд╕реЛрдбрдд рдирд╛рд╣реА
**рдПрдЬрдВрдЯ рддреИрдирд╛рддреАрд╕рд╛рдареА рдлреНрд░реЗрдорд╡рд░реНрдХ рдирд┐рд╡рдб**: рд▓рдХреНрд╖реНрдп рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдЖрдгрд┐ рдПрдЬрдВрдЯрдЪреНрдпрд╛ рдЧрд░рдЬрд╛рдВрдиреБрд╕рд╛рд░ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдлреНрд░реЗрдорд╡рд░реНрдХ рдирд┐рд╡рдбрд╛. CPU-рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭреНрдб рдПрдЬрдВрдЯ рддреИрдирд╛рддреАрд╕рд╛рдареА Llama.cpp рд╡рд╛рдкрд░рд╛, Apple Silicon рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА Apple MLX рд╡рд╛рдкрд░рд╛ рдЖрдгрд┐ рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рдПрдЬрдВрдЯ рд╕реБрд╕рдВрдЧрддрддреЗрд╕рд╛рдареА ONNX рд╡рд╛рдкрд░рд╛.

## рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ SLM рдПрдЬрдВрдЯ рд░реВрдкрд╛рдВрддрд░рдг рдЖрдгрд┐ рдЙрдкрдпреЛрдЧ рдкреНрд░рдХрд░рдгреЗ

### рд╡рд╛рд╕реНрддрд╡рд┐рдХ рдЬрдЧрд╛рддреАрд▓ рдПрдЬрдВрдЯ рддреИрдирд╛рддреА рдкрд░рд┐рджреГрд╢реНрдпреЗ

**рдореЛрдмрд╛рдЗрд▓ рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧ**: Q4_K рд╕реНрд╡рд░реВрдк рд╕реНрдорд╛рд░реНрдЯрдлреЛрди рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдХрдореА рдореЗрдорд░реА рд╡рд╛рдкрд░рд╛рд╕рд╣ рдЙрддреНрдХреГрд╖реНрдЯ рдЖрд╣реЗрдд, рддрд░ Q8_0 рдЯреЕрдмреНрд▓реЗрдЯ-рдЖрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯ рд╕рд┐рд╕реНрдЯрдорд╕рд╛рдареА рд╕рдВрддреБрд▓рд┐рдд рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ. Q5_K рд╕реНрд╡рд░реВрдк рдореЛрдмрд╛рдЗрд▓ рдЙрддреНрдкрд╛рджрдХрддрд╛ рдПрдЬрдВрдЯрд╕рд╛рдареА рдЙрдЪреНрдЪ рджрд░реНрдЬрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ.

**рдбреЗрд╕реНрдХрдЯреЙрдк рдЖрдгрд┐ рдПрдЬрдВрдЯ рдПрдЬ рдХреЙрдореНрдкреНрдпреБрдЯрд┐рдВрдЧ**: Q5_K рдбреЗрд╕реНрдХрдЯреЙрдк рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЙрддреНрдХреГрд╖реНрдЯ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ, Q8_0 рд╡рд░реНрдХрд╕реНрдЯреЗрд╢рди рдПрдЬрдВрдЯ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдЙрдЪреНрдЪ рджрд░реНрдЬрд╛рдЪреЗ рдЕрдиреБрдорд╛рди рдкреНрд░рджрд╛рди рдХрд░рддреЗ, рдЖрдгрд┐ Q4_K рдПрдЬ рдПрдЬрдВрдЯ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рд╡рд░ рдХрд╛рд░реНрдпрдХреНрд╖рдо рдкреНрд░рдХреНрд░рд┐рдпрд╛ рд╕рдХреНрд╖рдо рдХрд░рддреЗ.

**рд╕рдВрд╢реЛрдзрди рдЖрдгрд┐ рдкреНрд░рд╛рдпреЛрдЧрд┐рдХ рдПрдЬрдВрдЯ**: рдкреНрд░рдЧрдд рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рд╕реНрд╡рд░реВрдк рдЕрддреНрдпрдВрдд рдХрдореА рдЕрдЪреВрдХрддрд╛ рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рдПрдЬрдВрдЯ рдЕрдиреБрдорд╛рдирд╛рд╕рд╛рдареА рд╢реИрдХреНрд╖рдгрд┐рдХ рд╕рдВрд╢реЛрдзрди рдЖрдгрд┐ рдкреНрд░реВрдл-рдСрдл-рдХреЙрдиреНрд╕реЗрдкреНрдЯ рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЕрдиреНрд╡реЗрд╖рдг рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд.

### SLM рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдмреЗрдВрдЪрдорд╛рд░реНрдХ

**рдПрдЬрдВрдЯ рдЕрдиреБрдорд╛рди рдЧрддреА**: Q4_K рдореЛрдмрд╛рдЗрд▓ CPUs рд╡рд░ рд╕рд░реНрд╡рд╛рдд рд╡реЗрдЧрд╡рд╛рди рдПрдЬрдВрдЯ рдкреНрд░рддрд┐рд╕рд╛рдж рд╡реЗрд│рд╛ рдкреНрд░рд╛рдкреНрдд рдХрд░рддреЗ, Q5_K рд╕рд╛рдорд╛рдиреНрдп рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рд╕рдВрддреБрд▓рд┐рдд рдЧрддреА-рдЧреБрдгрд╡рддреНрддрд╛ рдЧреБрдгреЛрддреНрддрд░ рдкреНрд░рджрд╛рди рдХрд░рддреЗ, Q8_0 рдЬрдЯрд┐рд▓ рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рдЙрддреНрдХреГрд╖реНрдЯ рдЧреБрдгрд╡рддреНрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ, рдЖрдгрд┐ рдкреНрд░рд╛рдпреЛрдЧрд┐рдХ рд╕реНрд╡рд░реВрдк рд╡рд┐рд╢реЗрд╖ рдПрдЬрдВрдЯ рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рд╕рд╛рдареА рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рдереНрд░реВрдкреБрдЯ рдкреНрд░рджрд╛рди рдХрд░рддрд╛рдд.

**рдПрдЬрдВрдЯ рдореЗрдорд░реА рдЖрд╡рд╢реНрдпрдХрддрд╛**: рдПрдЬрдВрдЯрд╕рд╛рдареА рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рд╕реНрддрд░ Q2_K (рд▓рд╣рд╛рди рдПрдЬрдВрдЯ рдореЙрдбреЗрд▓рд╕рд╛рдареА 500MB рдкреЗрдХреНрд╖рд╛ рдХрдореА) рдкрд╛рд╕реВрди Q8_0 (рдореВрд│ рдЖрдХрд╛рд░рд╛рдЪреНрдпрд╛ рд╕реБрдорд╛рд░реЗ 50%) рдкрд░реНрдпрдВрдд рдЕрд╕рддрд╛рдд, рдкреНрд░рд╛рдпреЛрдЧрд┐рдХ рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди рд╕рдВрд╕рд╛рдзрди-рдЖрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рд╕рд╛рдареА рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд рд╕рдВрдХреНрд╖реЗрдкрдг рдкреНрд░рд╛рдкреНрдд рдХрд░рддрд╛рдд.

## SLM рдПрдЬрдВрдЯрд╕рд╛рдареА рдЖрд╡реНрд╣рд╛рдиреЗ рдЖрдгрд┐ рд╡рд┐рдЪрд╛рд░

### рдПрдЬрдВрдЯ рд╕рд┐рд╕реНрдЯрдордордзреАрд▓ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рд╡реНрдпрд╛рдкрд╛рд░-рдЙрдкрд╛рдп

SLM рдПрдЬрдВрдЯ рддреИрдирд╛рддреАрдордзреНрдпреЗ рдореЙрдбреЗрд▓ рдЖрдХрд╛рд░, рдПрдЬрдВрдЯ рдкреНрд░рддрд┐рд╕рд╛рдж рдЧрддреА рдЖрдгрд┐ рдЖрдЙрдЯрдкреБрдЯ рдЧреБрдгрд╡рддреНрддрд╛ рдпрд╛рдордзреАрд▓ рд╡реНрдпрд╛рдкрд╛рд░-рдЙрдкрд╛рдпрд╛рдВрдЪрд╛ рдХрд╛рд│рдЬреАрдкреВрд░реНрд╡рдХ рд╡рд┐рдЪрд╛рд░ рдХрд░рдгреЗ рдЖрд╡рд╢реНрдпрдХ рдЖрд╣реЗ. рдЬрд░реА Q4_K рдореЛрдмрд╛рдЗрд▓ рдПрдЬрдВрдЯрд╕рд╛рдареА рдЕрдкрд╡рд╛рджрд╛рддреНрдордХ рдЧрддреА рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ, Q8_0 рдЬрдЯрд┐рд▓ рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рдЙрддреНрдХреГрд╖реНрдЯ рдЧреБрдгрд╡рддреНрддрд╛ рдкреНрд░рджрд╛рди рдХрд░рддреЗ. Q5_K рдмрд╣реБрддреЗрдХ рд╕рд╛рдорд╛рдиреНрдп рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдпреЛрдЧреНрдп рдордзреНрдпрдо рдорд╛рд░реНрдЧ рдЖрд╣реЗ.

### SLM рдПрдЬрдВрдЯрд╕рд╛рдареА рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рд╕реБрд╕рдВрдЧрддрддрд╛

рд╡рд┐рд╡рд┐рдз рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕ SLM рдПрдЬрдВрдЯ рддреИрдирд╛рддреАрд╕рд╛рдареА рд╡реЗрдЧрд╡реЗрдЧрд│реНрдпрд╛ рдХреНрд╖рдорддрд╛ рдЕрд╕рддрд╛рдд. Q4_K рд╕рд╛рдзреНрдпрд╛ рдПрдЬрдВрдЯрд╕рд╛рдареА рдореВрд▓рднреВрдд рдкреНрд░реЛрд╕реЗрд╕рд░рд╡рд░ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдиреЗ рдЪрд╛рд▓рддреЗ, Q5_K рд╕рдВрддреБрд▓рд┐рдд рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрд╕рд╛рдареА рдордзреНрдпрдо рд╕рдВрдЧрдгрдХреАрдп рд╕рдВрд╕рд╛рдзрдирд╛рдВрдЪреА рдЖрд╡рд╢реНрдпрдХрддрд╛ рдЕрд╕рддреЗ, рдЖрдгрд┐ Q8_0 рдкреНрд░рдЧрдд рдПрдЬрдВрдЯ рдХреНрд╖рдорддрд╛ рд╕рд╛рдареА рдЙрдЪреНрдЪ-рд╕реНрддрд░реАрдп рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░рдЪрд╛ рд▓рд╛рдн рдШреЗрддреЛ.

### SLM рдПрдЬрдВрдЯ рд╕рд┐рд╕реНрдЯрдордордзреАрд▓ рд╕реБрд░рдХреНрд╖рд╛ рдЖрдгрд┐ рдЧреЛрдкрдиреАрдпрддрд╛

SLM рдПрдЬрдВрдЯреНрд╕ рд╕реНрдерд╛рдирд┐рдХ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд рдЬреНрдпрд╛рдореБрд│реЗ рдЧреЛрдкрдиреАрдпрддрд╛ рд╡рд╛рдврддреЗ, рдкрд░рдВрддреБ рдПрдЬ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рдПрдЬрдВрдЯ рдореЙрдбреЗрд▓реНрд╕ рдЖрдгрд┐ рдбреЗрдЯрд╛ рд╕рдВрд░рдХреНрд╖рд┐рдд рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдпреЛрдЧреНрдп рд╕реБрд░рдХреНрд╖рд╛ рдЙрдкрд╛рдп рд▓рд╛рдЧреВ рдХрд░рдгреЗ рдЖрд╡рд╢реНрдпрдХ рдЖрд╣реЗ. рд╣реЗ рд╡рд┐рд╢реЗрд╖рддрдГ рдорд╣рддреНрддреНрд╡рд╛рдЪреЗ рдЖрд╣реЗ рдЬреЗрд╡реНрд╣рд╛ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рдЙрдЪреНрдЪ-рдкрд░рд┐рд╢реБрджреНрдзрддрд╛ рдПрдЬрдВрдЯ рд╕реНрд╡рд░реВрдк рддреИрдирд╛рдд рдХреЗрд▓реЗ рдЬрд╛рддрд╛рдд рдХрд┐рдВрд╡рд╛ рд╕рдВрд╡реЗрджрдирд╢реАрд▓ рдбреЗрдЯрд╛ рд╣рд╛рддрд╛рд│рдгрд╛рд▒реНрдпрд╛ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдордзреНрдпреЗ рд╕рдВрдХреБрдЪрд┐рдд рдПрдЬрдВрдЯ рд╕реНрд╡рд░реВрдк рд╡рд╛рдкрд░рд▓реЗ рдЬрд╛рддрд╛рдд.

## SLM рдПрдЬрдВрдЯ рд╡рд┐рдХрд╛рд╕рд╛рддреАрд▓ рднрд╡рд┐рд╖реНрдпрд╛рддреАрд▓ рдкреНрд░рд╡реГрддреНрддреА

SLM рдПрдЬрдВрдЯ рд▓рдБрдбрд╕реНрдХреЗрдк рд╕рдВрдХреБрдЪрд┐рдд рддрдВрддреНрд░, рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдкрджреНрдзрддреА рдЖрдгрд┐ рдПрдЬ рддреИрдирд╛рддреА рдзреЛрд░рдгрд╛рдВрдордзреНрдпреЗ рдкреНрд░рдЧрддреАрд╕рд╣ рд╕рддрдд рд╡рд┐рдХрд╕рд┐рдд рд╣реЛрдд рдЖрд╣реЗ. рднрд╡рд┐рд╖реНрдпрд╛рддреАрд▓ рд╡рд┐рдХрд╛рд╕рд╛рдордзреНрдпреЗ рдПрдЬрдВрдЯ рдореЙрдбреЗрд▓рд╕рд╛рдареА рдЕрдзрд┐рдХ рдХрд╛рд░реНрдпрдХреНрд╖рдо рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рдЕрд▓реНрдЧреЛрд░рд┐рджрдо, рдПрдЬрдВрдЯ рд╡рд░реНрдХрдлреНрд▓реЛрд╕рд╛рдареА рд╕реБрдзрд╛рд░рд┐рдд рд╕рдВрдХреБрдЪрди рдкрджреНрдзрддреА рдЖрдгрд┐ рдПрдЬ рд╣рд╛рд░реНрдбрд╡реЗрдЕрд░ рдЕреЕрдХреНрд╕реЗрд▓рд░реЗрдЯрд░рд╕рд╣ рдПрдЬрдВрдЯ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдЪрд╛рдВрдЧрд▓реНрдпрд╛ рдкреНрд░рдХрд╛рд░реЗ рд╕рдорд╛рдХрд▓рд┐рдд рдХрд░рдгреЗ рдпрд╛рдВрдЪрд╛ рд╕рдорд╛рд╡реЗрд╢ рдЖрд╣реЗ.

**SLM рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рдмрд╛рдЬрд╛рд░ рдЕрдВрджрд╛рдЬ**: рдЕрд▓реАрдХрдбреАрд▓ рд╕рдВрд╢реЛрдзрдирд╛рдиреБрд╕рд╛рд░, рдПрдЬрдВрдЯ-рд╕рдХреНрд╖рдо рдСрдЯреЛрдореЗрд╢рди 2027 рдкрд░реНрдпрдВрдд рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╡рд░реНрдХрдлреНрд▓реЛрдордзреАрд▓ 40тАУ60% рдкреБрдирд░рд╛рд╡реГрддреНрддреА рд╣реЛрдгрд╛рд░реЗ рд╕рдВрдЬреНрдЮрд╛рдирд╛рддреНрдордХ рдХрд╛рд░реНрдп рдХрд╛рдвреВрди рдЯрд╛рдХреВ рд╢рдХрддреЗ, SLMs рддреНрдпрд╛рдВрдЪреНрдпрд╛ рдЦрд░реНрдЪ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдореБрд│реЗ рдЖрдгрд┐ рддреИрдирд╛рддреА рд▓рд╡рдЪрд┐рдХрддреЗрдореБрд│реЗ рдпрд╛ рдкрд░рд┐рд╡рд░реНрддрдирд╛рдЪреЗ рдиреЗрддреГрддреНрд╡ рдХрд░рдд рдЖрд╣реЗрдд.

**SLM рдПрдЬрдВрдЯреНрд╕рдордзреАрд▓ рддрд╛рдВрддреНрд░рд┐рдХ рдкреНрд░рд╡реГрддреНрддреА**:
- **рд╡рд┐рд╢реЗрд╖ SLM рдПрдЬрдВрдЯреНрд╕**: рд╡рд┐рд╢рд┐рд╖реНрдЯ рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпреЗ рдЖрдгрд┐ рдЙрджреНрдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдкреНрд░рд╢рд┐рдХреНрд╖рд┐рдд рдбреЛрдореЗрди-рд╡рд┐рд╢рд┐рд╖реНрдЯ рдореЙрдбреЗрд▓реНрд╕
- **рдПрдЬ рдПрдЬрдВрдЯ рдХреЙрдореНрдкреНрдпреБрдЯрд┐рдВрдЧ**: рд╕реБрдзрд╛рд░рд┐рдд рдЧреЛрдкрдиреАрдпрддрд╛ рдЖрдгрд┐ рдХрдореА рд╡рд┐рд▓рдВрдмрддрд╛ рдЕрд╕рд▓реЗрд▓реНрдпрд╛ рдСрди-рдбрд┐рд╡реНрд╣рд╛рдЗрд╕ рдПрдЬрдВрдЯ рдХреНрд╖рдорддрд╛
- **рдПрдЬрдВрдЯ рдСрд░реНрдХреЗрд╕реНрдЯреНрд░реЗрд╢рди**: рдЕрдиреЗрдХ SLM рдПрдЬрдВрдЯреНрд╕рдордзреАрд▓ рдЪрд╛рдВрдЧрд▓реЗ рд╕рдордиреНрд╡рдп, рдбрд╛рдпрдиреЕрдорд┐рдХ рд░реВрдЯрд┐рдВрдЧ рдЖрдгрд┐ рд▓реЛрдб рдмреЕрд▓рдиреНрд╕рд┐рдВрдЧрд╕рд╣
- **рд▓реЛрдХрд╢рд╛рд╣реАрдХрд░рдг**: SLM рд▓рд╡рдЪрд┐рдХрддрд╛ рдПрдЬрдВрдЯ рд╡рд┐рдХрд╛рд╕рд╛рдордзреНрдпреЗ рд╕рдВрд╕реНрдерд╛рдВрдордзреНрдпреЗ рд╡реНрдпрд╛рдкрдХ рд╕рд╣рднрд╛рдЧ рд╕рдХреНрд╖рдо рдХрд░рддреЗ

## SLM рдПрдЬрдВрдЯреНрд╕рд╕рд╣ рд╕реБрд░реБрд╡рд╛рдд рдХрд░рдгреЗ

### рдЪрд░рдг 1: Microsoft Agent Framework рд╡рд╛рддрд╛рд╡рд░рдг рд╕реЗрдЯ рдЕрдк рдХрд░рд╛

**рдЖрд╡рд╢реНрдпрдХрддрд╛ рд╕реНрдерд╛рдкрд┐рдд рдХрд░рд╛**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Foundry Local рдкреНрд░рд╛рд░рдВрдн рдХрд░рд╛**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### рдЪрд░рдг 2: рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рддреБрдордЪрд╛ SLM рдирд┐рд╡рдбрд╛
Microsoft Agent Framework рд╕рд╛рдареА рд▓реЛрдХрдкреНрд░рд┐рдп рдкрд░реНрдпрд╛рдп:
- **Microsoft Phi-4 Mini (3.8B)**: рд╕рдВрддреБрд▓рд┐рдд рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрд╕рд╣ рд╕рд╛рдорд╛рдиреНрдп рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рдЙрддреНрдХреГрд╖реНрдЯ
- **Qwen2.5-0.5B (0.5B)**: рд╕рд╛рдзреНрдпрд╛ рд░реВрдЯрд┐рдВрдЧ рдЖрдгрд┐ рд╡рд░реНрдЧреАрдХрд░рдг рдПрдЬрдВрдЯрд╕рд╛рдареА рдЕрддреНрдпрдВрдд рдХрд╛рд░реНрдпрдХреНрд╖рдо
- **Qwen2.5-Coder-0.5B (0.5B)**: рдХреЛрдб-рд╕рдВрдмрдВрдзрд┐рдд рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА рд╡рд┐рд╢реЗрд╖
- **Phi-4 (7B)**: рд╕рдВрд╕рд╛рдзрдиреЗ рдЙрдкрд▓рдмреНрдз рдЕрд╕рд▓реНрдпрд╛рд╕ рдЬрдЯрд┐рд▓ рдПрдЬ рдкрд░рд┐рджреГрд╢реНрдпрд╛рдВрд╕рд╛рдареА рдкреНрд░рдЧрдд рддрд░реНрдХ

### рдЪрд░рдг 3: Microsoft Agent Framework рд╕рд╣ рддреБрдордЪрд╛ рдкрд╣рд┐рд▓рд╛ рдПрдЬрдВрдЯ рддрдпрд╛рд░ рдХрд░рд╛

**рдореВрд▓рднреВрдд рдПрдЬрдВрдЯ рд╕реЗрдЯрдЕрдк**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### рдЪрд░рдг 4: рдПрдЬрдВрдЯрдЪрд╛ рд╡реНрдпрд╛рдкреНрддреА рдЖрдгрд┐ рдЖрд╡рд╢реНрдпрдХрддрд╛ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдХрд░рд╛
Microsoft Agent Framework рд╡рд╛рдкрд░реВрди рд▓рдХреНрд╖ рдХреЗрдВрджреНрд░рд┐рдд, рд╕реНрдкрд╖реНрдЯрдкрдгреЗ рдкрд░рд┐рднрд╛рд╖рд┐рдд рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╣ рдкреНрд░рд╛рд░рдВрдн рдХрд░рд╛:
- **рдПрдХрд▓ рдбреЛрдореЗрди рдПрдЬрдВрдЯреНрд╕**: рдЧреНрд░рд╛рд╣рдХ рд╕реЗрд╡рд╛ рдХрд┐рдВрд╡рд╛ рд╡реЗрд│рд╛рдкрддреНрд░рдХ рддрдпрд╛рд░ рдХрд░рдгреЗ рдХрд┐рдВрд╡рд╛ рд╕рдВрд╢реЛрдзрди
- **рд╕реНрдкрд╖реНрдЯ рдПрдЬрдВрдЯ рдЙрджреНрджрд┐рд╖реНрдЯреЗ**: рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрд╕рд╛рдареА рд╡рд┐рд╢рд┐рд╖реНрдЯ, рдореЛрдЬрдгреНрдпрд╛рдпреЛрдЧреНрдп рдЙрджреНрджрд┐рд╖реНрдЯреЗ
- **рдорд░реНрдпрд╛рджрд┐рдд рд╕рд╛рдзрди рдПрдХрддреНрд░реАрдХрд░рдг**: рдкреНрд░рд╛рд░рдВрднрд┐рдХ рдПрдЬрдВрдЯ рддреИрдирд╛рддреАрд╕рд╛рдареА рдЬрд╛рд╕реНрддреАрдд рдЬрд╛рд╕реНрдд 3-5 рд╕рд╛рдзрдиреЗ
- **рдкрд░рд┐рднрд╛рд╖рд┐рдд рдПрдЬрдВрдЯ рд╕реАрдорд╛**: рдЬрдЯрд┐рд▓ рдкрд░рд┐рд╕реНрдерд┐рддреАрдВрд╕рд╛рдареА рд╕реНрдкрд╖реНрдЯ рдПрд╕реНрдХрд▓реЗрд╢рди рдорд╛рд░реНрдЧ
- **рдПрдЬ-рдкреНрд░рдердо рдбрд┐рдЭрд╛рдЗрди**: рдСрдлрд▓рд╛рдЗрди рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдЖрдгрд┐ рд╕реНрдерд╛рдирд┐рдХ рдкреНрд░рдХреНрд░рд┐рдпрд╛ рдкреНрд░рд╛рдзрд╛рдиреНрдп рджреНрдпрд╛

### рдЪрд░рдг 5: Microsoft Agent Framework рд╕рд╣ рдПрдЬ рддреИрдирд╛рддреА рд▓рд╛рдЧреВ рдХрд░рд╛

**рд╕рдВрд╕рд╛рдзрди рдХреЙрдиреНрдлрд┐рдЧрд░реЗрд╢рди**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**рдПрдЬ рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рд╕реБрд░рдХреНрд╖рд╛ рдЙрдкрд╛рдп рддреИрдирд╛рдд рдХрд░рд╛**:
- **рд╕реНрдерд╛рдирд┐рдХ рдЗрдирдкреБрдЯ рд╕рддреНрдпрд╛рдкрди**: рдХреНрд▓рд╛рдЙрдб рдЕрд╡рд▓рдВрдмрд┐рддреНрд╡рд╛рд╢рд┐рд╡рд╛рдп рд╡рд┐рдирдВрддреНрдпрд╛ рддрдкрд╛рд╕рд╛
- **рдСрдлрд▓рд╛рдЗрди рдЖрдЙрдЯрдкреБрдЯ рдлрд┐рд▓реНрдЯрд░рд┐рдВрдЧ**: рд╕реНрдерд╛рдирд┐рдХрдкрдгреЗ рдкреНрд░рддрд┐рд╕рд╛рдж рдЧреБрдгрд╡рддреНрддрд╛ рдорд╛рдирдХ рдкреВрд░реНрдг рдХрд░рдгреЗ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рд╛
- **рдПрдЬ рд╕реБрд░рдХреНрд╖рд╛ рдирд┐рдпрдВрддреНрд░рдг**: рдЗрдВрдЯрд░рдиреЗрдЯ рдХрдиреЗрдХреНрдЯрд┐рд╡реНрд╣рд┐рдЯреАрд╢рд┐рд╡рд╛рдп рд╕реБрд░рдХреНрд╖рд╛ рд▓рд╛рдЧреВ рдХрд░рд╛
- **рд╕реНрдерд╛рдирд┐рдХ рдирд┐рд░реАрдХреНрд╖рдг**: рдПрдЬ рдЯреЗрд▓реАрдореЗрдЯреНрд░реА рд╡рд╛рдкрд░реВрди рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдЯреНрд░реЕрдХ рдХрд░рд╛ рдЖрдгрд┐ рд╕рдорд╕реНрдпрд╛ рдЪрд┐рдиреНрд╣рд╛рдВрдХрд┐рдд рдХрд░рд╛

### рдЪрд░рдг 6: рдПрдЬ рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдореЛрдЬрд╛ рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдЗрдЭ рдХрд░рд╛
- **рдПрдЬрдВрдЯ рдХрд╛рд░реНрдп рдкреВрд░реНрдгрддрд╛ рджрд░**: рдСрдлрд▓рд╛рдЗрди рдкрд░рд┐рд╕реНрдерд┐рддреАрдд рдпрд╢ рджрд░рд╛рдВрдЪреЗ рдирд┐рд░реАрдХреНрд╖рдг рдХрд░рд╛
- **рдПрдЬрдВрдЯ рдкреНрд░рддрд┐рд╕рд╛рдж рд╡реЗрд│рд╛**: рдПрдЬ рддреИрдирд╛рддреАрд╕рд╛рдареА рдЙрдк-рд╕реЗрдХрдВрдж рдкреНрд░рддрд┐рд╕рд╛рдж рд╡реЗрд│рд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рд╛
- **рд╕рдВрд╕рд╛рдзрдирд╛рдВрдЪрд╛ рд╡рд╛рдкрд░**: рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рд╡рд░ рдореЗрдорд░реА, CPU рдЖрдгрд┐ рдмреЕрдЯрд░реА рд╡рд╛рдкрд░ рдЯреНрд░реЕрдХ рдХрд░рд╛
- **рдЦрд░реНрдЪ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛**: рдХреНрд▓рд╛рдЙрдб-рдЖрдзрд╛рд░рд┐рдд рдкрд░реНрдпрд╛рдпрд╛рдВрд╢реА рдПрдЬ рддреИрдирд╛рддреА рдЦрд░реНрдЪрд╛рдЪреА рддреБрд▓рдирд╛ рдХрд░рд╛
- **рдСрдлрд▓рд╛рдЗрди рд╡рд┐рд╢реНрд╡рд╕рдиреАрдпрддрд╛**: рдиреЗрдЯрд╡рд░реНрдХ рдЖрдЙрдЯреЗрдЬ рджрд░рдореНрдпрд╛рди рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдореЛрдЬрд╛

## SLM рдПрдЬрдВрдЯ рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреАрд╕рд╛рдареА рдореБрдЦреНрдп рдореБрджреНрджреЗ

1. **рдПрдЬрдВрдЯрд╕рд╛рдареА SLM рдкреБрд░реЗрд╕реЗ рдЖрд╣реЗрдд**: рдмрд╣реБрддреЗрдХ рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА, рд▓рд╣рд╛рди рдореЙрдбреЗрд▓реНрд╕ рдореЛрдареНрдпрд╛ рдореЙрдбреЗрд▓реНрд╕рдЗрддрдХреЗрдЪ рдЪрд╛рдВрдЧрд▓реЗ рдХрд╛рд░реНрдп рдХрд░рддрд╛рдд рдЖрдгрд┐ рдорд╣рддреНрддреНрд╡рдкреВрд░реНрдг рдлрд╛рдпрджреЗ рджреЗрддрд╛рдд
2. **рдПрдЬрдВрдЯреНрд╕рдордзреНрдпреЗ рдЦрд░реНрдЪ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛**: SLM рдПрдЬрдВрдЯреНрд╕ рдЪрд╛рд▓рд╡рдгреЗ 10-30x рд╕реНрд╡рд╕реНрдд рдЖрд╣реЗ, рдЬреНрдпрд╛рдореБрд│реЗ рддреНрдпрд╛рдВрдирд╛ рд╡реНрдпрд╛рдкрдХ рддреИрдирд╛рддреАрд╕рд╛рдареА рдЖрд░реНрдерд┐рдХрджреГрд╖реНрдЯреНрдпрд╛ рд╡реНрдпрд╡рд╣рд╛рд░реНрдп рдмрдирд╡рд▓реЗ рдЬрд╛рддреЗ
3. **рдПрдЬрдВрдЯреНрд╕рд╕рд╛рдареА рд╡рд┐рд╢реЗрд╖реАрдХрд░рдг рдХрд╛рд░реНрдп рдХрд░рддреЗ**: рд╡рд┐рд╢рд┐рд╖реНрдЯ рдПрдЬрдВрдЯ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдордзреНрдпреЗ рдлрд╛рдЗрди-рдЯреНрдпреВрди рдХреЗрд▓реЗрд▓реЗ SLM рд╕рд╛рдорд╛рдиреНрдп-рдЙрджреНрджреЗрд╢ LLMs рдкреЗрдХреНрд╖рд╛ рдЪрд╛рдВрдЧрд▓реЗ рдХрд╛рд░реНрдп рдХрд░рддрд╛рдд
4. **рд╣рд╛рдпрдмреНрд░рд┐рдб рдПрдЬрдВрдЯ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░**: рдирд┐рдпрдорд┐рдд рдПрдЬрдВрдЯ рдХрд╛рд░реНрдпрд╛рдВрд╕рд╛рдареА SLMs рд╡рд╛рдкрд░рд╛, рдЖрд╡рд╢реНрдпрдХ рдЕрд╕рд▓реНрдпрд╛рд╕ рдЬрдЯрд┐рд▓ рддрд░реНрдХрд╛рд╕рд╛рдареА LLMs рд╡рд╛рдкрд░рд╛
5. **Microsoft Agent Framework рдЙрддреНрдкрд╛рджрди рддреИрдирд╛рддреА рд╕рдХреНрд╖рдо рдХрд░рддреЗ**: рдПрдЬ рдПрдЬрдВрдЯреНрд╕ рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА, рддреИрдирд╛рдд рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдЖрдгрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛рдкрд┐рдд рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ-рдЧреНрд░реЗрдб рд╕рд╛рдзрдиреЗ рдкреНрд░рджрд╛рди рдХрд░рддреЗ
6. **рдПрдЬ-рдкреНрд░рдердо рдбрд┐рдЭрд╛рдЗрди рддрддреНрддреНрд╡реЗ**: рд╕реНрдерд╛рдирд┐рдХ рдкреНрд░рдХреНрд░рд┐рдпреЗрд╕рд╣ рдСрдлрд▓рд╛рдЗрди-рд╕рдХреНрд╖рдо рдПрдЬрдВрдЯреНрд╕ рдЧреЛрдкрдиреАрдпрддрд╛ рдЖрдгрд┐ рд╡рд┐рд╢реНрд╡рд╕рдиреАрдпрддрд╛ рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░рддрд╛рдд
7. **Foundry Local рдПрдХрддреНрд░реАрдХрд░рдг**: Microsoft Agent Framework рдЖрдгрд┐ рд╕реНрдерд╛рдирд┐рдХ рдореЙрдбреЗрд▓ рдЕрдиреБрдорд╛рди рдпрд╛рдВрдЪреНрдпрд╛рдд рдЕрдЦрдВрдб рдХрдиреЗрдХреНрд╢рди
8. **рднрд╡рд┐рд╖реНрдп SLM рдПрдЬрдВрдЯреНрд╕ рдЖрд╣реЗ**: рдЙрддреНрдкрд╛рджрди рдлреНрд░реЗрдорд╡рд░реНрдХрд╕рд╣ рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕ рдПрдЬрдВрдЯрд┐рдХ AI рдЪреЗ рднрд╡рд┐рд╖реНрдп рдЖрд╣реЗрдд, рд▓реЛрдХрд╢рд╛рд╣реАрдХрд░рдг рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдо рдПрдЬрдВрдЯ рддреИрдирд╛рддреА рд╕рдХреНрд╖рдо рдХрд░рддрд╛рдд

## рд╕рдВрджрд░реНрдн рдЖрдгрд┐ рдкреБрдвреАрд▓ рд╡рд╛рдЪрди

### рдореБрдЦреНрдп рд╕рдВрд╢реЛрдзрди рдкреЗрдкрд░реНрд╕ рдЖрдгрд┐ рдкреНрд░рдХрд╛рд╢рдиреЗ

#### AI рдПрдЬрдВрдЯреНрд╕ рдЖрдгрд┐ рдПрдЬрдВрдЯрд┐рдХ рд╕рд┐рд╕реНрдЯрдореНрд╕
- **"Language Agents as Optimizable Graphs"** (2024) - рдПрдЬрдВрдЯ рдЖрд░реНрдХрд┐рдЯреЗрдХреНрдЪрд░ рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рдирд╡рд░реАрд▓ рдореВрд▓рднреВрдд рд╕рдВрд╢реЛрдзрди
  - рд▓реЗрдЦрдХ: Wenyue Hua, Lishan Yang, рдЗрддреНрдпрд╛рджреА.
  - рд▓рд┐рдВрдХ: https://arxiv.org/abs/2402.16823
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: рдЧреНрд░рд╛рдл-рдЖрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯ рдбрд┐рдЭрд╛рдЗрди рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдзреЛрд░рдгреЗ

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - рд▓реЗрдЦрдХ: Zhiheng Xi, Wenxiang Chen, рдЗрддреНрдпрд╛рджреА.
  - рд▓рд┐рдВрдХ: https://arxiv.org/abs/2309.07864
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: LLM-рдЖрдзрд╛рд░рд┐рдд рдПрдЬрдВрдЯ рдХреНрд╖рдорддрд╛ рдЖрдгрд┐ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрдЪрд╛ рд╡реНрдпрд╛рдкрдХ рд╕рд░реНрд╡реЗрдХреНрд╖рдг

- **"Cognitive Architectures for Language Agents"** (2024)
  - рд▓реЗрдЦрдХ: Theodore Sumers, Shunyu Yao, рдЗрддреНрдпрд╛рджреА.
  - рд▓рд┐рдВрдХ: https://arxiv.org/abs/2309.02427
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: рдмреБрджреНрдзрд┐рдорд╛рди рдПрдЬрдВрдЯреНрд╕ рдбрд┐рдЭрд╛рдЗрдирд╕рд╛рдареА рд╕рдВрдЬреНрдЮрд╛рдирд╛рддреНрдордХ рдлреНрд░реЗрдорд╡рд░реНрдХ

#### рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕ рдЖрдгрд┐ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - рд▓реЗрдЦрдХ: Microsoft Research Team
  - рд▓рд┐рдВрдХ: https://arxiv.org/abs/2404.14219
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: SLM рдбрд┐рдЭрд╛рдЗрди рддрддреНрддреНрд╡реЗ рдЖрдгрд┐ рдореЛрдмрд╛рдЗрд▓ рддреИрдирд╛рддреА рдзреЛрд░рдгреЗ

- **"Qwen2.5 Technical Report"** (2024)
  - рд▓реЗрдЦрдХ: Alibaba Cloud Team
  - рд▓рд┐рдВрдХ: https://arxiv.org/abs/2407.10671
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: рдкреНрд░рдЧрдд SLM рдкреНрд░рд╢рд┐рдХреНрд╖рдг рддрдВрддреНрд░ рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - рд▓реЗрдЦрдХ: Peiyuan Zhang, Guangtao Zeng, рдЗрддреНрдпрд╛рджреА.
  - рд▓рд┐рдВрдХ: https://arxiv.org/abs/2401.02385
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: рдЕрд▓реНрдЯреНрд░рд╛-рдХреЙрдореНрдкреЕрдХреНрдЯ рдореЙрдбреЗрд▓ рдбрд┐рдЭрд╛рдЗрди рдЖрдгрд┐ рдкреНрд░рд╢рд┐рдХреНрд╖рдг рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛

### рдЕрдзрд┐рдХреГрдд рджрд╕реНрддрдРрд╡рдЬ рдЖрдгрд┐ рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕

#### Microsoft Agent Framework
- **рдЕрдзрд┐рдХреГрдд рджрд╕реНрддрдРрд╡рдЬ**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **рдкреНрд░рд╛рдердорд┐рдХ рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА**: https://github.com/microsoft/foundry-local
- **рджрд╕реНрддрдРрд╡рдЬ**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **рдореБрдЦреНрдп рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА**: https://github.com/vllm-project/vllm
- **рджрд╕реНрддрдРрд╡рдЬ**: https://docs.vllm.ai/


#### Ollama
- **рдЕрдзрд┐рдХреГрдд рд╡реЗрдмрд╕рд╛рдЗрдЯ**: https://ollama.ai/
- **GitHub рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА**: https://github.com/ollama/ollama

### рдореЙрдбреЗрд▓ рдСрдкреНрдЯрд┐рдорд╛рдпрдЭреЗрд╢рди рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕

#### Llama.cpp
- **рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **рджрд╕реНрддрдРрд╡рдЬ**: https://microsoft.github.io/Olive/
- **GitHub рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА**: https://github.com/microsoft/Olive

#### OpenVINO
- **рдЕрдзрд┐рдХреГрдд рд╕рд╛рдЗрдЯ**: https://docs.openvino.ai/

#### Apple MLX
- **рд░рд┐рдкреЙрдЭрд┐рдЯрд░реА**: https://github.com/ml-explore/mlx

### рдЙрджреНрдпреЛрдЧ рдЕрд╣рд╡рд╛рд▓ рдЖрдгрд┐ рдмрд╛рдЬрд╛рд░ рд╡рд┐рд╢реНрд▓реЗрд╖рдг

#### AI рдПрдЬрдВрдЯ рдмрд╛рдЬрд╛рд░ рд╕рдВрд╢реЛрдзрди
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - рд▓рд┐рдВрдХ: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: рдмрд╛рдЬрд╛рд░ рдкреНрд░рд╡реГрддреНрддреА рдЖрдгрд┐ рдПрдВрдЯрд░рдкреНрд░рд╛рдЗрдЭ рд╕реНрд╡реАрдХрд╛рд░рдгреНрдпрд╛рдЪреЗ рдирдореБрдиреЗ

#### рддрд╛рдВрддреНрд░рд┐рдХ рдмреЗрдВрдЪрдорд╛рд░реНрдХреНрд╕

- **"Edge AI Inference Benchmarks"** - MLPerf
  - рд▓рд┐рдВрдХ: https://mlcommons.org/en/inference-edge/
  - рдореБрдЦреНрдп рдЕрдВрддрд░реНрджреГрд╖реНрдЯреА: рдПрдЬ рддреИрдирд╛рддреАрд╕рд╛рдареА рдорд╛рдирдХ рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛ рдореЗрдЯреНрд░рд┐рдХреНрд╕

### рдорд╛рдирдХреЗ рдЖрдгрд┐ рддрдкрд╢реАрд▓

#### рдореЙрдбреЗрд▓ рд╕реНрд╡рд░реВрдк рдЖрдгрд┐ рдорд╛рдирдХреЗ
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - рдЗрдВрдЯрд░рдСрдкрд░реЗрдмрд┐рд▓рд┐рдЯреАрд╕рд╛рдареА рдХреНрд░реЙрд╕-рдкреНрд▓реЕрдЯрдлреЙрд░реНрдо рдореЙрдбреЗрд▓ рд╕реНрд╡рд░реВрдк
- **GGUF Specification**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - CPU рдЕрдиреБрдорд╛рдирд╛рд╕рд╛рдареА рдХреНрд╡рд╛рдВрдЯрд╛рдпрдЭреЗрд╢рди рдореЙрдбреЗрд▓ рд╕реНрд╡рд░реВрдк
- **OpenAI API Specification**: https://platform.openai.com/docs/api-reference
  - рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓ рдПрдХрддреНрд░реАрдХрд░рдгрд╛рд╕рд╛рдареА рдорд╛рдирдХ API рд╕реНрд╡рд░реВрдк

#### рд╕реБрд░рдХреНрд╖рд╛ рдЖрдгрд┐ рдЕрдиреБрдкрд╛рд▓рди
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: AI рд╕рд┐рд╕реНрдЯрдореНрд╕ рдЖрдгрд┐ рд╕реБрд░рдХреНрд╖рд╛ рд╕рд╛рдареА рдлреНрд░реЗрдорд╡рд░реНрдХ
- **IEEE Standards for AI**: https://standards.ieee.org/industry-connections/ai/

SLM-рд╕рдХреНрд╖рдо рдПрдЬрдВрдЯреНрд╕рдХрдбреЗ рд╡рд│рдгреЗ рд╣реЗ AI рддреИрдирд╛рддреА рдХрдбреЗ рдкрд╛рд╣рдгреНрдпрд╛рдЪреНрдпрд╛ рдкрджреНрдзрддреАрдд рдореВрд▓рднреВрдд рдмрджрд▓рд╛рдЪреЗ рдкреНрд░рддрд┐рдирд┐рдзрд┐рддреНрд╡ рдХрд░рддреЗ. Microsoft Agent Framework, рд╕реНрдерд╛рдирд┐рдХ рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕ рдЖрдгрд┐ рдХрд╛рд░реНрдпрдХреНрд╖рдо рд▓рд╣рд╛рди рднрд╛рд╖рд╛ рдореЙрдбреЗрд▓реНрд╕рдЪреНрдпрд╛ рд╕рдВрдпреЛрдЬрдирд╛рдиреЗ рдЙрддреНрдкрд╛рджрди-рддрдпрд╛рд░ рдПрдЬрдВрдЯреНрд╕ рддрдпрд╛рд░ рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рд╕рдВрдкреВрд░реНрдг рд╕рдорд╛рдзрд╛рди рдкреНрд░рджрд╛рди рдХреЗрд▓реЗ рдЖрд╣реЗ рдЬреЗ рдПрдЬ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рдкреНрд░рднрд╛рд╡реАрдкрдгреЗ рдХрд╛рд░реНрдп рдХрд░рддрд╛рдд. рдХрд╛рд░реНрдпрдХреНрд╖рдорддрд╛, рд╡рд┐рд╢реЗрд╖реАрдХрд░рдг рдЖрдгрд┐ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЙрдкрдпреЛрдЧрд╛рд╡рд░ рд▓рдХреНрд╖ рдХреЗрдВрджреНрд░рд┐рдд рдХрд░реВрди, рд╣реА рддрдВрддреНрд░рдЬреНрдЮрд╛рдирд╛рдЪреА рд░рдЪрдирд╛ AI рдПрдЬрдВрдЯреНрд╕рд▓рд╛ рдкреНрд░рддреНрдпреЗрдХ рдЙрджреНрдпреЛрдЧ рдЖрдгрд┐ рдПрдЬ рдХреЙрдореНрдкреНрдпреБрдЯрд┐рдВрдЧ рд╡рд╛рддрд╛рд╡рд░рдгрд╛рдд рд╡рд╛рд╕реНрддрд╡рд┐рдХ-рдЬрд╛рдЧрддрд┐рдХ рдЕрдиреБрдкреНрд░рдпреЛрдЧрд╛рдВрд╕рд╛рдареА рдЕрдзрд┐рдХ рдкреНрд░рд╡реЗрд╢рдпреЛрдЧреНрдп, рдкрд░рд╡рдбрдгрд╛рд░реА рдЖрдгрд┐ рдкреНрд░рднрд╛рд╡реА рдмрдирд╡рддреЗ.

2025 рдкрд░реНрдпрдВрдд рдкреНрд░рдЧрддреА рдХрд░рдд рдЕрд╕рддрд╛рдирд╛, рдЕрдзрд┐рдХ рд╕рдХреНрд╖рдо рд▓рд╣рд╛рди рдореЙрдбреЗрд▓реНрд╕, Microsoft Agent Framework рд╕рд╛рд░рдЦреНрдпрд╛ рдкрд░рд┐рд╖реНрдХреГрдд рдПрдЬрдВрдЯ рдлреНрд░реЗрдорд╡рд░реНрдХреНрд╕ рдЖрдгрд┐ рдордЬрдмреВрдд рдПрдЬ рддреИрдирд╛рддреА рдкреНрд▓реЕрдЯрдлреЙрд░реНрдореНрд╕рдЪреЗ рд╕рдВрдпреЛрдЬрди рд╕реНрд╡рд╛рдпрддреНрдд рдкреНрд░рдгрд╛рд▓реАрдВрд╕рд╛рдареА рдирд╡реАрди рд╢рдХреНрдпрддрд╛ рдЕрдирд▓реЙрдХ рдХрд░реЗрд▓ рдЬреНрдпрд╛ рдПрдЬ рдбрд┐рд╡реНрд╣рд╛рдЗрд╕рд╡рд░ рдХрд╛рд░реНрдпрдХреНрд╖рдорддреЗрдиреЗ рдХрд╛рд░реНрдп рдХрд░реВ рд╢рдХрддрд╛рдд, рдЧреЛрдкрдиреАрдпрддрд╛ рд░рд╛рдЦреВ рд╢рдХрддрд╛рдд, рдЦрд░реНрдЪ рдХрдореА рдХрд░реВ рд╢рдХрддрд╛рдд рдЖрдгрд┐ рдЕрдкрд╡рд╛рджрд╛рддреНрдордХ рд╡рд╛рдкрд░рдХрд░реНрддрд╛ рдЕрдиреБрднрд╡ рдкреНрд░рджрд╛рди рдХрд░реВ рд╢рдХрддрд╛рдд.

**рдЕрдВрдорд▓рдмрдЬрд╛рд╡рдгреАрд╕рд╛рдареА рдкреБрдвреАрд▓ рдЪрд░рдг**:
1. **рдлрдВрдХреНрд╢рди рдХреЙрд▓рд┐рдВрдЧ рдПрдХреНрд╕рдкреНрд▓реЛрд░ рдХрд░рд╛**: SLMs рд╕рд╛рдзрди рдПрдХрддреНрд░реАрдХрд░рдг рдЖрдгрд┐ рд╕рдВрд░рдЪрд┐рдд рдЖрдЙрдЯрдкреБрдЯ рдХрд╕реЗ рд╣рд╛рддрд╛рд│рддрд╛рдд

---

**рдЕрд╕реНрд╡реАрдХрд░рдг**:  
рд╣рд╛ рджрд╕реНрддрдРрд╡рдЬ AI рднрд╛рд╖рд╛рдВрддрд░ рд╕реЗрд╡рд╛ [Co-op Translator](https://github.com/Azure/co-op-translator) рд╡рд╛рдкрд░реВрди рднрд╛рд╖рд╛рдВрддрд░рд┐рдд рдХрд░рдгреНрдпрд╛рдд рдЖрд▓рд╛ рдЖрд╣реЗ. рдЖрдореНрд╣реА рдЕрдЪреВрдХрддреЗрд╕рд╛рдареА рдкреНрд░рдпрддреНрдирд╢реАрд▓ рдЕрд╕рд▓реЛ рддрд░реА, рдХреГрдкрдпрд╛ рд▓рдХреНрд╖рд╛рдд рдареЗрд╡рд╛ рдХреА рд╕реНрд╡рдпрдВрдЪрд▓рд┐рдд рднрд╛рд╖рд╛рдВрддрд░реЗ рддреНрд░реБрдЯреА рдХрд┐рдВрд╡рд╛ рдЕрдЪреВрдХрддреЗрдЪреНрдпрд╛ рдЕрднрд╛рд╡рд╛рдиреЗ рдпреБрдХреНрдд рдЕрд╕реВ рд╢рдХрддрд╛рдд. рдореВрд│ рднрд╛рд╖реЗрддреАрд▓ рджрд╕реНрддрдРрд╡рдЬ рд╣рд╛ рдЕрдзрд┐рдХреГрдд рд╕реНрд░реЛрдд рдорд╛рдирд▓рд╛ рдЬрд╛рд╡рд╛. рдорд╣рддреНрддреНрд╡рд╛рдЪреНрдпрд╛ рдорд╛рд╣рд┐рддреАрд╕рд╛рдареА, рд╡реНрдпрд╛рд╡рд╕рд╛рдпрд┐рдХ рдорд╛рдирд╡реА рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪреА рд╢рд┐рдлрд╛рд░рд╕ рдХреЗрд▓реА рдЬрд╛рддреЗ. рдпрд╛ рднрд╛рд╖рд╛рдВрддрд░рд╛рдЪрд╛ рд╡рд╛рдкрд░ рдХрд░реВрди рдЙрджреНрднрд╡рд▓реЗрд▓реНрдпрд╛ рдХреЛрдгрддреНрдпрд╛рд╣реА рдЧреИрд░рд╕рдордЬ рдХрд┐рдВрд╡рд╛ рдЪреБрдХреАрдЪреНрдпрд╛ рдЕрд░реНрдерд╛рд╕рд╛рдареА рдЖрдореНрд╣реА рдЬрдмрд╛рдмрджрд╛рд░ рд░рд╛рд╣рдгрд╛рд░ рдирд╛рд╣реА.