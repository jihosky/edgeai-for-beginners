<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T11:54:57+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "mr"
}
-->
# विभाग 3 : Microsoft Olive ऑप्टिमायझेशन सूट

## विषय सूची
1. [परिचय](../../../Module04)
2. [Microsoft Olive म्हणजे काय?](../../../Module04)
3. [इंस्टॉलेशन](../../../Module04)
4. [जलद प्रारंभ मार्गदर्शक](../../../Module04)
5. [उदाहरण: Qwen3 ला ONNX INT4 मध्ये रूपांतरित करणे](../../../Module04)
6. [प्रगत वापर](../../../Module04)
7. [Olive Recipes रिपॉझिटरी](../../../Module04)
8. [सर्वोत्तम पद्धती](../../../Module04)
9. [समस्या निवारण](../../../Module04)
10. [अतिरिक्त संसाधने](../../../Module04)

## परिचय

Microsoft Olive हे एक शक्तिशाली, वापरण्यास सोपे हार्डवेअर-जाणकार मॉडेल ऑप्टिमायझेशन टूलकिट आहे जे मशीन लर्निंग मॉडेल्सना विविध हार्डवेअर प्लॅटफॉर्मवर तैनात करण्यासाठी ऑप्टिमायझेशन प्रक्रिया सुलभ करते. तुम्ही CPU, GPU किंवा विशेष AI अॅक्सेलरेटरसाठी लक्ष्य करत असाल, Olive तुम्हाला मॉडेल अचूकता टिकवून ठेवताना उत्कृष्ट कार्यप्रदर्शन साध्य करण्यात मदत करते.

## Microsoft Olive म्हणजे काय?

Olive हे एक सोपे हार्डवेअर-जाणकार मॉडेल ऑप्टिमायझेशन टूल आहे जे मॉडेल कम्प्रेशन, ऑप्टिमायझेशन आणि कंपायलेशनसाठी उद्योग-अग्रणी तंत्रांचा समावेश करते. हे ONNX Runtime सह E2E इनफरन्स ऑप्टिमायझेशन सोल्यूशन म्हणून कार्य करते.

### मुख्य वैशिष्ट्ये

- **हार्डवेअर-जाणकार ऑप्टिमायझेशन**: तुमच्या लक्ष्य हार्डवेअरसाठी सर्वोत्तम ऑप्टिमायझेशन तंत्र आपोआप निवडते
- **40+ अंगभूत ऑप्टिमायझेशन घटक**: मॉडेल कम्प्रेशन, क्वांटायझेशन, ग्राफ ऑप्टिमायझेशन आणि बरेच काही समाविष्ट
- **सोपे CLI इंटरफेस**: सामान्य ऑप्टिमायझेशन कार्यांसाठी सोपे आदेश
- **मल्टी-फ्रेमवर्क सपोर्ट**: PyTorch, Hugging Face मॉडेल्स आणि ONNX सह कार्य करते
- **लोकप्रिय मॉडेल सपोर्ट**: Olive Llama, Phi, Qwen, Gemma यांसारख्या लोकप्रिय मॉडेल आर्किटेक्चरला स्वयंचलितपणे ऑप्टिमाइझ करू शकते

### फायदे

- **विकास वेळ कमी**: विविध ऑप्टिमायझेशन तंत्रांसह मॅन्युअली प्रयोग करण्याची गरज नाही
- **कार्यप्रदर्शन सुधारणा**: लक्षणीय गती सुधारणा (काही प्रकरणांमध्ये 6x पर्यंत)
- **क्रॉस-प्लॅटफॉर्म तैनाती**: ऑप्टिमाइझ केलेले मॉडेल्स विविध हार्डवेअर आणि ऑपरेटिंग सिस्टमवर कार्य करतात
- **अचूकता टिकवून ठेवली**: ऑप्टिमायझेशन कार्यप्रदर्शन सुधारताना मॉडेल गुणवत्ता टिकवून ठेवते

## इंस्टॉलेशन

### पूर्वापेक्षित

- Python 3.8 किंवा त्याहून अधिक
- pip पॅकेज मॅनेजर
- व्हर्च्युअल एन्व्हायर्नमेंट (शिफारस केलेले)

### मूलभूत इंस्टॉलेशन

व्हर्च्युअल एन्व्हायर्नमेंट तयार करा आणि सक्रिय करा:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ऑटो-ऑप्टिमायझेशन वैशिष्ट्यांसह Olive इंस्टॉल करा:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### पर्यायी अवलंबित्वे

Olive अतिरिक्त वैशिष्ट्यांसाठी विविध पर्यायी अवलंबित्वे ऑफर करते:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### इंस्टॉलेशन सत्यापित करा

```bash
olive --help
```

यशस्वी असल्यास, तुम्हाला Olive CLI मदत संदेश दिसेल.

## जलद प्रारंभ मार्गदर्शक

### तुमचे पहिले ऑप्टिमायझेशन

Olive च्या ऑटो-ऑप्टिमायझेशन वैशिष्ट्याचा वापर करून एक छोटे भाषा मॉडेल ऑप्टिमाइझ करूया:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### या आदेशाचे कार्य काय आहे

ऑप्टिमायझेशन प्रक्रियेमध्ये: स्थानिक कॅशमधून मॉडेल मिळवणे, ONNX ग्राफ कॅप्चर करणे आणि ONNX डेटा फाइलमध्ये वेट्स संग्रहित करणे, ONNX ग्राफ ऑप्टिमाइझ करणे आणि RTN पद्धतीचा वापर करून मॉडेलला int4 मध्ये क्वांटाइझ करणे समाविष्ट आहे.

### आदेशाचे पॅरामीटर्स स्पष्ट केले

- `--model_name_or_path`: Hugging Face मॉडेल आयडेंटिफायर किंवा स्थानिक पथ
- `--output_path`: ऑप्टिमाइझ केलेले मॉडेल जिथे सेव्ह केले जाईल ती डायरेक्टरी
- `--device`: लक्ष्य डिव्हाइस (cpu, gpu)
- `--provider`: एक्झिक्युशन प्रोव्हायडर (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: इनफरन्ससाठी ONNX Runtime Generate AI वापरा
- `--precision`: क्वांटायझेशन अचूकता (int4, int8, fp16)
- `--log_level`: लॉगिंगची तीव्रता (0=minimal, 1=verbose)

## उदाहरण: Qwen3 ला ONNX INT4 मध्ये रूपांतरित करणे

[lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) येथे दिलेल्या Hugging Face उदाहरणावर आधारित, Qwen3 मॉडेल ऑप्टिमाइझ कसे करायचे ते येथे आहे:

### चरण 1: मॉडेल डाउनलोड करा (पर्यायी)

डाउनलोड वेळ कमी करण्यासाठी, फक्त आवश्यक फाइल्स कॅश करा:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### चरण 2: Qwen3 मॉडेल ऑप्टिमाइझ करा

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### चरण 3: ऑप्टिमाइझ केलेले मॉडेल तपासा

तुमचे ऑप्टिमाइझ केलेले मॉडेल तपासण्यासाठी एक साधी Python स्क्रिप्ट तयार करा:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### आउटपुट संरचना

ऑप्टिमायझेशननंतर, तुमच्या आउटपुट डायरेक्टरीमध्ये खालील गोष्टी असतील:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## प्रगत वापर

### कॉन्फिगरेशन फाइल्स

अधिक जटिल ऑप्टिमायझेशन वर्कफ्लो साठी, तुम्ही JSON कॉन्फिगरेशन फाइल्स वापरू शकता:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

कॉन्फिगरेशनसह चालवा:

```bash
olive run --config config.json
```

### GPU ऑप्टिमायझेशन

CUDA GPU ऑप्टिमायझेशनसाठी:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) साठी:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive सह फाइन-ट्यूनिंग

Olive मॉडेल्सचे फाइन-ट्यूनिंग देखील समर्थन करते:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## सर्वोत्तम पद्धती

### 1. मॉडेल निवड
- चाचणीसाठी लहान मॉडेल्ससह प्रारंभ करा (उदा., 0.5B-7B पॅरामीटर्स)
- तुमचे लक्ष्य मॉडेल आर्किटेक्चर Olive द्वारे समर्थित आहे याची खात्री करा

### 2. हार्डवेअर विचार
- तुमचे ऑप्टिमायझेशन लक्ष्य तुमच्या तैनाती हार्डवेअरशी जुळवा
- CUDA-सुसंगत हार्डवेअर असल्यास GPU ऑप्टिमायझेशन वापरा
- Windows मशीनसाठी DirectML विचार करा

### 3. अचूकता निवड
- **INT4**: जास्तीत जास्त कम्प्रेशन, थोडासा अचूकता तोटा
- **INT8**: आकार आणि अचूकतेचा चांगला समतोल
- **FP16**: किमान अचूकता तोटा, मध्यम आकार कमी

### 4. चाचणी आणि सत्यापन
- तुमच्या विशिष्ट वापर प्रकरणांसह ऑप्टिमाइझ केलेले मॉडेल्स नेहमी चाचणी करा
- कार्यप्रदर्शन मेट्रिक्सची तुलना करा (लेटन्सी, थ्रूपुट, अचूकता)
- मूल्यांकनासाठी प्रतिनिधी इनपुट डेटा वापरा

### 5. पुनरावृत्ती ऑप्टिमायझेशन
- जलद परिणामांसाठी ऑटो-ऑप्टिमायझेशनसह प्रारंभ करा
- सूक्ष्म नियंत्रणासाठी कॉन्फिगरेशन फाइल्स वापरा
- विविध ऑप्टिमायझेशन पासेससह प्रयोग करा

## समस्या निवारण

### सामान्य समस्या

#### 1. इंस्टॉलेशन समस्या
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU समस्या
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. मेमरी समस्या
- ऑप्टिमायझेशन दरम्यान लहान बॅच साइज वापरा
- प्रथम उच्च अचूकतेसह क्वांटायझेशनचा प्रयत्न करा (int8 ऐवजी int4)
- मॉडेल कॅशिंगसाठी पुरेशी डिस्क जागा सुनिश्चित करा

#### 4. मॉडेल लोडिंग त्रुटी
- मॉडेल पथ आणि प्रवेश परवानग्या सत्यापित करा
- मॉडेलला `trust_remote_code=True` आवश्यक आहे का ते तपासा
- सर्व आवश्यक मॉडेल फाइल्स डाउनलोड झाल्या आहेत याची खात्री करा

### मदत मिळवा

- **डॉक्युमेंटेशन**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **उदाहरणे**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes रिपॉझिटरी

### Olive Recipes ची ओळख

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) रिपॉझिटरी मुख्य Olive टूलकिटला पूरक आहे आणि लोकप्रिय AI मॉडेल्ससाठी तयार-टू-यूज ऑप्टिमायझेशन रेसिपींचा व्यापक संग्रह प्रदान करते. ही रिपॉझिटरी सार्वजनिकपणे उपलब्ध मॉडेल्स ऑप्टिमाइझ करण्यासाठी आणि मालकीच्या मॉडेल्ससाठी ऑप्टिमायझेशन वर्कफ्लो तयार करण्यासाठी व्यावहारिक संदर्भ म्हणून काम करते.

### मुख्य वैशिष्ट्ये

- **100+ प्री-बिल्ट रेसिपी**: लोकप्रिय मॉडेल्ससाठी तयार-टू-यूज ऑप्टिमायझेशन कॉन्फिगरेशन्स
- **मल्टी-आर्किटेक्चर सपोर्ट**: ट्रान्सफॉर्मर मॉडेल्स, व्हिजन मॉडेल्स आणि मल्टीमोडल आर्किटेक्चर समाविष्ट
- **हार्डवेअर-विशिष्ट ऑप्टिमायझेशन**: CPU, GPU आणि विशेष अॅक्सेलरेटरसाठी तयार केलेल्या रेसिपी
- **लोकप्रिय मॉडेल कुटुंबे**: Phi, Llama, Qwen, Gemma, Mistral आणि बरेच काही समाविष्ट

### समर्थित मॉडेल कुटुंबे

रिपॉझिटरीमध्ये ऑप्टिमायझेशन रेसिपी समाविष्ट आहेत:

#### भाषा मॉडेल्स
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 मालिका (0.5B ते 14B)
- **Google Gemma**: विविध Gemma मॉडेल कॉन्फिगरेशन्स
- **Mistral AI**: Mistral-7B मालिका
- **DeepSeek**: R1-Distill मालिका मॉडेल्स

#### व्हिजन आणि मल्टीमोडल मॉडेल्स
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP मॉडेल्स**: विविध CLIP-ViT कॉन्फिगरेशन्स
- **ResNet**: ResNet-50 ऑप्टिमायझेशन
- **व्हिजन ट्रान्सफॉर्मर्स**: ViT-base-patch16-224

#### विशेष मॉडेल्स
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: बेस आणि मल्टीलिंग्वल व्हेरिएंट्स
- **सेंटन्स ट्रान्सफॉर्मर्स**: all-MiniLM-L6-v2

### Olive Recipes वापरणे

#### पद्धत 1: विशिष्ट रेसिपी क्लोन करा

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### पद्धत 2: रेसिपी टेम्पलेट म्हणून वापरा

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### रेसिपी संरचना

प्रत्येक रेसिपी डायरेक्टरीमध्ये सामान्यतः खालील गोष्टी असतात:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### उदाहरण: Phi-4-mini रेसिपी वापरणे

Phi-4-mini रेसिपीचा एक उदाहरण म्हणून वापर करूया:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

कॉन्फिगरेशन फाइलमध्ये सामान्यतः समाविष्ट असते:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### रेसिपी सानुकूलित करणे

#### लक्ष्य हार्डवेअर बदलणे

`systems` विभाग अपडेट करा:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### ऑप्टिमायझेशन पॅरामीटर्स समायोजित करणे

`passes` विभाग बदलून विविध ऑप्टिमायझेशन स्तरांसाठी:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### तुमची स्वतःची रेसिपी तयार करणे

1. **समान मॉडेलसह प्रारंभ करा**: समान आर्किटेक्चर असलेल्या मॉडेलसाठी रेसिपी शोधा
2. **मॉडेल कॉन्फिगरेशन अपडेट करा**: कॉन्फिगरेशनमध्ये मॉडेल नाव/पथ बदला
3. **पॅरामीटर्स समायोजित करा**: आवश्यकतेनुसार ऑप्टिमायझेशन पॅरामीटर्स बदला
4. **चाचणी आणि सत्यापन करा**: ऑप्टिमायझेशन चालवा आणि परिणाम सत्यापित करा
5. **परत योगदान द्या**: तुमची रेसिपी रिपॉझिटरीमध्ये योगदान देण्याचा विचार करा

### रेसिपी वापरण्याचे फायदे

#### 1. **प्रमाणित कॉन्फिगरेशन्स**
- विशिष्ट मॉडेल्ससाठी चाचणी केलेली ऑप्टिमायझेशन सेटिंग्ज
- इष्टतम पॅरामीटर्स शोधण्यात ट्रायल-एंड-एरर टाळते

#### 2. **हार्डवेअर-विशिष्ट ट्यूनिंग**
- विविध एक्झिक्युशन प्रोव्हायडर्ससाठी पूर्व-ऑप्टिमाइझ केलेले
- CPU, GPU आणि NPU लक्ष्यांसाठी तयार-टू-यूज कॉन्फिगरेशन्स

#### 3. **व्यापक कव्हरेज**
- सर्वात लोकप्रिय ओपन-सोर्स मॉडेल्सला समर्थन देते
- नवीन मॉडेल रिलीजसह नियमित अद्यतने

#### 4. **समुदाय योगदान**
- AI समुदायासह सहयोगी विकास
- सामायिक ज्ञान आणि सर्वोत्तम पद्धती

### Olive Recipes मध्ये योगदान देणे

जर तुम्ही रिपॉझिटरीमध्ये समाविष्ट नसलेले मॉडेल ऑप्टिमाइझ केले असेल:

1. **रिपॉझिटरी फोर्क करा**: Olive Recipes ची स्वतःची फोर्क तयार करा
2. **रेसिपी डायरेक्टरी तयार करा**: तुमच्या मॉडेलसाठी नवीन डायरेक्टरी जोडा
3. **कॉन्फिगरेशन समाविष्ट करा**: olive_config.json आणि सपोर्टिंग फाइल्स जोडा
4. **वापराचे दस्तऐवजीकरण करा**: स्पष्ट README सह सूचना प्रदान करा
5. **पुल रिक्वेस्ट सबमिट करा**: समुदायाला परत योगदान द्या

### कार्यप्रदर्शन बेंचमार्क्स

अनेक रेसिपीमध्ये कार्यप्रदर्शन बेंचमार्क्स समाविष्ट आहेत जे दर्शवतात:
- **लेटन्सी सुधारणा**: बेसलाइनच्या तुलनेत सामान्यतः 2-6x गती वाढ
- **मेमरी कमी करणे**: क्वांटायझेशनसह 50-75% मेमरी वापर कमी
- **अचूकता टिकवून ठेवणे**: 95-99% अचूकता टिकवून ठेवणे

### AI टूलकिटसह एकत्रीकरण

रेसिपी सहजतेने कार्य करतात:
- **VS Code AI टूलकिट**: मॉडेल ऑप्टिमायझेशनसाठी थेट एकत्रीकरण
- **Azure Machine Learning**: क्लाउड-आधारित ऑप्टिमायझेशन वर्कफ्लो
- **ONNX Runtime**: ऑप्टिमाइझ इनफरन्स तैनाती

## अतिरिक्त संसाधने

### अधिकृत दुवे
- **GitHub रिपॉझिटरी**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes रिपॉझिटरी**: [github.com/m

---

**अस्वीकरण**:  
हा दस्तऐवज AI भाषांतर सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) वापरून भाषांतरित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी कृपया लक्षात ठेवा की स्वयंचलित भाषांतरे त्रुटी किंवा अचूकतेच्या अभावाने युक्त असू शकतात. मूळ भाषेतील दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर करून उद्भवलेल्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.