<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T16:00:42+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "sr"
}
-->
# Секција 7: Qualcomm QNN (Qualcomm Neural Network) Оптимизациони пакет

## Садржај
1. [Увод](../../../Module04)
2. [Шта је Qualcomm QNN?](../../../Module04)
3. [Инсталација](../../../Module04)
4. [Водич за брзи почетак](../../../Module04)
5. [Пример: Конверзија и оптимизација модела са QNN](../../../Module04)
6. [Напредна употреба](../../../Module04)
7. [Најбоље праксе](../../../Module04)
8. [Решавање проблема](../../../Module04)
9. [Додатни ресурси](../../../Module04)

## Увод

Qualcomm QNN (Qualcomm Neural Network) је свеобухватни оквир за AI инференцију, дизајниран да искористи пун потенцијал Qualcomm-ових AI хардверских акцелератора, укључујући Hexagon NPU, Adreno GPU и Kryo CPU. Без обзира да ли циљате мобилне уређаје, платформе за edge рачунарство или аутомобилске системе, QNN пружа оптимизоване могућности инференције које користе специјализоване AI процесне јединице Qualcomm-а за максималне перформансе и енергетску ефикасност.

## Шта је Qualcomm QNN?

Qualcomm QNN је унифицирани оквир за AI инференцију који омогућава програмерима да ефикасно имплементирају AI моделе на Qualcomm-овој хетерогеној рачунарској архитектури. Пружа унифицирани програмски интерфејс за приступ Hexagon NPU (Neural Processing Unit), Adreno GPU и Kryo CPU, аутоматски бирајући оптималну процесну јединицу за различите слојеве модела и операције.

### Кључне карактеристике

- **Хетерогено рачунарство**: Унифицирани приступ NPU, GPU и CPU са аутоматском расподелом оптерећења
- **Оптимизација прилагођена хардверу**: Специјализоване оптимизације за Qualcomm Snapdragon платформе
- **Подршка за квантовање**: Напредне технике INT8, INT16 и мешовите прецизности квантовања
- **Алатке за конверзију модела**: Директна подршка за TensorFlow, PyTorch, ONNX и Caffe моделе
- **Оптимизовано за Edge AI**: Дизајнирано посебно за мобилне и edge сценарије са фокусом на енергетску ефикасност

### Предности

- **Максималне перформансе**: Искористите специјализовани AI хардвер за до 15x побољшања перформанси
- **Енергетска ефикасност**: Оптимизовано за мобилне и уређаје на батерије са интелигентним управљањем енергијом
- **Ниска латенција**: Инференција убрзана хардвером са минималним кашњењем за апликације у реалном времену
- **Скалабилна имплементација**: Од паметних телефона до аутомобилских платформи у Qualcomm-овом екосистему
- **Спремно за производњу**: Оквир тестиран у милионима уређаја у употреби

## Инсталација

### Предуслови

- Qualcomm QNN SDK (потребна је регистрација код Qualcomm-а)
- Python 3.7 или новији
- Компатибилан Qualcomm хардвер или симулатор
- Android NDK (за мобилну имплементацију)
- Linux или Windows развојно окружење

### Подешавање QNN SDK-а

1. **Регистрација и преузимање**: Посетите Qualcomm Developer Network за регистрацију и преузимање QNN SDK-а
2. **Распакујте SDK**: Распакујте QNN SDK у ваш развојни директоријум
3. **Подесите променљиве окружења**: Конфигуришите путање за QNN алатке и библиотеке

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Подешавање Python окружења

Креирајте и активирајте виртуелно окружење:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Инсталирајте потребне Python пакете:

```bash
pip install numpy tensorflow torch onnx
```

### Провера инсталације

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Ако је успешно, требало би да видите информације о помоћи за сваку QNN алатку.

## Водич за брзи почетак

### Ваша прва конверзија модела

Хајде да конвертујемо једноставан PyTorch модел за рад на Qualcomm хардверу:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Конверзија ONNX у QNN формат

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Генерисање QNN библиотеке модела

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Шта овај процес ради

Процес оптимизације укључује: конверзију оригиналног модела у ONNX формат, превођење ONNX-а у QNN интермедијарну репрезентацију, примену оптимизација специфичних за хардвер и генерисање компајлиране библиотеке модела за имплементацију.

### Објашњење кључних параметара

- `--input_network`: Изворна ONNX датотека модела
- `--output_path`: Генерисана C++ изворна датотека
- `--input_dim`: Димензије улазног тензора за оптимизацију
- `--quantization_overrides`: Прилагођена конфигурација квантовања
- `-t x86_64-linux-clang`: Циљна архитектура и компајлер

## Пример: Конверзија и оптимизација модела са QNN

### Корак 1: Напредна конверзија модела са квантовањем

Ево како да примените прилагођено квантовање током конверзије:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Конверзија са прилагођеним квантовањем:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Корак 2: Оптимизација за више позадина

Конфигуришите за хетерогену извршење преко NPU, GPU и CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Корак 3: Креирање бинарног контекста за имплементацију

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Корак 4: Инференција са QNN Runtime-ом

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Структура излазних података

Након оптимизације, ваш директоријум за имплементацију ће садржати:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Напредна употреба

### Конфигурација прилагођене позадине

Конфигуришите специфичне оптимизације позадине:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Динамичко квантовање

Примените квантовање у реалном времену за бољу тачност:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Профилисање перформанси

Пратите перформансе на различитим позадинама:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Аутоматски избор позадине

Имплементирајте интелигентан избор позадине на основу карактеристика модела:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Најбоље праксе

### 1. Оптимизација архитектуре модела
- **Фузија слојева**: Комбинујте операције као што су Conv+BatchNorm+ReLU за бољу искоришћеност NPU-а
- **Дубински раздвојиве конволуције**: Преферирајте их уместо стандардних конволуција за мобилну имплементацију
- **Дизајни прилагођени квантовању**: Користите ReLU активације и избегавајте операције које се тешко квантизују

### 2. Стратегија квантовања
- **Квантовање након тренинга**: Почните са овим за брзу имплементацију
- **Калибрациони скуп података**: Користите репрезентативне податке који покривају све варијације улаза
- **Мешовита прецизност**: Користите INT8 за већину слојева, а критичне слојеве оставите у вишој прецизности

### 3. Смјернице за избор позадине
- **NPU (HTP)**: Најбоље за CNN радне оптерећења, квантоване моделе и апликације осетљиве на потрошњу енергије
- **GPU**: Оптимално за операције које захтевају високе рачунарске ресурсе, веће моделе и FP16 прецизност
- **CPU**: Резервна опција за неподржане операције и отклањање грешака

### 4. Оптимизација перформанси
- **Величина серије**: Користите величину серије 1 за апликације у реалном времену, веће серије за пропусност
- **Предобрада улаза**: Минимизирајте копирање података и трошкове конверзије
- **Поновна употреба контекста**: Прекомпајлирајте контексте да бисте избегли трошкове компајлирања у реалном времену

### 5. Управљање меморијом
- **Додела тензора**: Користите статичку доделу кад год је могуће да бисте избегли трошкове у реалном времену
- **Меморијски базени**: Имплементирајте прилагођене меморијске базене за често додељиване тензоре
- **Поновна употреба бафера**: Поново користите улазне/излазне бафере током позива инференције

### 6. Оптимизација потрошње енергије
- **Модови перформанси**: Користите одговарајуће модове перформанси на основу термалних ограничења
- **Динамичко скалирање фреквенције**: Дозволите систему да скалира фреквенцију на основу оптерећења
- **Управљање стањем мировања**: Правилно ослободите ресурсе када нису у употреби

## Решавање проблема

### Уобичајени проблеми

#### 1. Проблеми са инсталацијом SDK-а
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Грешке у конверзији модела
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Проблеми са квантовањем
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Проблеми са перформансама
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Проблеми са меморијом
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Компатибилност позадине
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Дебаговање перформанси

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Добијање помоћи

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN документација**: Доступна у SDK пакету
- **Форуми заједнице**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Техничка подршка**: Преко Qualcomm портала за програмере

## Додатни ресурси

### Званични линкови
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon платформе**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Портал за програмере**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Ресурси за учење
- **Водич за почетак**: Доступан у QNN SDK документацији
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Водич за оптимизацију**: SDK документација укључује свеобухватне смернице за оптимизацију
- **Видео туторијали**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Алатке за интеграцију
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Унапред оптимизовани модели за Qualcomm хардвер
- **Android Neural Networks API**: Интеграција са Android NNAPI
- **TensorFlow Lite Delegate**: Qualcomm делегат за TFLite

### Бенчмаркови перформанси
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Примери из заједнице
- **Пример апликација**: Доступно у QNN SDK директоријуму примера
- **GitHub репозиторијуми**: Примери и алатке које је допринела заједница
- **Технички блогови**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Повезане алатке
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Напредне технике квантовања и компресије
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - За поређење и резервну имплементацију
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Мултиплатформски инференцијски мотор

### Спецификације хардвера
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon платформе**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Шта је следеће

Наставите своје Edge AI путовање истражујући [Модул 5: SLMOps и имплементација у производњи](../Module05/README.md) да бисте научили о оперативним аспектима управљања животним циклусом малих језичких модела.

---

**Одрицање од одговорности**:  
Овај документ је преведен помоћу услуге за превођење уз помоћ вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако настојимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.