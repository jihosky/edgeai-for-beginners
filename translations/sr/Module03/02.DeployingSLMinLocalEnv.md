<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T10:01:51+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "sr"
}
-->
# Одељак 2: Развој локалног окружења - решења која првенствено чувају приватност

Локално постављање малих језичких модела (SLM) представља промену парадигме ка решењима за вештачку интелигенцију која чувају приватност и смањују трошкове. Овај свеобухватни водич истражује два моћна оквира—Ollama и Microsoft Foundry Local—који омогућавају програмерима да искористе пуни потенцијал SLM-а уз потпуну контролу над окружењем за постављање.

## Увод

У овом лекцији истражићемо напредне стратегије за постављање малих језичких модела у локалним окружењима. Покрићемо основне концепте локалног постављања вештачке интелигенције, анализирати два водећа платформе (Ollama и Microsoft Foundry Local) и пружити практичне смернице за имплементацију решења спремних за производњу.

## Циљеви учења

На крају ове лекције, моћи ћете да:

- Разумете архитектуру и предности оквира за локално постављање SLM-а.
- Имплементирате решења спремна за производњу користећи Ollama и Microsoft Foundry Local.
- Упоредите и изаберете одговарајућу платформу на основу специфичних захтева и ограничења.
- Оптимизујете локална постављања за перформансе, безбедност и скалабилност.

## Разумевање архитектура за локално постављање SLM-а

Локално постављање SLM-а представља фундаменталну промену од услуга вештачке интелигенције зависних од облака ка решењима која чувају приватност и раде на лицу места. Овај приступ омогућава организацијама да задрже потпуну контролу над својом инфраструктуром за вештачку интелигенцију уз обезбеђивање суверенитета података и оперативне независности.

### Класификације оквира за постављање

Разумевање различитих приступа постављању помаже у избору праве стратегије за специфичне случајеве употребе:

- **Фокусирано на развој**: Поједностављено подешавање за експериментисање и прототипирање
- **За предузећа**: Решeња спремна за производњу са могућностима интеграције у предузећима  
- **Крос-платформско**: Универзална компатибилност на различитим оперативним системима и хардверу

### Кључне предности локалног постављања SLM-а

Локално постављање SLM-а нуди неколико основних предности које га чине идеалним за апликације осетљиве на приватност и предузећа:

**Приватност и безбедност**: Локална обрада осигурава да осетљиви подаци никада не напуштају инфраструктуру организације, омогућавајући усклађеност са GDPR, HIPAA и другим регулаторним захтевима. Постављања без интернет везе су могућа за класификована окружења, док комплетни записи омогућавају надзор безбедности.

**Исплативост**: Елиминација модела наплате по токену значајно смањује оперативне трошкове. Мањи захтеви за пропусним опсегом и смањена зависност од облака пружају предвидљиве структуре трошкова за буџетирање у предузећима.

**Перформансе и поузданост**: Брже време извршавања без кашњења у мрежи омогућава апликације у реалном времену. Функционалност ван мреже осигурава континуиран рад без обзира на интернет конекцију, док оптимизација локалних ресурса пружа конзистентне перформансе.

## Ollama: Универзална платформа за локално постављање

### Основна архитектура и филозофија

Ollama је дизајнирана као универзална, програмерски пријатељска платформа која демократизује локално постављање LLM-а на различитим хардверским конфигурацијама и оперативним системима.

**Техничка основа**: Изграђена на робусном оквиру llama.cpp, Ollama користи ефикасан GGUF формат модела за оптималне перформансе. Крос-платформска компатибилност осигурава конзистентно понашање на Windows, macOS и Linux окружењима, док интелигентно управљање ресурсима оптимизује употребу CPU-а, GPU-а и меморије.

**Филозофија дизајна**: Ollama приоритетно поставља једноставност без жртвовања функционалности, нудећи постављање без конфигурације за тренутну продуктивност. Платформа одржава широку компатибилност модела уз пружање конзистентних API-ја за различите архитектуре модела.

### Напредне функције и могућности

**Изврсност у управљању моделима**: Ollama пружа свеобухватно управљање животним циклусом модела са аутоматским преузимањем, кеширањем и верзионисањем. Платформа подржава обиман екосистем модела укључујући Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral и специјализоване моделе за уграђивање.

**Прилагођавање кроз Modelfiles**: Напредни корисници могу креирати прилагођене конфигурације модела са специфичним параметрима, системским упутствима и модификацијама понашања. Ово омогућава оптимизације специфичне за домен и захтеве за специјализоване апликације.

**Оптимизација перформанси**: Ollama аутоматски открива и користи доступно хардверско убрзање укључујући NVIDIA CUDA, Apple Metal и OpenCL. Интелигентно управљање меморијом осигурава оптималну употребу ресурса на различитим хардверским конфигурацијама.

### Стратегије имплементације у производњи

**Инсталација и подешавање**: Ollama пружа поједностављену инсталацију на платформама кроз домаће инсталере, менаџере пакета (WinGet, Homebrew, APT) и Docker контејнере за контејнеризована постављања.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Основне команде и операције**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Напредна конфигурација**: Modelfiles омогућавају софистицирано прилагођавање за захтеве предузећа:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Примери интеграције за програмере

**Интеграција Python API-ја**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Интеграција JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Коришћење RESTful API-ја са cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Подешавање и оптимизација перформанси

**Конфигурација меморије и нити**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Избор квантизације за различит хардвер**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Платформа за AI на ивици за предузећа

### Архитектура за предузећа

Microsoft Foundry Local представља свеобухватно решење за предузећа, посебно дизајнирано за производна постављања AI-а на ивици са дубоком интеграцијом у Microsoft екосистем.

**Основа заснована на ONNX-у**: Изграђена на индустријском стандарду ONNX Runtime, Foundry Local пружа оптимизоване перформансе на различитим хардверским архитектурама. Платформа користи интеграцију Windows ML-а за оптимизацију на Windows-у уз одржавање крос-платформске компатибилности.

**Изврсност у хардверском убрзању**: Foundry Local карактерише интелигентно откривање хардвера и оптимизација на CPU-има, GPU-има и NPU-има. Дубока сарадња са произвођачима хардвера (AMD, Intel, NVIDIA, Qualcomm) осигурава оптималне перформансе на хардверским конфигурацијама за предузећа.

### Напредно искуство за програмере

**Приступ преко више интерфејса**: Foundry Local пружа свеобухватне интерфејсе за развој, укључујући моћан CLI за управљање моделима и постављање, SDK-ове за више језика (Python, NodeJS) за домаћу интеграцију и RESTful API-је са компатибилношћу са OpenAI-јем за беспрекорну миграцију.

**Интеграција са Visual Studio-ом**: Платформа се беспрекорно интегрише са AI Toolkit-ом за VS Code, пружајући алате за конверзију модела, квантизацију и оптимизацију унутар окружења за развој. Ова интеграција убрзава радне токове развоја и смањује сложеност постављања.

**Платформа за оптимизацију модела**: Интеграција Microsoft Olive-а омогућава софистициране радне токове оптимизације модела, укључујући динамичку квантизацију, оптимизацију графа и подешавање специфично за хардвер. Могућности конверзије засноване на облаку кроз Azure ML пружају скалабилну оптимизацију за велике моделе.

### Стратегије имплементације у производњи

**Инсталација и конфигурација**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Операције управљања моделима**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Напредна конфигурација постављања**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Интеграција у екосистем предузећа

**Безбедност и усклађеност**: Foundry Local пружа функције безбедности на нивоу предузећа, укључујући контролу приступа засновану на улогама, евиденцију активности, извештавање о усклађености и шифровану складиштење модела. Интеграција са Microsoft инфраструктуром за безбедност осигурава поштовање политика безбедности предузећа.

**Уграђене AI услуге**: Платформа нуди готове AI могућности, укључујући Phi Silica за локалну обраду језика, AI Imaging за побољшање и анализу слика и специјализоване API-је за уобичајене AI задатке у предузећима.

## Упоредна анализа: Ollama vs Foundry Local

### Упоредна анализа техничке архитектуре

| **Аспект** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Формат модела** | GGUF (преко llama.cpp) | ONNX (преко ONNX Runtime) |
| **Фокус платформе** | Универзална крос-платформска | Оптимизација за Windows/предузећа |
| **Интеграција хардвера** | Општа подршка за GPU/CPU | Дубока подршка за Windows ML, NPU |
| **Оптимизација** | Квантизација преко llama.cpp | Microsoft Olive + ONNX Runtime |
| **Функције за предузећа** | Вођено заједницом | На нивоу предузећа са SLA-овима |

### Карактеристике перформанси

**Предности Ollama перформанси**:
- Изузетне перформансе CPU-а кроз оптимизацију llama.cpp
- Конзистентно понашање на различитим платформама и хардверу
- Ефикасно коришћење меморије уз интелигентно учитавање модела
- Брзо време покретања за сценарије развоја и тестирања

**Предности Foundry Local перформанси**:
- Супериорна употреба NPU-а на модерном Windows хардверу
- Оптимизовано GPU убрзање кроз партнерства са произвођачима
- Надзор перформанси на нивоу предузећа и оптимизација
- Скалабилне могућности постављања за производна окружења

### Анализа искуства за програмере

**Искуство програмера са Ollama**:
- Минимални захтеви за подешавање уз тренутну продуктивност
- Интуитиван интерфејс командне линије за све операције
- Обимна подршка заједнице и документација
- Флексибилно прилагођавање кроз Modelfiles

**Искуство програмера са Foundry Local**:
- Свеобухватна интеграција IDE-а са Visual Studio екосистемом
- Радни токови развоја за предузећа са функцијама за сарадњу тимова
- Професионални канали подршке уз Microsoft подршку
- Напредни алати за дебаговање и оптимизацију

### Оптимизација случајева употребе

**Изаберите Ollama када**:
- Развијате крос-платформске апликације које захтевају конзистентно понашање
- Приоритет стављате на транспарентност отвореног кода и доприносе заједнице
- Радите са ограниченим ресурсима или буџетским ограничењима
- Градите експерименталне или апликације фокусиране на истраживање
- Захтевате широку компатибилност модела на различитим архитектурама

**Изаберите Foundry Local када**:
- Постављате апликације за предузећа са строгим захтевима за перформансе
- Искористите оптимизације хардвера специфичне за Windows (NPU, Windows ML)
- Захтевате подршку за предузећа, SLA-ове и функције усклађености
- Градите производне апликације са интеграцијом у Microsoft екосистем
- Потребни су вам напредни алати за оптимизацију и професионални радни токови развоја

## Напредне стратегије постављања

### Шаблони за контејнеризовано постављање

**Контејнеризација Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Постављање Foundry Local за предузећа**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Технике оптимизације перформанси

**Стратегије оптимизације Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Оптимизација Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Разматрања о безбедности и усклађености

### Имплементација безбедности у предузећима

**Најбоље праксе за безбедност Ollama**:
- Изолација мреже уз правила заштитног зида и VPN приступ
- Аутентификација кроз интеграцију реверзног проксија
- Верификација интегритета модела и сигурна дистрибуција модела
- Евиденција активности за API приступ и операције модела

**Безбедност Foundry Local за предузећа**:
- Уграђена контрола приступа заснована на улогама уз интеграцију Active Directory-а
- Свеобухватни записи активности уз извештавање о усклађености
- Шифрована складиштење модела и сигурно постављање модела
- Интеграција са Microsoft инфраструктуром за безбедност

### Усклађеност и регулаторни захтеви

Обе платформе подржавају усклађеност са регулативама кроз:
- Контроле резиденције података које осигуравају локалну обраду
- Евиденцију активности за захтеве извештавања о регулативама
- Контроле приступа за руковање осетљивим подацима
- Шифровање у мировању и током преноса ради заштите података

## Најбоље праксе за постављање у производњи

### Надзор и посматрање

**Кључне метрике за надзор**:
- К

---

**Одрицање од одговорности**:  
Овај документ је преведен помоћу услуге за превођење вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако настојимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати меродавним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.