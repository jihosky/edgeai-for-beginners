<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T10:21:51+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "lt"
}
-->
# 1 skyrius: EdgeAI pagrindai

EdgeAI reiškia paradigmos pokytį dirbtinio intelekto diegime, perkeliant AI galimybes tiesiai į kraštinius įrenginius, o ne pasikliaujant vien debesų pagrindu veikiančiu apdorojimu. Svarbu suprasti, kaip EdgeAI leidžia vietinį AI apdorojimą ribotų resursų įrenginiuose, išlaikant tinkamą našumą ir sprendžiant tokias problemas kaip privatumas, delsimas ir veikimas neprisijungus.

## Įvadas

Šioje pamokoje nagrinėsime EdgeAI ir jo pagrindines sąvokas. Aptarsime tradicinę AI skaičiavimo paradigmą, kraštinio skaičiavimo iššūkius, pagrindines technologijas, leidžiančias EdgeAI, ir praktines taikymo sritis įvairiose pramonės šakose.

## Mokymosi tikslai

Pamokos pabaigoje galėsite:

- Suprasti skirtumą tarp tradicinio debesų pagrindu veikiančio AI ir EdgeAI požiūrių.
- Identifikuoti pagrindines technologijas, leidžiančias AI apdorojimą kraštiniuose įrenginiuose.
- Atpažinti EdgeAI įgyvendinimo privalumus ir apribojimus.
- Pritaikyti EdgeAI žinias realioms situacijoms ir naudojimo atvejams.

## Tradicinės AI skaičiavimo paradigmos supratimas

Tradiciškai generatyvios AI programos remiasi aukštos našumo skaičiavimo infrastruktūra, kad efektyviai vykdytų didelius kalbos modelius (LLMs). Organizacijos paprastai diegia šiuos modelius GPU klasteriuose debesų aplinkoje, pasiekdamos jų galimybes per API sąsajas.

Šis centralizuotas modelis gerai veikia daugelyje programų, tačiau turi esminių apribojimų kraštinio skaičiavimo scenarijuose. Tradicinis požiūris apima vartotojo užklausų siuntimą į nuotolinius serverius, jų apdorojimą naudojant galingą aparatinę įrangą ir rezultatų grąžinimą internetu. Nors šis metodas suteikia prieigą prie pažangiausių modelių, jis sukuria priklausomybę nuo interneto ryšio, sukelia delsimo problemas ir kelia privatumo klausimus, kai jautri informacija turi būti perduodama išoriniams serveriams.

Yra keletas pagrindinių sąvokų, kurias reikia suprasti dirbant su tradicinėmis AI skaičiavimo paradigmomis, būtent:

- **☁️ Debesų pagrindu veikiančio apdorojimo**: AI modeliai veikia galingoje serverių infrastruktūroje su dideliais skaičiavimo resursais.
- **🔌 API pagrindu veikiančio prieigos**: Programos pasiekia AI galimybes per nuotolinius API skambučius, o ne vietinį apdorojimą.
- **🎛️ Centralizuotas modelių valdymas**: Modeliai yra centralizuotai prižiūrimi ir atnaujinami, užtikrinant nuoseklumą, bet reikalaujant tinklo ryšio.
- **📈 Resursų mastelio keitimas**: Debesų infrastruktūra gali dinamiškai prisitaikyti prie kintančių skaičiavimo poreikių.

## Kraštinio skaičiavimo iššūkiai

Kraštiniai įrenginiai, tokie kaip nešiojamieji kompiuteriai, mobilieji telefonai ir daiktų interneto (IoT) įrenginiai, pvz., Raspberry Pi ir NVIDIA Orin Nano, turi unikalius skaičiavimo apribojimus. Šie įrenginiai paprastai turi mažesnę apdorojimo galią, atmintį ir energijos resursus, palyginti su duomenų centrų infrastruktūra.

Tradiciškai LLM vykdymas tokiuose įrenginiuose buvo sudėtingas dėl šių aparatinės įrangos apribojimų. Tačiau kraštinio AI apdorojimo poreikis tampa vis svarbesnis įvairiose situacijose. Apsvarstykite situacijas, kai interneto ryšys yra nepatikimas arba nepasiekiamas, pvz., atokiose pramonės vietose, transporto priemonėse kelionės metu arba vietovėse su prastu tinklo aprėptimi. Be to, programos, kurioms reikalingi aukšti saugumo standartai, pvz., medicinos prietaisai, finansų sistemos ar vyriausybės programos, gali reikalauti vietinio jautrios informacijos apdorojimo, kad būtų užtikrintas privatumas ir atitiktis.

### Pagrindiniai kraštinio skaičiavimo apribojimai

Kraštinio skaičiavimo aplinkos susiduria su keliais pagrindiniais apribojimais, kurių tradiciniai debesų pagrindu veikiantys AI sprendimai nepatiria:

- **Ribota apdorojimo galia**: Kraštiniai įrenginiai paprastai turi mažiau CPU branduolių ir mažesnį taktų dažnį, palyginti su serverio klasės aparatine įranga.
- **Atminties apribojimai**: RAM ir saugojimo talpa kraštiniuose įrenginiuose yra žymiai mažesnė.
- **Energijos apribojimai**: Baterijomis maitinami įrenginiai turi subalansuoti našumą su energijos suvartojimu, kad veiktų ilgiau.
- **Šilumos valdymas**: Kompaktiški formos faktoriai riboja aušinimo galimybes, paveikdami ilgalaikį našumą esant apkrovai.

## Kas yra EdgeAI?

### Sąvoka: EdgeAI apibrėžimas

EdgeAI reiškia dirbtinio intelekto algoritmų diegimą ir vykdymą tiesiai kraštiniuose įrenginiuose – fizinėje aparatinėje įrangoje, esančioje tinklo „krašte“, arti vietos, kur generuojami ir renkami duomenys. Šie įrenginiai apima išmaniuosius telefonus, IoT jutiklius, išmaniąsias kameras, autonomines transporto priemones, nešiojamus prietaisus ir pramoninę įrangą. Skirtingai nuo tradicinių AI sistemų, kurios remiasi debesų serveriais apdorojimui, EdgeAI perkelia intelektą tiesiai į duomenų šaltinį.

EdgeAI esmė yra AI apdorojimo decentralizavimas, perkeliant jį iš centralizuotų duomenų centrų ir paskirstant per plačią įrenginių tinklą, sudarantį mūsų skaitmeninę ekosistemą. Tai reiškia esminį architektūrinį pokytį, kaip AI sistemos yra kuriamos ir diegiamos.

Pagrindiniai EdgeAI konceptualūs principai apima:

- **Artumo apdorojimas**: Skaičiavimas vyksta fiziškai arti vietos, kur generuojami duomenys.
- **Decentralizuotas intelektas**: Sprendimų priėmimo galimybės paskirstomos per kelis įrenginius.
- **Duomenų suverenitetas**: Informacija lieka vietinėje kontrolėje, dažnai niekada nepaliekant įrenginio.
- **Autonominis veikimas**: Įrenginiai gali veikti protingai be nuolatinio ryšio.
- **Įterptinis AI**: Intelektas tampa įprasta kasdieninių įrenginių savybe.

### EdgeAI architektūros vizualizacija

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI reiškia paradigmos pokytį dirbtinio intelekto diegime, perkeliant AI galimybes tiesiai į kraštinius įrenginius, o ne pasikliaujant vien debesų pagrindu veikiančiu apdorojimu. Šis požiūris leidžia AI modeliams veikti vietoje įrenginiuose su ribotais skaičiavimo resursais, suteikiant realaus laiko išvadų galimybes be nuolatinio interneto ryšio.

EdgeAI apima įvairias technologijas ir metodus, skirtus AI modeliams padaryti efektyvesnius ir tinkamus diegti ribotų resursų įrenginiuose. Tikslas yra išlaikyti tinkamą našumą, tuo pačiu žymiai sumažinant AI modelių skaičiavimo ir atminties reikalavimus.

Pažvelkime į pagrindinius požiūrius, leidžiančius EdgeAI diegimą įvairiuose įrenginiuose ir naudojimo atvejais.

### Pagrindiniai EdgeAI principai

EdgeAI yra pagrįstas keliais pagrindiniais principais, kurie jį išskiria iš tradicinio debesų pagrindu veikiančio AI:

- **Vietinis apdorojimas**: AI išvados vyksta tiesiai kraštiniame įrenginyje, nereikalaujant išorinio ryšio.
- **Resursų optimizavimas**: Modeliai yra specialiai optimizuoti pagal tikslinių įrenginių aparatinės įrangos apribojimus.
- **Realaus laiko našumas**: Apdorojimas vyksta su minimaliu delsimu, skirtas laiko atžvilgiu jautrioms programoms.
- **Privatumas pagal dizainą**: Jautrūs duomenys lieka įrenginyje, didinant saugumą ir atitiktį.

## Pagrindinės technologijos, leidžiančios EdgeAI

### Modelio kvantavimas

Viena iš svarbiausių EdgeAI technikų yra modelio kvantavimas. Šis procesas apima modelio parametrų tikslumo sumažinimą, paprastai nuo 32 bitų slankiojo kablelio skaičių iki 8 bitų sveikųjų skaičių arba dar mažesnio tikslumo formatų. Nors šis tikslumo sumažinimas gali atrodyti nerimą keliantis, tyrimai parodė, kad daugelis AI modelių gali išlaikyti savo našumą net ir esant žymiai sumažintam tikslumui.

Kvantavimas veikia, susiedamas slankiojo kablelio reikšmių diapazoną su mažesniu diskrečių reikšmių rinkiniu. Pavyzdžiui, vietoj 32 bitų naudojimo kiekvienam parametrui, kvantavimas gali naudoti tik 8 bitus, dėl to sumažėja atminties reikalavimai 4 kartus ir dažnai pasiekiamas greitesnis išvadų laikas.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Skirtingos kvantavimo technikos apima:

- **Po treniravimo kvantavimas (PTQ)**: Taikomas po modelio treniravimo, nereikalaujant pakartotinio treniravimo.
- **Kvantavimo suvokimo treniravimas (QAT)**: Įtraukia kvantavimo efektus treniravimo metu, siekiant geresnio tikslumo.
- **Dinaminis kvantavimas**: Kvantuoja svorius į int8, bet aktyvacijas skaičiuoja dinamiškai.
- **Statinis kvantavimas**: Iš anksto apskaičiuoja visus kvantavimo parametrus tiek svoriams, tiek aktyvacijoms.

EdgeAI diegimui tinkamos kvantavimo strategijos pasirinkimas priklauso nuo konkrečios modelio architektūros, našumo reikalavimų ir tikslinio įrenginio aparatinės įrangos galimybių.

### Modelio suspaudimas ir optimizavimas

Be kvantavimo, įvairios suspaudimo technikos padeda sumažinti modelio dydį ir skaičiavimo reikalavimus. Tai apima:

**Pruning**: Ši technika pašalina nereikalingus ryšius arba neuronus iš neuroninių tinklų. Identifikuojant ir pašalinant parametrus, kurie mažai prisideda prie modelio našumo, pruning gali žymiai sumažinti modelio dydį, išlaikant tikslumą.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Žinių distiliacija**: Šis metodas apima mažesnio „studento“ modelio treniravimą, kad jis imituotų didesnio „mokytojo“ modelio elgesį. Studentas mokosi apytiksliai atkartoti mokytojo išvestis, dažnai pasiekdamas panašų našumą su žymiai mažesniais parametrais.

**Modelio architektūros optimizavimas**: Tyrėjai sukūrė specializuotas architektūras, skirtas kraštiniam diegimui, tokias kaip MobileNets, EfficientNets ir kitas lengvas architektūras, kurios subalansuoja našumą su skaičiavimo efektyvumu.

### Maži kalbos modeliai (SLMs)

Kylanti EdgeAI tendencija yra mažų kalbos modelių (SLMs) kūrimas. Šie modeliai yra sukurti nuo pat pradžių, kad būtų kompaktiški ir efektyvūs, tuo pačiu teikiant reikšmingas natūralios kalbos galimybes. SLMs tai pasiekia per kruopščius architektūrinius sprendimus, efektyvias treniravimo technikas ir orientuotą treniravimą konkrečiose srityse ar užduotyse.

Skirtingai nuo tradicinių metodų, kurie apima didelių modelių suspaudimą, SLMs dažnai treniruojami su mažesniais duomenų rinkiniais ir optimizuotomis architektūromis, specialiai sukurtomis kraštiniam diegimui. Šis požiūris gali sukurti modelius, kurie yra ne tik mažesni, bet ir efektyvesni konkretiems naudojimo atvejams.

## Aparatinės įrangos pagreitinimas EdgeAI

Šiuolaikiniai kraštiniai įrenginiai vis dažniau apima specializuotą aparatinę įrangą, skirtą AI darbo krūviams pagreitinti:

### Neuronų apdorojimo vienetai (NPUs)

NPUs yra specializuoti procesoriai, skirti neuroninių tinklų skaičiavimams. Šie lustai gali vykdyti AI išvadų užduotis daug efektyviau nei tradiciniai CPU, dažnai su mažesniu energijos suvartojimu. Daugelis šiuolaikinių išmaniųjų telefonų, nešiojamųjų kompiuterių ir IoT įrenginių dabar apima NPUs, kad būtų galima vykdyti AI apdorojimą įrenginyje.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Įrenginiai su NPUs apima:

- **Apple**: A serijos ir M serijos lustai su Neural Engine
- **Qualcomm**: Snapdragon procesoriai su Hexagon DSP/NPU
- **Samsung**: Exynos procesoriai su NPU
- **Intel**: Movidius VPUs ir Habana Labs pagreitinimo įrenginiai
- **Microsoft**: Windows Copilot+ kompiuteriai su NPUs

### 🎮 GPU pagreitinimas

Nors kraštiniai įrenginiai gali neturėti galingų GPU, esančių duomenų centruose, daugelis vis tiek apima integruotus arba atskirus GPU, kurie gali pagreitinti AI darbo krūvius. Šiuolaikiniai mobilieji GPU ir integruoti grafikos procesoriai gali suteikti reikšmingą našumo pagerėjimą AI išvadų užduotims.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU optimizavimas

Net CPU tik įrenginiai gali pasinaudoti EdgeAI per optimizuotas įgyvendinimo strategijas. Šiuolaikiniai CPU apima specializuotas instrukcijas AI darbo krūviams, o programinės įrangos sistemos buvo sukurtos maksimaliai išnaudoti CPU našumą AI išvadoms.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Programinės įrangos inžinieriams, dirbantiems su EdgeAI, supratimas, kaip pasinaudoti šiais aparatinės įrangos pagreitinimo pasirinkimais, yra labai svarbus optimizuojant išvadų našumą ir energijos efektyvumą tiksliniuose įrenginiuose.

## EdgeAI privalumai

### Privatumas ir saugumas

Vienas iš didžiausių EdgeAI privalumų yra sustiprintas privatumas ir saugumas. Apdorojant duomenis vietoje įrenginyje, jautri informacija niekada nepalieka vartotojo kontrolės. Tai ypač svarbu programoms, tvarkančioms asmeninius duomenis, medicininę informaciją ar konfidencialius verslo duomenis.

### Sumažintas delsimas

EdgeAI pašalina poreikį siųsti duomenis į nuotolinius serverius apdorojimui, žymiai sumažindamas delsą. Tai labai svarbu realaus laiko programoms, tokioms kaip autonominės transporto priemonės, pramoninė automatizacija ar interaktyvios programos, kuriose reikalingi greiti atsakymai.

### Veikimas neprisijungus

EdgeAI leidžia AI funkcionalumą net ir tada, kai interneto ryšys yra nepasiekiamas. Tai vertinga programoms atokiose vietose, kelionės metu arba situacijose
- [02: EdgeAI Pritaikymas](02.RealWorldCaseStudies.md)

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors stengiamės užtikrinti tikslumą, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Kritinei informacijai rekomenduojama naudoti profesionalų žmogaus vertimą. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus interpretavimus, atsiradusius dėl šio vertimo naudojimo.