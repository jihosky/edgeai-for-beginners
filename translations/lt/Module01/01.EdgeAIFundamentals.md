<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T10:05:17+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "lt"
}
-->
# 1 skyrius: EdgeAI pagrindai

EdgeAI reiškia paradigmos pokytį dirbtinio intelekto diegime, kai DI galimybės perkeliamos tiesiai į kraštinius įrenginius, o ne remiamasi tik debesų kompiuterija. Svarbu suprasti, kaip EdgeAI leidžia vietinį DI apdorojimą ribotų resursų įrenginiuose, išlaikant tinkamą našumą ir sprendžiant tokias problemas kaip privatumas, delsimas ir veikimas neprisijungus.

## Įvadas

Šioje pamokoje nagrinėsime EdgeAI ir jo pagrindines sąvokas. Aptarsime tradicinę DI kompiuterijos paradigmą, kraštinės kompiuterijos iššūkius, pagrindines technologijas, leidžiančias EdgeAI, ir praktines taikymo sritis įvairiose pramonės šakose.

## Mokymosi tikslai

Pamokos pabaigoje galėsite:

- Suprasti skirtumą tarp tradicinio debesų pagrindu veikiančio DI ir EdgeAI metodų.
- Identifikuoti pagrindines technologijas, leidžiančias DI apdorojimą kraštiniuose įrenginiuose.
- Atpažinti EdgeAI įgyvendinimo privalumus ir apribojimus.
- Taikyti EdgeAI žinias realioms situacijoms ir naudojimo atvejams.

## Tradicinės DI kompiuterijos paradigmos supratimas

Tradiciškai generatyvios DI programos remiasi aukštos našumo kompiuterijos infrastruktūra, kad efektyviai vykdytų didelius kalbos modelius (LLMs). Organizacijos paprastai diegia šiuos modelius GPU klasteriuose debesų aplinkoje, pasiekdamos jų galimybes per API sąsajas.

Šis centralizuotas modelis gerai veikia daugelyje programų, tačiau turi esminių apribojimų kraštinės kompiuterijos scenarijuose. Tradicinis metodas apima vartotojo užklausų siuntimą į nuotolinius serverius, jų apdorojimą naudojant galingą aparatinę įrangą ir rezultatų grąžinimą internetu. Nors šis metodas suteikia prieigą prie pažangiausių modelių, jis sukuria priklausomybę nuo interneto ryšio, sukelia delsimo problemų ir kelia privatumo klausimų, kai jautrūs duomenys turi būti perduodami išoriniams serveriams.

Yra keletas pagrindinių sąvokų, kurias reikia suprasti dirbant su tradicinėmis DI kompiuterijos paradigmomis, būtent:

- **☁️ Debesų pagrindu veikiantis apdorojimas**: DI modeliai veikia galingoje serverių infrastruktūroje su dideliais skaičiavimo resursais.
- **🔌 API pagrindu veikianti prieiga**: Programos pasiekia DI galimybes per nuotolinius API iškvietimus, o ne vietinį apdorojimą.
- **🎛️ Centralizuotas modelių valdymas**: Modeliai yra palaikomi ir atnaujinami centralizuotai, užtikrinant nuoseklumą, bet reikalaujant tinklo ryšio.
- **📈 Resursų skalavimas**: Debesų infrastruktūra gali dinamiškai prisitaikyti prie kintančių skaičiavimo poreikių.

## Kraštinės kompiuterijos iššūkiai

Kraštiniai įrenginiai, tokie kaip nešiojamieji kompiuteriai, mobilieji telefonai ir daiktų interneto (IoT) įrenginiai, pvz., Raspberry Pi ir NVIDIA Orin Nano, turi unikalius skaičiavimo apribojimus. Šie įrenginiai paprastai turi mažesnę apdorojimo galią, atmintį ir energijos išteklius, palyginti su duomenų centrų infrastruktūra.

Tradiciškai vykdyti didelius kalbos modelius tokiuose įrenginiuose buvo sudėtinga dėl šių techninės įrangos apribojimų. Tačiau kraštinės DI apdorojimo poreikis tampa vis svarbesnis įvairiose situacijose. Pavyzdžiui, kai interneto ryšys yra nepatikimas arba nepasiekiamas, kaip nuotolinėse pramonės vietose, transporto priemonėse kelionės metu ar vietovėse su prastu tinklo aprėptimi. Be to, programos, kurioms reikalingi aukšti saugumo standartai, tokios kaip medicinos prietaisai, finansų sistemos ar vyriausybinės programos, gali reikalauti vietinio jautrių duomenų apdorojimo, kad būtų užtikrintas privatumas ir atitiktis reikalavimams.

### Pagrindiniai kraštinės kompiuterijos apribojimai

Kraštinės kompiuterijos aplinkos susiduria su keliais esminiais apribojimais, kurių tradicinės debesų pagrindu veikiančios DI sprendimai nepatiria:

- **Ribota apdorojimo galia**: Kraštiniai įrenginiai paprastai turi mažiau CPU branduolių ir mažesnį taktų dažnį, palyginti su serverio klasės aparatine įranga.
- **Atminties apribojimai**: RAM ir saugojimo talpa kraštiniuose įrenginiuose yra žymiai mažesnė.
- **Energijos apribojimai**: Baterijomis maitinami įrenginiai turi subalansuoti našumą ir energijos suvartojimą, kad veiktų ilgą laiką.
- **Šiluminis valdymas**: Kompaktiški formos faktoriai riboja aušinimo galimybes, paveikdami ilgalaikį našumą esant apkrovai.

## Kas yra EdgeAI?

### Sąvoka: EdgeAI apibrėžimas

EdgeAI reiškia dirbtinio intelekto algoritmų diegimą ir vykdymą tiesiogiai kraštiniuose įrenginiuose – fizinėje aparatinėje įrangoje, esančioje „tinklo krašte“, arti duomenų generavimo ir rinkimo vietos. Šie įrenginiai apima išmaniuosius telefonus, IoT jutiklius, išmaniąsias kameras, autonomines transporto priemones, nešiojamus įrenginius ir pramoninę įrangą. Skirtingai nuo tradicinių DI sistemų, kurios remiasi debesų serveriais apdorojimui, EdgeAI perkelia intelektą tiesiai į duomenų šaltinį.

EdgeAI esmė yra DI apdorojimo decentralizavimas, perkeliant jį iš centralizuotų duomenų centrų ir paskirstant per plačią įrenginių tinklą, sudarantį mūsų skaitmeninę ekosistemą. Tai reiškia esminį architektūrinį pokytį, kaip kuriamos ir diegiamos DI sistemos.

Pagrindiniai EdgeAI konceptualūs principai apima:

- **Artumo apdorojimas**: Skaičiavimai vyksta fiziškai arti duomenų kilmės vietos.
- **Decentralizuotas intelektas**: Sprendimų priėmimo galimybės paskirstomos per kelis įrenginius.
- **Duomenų suverenitetas**: Informacija lieka vietinėje kontrolėje, dažnai niekada nepaliekant įrenginio.
- **Autonominis veikimas**: Įrenginiai gali veikti protingai be nuolatinio ryšio.
- **Įterptinis DI**: Intelektas tampa įprastų įrenginių esmine savybe.

### EdgeAI architektūros vizualizacija

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI reiškia paradigmos pokytį dirbtinio intelekto diegime, perkeliant DI galimybes tiesiai į kraštinius įrenginius, o ne remiantis tik debesų kompiuterija. Šis metodas leidžia DI modeliams veikti vietoje įrenginiuose su ribotais skaičiavimo resursais, suteikiant realaus laiko išvadų galimybes be nuolatinio interneto ryšio.

EdgeAI apima įvairias technologijas ir metodus, skirtus DI modeliams padaryti efektyvesnius ir tinkamus diegimui ribotų resursų įrenginiuose. Tikslas yra išlaikyti tinkamą našumą, tuo pačiu žymiai sumažinant DI modelių skaičiavimo ir atminties reikalavimus.

Pažvelkime į pagrindinius metodus, leidžiančius EdgeAI įgyvendinimą įvairiuose įrenginiuose ir naudojimo atvejais.

### Pagrindiniai EdgeAI principai

EdgeAI yra pagrįstas keliais pagrindiniais principais, kurie jį skiria nuo tradicinio debesų pagrindu veikiančio DI:

- **Vietinis apdorojimas**: DI išvados vyksta tiesiogiai kraštiniame įrenginyje be išorinio ryšio poreikio.
- **Resursų optimizavimas**: Modeliai yra specialiai optimizuoti pagal tikslinių įrenginių techninės įrangos apribojimus.
- **Realus laikas**: Apdorojimas vyksta su minimaliu delsimu laiko jautrioms programoms.
- **Privatumas pagal dizainą**: Jautrūs duomenys lieka įrenginyje, didinant saugumą ir atitiktį.

## Pagrindinės technologijos, leidžiančios EdgeAI

### Modelio kvantavimas

Viena svarbiausių EdgeAI technikų yra modelio kvantavimas. Šis procesas apima modelio parametrų tikslumo sumažinimą, paprastai nuo 32 bitų slankiojo kablelio skaičių iki 8 bitų sveikųjų skaičių ar net mažesnio tikslumo formatų. Nors šis tikslumo sumažinimas gali atrodyti nerimą keliantis, tyrimai parodė, kad daugelis DI modelių gali išlaikyti savo našumą net ir esant žymiai sumažintam tikslumui.

Kvantavimas veikia, susiedamas slankiojo kablelio reikšmių diapazoną su mažesniu diskrečių reikšmių rinkiniu. Pavyzdžiui, vietoj 32 bitų naudojimo kiekvienam parametrui, kvantavimas gali naudoti tik 8 bitus, taip sumažinant atminties reikalavimus 4 kartus ir dažnai paspartinant išvadų laiką.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Skirtingos kvantavimo technikos apima:

- **Po treniravimo kvantavimas (PTQ)**: Taikomas po modelio treniravimo, nereikalaujant pakartotinio treniravimo.
- **Kvantavimo suvokimo treniravimas (QAT)**: Įtraukia kvantavimo efektus treniravimo metu, siekiant geresnio tikslumo.
- **Dinaminis kvantavimas**: Kvantuoja svorius į int8, bet aktyvacijas skaičiuoja dinamiškai.
- **Statinis kvantavimas**: Iš anksto apskaičiuoja visus kvantavimo parametrus tiek svoriams, tiek aktyvacijoms.

EdgeAI diegimui tinkamos kvantavimo strategijos pasirinkimas priklauso nuo konkrečios modelio architektūros, našumo reikalavimų ir tikslinio įrenginio techninės įrangos galimybių.

### Modelio suspaudimas ir optimizavimas

Be kvantavimo, įvairios suspaudimo technikos padeda sumažinti modelio dydį ir skaičiavimo reikalavimus. Tai apima:

**Pruning**: Ši technika pašalina nereikalingus ryšius ar neuronus iš neuroninių tinklų. Identifikuojant ir pašalinant parametrus, kurie mažai prisideda prie modelio našumo, pruning gali žymiai sumažinti modelio dydį, išlaikant tikslumą.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Žinių distiliacija**: Šis metodas apima mažesnio „studento“ modelio treniravimą, kad jis imituotų didesnio „mokytojo“ modelio elgesį. Studentų modelis mokosi apytiksliai atkartoti mokytojo rezultatus, dažnai pasiekdamas panašų našumą su žymiai mažesniais parametrais.

**Modelio architektūros optimizavimas**: Tyrėjai sukūrė specializuotas architektūras, skirtas kraštiniam diegimui, tokias kaip MobileNets, EfficientNets ir kitas lengvas architektūras, kurios subalansuoja našumą su skaičiavimo efektyvumu.

### Maži kalbos modeliai (SLMs)

Auganti EdgeAI tendencija yra mažų kalbos modelių (SLMs) kūrimas. Šie modeliai yra sukurti nuo pat pradžių, kad būtų kompaktiški ir efektyvūs, tuo pačiu teikiant reikšmingas natūralios kalbos galimybes. SLMs tai pasiekia per kruopščiai parinktas architektūras, efektyvius treniravimo metodus ir orientuotą treniravimą konkrečiose srityse ar užduotyse.

Skirtingai nuo tradicinių metodų, kurie apima didelių modelių suspaudimą, SLMs dažnai treniruojami su mažesniais duomenų rinkiniais ir optimizuotomis architektūromis, specialiai sukurtomis kraštiniam diegimui. Šis metodas gali sukurti modelius, kurie yra ne tik mažesni, bet ir efektyvesni konkretiems naudojimo atvejams.

## Aparatinės įrangos pagreitinimas EdgeAI

Šiuolaikiniai kraštiniai įrenginiai vis dažniau apima specializuotą aparatinę įrangą, skirtą DI darbo krūviams pagreitinti:

### Neuronų apdorojimo vienetai (NPUs)

NPUs yra specializuoti procesoriai, skirti neuroninių tinklų skaičiavimams. Šie lustai gali vykdyti DI išvadų užduotis daug efektyviau nei tradiciniai CPU, dažnai su mažesniu energijos suvartojimu. Daugelis šiuolaikinių išmaniųjų telefonų, nešiojamųjų kompiuterių ir IoT įrenginių dabar apima NPUs, kad būtų galima vykdyti DI apdorojimą įrenginyje.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Įrenginiai su NPUs apima:

- **Apple**: A serijos ir M serijos lustai su Neural Engine
- **Qualcomm**: Snapdragon procesoriai su Hexagon DSP/NPU
- **Samsung**: Exynos procesoriai su NPU
- **Intel**: Movidius VPUs ir Habana Labs akceleratoriai
- **Microsoft**: Windows Copilot+ kompiuteriai su NPUs

### 🎮 GPU pagreitinimas

Nors kraštiniai įrenginiai gali neturėti galingų GPU, esančių duomenų centruose, daugelis vis dar apima integruotus arba atskirus GPU, kurie gali pagreitinti DI darbo krūvius. Šiuolaikiniai mobilieji GPU ir integruoti grafikos procesoriai gali suteikti reikšmingą našumo pagerėjimą DI išvadų užduotims.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU optimizavimas

Net tik CPU turintys įrenginiai gali pasinaudoti EdgeAI per optimizuotas įgyvendinimo strategijas. Šiuolaikiniai CPU apima specializuotas instrukcijas DI darbo krūviams, o programinės įrangos pagrindai buvo sukurti siekiant maksimaliai padidinti CPU našumą DI išvadoms.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Programinės įrangos inžinieriams, dirbantiems su EdgeAI, supratimas, kaip pasinaudoti šiais aparatinės įrangos pagreitinimo pasirinkimais, yra labai svarbus optimizuojant išvadų našumą ir energijos efektyvumą tiksliniuose įrenginiuose.

## EdgeAI privalumai

### Privatumas ir saugumas

Vienas iš didžiausių EdgeAI privalumų yra pagerintas privatumas ir saugumas. Apdorojant duomenis vietoje įrenginyje, jautri informacija niekada nepalieka vartotojo kontrolės. Tai ypač svarbu programoms, tvarkančioms asmeninius duomenis, medicininę informaciją ar konfidencialius verslo duomenis.

### Sumažintas delsimas

EdgeAI pašalina poreikį siųsti duomenis į nuotolinius serverius apdorojimui, žymiai sumažindamas delsą. Tai labai svarbu realaus laiko programoms, tokioms kaip autonominės transporto priemonės, pramoninė automatizacija ar interaktyvios programos, kuriose reikalingi greiti atsakymai.

### Veikimas neprisijungus

EdgeAI leidžia DI funkcionalumą net tada, kai interneto ryšys yra nepasiekiamas. Tai vertinga programoms atokiose vietovėse, kelionių metu ar situacijose, kai tinklo patikimumas kelia susirūpinimą.

### Ekonominis efektyvumas

Sumažinus priklausomybę
- [02: EdgeAI Pritaikymas](02.RealWorldCaseStudies.md)

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Dėl svarbios informacijos rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus aiškinimus, atsiradusius naudojant šį vertimą.