<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T15:21:49+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "lt"
}
-->
# AI agentai ir mažieji kalbos modeliai: išsamus vadovas

## Įvadas

Šiame vadove nagrinėsime AI agentus ir mažus kalbos modelius (SLM) bei jų pažangias diegimo strategijas kraštinio skaičiavimo aplinkose. Aptarsime pagrindines agentinio AI koncepcijas, SLM optimizavimo technikas, praktines diegimo strategijas išteklių ribojamuose įrenginiuose ir „Microsoft Agent Framework“ sistemą, skirtą gamybai paruoštų agentų sistemų kūrimui.

Dirbtinio intelekto sritis 2025 metais patiria paradigminį pokytį. Nors 2023-ieji buvo pokalbių robotų metai, o 2024-aisiais įvyko bumas kopilotų srityje, 2025-ieji priklauso AI agentams – intelektinėms sistemoms, kurios mąsto, planuoja, naudoja įrankius ir vykdo užduotis su minimaliu žmogaus įsikišimu, vis dažniau naudojant efektyvius mažus kalbos modelius. „Microsoft Agent Framework“ tampa pagrindiniu sprendimu kuriant šias intelektines sistemas su neprisijungimo galimybėmis kraštiniuose įrenginiuose.

## Mokymosi tikslai

Šio vadovo pabaigoje galėsite:

- 🤖 Suprasti pagrindines AI agentų ir agentinių sistemų koncepcijas
- 🔬 Identifikuoti mažų kalbos modelių pranašumus prieš didelius kalbos modelius agentinėse aplikacijose
- 🚀 Išmokti pažangias SLM diegimo strategijas kraštinio skaičiavimo aplinkose
- 📱 Įgyvendinti praktinius SLM pagrįstus agentus realioms aplikacijoms
- 🏗️ Kurti gamybai paruoštus agentus naudojant „Microsoft Agent Framework“
- 🌐 Diegti neprisijungimo agentus su vietiniu LLM ir SLM integravimu
- 🔧 Integruoti „Microsoft Agent Framework“ su „Foundry Local“ kraštiniam diegimui

## AI agentų supratimas: pagrindai ir klasifikacija

### Apibrėžimas ir pagrindinės koncepcijos

Dirbtinio intelekto (AI) agentas – tai sistema arba programa, galinti savarankiškai atlikti užduotis vartotojo ar kitos sistemos vardu, kurdama savo darbo eigą ir naudodama turimus įrankius. Skirtingai nuo tradicinio AI, kuris tiesiog atsako į jūsų klausimus, agentas gali veikti savarankiškai, siekdamas tikslų.

### Agentų klasifikavimo sistema

Agentų ribų supratimas padeda pasirinkti tinkamus agentų tipus skirtingoms skaičiavimo situacijoms:

- **🔬 Paprasti refleksiniai agentai**: Taisyklėmis pagrįstos sistemos, reaguojančios į tiesioginius suvokimus (termostatai, pagrindinė automatizacija)
- **📱 Modeliu pagrįsti agentai**: Sistemos, palaikančios vidinę būseną ir atmintį (robotai dulkių siurbliai, navigacijos sistemos)
- **⚖️ Tikslų siekiantys agentai**: Sistemos, planuojančios ir vykdančios sekas, kad pasiektų tikslus (maršrutų planuotojai, užduočių planuotojai)
- **🧠 Mokymosi agentai**: Prisitaikančios sistemos, kurios laikui bėgant gerina našumą (rekomendacijų sistemos, personalizuoti asistentai)

### Pagrindiniai AI agentų pranašumai

AI agentai siūlo keletą pagrindinių pranašumų, kurie daro juos idealiais kraštinio skaičiavimo aplikacijoms:

**Operacinė autonomija**: Agentai suteikia nepriklausomą užduočių vykdymą be nuolatinės žmogaus priežiūros, todėl jie yra idealūs realaus laiko aplikacijoms. Jie reikalauja minimalios priežiūros, išlaikydami prisitaikantį elgesį, leidžiantį juos diegti išteklių ribojamuose įrenginiuose su sumažintomis operacinėmis sąnaudomis.

**Diegimo lankstumas**: Šios sistemos leidžia AI veikti įrenginyje be interneto ryšio, didina privatumą ir saugumą per vietinį apdorojimą, gali būti pritaikytos specifinėms sritims ir yra tinkamos įvairioms kraštinio skaičiavimo aplinkoms.

**Ekonomiškumas**: Agentų sistemos siūlo ekonomišką diegimą, palyginti su debesų sprendimais, su sumažintomis operacinėmis sąnaudomis ir mažesniais pralaidumo reikalavimais kraštinėms aplikacijoms.

## Pažangios mažų kalbos modelių strategijos

### SLM (Mažų kalbos modelių) pagrindai

Mažas kalbos modelis (SLM) – tai kalbos modelis, kuris gali veikti įprastame vartotojų elektronikos įrenginyje ir atlikti užklausų apdorojimą su pakankamai mažu vėlavimu, kad būtų praktiškas agentinių užklausų aptarnavimui vienam vartotojui. Praktikoje SLM paprastai yra modeliai, turintys mažiau nei 10 milijardų parametrų.

**Formatų atradimo funkcijos**: SLM siūlo pažangų palaikymą įvairiems kvantavimo lygiams, suderinamumą tarp platformų, realaus laiko našumo optimizavimą ir kraštinio diegimo galimybes. Vartotojai gali pasiekti patobulintą privatumą per vietinį apdorojimą ir „WebGPU“ palaikymą naršyklės pagrindu veikiančiam diegimui.

**Kvantavimo lygių kolekcijos**: Populiarūs SLM formatai apima Q4_K_M, skirtą subalansuotam suspaudimui mobilioms aplikacijoms, Q5_K_S seriją, skirtą kokybei orientuotam kraštiniam diegimui, Q8_0, skirtą beveik originaliam tikslumui galinguose kraštiniuose įrenginiuose, ir eksperimentinius formatus, tokius kaip Q2_K, skirtus itin mažų išteklių scenarijams.

### GGUF (Bendras GGML universalus formatas) SLM diegimui

GGUF yra pagrindinis formatas, skirtas kvantuotų SLM diegimui CPU ir kraštiniuose įrenginiuose, specialiai optimizuotas agentinėms aplikacijoms:

**Agentams optimizuotos funkcijos**: Formatui būdingi išsamūs ištekliai SLM konversijai ir diegimui su patobulintu įrankių kvietimu, struktūrizuoto išvesties generavimu ir daugiapakopiais pokalbiais. Suderinamumas tarp platformų užtikrina nuoseklų agentų elgesį skirtinguose kraštiniuose įrenginiuose.

**Našumo optimizavimas**: GGUF leidžia efektyviai naudoti atmintį agentų darbo eigoms, palaiko dinaminį modelių įkėlimą daugiagentinėms sistemoms ir užtikrina optimizuotą užklausų apdorojimą realaus laiko agentų sąveikoms.

### Kraštui optimizuotos SLM sistemos

#### Llama.cpp optimizavimas agentams

Llama.cpp siūlo pažangias kvantavimo technikas, specialiai optimizuotas agentiniam SLM diegimui:

**Agentams specifinis kvantavimas**: Sistema palaiko Q4_0 (optimalus mobiliesiems agentams su 75% dydžio sumažinimu), Q5_1 (subalansuota kokybė-suspaudimas kraštiniams agentams) ir Q8_0 (beveik originali kokybė gamybos agentų sistemoms). Pažangūs formatai leidžia itin suspaustus agentus ekstremaliems kraštiniams scenarijams.

**Įgyvendinimo privalumai**: CPU optimizuotas užklausų apdorojimas su SIMD pagreitinimu užtikrina efektyvų agentų vykdymą. Suderinamumas tarp platformų, įskaitant x86, ARM ir „Apple Silicon“ architektūras, leidžia universalias agentų diegimo galimybes.

#### Apple MLX sistema SLM agentams

Apple MLX siūlo natyvią optimizaciją, specialiai sukurtą SLM pagrįstiems agentams „Apple Silicon“ įrenginiuose:

**Apple Silicon agentų optimizavimas**: Sistema naudoja vieningą atminties architektūrą su „Metal Performance Shaders“ integracija, automatinį mišrų tikslumą agentų užklausų apdorojimui ir optimizuotą atminties pralaidumą daugiagentinėms sistemoms. SLM agentai rodo išskirtinį našumą M serijos lustuose.

**Kūrimo funkcijos**: Python ir Swift API palaikymas su agentams specifinėmis optimizacijomis, automatinė diferenciacija agentų mokymuisi ir sklandi integracija su „Apple“ kūrimo įrankiais suteikia išsamias agentų kūrimo aplinkas.

#### ONNX Runtime kryžminės platformos SLM agentams

ONNX Runtime siūlo universalų užklausų apdorojimo variklį, leidžiantį SLM agentams veikti nuosekliai įvairiose aparatinės įrangos platformose ir operacinėse sistemose:

**Universalus diegimas**: ONNX Runtime užtikrina nuoseklų SLM agentų elgesį „Windows“, „Linux“, „macOS“, „iOS“ ir „Android“ platformose. Šis suderinamumas tarp platformų leidžia kūrėjams rašyti vieną kartą ir diegti visur, žymiai sumažinant kūrimo ir priežiūros sąnaudas daugiaplatformėms aplikacijoms.

**Aparatinės įrangos pagreitinimo galimybės**: Sistema siūlo optimizuotus vykdymo tiekėjus įvairioms aparatinės įrangos konfigūracijoms, įskaitant CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) ir specializuotus akceleratorius (Intel VPU, Qualcomm NPU). SLM agentai gali automatiškai pasinaudoti geriausia turima aparatine įranga be kodo pakeitimų.

**Gamybai paruoštos funkcijos**: ONNX Runtime siūlo įmonės lygio funkcijas, būtinas gamybos agentų diegimui, įskaitant grafų optimizavimą greitesniam užklausų apdorojimui, atminties valdymą išteklių ribojamoms aplinkoms ir išsamias profiliavimo priemones našumo analizei. Sistema palaiko tiek Python, tiek C++ API, užtikrinant lankstų integravimą.

## SLM ir LLM palyginimas agentinėse sistemose: pažangus palyginimas

### SLM pranašumai agentinėse aplikacijose

**Operacinis efektyvumas**: SLM siūlo 10-30× sąnaudų sumažinimą, palyginti su LLM, agentų užduotims, leidžiant realaus laiko agentinius atsakymus mastu. Jie siūlo greitesnį užklausų apdorojimą dėl sumažinto skaičiavimo sudėtingumo, todėl yra idealūs interaktyvioms agentinėms aplikacijoms.

**Kraštinio diegimo galimybės**: SLM leidžia agentams veikti įrenginyje be interneto priklausomybės, didina privatumą per vietinį agentų apdorojimą ir pritaikymą specifinėms agentų aplikacijoms, tinkamoms įvairioms kraštinio skaičiavimo aplinkoms.

**Agentams specifinė optimizacija**: SLM puikiai tinka įrankių kvietimui, struktūrizuoto išvesties generavimui ir įprastiems sprendimų priėmimo darbo eigoms, kurios sudaro 70-80% tipinių agentų užduočių.

### Kada naudoti SLM ir LLM agentinėse sistemose

**Puikiai tinka SLM**:
- **Pasikartojančios agentų užduotys**: Duomenų įvedimas, formų pildymas, įprasti API kvietimai
- **Įrankių integracija**: Duomenų bazės užklausos, failų operacijos, sistemos sąveikos
- **Struktūrizuotos darbo eigos**: Sekant iš anksto nustatytus agentų procesus
- **Specifinės srities agentai**: Klientų aptarnavimas, planavimas, pagrindinė analizė
- **Vietinis apdorojimas**: Privatumui jautrios agentų operacijos

**Geriau tinka LLM**:
- **Sudėtingas mąstymas**: Nauji problemų sprendimai, strateginis planavimas
- **Atviri pokalbiai**: Bendras pokalbis, kūrybinės diskusijos
- **Plataus žinių užduotys**: Tyrimai, reikalaujantys plačių bendrųjų žinių
- **Naujos situacijos**: Visiškai naujų agentų scenarijų valdymas

### Hibridinė agentų architektūra

Optimalus požiūris sujungia SLM ir LLM heterogeninėse agentinėse sistemose:

**Išmanus agentų orkestravimas**:
1. **SLM kaip pagrindinis**: Tvarkyti 70-80% įprastų agentų užduočių vietoje
2. **LLM prireikus**: Nukreipti sudėtingas užklausas į debesų pagrindu veikiančius didesnius modelius
3. **Specializuoti SLM**: Skirtingi maži modeliai skirtingoms agentų sritims
4. **Sąnaudų optimizavimas**: Minimizuoti brangius LLM kvietimus per išmanų nukreipimą

## Gamybos SLM agentų diegimo strategijos

### Foundry Local: Įmonės lygio kraštinio AI vykdymo aplinka

Foundry Local (https://github.com/microsoft/foundry-local) yra pagrindinis „Microsoft“ sprendimas, skirtas mažų kalbos modelių diegimui gamybos kraštinėse aplinkose. Ji siūlo pilną vykdymo aplinką, specialiai sukurtą SLM pagrįstiems agentams su įmonės lygio funkcijomis ir sklandžiomis integravimo galimybėmis.

**Pagrindinė architektūra ir funkcijos**:
- **OpenAI suderinama API**: Pilnas suderinamumas su „OpenAI SDK“ ir „Agent Framework“ integracijomis
- **Automatinė aparatinės įrangos optimizacija**: Išmanus modelių variantų pasirinkimas pagal turimą aparatinę įrangą (CUDA GPU, Qualcomm NPU, CPU)
- **Modelių valdymas**: Automatinis modelių atsisiuntimas, talpinimas ir gyvavimo ciklo valdymas
- **Paslaugų atradimas**: Nulinės konfigūracijos paslaugų aptikimas agentų sistemoms
- **Išteklių optimizavimas**: Išmanus atminties valdymas ir energijos efektyvumas kraštiniam diegimui

#### Diegimas ir nustatymas

**Kryžminės platformos diegimas**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Greitas startas agentų kūrimui**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Agentų sistemos integracija

**Foundry Local SDK integracija**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automatinis modelių pasirinkimas ir aparatinės įrangos optimizavimas**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Gamybos diegimo modeliai

**Vieno agento gamybos nustatymas**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Daugiagentinė gamybos orkestracija**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Įmonės funkcijos ir stebėjimas

**Sveikatos stebėjimas ir stebimumas**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Išteklių valdymas ir automatinis mastelio keitimas**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Pažangi konfigūracija ir optimizavimas

**Individuali modelių konfigūracija**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Gamybos diegimo kontrolinis sąrašas**:

✅ **Paslaugų konfigūracija**:
- Konfigūruoti tinkamus modelių aliasus pagal naudojimo atvejus
- Nustatyti išteklių ribas ir stebėjimo slenksčius
- Įjungti sveikatos patikrinimus ir metrikų rinkimą
- Konfigūruoti automatinį paleidimą iš naujo ir gedimų valdymą

✅ **Saugumo nustatymas**:
- Įjungti tik vietinį API prieinamumą (be išorinio ryšio)
- Konfigūruoti tinkamą API raktų valdymą
- Nustaty
- Testuoti Microsoft Agent Framework integraciją  
- Patikrinti neprisijungus veikiančias funkcijas  
- Išbandyti gedimų scenarijus ir klaidų tvarkymą  
- Patvirtinti agentų darbo eigą nuo pradžios iki pabaigos  

**Palyginimas su Foundry Local**:

| Funkcija | Foundry Local | Ollama |
|----------|---------------|--------|
| **Tikslinė paskirtis** | Įmonių gamyba | Kūrimas ir bendruomenė |
| **Modelių ekosistema** | Microsoft atrinkta | Plati bendruomenė |
| **Aparatūros optimizavimas** | Automatinis (CUDA/NPU/CPU) | Rankinis konfigūravimas |
| **Įmonių funkcijos** | Įmontuotas stebėjimas, saugumas | Bendruomenės įrankiai |
| **Diegimo sudėtingumas** | Paprasta (winget install) | Paprasta (curl install) |
| **API suderinamumas** | OpenAI + praplėtimai | OpenAI standartas |
| **Palaikymas** | Microsoft oficialus | Bendruomenės valdomas |
| **Geriausia naudoti** | Gamybiniai agentai | Prototipų kūrimas, tyrimai |

**Kada rinktis Ollama**:  
- **Kūrimas ir prototipų kūrimas**: Greitas eksperimentavimas su skirtingais modeliais  
- **Bendruomenės modeliai**: Prieiga prie naujausių bendruomenės sukurtų modelių  
- **Švietimo tikslai**: AI agentų kūrimo mokymasis ir mokymas  
- **Tyrimų projektai**: Akademiniai tyrimai, reikalaujantys įvairių modelių prieigos  
- **Individualūs modeliai**: Individualiai pritaikytų modelių kūrimas ir testavimas  

### VLLM: Didelio našumo SLM agentų inferencija  

VLLM (Very Large Language Model inference) siūlo didelio našumo, atmintį taupantį inferencijos variklį, specialiai optimizuotą gamybiniams SLM diegimams dideliu mastu. Nors Foundry Local orientuojasi į naudojimo paprastumą, o Ollama pabrėžia bendruomenės modelius, VLLM išsiskiria didelio našumo scenarijais, reikalaujančiais maksimalaus našumo ir efektyvaus resursų panaudojimo.  

**Pagrindinė architektūra ir funkcijos**:  
- **PagedAttention**: Revoliucinė atminties valdymo sistema efektyviam dėmesio skaičiavimui  
- **Dinaminis grupavimas**: Protingas užklausų grupavimas optimaliam našumui  
- **GPU optimizavimas**: Pažangūs CUDA branduoliai ir tensorių paralelizmo palaikymas  
- **OpenAI suderinamumas**: Pilnas API suderinamumas sklandžiai integracijai  
- **Spekuliatyvus dekodavimas**: Pažangios inferencijos pagreitinimo technikos  
- **Kvantizacijos palaikymas**: INT4, INT8 ir FP16 kvantizacija atminties efektyvumui  

#### Diegimas ir nustatymas  

**Diegimo parinktys**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Greitas startas agentų kūrimui**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### Agentų Framework integracija  

**VLLM su Microsoft Agent Framework**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**Didelio našumo daugiagentė sąranka**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### Gamybinio diegimo modeliai  

**Įmonių VLLM gamybos paslauga**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### Įmonių funkcijos ir stebėjimas  

**Pažangus VLLM našumo stebėjimas**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### Pažangi konfigūracija ir optimizavimas  

**Gamybinės VLLM konfigūracijos šablonai**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Gamybinio diegimo kontrolinis sąrašas VLLM**:  

✅ **Aparatūros optimizavimas**:  
- Konfigūruoti tensorių paralelizmą daugi-GPU sąrankoms  
- Įjungti kvantizaciją (AWQ/GPTQ) atminties efektyvumui  
- Nustatyti optimalų GPU atminties panaudojimą (85-95%)  
- Konfigūruoti tinkamus grupių dydžius našumui  

✅ **Našumo derinimas**:  
- Įjungti prefikso talpyklą pasikartojančioms užklausoms  
- Konfigūruoti suskaidytą užpildymą ilgoms sekoms  
- Nustatyti spekuliatyvų dekodavimą greitesnei inferencijai  
- Optimizuoti max_num_seqs pagal aparatinę įrangą  

✅ **Gamybinės funkcijos**:  
- Nustatyti sveikatos stebėjimą ir metrikų rinkimą  
- Konfigūruoti automatinį paleidimą iš naujo ir gedimų valdymą  
- Įgyvendinti užklausų eilės sudarymą ir apkrovos balansavimą  
- Nustatyti išsamų žurnalų vedimą ir įspėjimus  

✅ **Saugumas ir patikimumas**:  
- Konfigūruoti ugniasienės taisykles ir prieigos kontrolę  
- Nustatyti API užklausų ribojimą ir autentifikaciją  
- Įgyvendinti sklandų išjungimą ir valymą  
- Konfigūruoti atsargines kopijas ir nelaimių atkūrimą  

✅ **Integracijos testavimas**:  
- Testuoti Microsoft Agent Framework integraciją  
- Patvirtinti didelio našumo scenarijus  
- Išbandyti gedimų ir atkūrimo procedūras  
- Atlikti našumo testus apkrovos sąlygomis  

**Palyginimas su kitais sprendimais**:

| Funkcija | VLLM | Foundry Local | Ollama |
|----------|------|---------------|--------|
| **Tikslinė paskirtis** | Didelio našumo gamyba | Įmonių naudojimo paprastumas | Kūrimas ir bendruomenė |
| **Našumas** | Maksimalus našumas | Subalansuotas | Geras |
| **Atminties efektyvumas** | PagedAttention optimizacija | Automatinė optimizacija | Standartinis |
| **Diegimo sudėtingumas** | Aukštas (daug parametrų) | Žemas (automatinis) | Žemas (paprastas) |
| **Mastelio keitimas** | Puikus (tensorinis/pipeline paralelizmas) | Geras | Ribotas |
| **Kvantizacija** | Pažangi (AWQ, GPTQ, FP8) | Automatinė | Standartinė GGUF |
| **Įmonių funkcijos** | Reikalinga individuali įgyvendinimas | Įmontuota | Bendruomenės įrankiai |
| **Geriausia naudoti** | Didelio masto gamybiniai agentai | Įmonių gamyba | Kūrimas |

**Kada rinktis VLLM**:  
- **Didelio našumo poreikiai**: Šimtų užklausų per sekundę apdorojimas  
- **Didelio masto diegimai**: Daugi-GPU, daugi-mazgiai diegimai  
- **Našumo kritiškumas**: Atsakymo laikas mažesnis nei sekundė dideliu mastu  
- **Pažangi optimizacija**: Reikalinga individuali kvantizacija ir grupavimas  
- **Resursų efektyvumas**: Maksimalus brangios GPU aparatinės įrangos panaudojimas  

## Realūs SLM agentų pritaikymo pavyzdžiai  

### Klientų aptarnavimo SLM agentai  
- **SLM galimybės**: Paskyrų paieška, slaptažodžių atstatymas, užsakymų būsenos tikrinimas  
- **Kaštų privalumai**: 10 kartų mažesnės inferencijos išlaidos, palyginti su LLM agentais  
- **Našumas**: Greitesnis atsakymo laikas su nuoseklia kokybe įprastoms užklausoms  

### Verslo procesų SLM agentai  
- **Sąskaitų faktūrų apdorojimo agentai**: Duomenų ištraukimas, informacijos tikrinimas, nukreipimas patvirtinimui  
- **El. pašto valdymo agentai**: Kategorizavimas, prioritetų nustatymas, automatinis atsakymų rengimas  
- **Tvarkaraščių sudarymo agentai**: Susitikimų koordinavimas, kalendorių valdymas, priminimų siuntimas  

### Asmeniniai SLM skaitmeniniai asistentai  
- **Užduočių valdymo agentai**: Efektyvus užduočių sąrašų kūrimas, atnaujinimas, organizavimas  
- **Informacijos rinkimo agentai**: Temų tyrimas, išvadų apibendrinimas vietoje  
- **Komunikacijos agentai**: El. laiškų, žinučių, socialinių tinklų įrašų rengimas privačiai  

### Prekybos ir finansų SLM agentai  
- **Rinkos stebėjimo agentai**: Kainų sekimas, tendencijų nustatymas realiu laiku  
- **Ataskaitų rengimo agentai**: Automatinis dienos/savaitės santraukų kūrimas  
- **Rizikos vertinimo agentai**: Portfelio pozicijų vertinimas naudojant vietinius duomenis  

### Sveikatos priežiūros SLM agentai  
- **Pacientų tvarkaraščių sudarymo agentai**: Susitikimų koordinavimas, automatinių priminimų siuntimas  
- **Dokumentacijos agentai**: Medicininių santraukų, ataskaitų generavimas vietoje  
- **Receptų valdymo agentai**: Papildymų sekimas, sąveikų tikrinimas privačiai  

## Microsoft Agent Framework: Gamybai paruoštų agentų kūrimas  

### Apžvalga ir architektūra  

Microsoft Agent Framework siūlo išsamų, įmonių lygio platformą AI agentų kūrimui, diegimui ir valdymui, kurie gali veikti tiek debesyje, tiek neprisijungus prie tinklo. Framework yra specialiai sukurtas veikti sklandžiai su mažais kalbos modeliais ir edge kompiuterijos scenarijais, todėl jis idealiai tinka privatumui jautriems ir resursų apribotiems diegimams.  

**Pagrindiniai Framework komponentai**:  
- **Agentų vykdymo aplinka**: Lengvas vykdymo aplinkos modulis, optimizuotas edge įrenginiams  
- **Įrankių integravimo sistema**: Išplečiama papildinių architektūra išorinių paslaugų ir API prijungimui  
- **Būsenos valdymas**: Nuolatinė agento atmintis ir konteksto valdymas tarp sesijų  
- **Saugumo sluoksnis**: Įmontuotos saugumo kontrolės įmonių diegimui  
- **Orkestracijos variklis**: Daugiagentė koordinacija ir darbo eigos valdymas  

### Pagrindinės funkcijos edge diegimui  

**Neprisijungus pirmiausia architektūra**: Microsoft Agent Framework sukurtas laikantis neprisijungus pirmiausia principų, leidžiant agentams efektyviai veikti be nuolatinio interneto ryšio. Tai apima vietinę modelių inferenciją, talpykloje saugomus žinių bazes, neprisijungus veikiančių įrankių vykdymą ir sklandų degradavimą, kai debesų paslaugos nepasiekiamos.  

**Resursų optimizavimas**: Framework siūlo protingą resursų valdymą su automatine atminties optimizacija SLM, CPU/GPU apkrovos balansavimu edge įrenginiams, adaptaciniu modelių pasirinkimu pagal turimus resursus ir energiją taupančiais inferencijos modeliais mobiliesiems diegimams.  

**Saugumas ir privatumas**: Įmonių lygio saugumo funkcijos apima vietinį duomenų apdorojimą privatumui išlaikyti, užšifruotus agentų komunikacijos kanalus, rolėmis pagrįstą prieigos kontrolę agentų galimybėms ir audito žurnalų vedimą atitikties reikalavimams.  

### Integracija su Foundry Local  

Microsoft Agent Framework sklandžiai integruojasi su Foundry Local, kad būtų užtikrintas pilnas edge AI sprendimas:  

**Automatinis modelių aptikimas**: Framework automatiškai aptinka ir prisijungia prie Foundry Local instancijų, aptinka turimus SLM modelius ir pasirenka optimalius modelius pagal agentų reikalavimus ir aparatinės įrangos galimybes.  

**Dinaminis modelių įkrovimas**: Agentai gali dinamiškai įkrauti skirtingus SLM modelius specifinėms užduotims, leidžiant daugiamodelių agentų sistemas, kur skirtingi modeliai tvarko skirtingų tipų užklausas, ir automatinį perjungimą tarp modelių pagal prieinamumą ir našumą.  

**Našumo optimizavimas**: Integruoti talpyklos mechanizmai mažina modelių įkrovimo laiką, jungčių grupavimas optimizuoja API skambučius į Foundry Local, o protingas grupavimas gerina našumą kelių agentų užklausoms.  

### Agentų kūrimas su Microsoft Agent Framework  

#### Agentų apibrėžimas ir konfigūracija  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### Įrankių integracija edge scenarijams  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### Daugiagentė orkestracija  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### Pažangūs edge diegimo modeliai  

#### Hierarchinė agentų architektūra  

**Vietiniai agentų klasteriai**: Diegti kelis specializuotus SLM agentus edge įrenginiuose, kiekvieną optimizuotą specifinėms užduotims. Naudoti lengvus modelius, tokius kaip Qwen2.5-0.5B, paprastam maršrutavimui ir tvarkaraščių sudarymui, vidutinius modelius, tokius kaip Phi-4-Mini, klientų aptarnavimui ir dokumentacijai, ir didesnius modelius sudėtingam samprotavimui, kai resursai leidžia.  

**Edge-to-Cloud koordinacija**: Įgyvendinti protingus eskalacijos modelius, kur vietiniai agentai tvarko įprastas užduotis, debesų agentai teikia sudėtingą samprotavimą, kai ryšys leidžia, ir sklandus perėjimas tarp edge ir debesų apdorojimo išlaiko tęstinumą.  

#### Diegimo konfigūracijos  

**Vieno įrenginio diegimas**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**Paskirstytas edge diegimas**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### Našumo optimizavimas edge agentams  

#### Modelių pasirinkimo strategijos  

**Užduočių pagrindu modelių priskyrimas**: Microsoft Agent Framework leidžia protingą modelių pasirinkimą pagal užduočių sudėtingumą ir reikalavimus:  

- **Paprastos užduotys** (Q&A, maršrutavimas): Qwen2.5-0.5B (500MB, <100ms atsakymas)  
- **Vidutinės užduotys** (klientų aptarnavimas, tvarkaraščių sudarymas): Phi-4-Mini (2.4GB, 200-500ms atsakymas)  
- **Sudėtingos užduotys** (techninė analizė, planavimas): Phi-4 (7GB, 1-3s atsakymas, kai resursai leidžia)  

**Dinaminis modelių perjungimas**: Agentai gali perjungti modelius pagal esamą sistemos apkrovą, užduočių sudėtingumo vertinimą, vartotojo prioritetų lygius ir turimus aparatinės įrangos resursus.  

#### Atminties ir resursų valdymas  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  

### Įmonių integracijos modeliai  

#### Saugumas ir atitiktis  

**Vietinis duomenų apdorojimas**: Visi agentų apdorojimai vyksta vietoje, užtikrinant, kad jautrūs duomenys niekada nepalieka edge įrenginio. Tai apima klientų informacijos apsaugą, HIPAA atitiktį sveikatos priežiūros agentams, finansinių duomenų saugumą bankininkystės agentams ir GDPR atitiktį Europos diegimams.  

**Prieigos kontrolė**: Rolėmis pagrįstos teisės kontroliuoja, kokius įrankius agentai gali pas
**Karkaso pasirinkimas agentų diegimui**: Pasirinkite optimizavimo karkasus pagal tikslinę aparatinę įrangą ir agentų reikalavimus. Naudokite Llama.cpp CPU optimizuotam agentų diegimui, Apple MLX Apple Silicon agentų programoms ir ONNX kryžminės platformos agentų suderinamumui.

## Praktinis SLM agentų konvertavimas ir panaudojimo atvejai

### Agentų diegimo scenarijai realiame pasaulyje

**Mobilios agentų programos**: Q4_K formatas puikiai tinka išmaniesiems telefonams dėl mažo atminties naudojimo, o Q8_0 užtikrina subalansuotą našumą planšetėse. Q5_K formatas siūlo aukščiausią kokybę mobiliesiems produktyvumo agentams.

**Darbalaukio ir kraštinių agentų kompiuterija**: Q5_K užtikrina optimalų našumą darbalaukio agentų programoms, Q8_0 siūlo aukštos kokybės išvadą darbo stočių agentų aplinkoje, o Q4_K leidžia efektyviai apdoroti kraštinių agentų įrenginiuose.

**Tyrimų ir eksperimentiniai agentai**: Pažangūs kvantavimo formatai leidžia tyrinėti itin mažo tikslumo agentų išvadas akademiniams tyrimams ir koncepcijos įrodymo agentų programoms, kurioms reikalingi ekstremalūs resursų apribojimai.

### SLM agentų našumo etalonai

**Agentų išvados greitis**: Q4_K pasiekia greičiausią agentų atsako laiką mobiliose CPU, Q5_K užtikrina subalansuotą greičio ir kokybės santykį bendroms agentų programoms, Q8_0 siūlo aukščiausią kokybę sudėtingoms agentų užduotims, o eksperimentiniai formatai užtikrina maksimalų pralaidumą specializuotai agentų aparatiniai įrangai.

**Agentų atminties reikalavimai**: Agentų kvantavimo lygiai svyruoja nuo Q2_K (mažiau nei 500MB mažiems agentų modeliams) iki Q8_0 (apie 50% originalaus dydžio), o eksperimentinės konfigūracijos pasiekia maksimalų suspaudimą resursų apribotoms agentų aplinkoms.

## SLM agentų iššūkiai ir svarstymai

### Našumo kompromisai agentų sistemose

SLM agentų diegimas reikalauja atidžiai apsvarstyti kompromisus tarp modelio dydžio, agentų atsako greičio ir išvados kokybės. Nors Q4_K siūlo išskirtinį greitį ir efektyvumą mobiliesiems agentams, Q8_0 užtikrina aukščiausią kokybę sudėtingoms agentų užduotims. Q5_K yra vidurio variantas, tinkamas daugumai bendrų agentų programų.

### Aparatinės įrangos suderinamumas SLM agentams

Skirtingi kraštiniai įrenginiai turi skirtingas galimybes SLM agentų diegimui. Q4_K efektyviai veikia paprastuose procesoriuose paprastiems agentams, Q5_K reikalauja vidutinių skaičiavimo resursų subalansuotam agentų našumui, o Q8_0 naudoja aukščiausios klasės aparatinę įrangą pažangioms agentų galimybėms.

### Saugumas ir privatumas SLM agentų sistemose

Nors SLM agentai leidžia vietinį apdorojimą, užtikrinantį didesnį privatumą, būtina įgyvendinti tinkamas saugumo priemones, kad apsaugotumėte agentų modelius ir duomenis kraštinėse aplinkose. Tai ypač svarbu diegiant aukšto tikslumo agentų formatus įmonių aplinkose arba suspaustus agentų formatus programose, kurios tvarko jautrius duomenis.

## Ateities tendencijos SLM agentų kūrime

SLM agentų sritis toliau vystosi su pažanga suspaudimo technikose, optimizavimo metodais ir kraštinių diegimo strategijomis. Ateities plėtra apima efektyvesnius agentų modelių kvantavimo algoritmus, patobulintus suspaudimo metodus agentų darbo eigoms ir geresnę integraciją su kraštinių aparatinės įrangos akceleratoriais agentų apdorojimui.

**SLM agentų rinkos prognozės**: Remiantis naujausiais tyrimais, agentų valdoma automatizacija galėtų pašalinti 40–60% pasikartojančių kognityvinių užduočių įmonių darbo eigoje iki 2027 m., o SLM bus šios transformacijos lyderiai dėl jų ekonomiškumo ir diegimo lankstumo.

**Technologijų tendencijos SLM agentuose**:
- **Specializuoti SLM agentai**: Modeliai, pritaikyti konkrečioms agentų užduotims ir pramonės šakoms
- **Kraštinė agentų kompiuterija**: Patobulintos įrenginio agentų galimybės su geresniu privatumu ir sumažintu vėlavimu
- **Agentų koordinavimas**: Geresnis kelių SLM agentų koordinavimas su dinamišku maršrutizavimu ir apkrovos balansavimu
- **Demokratizacija**: SLM lankstumas leidžia platesnį dalyvavimą agentų kūrime įvairiose organizacijose

## Pradžia su SLM agentais

### 1 žingsnis: Microsoft Agent Framework aplinkos nustatymas

**Įdiekite priklausomybes**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicializuokite Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### 2 žingsnis: Pasirinkite savo SLM agentų programoms
Populiarios parinktys Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Puikiai tinka bendroms agentų užduotims su subalansuotu našumu
- **Qwen2.5-0.5B (0.5B)**: Itin efektyvus paprastiems maršrutizavimo ir klasifikavimo agentams
- **Qwen2.5-Coder-0.5B (0.5B)**: Specializuotas kodui susijusioms agentų užduotims
- **Phi-4 (7B)**: Pažangus samprotavimas sudėtingoms kraštinėms situacijoms, kai leidžia resursai

### 3 žingsnis: Sukurkite savo pirmąjį agentą su Microsoft Agent Framework

**Pagrindinis agento nustatymas**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### 4 žingsnis: Apibrėžkite agento apimtį ir reikalavimus
Pradėkite nuo sutelktų, gerai apibrėžtų agentų programų, naudodami Microsoft Agent Framework:
- **Vieno domeno agentai**: Klientų aptarnavimas ARBA planavimas ARBA tyrimai
- **Aiškūs agento tikslai**: Konkretūs, išmatuojami agento našumo tikslai
- **Ribota įrankių integracija**: 3–5 įrankiai maksimaliai pradiniam agentų diegimui
- **Apibrėžtos agento ribos**: Aiškūs eskalavimo keliai sudėtingoms situacijoms
- **Kraštinė pirmoji dizaino koncepcija**: Pirmenybė neprisijungus veikiančioms funkcijoms ir vietiniam apdorojimui

### 5 žingsnis: Įgyvendinkite kraštinį diegimą su Microsoft Agent Framework

**Resursų konfigūracija**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Diegimo saugumo priemonės kraštiniams agentams**:
- **Vietinis įvesties patikrinimas**: Tikrinkite užklausas be debesų priklausomybių
- **Neprisijungus išvesties filtravimas**: Užtikrinkite, kad atsakymai atitiktų kokybės standartus vietoje
- **Kraštinės saugumo kontrolės**: Įgyvendinkite saugumą be interneto ryšio
- **Vietinis stebėjimas**: Sekite našumą ir žymėkite problemas naudodami kraštinę telemetriją

### 6 žingsnis: Matavimas ir optimizavimas kraštinių agentų našumui
- **Agentų užduočių užbaigimo rodikliai**: Stebėkite sėkmės rodiklius neprisijungus scenarijuose
- **Agentų atsako laikai**: Užtikrinkite subsekundinius atsako laikus kraštiniam diegimui
- **Resursų naudojimas**: Sekite atminties, CPU ir baterijos naudojimą kraštiniuose įrenginiuose
- **Ekonominis efektyvumas**: Palyginkite kraštinio diegimo išlaidas su debesų pagrindu veikiančiomis alternatyvomis
- **Neprisijungus patikimumas**: Matavimas agentų našumo tinklo sutrikimų metu

## Pagrindinės išvados apie SLM agentų įgyvendinimą

1. **SLM pakanka agentams**: Daugumai agentų užduočių maži modeliai veikia taip pat gerai kaip dideli, tuo pačiu siūlydami reikšmingus pranašumus
2. **Ekonominis efektyvumas agentuose**: 10–30 kartų pigiau valdyti SLM agentus, todėl jie tampa ekonomiškai perspektyvūs plačiam diegimui
3. **Specializacija veikia agentams**: Smulkiai pritaikyti SLM dažnai pranoksta bendros paskirties LLM specifinėse agentų programose
4. **Hibridinė agentų architektūra**: Naudokite SLM įprastoms agentų užduotims, LLM sudėtingam samprotavimui, kai būtina
5. **Microsoft Agent Framework leidžia gamybos diegimą**: Siūlo įmonės lygio įrankius agentų kūrimui, diegimui ir valdymui kraštinėse aplinkose
6. **Kraštinė pirmoji dizaino koncepcija**: Neprisijungus veikiantys agentai su vietiniu apdorojimu užtikrina privatumą ir patikimumą
7. **Foundry Local integracija**: Sklandi jungtis tarp Microsoft Agent Framework ir vietinio modelio išvados
8. **Ateitis yra SLM agentai**: Maži kalbos modeliai su gamybos karkasais yra agentinės AI ateitis, leidžianti demokratizuotą ir efektyvų agentų diegimą

## Nuorodos ir papildoma literatūra

### Pagrindiniai tyrimų darbai ir publikacijos

#### AI agentai ir agentinės sistemos
- **"Kalbos agentai kaip optimizuojami grafai"** (2024) - Pagrindiniai tyrimai apie agentų architektūrą ir optimizavimo strategijas
  - Autoriai: Wenyue Hua, Lishan Yang ir kt.
  - Nuoroda: https://arxiv.org/abs/2402.16823
  - Pagrindinės įžvalgos: Grafų pagrindu sukurta agentų dizaino ir optimizavimo strategija

- **"Didelių kalbos modelių pagrindu sukurtų agentų kilimas ir potencialas"** (2023)
  - Autoriai: Zhiheng Xi, Wenxiang Chen ir kt.
  - Nuoroda: https://arxiv.org/abs/2309.07864
  - Pagrindinės įžvalgos: Išsamus LLM pagrindu sukurtų agentų galimybių ir pritaikymo apžvalga

- **"Kognityvinės architektūros kalbos agentams"** (2024)
  - Autoriai: Theodore Sumers, Shunyu Yao ir kt.
  - Nuoroda: https://arxiv.org/abs/2309.02427
  - Pagrindinės įžvalgos: Kognityviniai pagrindai intelektualių agentų kūrimui

#### Maži kalbos modeliai ir optimizacija
- **"Phi-3 techninė ataskaita: Labai pajėgus kalbos modelis vietoje jūsų telefone"** (2024)
  - Autoriai: Microsoft Research Team
  - Nuoroda: https://arxiv.org/abs/2404.14219
  - Pagrindinės įžvalgos: SLM dizaino principai ir mobiliojo diegimo strategijos

- **"Qwen2.5 techninė ataskaita"** (2024)
  - Autoriai: Alibaba Cloud Team
  - Nuoroda: https://arxiv.org/abs/2407.10671
  - Pagrindinės įžvalgos: Pažangios SLM mokymo technikos ir našumo optimizacija

- **"TinyLlama: Atvirojo kodo mažas kalbos modelis"** (2024)
  - Autoriai: Peiyuan Zhang, Guangtao Zeng ir kt.
  - Nuoroda: https://arxiv.org/abs/2401.02385
  - Pagrindinės įžvalgos: Itin kompaktiško modelio dizainas ir mokymo efektyvumas

### Oficialūs dokumentai ir karkasai

#### Microsoft Agent Framework
- **Oficialūs dokumentai**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub saugykla**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Pagrindinė saugykla**: https://github.com/microsoft/foundry-local
- **Dokumentai**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Pagrindinė saugykla**: https://github.com/vllm-project/vllm
- **Dokumentai**: https://docs.vllm.ai/


#### Ollama
- **Oficiali svetainė**: https://ollama.ai/
- **GitHub saugykla**: https://github.com/ollama/ollama

### Modelių optimizavimo karkasai

#### Llama.cpp
- **Saugykla**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentai**: https://microsoft.github.io/Olive/
- **GitHub saugykla**: https://github.com/microsoft/Olive

#### OpenVINO
- **Oficiali svetainė**: https://docs.openvino.ai/

#### Apple MLX
- **Saugykla**: https://github.com/ml-explore/mlx

### Pramonės ataskaitos ir rinkos analizė

#### AI agentų rinkos tyrimai
- **"AI agentų būklė 2025"** - McKinsey Global Institute
  - Nuoroda: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Pagrindinės įžvalgos: Rinkos tendencijos ir įmonių pritaikymo modeliai

#### Techniniai etalonai

- **"Kraštinių AI išvadų etalonai"** - MLPerf
  - Nuoroda: https://mlcommons.org/en/inference-edge/
  - Pagrindinės įžvalgos: Standartizuoti našumo rodikliai kraštiniam diegimui

### Standartai ir specifikacijos

#### Modelių formatai ir standartai
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Kryžminės platformos modelio formatas suderinamumui
- **GGUF specifikacija**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantuotas modelio formatas CPU išvadai
- **OpenAI API specifikacija**: https://platform.openai.com/docs/api-reference
  - Standartinis API formatas kalbos modelio integracijai

#### Saugumas ir atitiktis
- **NIST AI rizikos valdymo karkasas**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI sistemos**: Karkasas AI sistemoms ir saugumui
- **IEEE standartai AI**: https://standards.ieee.org/industry-connections/ai/

SLM agentų perėjimas žymi esminį pokytį, kaip mes žiūrime į AI diegimą. Microsoft Agent Framework, kartu su vietinėmis platformomis ir efektyviais mažais kalbos modeliais, siūlo pilną sprendimą gamybai paruoštų agentų kūrimui, kurie efektyviai veikia kraštinėse aplinkose. Dėmesys efektyvumui, specializacijai ir praktiniam pritaikymui daro šį technologijų rinkinį labiau prieinamą, ekonomišką ir efektyvų realaus pasaulio programoms įvairiose pramonės šakose ir kraštinės kompiuterijos aplinkose.

Judant į priekį iki 2025 m., vis labiau pajėgūs maži modeliai, sudėtingi agentų karkas

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Dėl svarbios informacijos rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus aiškinimus, atsiradusius naudojant šį vertimą.