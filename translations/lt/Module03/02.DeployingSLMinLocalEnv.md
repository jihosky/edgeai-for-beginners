<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T10:10:58+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "lt"
}
-->
# 2 skyrius: Vietinio aplinkos diegimas – privatumo pirmumo sprendimai

Vietinis Mažų Kalbos Modelių (SLM) diegimas žymi paradigmos pokytį link privatumo išsaugojimo ir ekonomiškai efektyvių AI sprendimų. Ši išsami instrukcija nagrinėja du galingus pagrindus – Ollama ir Microsoft Foundry Local – kurie leidžia kūrėjams maksimaliai išnaudoti SLM galimybes, išlaikant visišką kontrolę savo diegimo aplinkoje.

## Įvadas

Šioje pamokoje nagrinėsime pažangias Mažų Kalbos Modelių diegimo strategijas vietinėse aplinkose. Aptarsime pagrindines vietinio AI diegimo koncepcijas, išanalizuosime dvi pirmaujančias platformas (Ollama ir Microsoft Foundry Local) ir pateiksime praktinius patarimus, kaip įgyvendinti sprendimus, paruoštus gamybai.

## Mokymosi tikslai

Pamokos pabaigoje galėsite:

- Suprasti vietinių SLM diegimo pagrindų architektūrą ir privalumus.
- Įgyvendinti gamybai paruoštus diegimus naudojant Ollama ir Microsoft Foundry Local.
- Palyginti ir pasirinkti tinkamą platformą pagal specifinius reikalavimus ir apribojimus.
- Optimizuoti vietinius diegimus našumui, saugumui ir masteliui.

## Vietinių SLM diegimo architektūrų supratimas

Vietinis SLM diegimas žymi esminį pokytį nuo debesų priklausomų AI paslaugų link vietinių, privatumo išsaugojimo sprendimų. Šis požiūris leidžia organizacijoms išlaikyti visišką kontrolę savo AI infrastruktūrai, užtikrinant duomenų suverenitetą ir operacinę nepriklausomybę.

### Diegimo pagrindų klasifikacija

Skirtingų diegimo metodų supratimas padeda pasirinkti tinkamą strategiją specifiniams naudojimo atvejams:

- **Skirta kūrimui**: Supaprastintas nustatymas eksperimentavimui ir prototipų kūrimui.
- **Įmonės lygio**: Sprendimai, paruošti gamybai, su įmonės integracijos galimybėmis.  
- **Daugiaplatformė**: Universalus suderinamumas su skirtingomis operacinėmis sistemomis ir aparatine įranga.

### Pagrindiniai vietinio SLM diegimo privalumai

Vietinis SLM diegimas siūlo keletą esminių privalumų, kurie daro jį idealų įmonėms ir privatumo jautrioms programoms:

**Privatumas ir saugumas**: Vietinis apdorojimas užtikrina, kad jautrūs duomenys niekada nepaliks organizacijos infrastruktūros, leidžiant laikytis GDPR, HIPAA ir kitų reguliavimo reikalavimų. Galimos izoliuotos diegimo aplinkos klasifikuotoms aplinkoms, o visiškos audito ataskaitos užtikrina saugumo priežiūrą.

**Ekonomiškumas**: Per-token kainodaros modelių panaikinimas žymiai sumažina eksploatacines išlaidas. Mažesni pralaidumo reikalavimai ir sumažinta priklausomybė nuo debesų suteikia prognozuojamas išlaidų struktūras įmonės biudžetui.

**Našumas ir patikimumas**: Greitesnis išvedimo laikas be tinklo vėlavimo leidžia realaus laiko programoms. Neprisijungimo funkcionalumas užtikrina nenutrūkstamą veikimą nepriklausomai nuo interneto ryšio, o vietinių išteklių optimizavimas suteikia nuoseklų našumą.

## Ollama: universali vietinio diegimo platforma

### Pagrindinė architektūra ir filosofija

Ollama sukurta kaip universali, kūrėjams draugiška platforma, kuri demokratizuoja vietinį LLM diegimą įvairiose aparatinės įrangos konfigūracijose ir operacinėse sistemose.

**Techninis pagrindas**: Sukurta ant patikimos llama.cpp pagrindo, Ollama naudoja efektyvų GGUF modelio formatą optimaliam našumui. Daugiaplatformis suderinamumas užtikrina nuoseklų veikimą Windows, macOS ir Linux aplinkose, o intelektualus išteklių valdymas optimizuoja CPU, GPU ir atminties naudojimą.

**Dizaino filosofija**: Ollama teikia pirmenybę paprastumui, neaukojant funkcionalumo, siūlydama diegimą be konfigūracijos, kad būtų galima iškart pradėti produktyviai dirbti. Platforma palaiko platų modelių suderinamumą, tuo pačiu užtikrindama nuoseklius API skirtingoms modelių architektūroms.

### Pažangios funkcijos ir galimybės

**Modelių valdymo meistriškumas**: Ollama siūlo išsamų modelių gyvavimo ciklo valdymą su automatiniu atsisiuntimu, talpyklos naudojimu ir versijavimu. Platforma palaiko platų modelių ekosistemą, įskaitant Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral ir specializuotus įterpimo modelius.

**Tinkinti per Modelfiles**: Pažangūs vartotojai gali kurti individualizuotas modelių konfigūracijas su specifiniais parametrais, sistemos raginimais ir elgesio pakeitimais. Tai leidžia optimizuoti pagal sritį ir specializuotus programų reikalavimus.

**Našumo optimizavimas**: Ollama automatiškai aptinka ir naudoja galimą aparatinės įrangos spartinimą, įskaitant NVIDIA CUDA, Apple Metal ir OpenCL. Intelektualus atminties valdymas užtikrina optimalų išteklių naudojimą skirtingose aparatinės įrangos konfigūracijose.

### Gamybos įgyvendinimo strategijos

**Įdiegimas ir nustatymas**: Ollama siūlo supaprastintą diegimą įvairiose platformose per vietinius diegimo įrankius, paketų tvarkykles (WinGet, Homebrew, APT) ir Docker konteinerius konteinerizuotiems diegimams.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Pagrindinės komandos ir operacijos**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Pažangi konfigūracija**: Modelfiles leidžia sudėtingą pritaikymą įmonės poreikiams:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Kūrėjų integracijos pavyzdžiai

**Python API integracija**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript integracija (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API naudojimas su cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Našumo derinimas ir optimizavimas

**Atminties ir gijų konfigūracija**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Kvantizacijos pasirinkimas skirtingai aparatine įrangai**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Įmonės Edge AI platforma

### Įmonės lygio architektūra

Microsoft Foundry Local yra išsamus įmonės sprendimas, specialiai sukurtas gamybos Edge AI diegimams su gilia integracija į Microsoft ekosistemą.

**ONNX pagrindas**: Sukurta ant pramonės standarto ONNX Runtime, Foundry Local užtikrina optimizuotą našumą įvairiose aparatinės įrangos architektūrose. Platforma naudoja Windows ML integraciją, kad optimizuotų Windows aplinką, tuo pačiu išlaikydama daugiaplatformį suderinamumą.

**Aparatinės įrangos spartinimo meistriškumas**: Foundry Local siūlo intelektualų aparatinės įrangos aptikimą ir optimizavimą CPU, GPU ir NPU. Glaudus bendradarbiavimas su aparatinės įrangos tiekėjais (AMD, Intel, NVIDIA, Qualcomm) užtikrina optimalų našumą įmonės aparatinės įrangos konfigūracijose.

### Pažangi kūrėjų patirtis

**Daugiainterfeisinė prieiga**: Foundry Local siūlo išsamias kūrimo sąsajas, įskaitant galingą CLI modelių valdymui ir diegimui, daugiakalbius SDK (Python, NodeJS) natūraliai integracijai ir RESTful API su OpenAI suderinamumu, kad būtų užtikrintas sklandus migravimas.

**Visual Studio integracija**: Platforma sklandžiai integruojasi su AI Toolkit for VS Code, siūlydama modelių konvertavimo, kvantizacijos ir optimizavimo įrankius kūrimo aplinkoje. Ši integracija pagreitina kūrimo procesus ir sumažina diegimo sudėtingumą.

**Modelių optimizavimo procesas**: Microsoft Olive integracija leidžia sudėtingus modelių optimizavimo procesus, įskaitant dinaminę kvantizaciją, grafų optimizavimą ir aparatinės įrangos specifinį derinimą. Debesų pagrindu veikiančios konvertavimo galimybės per Azure ML suteikia mastelio optimizavimą dideliems modeliams.

### Gamybos įgyvendinimo strategijos

**Įdiegimas ir konfigūracija**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Modelių valdymo operacijos**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Pažangi diegimo konfigūracija**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Įmonės ekosistemos integracija

**Saugumas ir atitiktis**: Foundry Local siūlo įmonės lygio saugumo funkcijas, įskaitant prieigos kontrolę pagal vaidmenis, audito žurnalus, atitikties ataskaitas ir užšifruotą modelių saugojimą. Integracija su Microsoft saugumo infrastruktūra užtikrina laikymąsi įmonės saugumo politikos.

**Įmontuotos AI paslaugos**: Platforma siūlo paruoštas naudoti AI galimybes, įskaitant Phi Silica vietiniam kalbos apdorojimui, AI Imaging vaizdų gerinimui ir analizei bei specializuotus API dažniems įmonės AI užduotims.

## Lyginamoji analizė: Ollama vs Foundry Local

### Techninės architektūros palyginimas

| **Aspektas** | **Ollama** | **Foundry Local** |
|--------------|------------|-------------------|
| **Modelio formatas** | GGUF (per llama.cpp) | ONNX (per ONNX Runtime) |
| **Platformos fokusas** | Universalus daugiaplatformis | Windows/Įmonės optimizacija |
| **Aparatinės įrangos integracija** | Bendras GPU/CPU palaikymas | Gili Windows ML, NPU palaikymas |
| **Optimizacija** | llama.cpp kvantizacija | Microsoft Olive + ONNX Runtime |
| **Įmonės funkcijos** | Bendruomenės valdomas | Įmonės lygio su SLA |

### Našumo charakteristikos

**Ollama našumo stiprybės**:
- Išskirtinis CPU našumas per llama.cpp optimizaciją.
- Nuoseklus veikimas skirtingose platformose ir aparatinėje įrangoje.
- Efektyvus atminties naudojimas su intelektualiu modelių įkrovimu.
- Greitas šalto paleidimo laikas kūrimui ir testavimo scenarijams.

**Foundry Local našumo privalumai**:
- Puikus NPU naudojimas modernioje Windows aparatinėje įrangoje.
- Optimizuotas GPU spartinimas per tiekėjų partnerystes.
- Įmonės lygio našumo stebėjimas ir optimizavimas.
- Mastelio diegimo galimybės gamybos aplinkoms.

### Kūrimo patirties analizė

**Ollama kūrėjų patirtis**:
- Minimalūs nustatymo reikalavimai su momentiniu produktyvumu.
- Intuityvi komandinės eilutės sąsaja visoms operacijoms.
- Plati bendruomenės parama ir dokumentacija.
- Lankstus pritaikymas per Modelfiles.

**Foundry Local kūrėjų patirtis**:
- Išsami IDE integracija su Visual Studio ekosistema.
- Įmonės kūrimo procesai su komandos bendradarbiavimo funkcijomis.
- Profesionalūs palaikymo kanalai su Microsoft užnugariu.
- Pažangūs derinimo ir optimizavimo įrankiai.

### Naudojimo atvejų optimizavimas

**Pasirinkite Ollama, kai**:
- Kuriate daugiaplatformes programas, reikalaujančias nuoseklaus veikimo.
- Pirmenybę teikiate atvirojo kodo skaidrumui ir bendruomenės indėliui.
- Dirbate su ribotais ištekliais arba biudžeto apribojimais.
- Kuriate eksperimentines arba mokslinių tyrimų orientuotas programas.
- Reikia plataus modelių suderinamumo skirtingose architektūrose.

**Pasirinkite Foundry Local, kai**:
- Diegiate įmonės programas su griežtais našumo reikalavimais.
- Naudojate Windows specifines aparatinės įrangos optimizacijas (NPU, Windows ML).
- Reikia įmonės palaikymo, SLA ir atitikties funkcijų.
- Kuriate gamybos programas su Microsoft ekosistemos integracija.
- Reikia pažangių optimizavimo įrankių ir profesionalių kūrimo procesų.

## Pažangios diegimo strategijos

### Konteinerizuoti diegimo modeliai

**Ollama konteinerizacija**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local įmonės diegimas**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Našumo optimizavimo technikos

**Ollama optimizavimo strategijos**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local optimizavimas**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Saugumo ir atitikties svarstymai

### Įmonės saugumo įgyvendinimas

**Ollama saugumo geriausios praktikos**:
- Tinklo izoliacija su ugniasienės taisyklėmis ir VPN prieiga.
- Autentifikacija per atvirkštinio proxy integraciją.
- Modelio vientisumo patikrinimas ir saugus modelių platinimas.
- Audito žurnalai API prieigai ir modelių operacijoms.

**Foundry Local įmonės saugumas**:
- Įmontuota prieigos kontrolė pagal vaidmenis su Active Directory integracija.
- Išsamūs audito žurnalai su atitikties ataskaitomis.
- Užšifruotas modelių saugojimas ir saugus modelių diegimas.
- Integracija su Microsoft saugumo infrastruktūra.

### Atitikties ir reguliavimo reikalavimai

Abi platformos palaiko reguliavimo atitiktį per:
- Duomenų rezidencijos kontrolę, užtikrinančią vietinį apdorojimą.
- Audito žurnalus reguliavimo ataskaitų reikalavimams.
- Prieigos kontrolę jautrių duomenų tvarkymui.
- Duomenų apsaugą užšifruojant tiek saugojimo, tiek perdavimo metu.

## Geriausios praktikos gamybos diegimui

### Stebėjimas ir stebimumas

**Pagrindiniai stebimi rodikliai**:
- Modelio išvedimo vėlavimas ir pralaidumas.
- Išteklių naudojimas (CPU, GPU, atmintis).
- API atsako laikas ir klaidų dažnis.
- Modelio tikslumas ir našumo nukrypimai.

**Stebėjimo įgyvendinimas**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Nuolatinė integracija ir diegimas

**CI/CD proceso integracija**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Ateities tendencijos ir svarstymai

### Atsirandančios technologijos

Vietinio SLM diegimo kraštovaizdis toliau vystosi su keliomis pagrindinėmis tendencijomis:

**Pažangios modelių architektūros**: Atsiranda naujos kartos SLM su patobulintu efektyvumu ir galimybių santykiu, įskaitant ekspertų mišinio modelius dinamiškam masteliui ir specializuotas architektūras Edge diegimui.

**Aparatinės įrangos integracija**: Gilesnė integracija su specializuota AI aparatine įranga, įskaitant NPU, pritaikytą silicio technologiją ir Edge kompiuterijos spartintuvus, suteiks patobulintas našumo galimybes.

**Ekosistemos evoliucija**: Standartizavimo pastangos tarp diegimo platformų ir patobulintas tarpusavio suderinamumas tarp skirtingų pagrindų supaprast

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors stengiamės užtikrinti tikslumą, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Dėl svarbios informacijos rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus aiškinimus, atsiradusius naudojant šį vertimą.