<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T15:24:08+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "lt"
}
-->
# 3 skyrius: Microsoft Olive optimizavimo rinkinys

## Turinys
1. [Įvadas](../../../Module04)
2. [Kas yra Microsoft Olive?](../../../Module04)
3. [Įdiegimas](../../../Module04)
4. [Greito starto vadovas](../../../Module04)
5. [Pavyzdys: Qwen3 konvertavimas į ONNX INT4](../../../Module04)
6. [Išplėstinis naudojimas](../../../Module04)
7. [Olive receptų saugykla](../../../Module04)
8. [Geriausios praktikos](../../../Module04)
9. [Trikčių šalinimas](../../../Module04)
10. [Papildomi ištekliai](../../../Module04)

## Įvadas

Microsoft Olive yra galingas ir lengvai naudojamas įrankis, skirtas modelių optimizavimui, atsižvelgiant į aparatinę įrangą. Jis supaprastina mašininio mokymosi modelių optimizavimo procesą, kad jie būtų pritaikyti įvairioms aparatinės įrangos platformoms. Nesvarbu, ar taikote procesoriams, grafikos plokštėms, ar specializuotiems AI akceleratoriams, Olive padeda pasiekti optimalų našumą išlaikant modelio tikslumą.

## Kas yra Microsoft Olive?

Olive yra lengvai naudojamas įrankis, skirtas modelių optimizavimui, atsižvelgiant į aparatinę įrangą. Jis apjungia pažangiausias technologijas, tokias kaip modelių suspaudimas, optimizavimas ir kompiliavimas. Olive veikia kartu su ONNX Runtime kaip pilnas sprendimas optimizuotam inferencijos procesui.

### Pagrindinės savybės

- **Optimizavimas pagal aparatinę įrangą**: Automatiškai parenka geriausius optimizavimo metodus jūsų tikslinės aparatinės įrangos atžvilgiu
- **40+ įmontuotų optimizavimo komponentų**: Apima modelių suspaudimą, kvantizaciją, grafų optimizavimą ir daugiau
- **Paprasta CLI sąsaja**: Lengvi komandų įvedimai dažniausiai naudojamiems optimizavimo užduotims
- **Daugiaplatformė palaikymas**: Veikia su PyTorch, Hugging Face modeliais ir ONNX
- **Populiarių modelių palaikymas**: Olive automatiškai optimizuoja populiarias modelių architektūras, tokias kaip Llama, Phi, Qwen, Gemma ir kt.

### Privalumai

- **Sutrumpintas kūrimo laikas**: Nebereikia rankiniu būdu eksperimentuoti su skirtingais optimizavimo metodais
- **Našumo padidėjimas**: Žymiai greitesnis veikimas (kai kuriais atvejais iki 6 kartų)
- **Daugiaplatformė diegimas**: Optimizuoti modeliai veikia skirtingose aparatinės įrangos ir operacinių sistemų platformose
- **Išlaikytas tikslumas**: Optimizacijos išlaiko modelio kokybę, tuo pačiu gerindamos našumą

## Įdiegimas

### Būtinos sąlygos

- Python 3.8 ar naujesnė versija
- pip paketų tvarkyklė
- Virtuali aplinka (rekomenduojama)

### Pagrindinis įdiegimas

Sukurkite ir aktyvuokite virtualią aplinką:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Įdiekite Olive su automatinio optimizavimo funkcijomis:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Pasirenkamos priklausomybės

Olive siūlo įvairias pasirenkamas priklausomybes papildomoms funkcijoms:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Įdiegimo patikrinimas

```bash
olive --help
```

Jei sėkmingai, turėtumėte matyti Olive CLI pagalbos pranešimą.

## Greito starto vadovas

### Pirmasis optimizavimas

Optimizuokime mažą kalbos modelį naudodami Olive automatinio optimizavimo funkciją:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ką daro ši komanda

Optimizavimo procesas apima: modelio gavimą iš vietinės talpyklos, ONNX grafų užfiksavimą ir svorių saugojimą ONNX duomenų faile, ONNX grafų optimizavimą ir modelio kvantizavimą į int4 naudojant RTN metodą.

### Komandos parametrų paaiškinimas

- `--model_name_or_path`: Hugging Face modelio identifikatorius arba vietinis kelias
- `--output_path`: Katalogas, kuriame bus išsaugotas optimizuotas modelis
- `--device`: Tikslinis įrenginys (cpu, gpu)
- `--provider`: Vykdymo tiekėjas (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Naudoti ONNX Runtime Generate AI inferencijai
- `--precision`: Kvantizacijos tikslumas (int4, int8, fp16)
- `--log_level`: Žurnalų detalumo lygis (0=minimalus, 1=detalus)

## Pavyzdys: Qwen3 konvertavimas į ONNX INT4

Remiantis pateiktu Hugging Face pavyzdžiu [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), štai kaip optimizuoti Qwen3 modelį:

### 1 žingsnis: Atsisiųskite modelį (pasirinktinai)

Norėdami sumažinti atsisiuntimo laiką, talpinkite tik būtinus failus:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### 2 žingsnis: Optimizuokite Qwen3 modelį

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 3 žingsnis: Testuokite optimizuotą modelį

Sukurkite paprastą Python skriptą, kad išbandytumėte savo optimizuotą modelį:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Išvesties struktūra

Po optimizavimo jūsų išvesties kataloge bus:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Išplėstinis naudojimas

### Konfigūracijos failai

Sudėtingesniems optimizavimo procesams galite naudoti JSON konfigūracijos failus:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Vykdykite su konfigūracija:

```bash
olive run --config config.json
```

### GPU optimizavimas

CUDA GPU optimizavimui:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) optimizavimui:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Modelių pritaikymas su Olive

Olive taip pat palaiko modelių pritaikymą:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Geriausios praktikos

### 1. Modelio pasirinkimas
- Pradėkite nuo mažesnių modelių testavimui (pvz., 0.5B-7B parametrų)
- Įsitikinkite, kad jūsų tikslinė modelio architektūra yra palaikoma Olive

### 2. Aparatinės įrangos apsvarstymai
- Suderinkite optimizavimo tikslą su jūsų diegimo aparatine įranga
- Naudokite GPU optimizavimą, jei turite CUDA suderinamą aparatinę įrangą
- Apsvarstykite DirectML Windows kompiuteriams su integruota grafika

### 3. Tikslumo pasirinkimas
- **INT4**: Maksimalus suspaudimas, nedidelis tikslumo praradimas
- **INT8**: Geras dydžio ir tikslumo balansas
- **FP16**: Minimalus tikslumo praradimas, vidutinis dydžio sumažinimas

### 4. Testavimas ir validacija
- Visada testuokite optimizuotus modelius su savo specifiniais naudojimo atvejais
- Palyginkite našumo metrikas (vėlavimą, pralaidumą, tikslumą)
- Naudokite reprezentatyvius įvesties duomenis vertinimui

### 5. Iteratyvus optimizavimas
- Pradėkite nuo automatinio optimizavimo greitiems rezultatams
- Naudokite konfigūracijos failus detaliam valdymui
- Eksperimentuokite su skirtingais optimizavimo etapais

## Trikčių šalinimas

### Dažnos problemos

#### 1. Įdiegimo problemos
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU problemos
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Atminties problemos
- Naudokite mažesnius partijų dydžius optimizavimo metu
- Pirmiausia bandykite kvantizaciją su aukštesniu tikslumu (int8 vietoj int4)
- Įsitikinkite, kad yra pakankamai disko vietos modelio talpinimui

#### 4. Modelio įkėlimo klaidos
- Patikrinkite modelio kelią ir prieigos leidimus
- Patikrinkite, ar modelis reikalauja `trust_remote_code=True`
- Įsitikinkite, kad visi reikalingi modelio failai yra atsisiųsti

### Pagalbos gavimas

- **Dokumentacija**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub problemos**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Pavyzdžiai**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive receptų saugykla

### Įvadas į Olive receptus

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) saugykla papildo pagrindinį Olive įrankį, pateikdama išsamų paruoštų optimizavimo receptų rinkinį populiariems AI modeliams. Ši saugykla yra praktinis šaltinis tiek viešai prieinamų modelių optimizavimui, tiek optimizavimo darbo eigų kūrimui nuosaviems modeliams.

### Pagrindinės savybės

- **100+ paruoštų receptų**: Paruoštos optimizavimo konfigūracijos populiariems modeliams
- **Daugiaplatformė palaikymas**: Apima transformatorių modelius, vizualinius modelius ir multimodalines architektūras
- **Aparatinės įrangos specifinės optimizacijos**: Receptai pritaikyti CPU, GPU ir specializuotiems akceleratoriams
- **Populiarios modelių šeimos**: Įtraukia Phi, Llama, Qwen, Gemma, Mistral ir daugelį kitų

### Palaikomos modelių šeimos

Saugykla apima optimizavimo receptus:

#### Kalbos modeliai
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 serija (0.5B iki 14B)
- **Google Gemma**: Įvairios Gemma modelių konfigūracijos
- **Mistral AI**: Mistral-7B serija
- **DeepSeek**: R1-Distill serijos modeliai

#### Vizualiniai ir multimodaliniai modeliai
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP modeliai**: Įvairios CLIP-ViT konfigūracijos
- **ResNet**: ResNet-50 optimizacijos
- **Vizualiniai transformatoriai**: ViT-base-patch16-224

#### Specializuoti modeliai
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Bazinės ir daugiakalbės versijos
- **Sakinio transformatoriai**: all-MiniLM-L6-v2

### Naudojimasis Olive receptais

#### 1 metodas: Atsisiųskite konkretų receptą

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### 2 metodas: Naudokite receptą kaip šabloną

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Recepto struktūra

Kiekviename recepto kataloge paprastai yra:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Pavyzdys: Phi-4-mini recepto naudojimas

Naudokime Phi-4-mini receptą kaip pavyzdį:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfigūracijos failas paprastai apima:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Receptų pritaikymas

#### Tikslinės aparatinės įrangos keitimas

Norėdami pakeisti tikslinę aparatinę įrangą, atnaujinkite `systems` skyrių:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimizavimo parametrų koregavimas

Keiskite `passes` skyrių skirtingiems optimizavimo lygiams:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Sukurkite savo receptą

1. **Pradėkite nuo panašaus modelio**: Suraskite receptą modeliui su panašia architektūra
2. **Atnaujinkite modelio konfigūraciją**: Pakeiskite modelio pavadinimą/kelia konfigūracijoje
3. **Koreguokite parametrus**: Pakeiskite optimizavimo parametrus pagal poreikį
4. **Testuokite ir validuokite**: Vykdykite optimizavimą ir patikrinkite rezultatus
5. **Prisidėkite prie bendruomenės**: Apsvarstykite galimybę prisidėti savo receptu prie saugyklos

### Receptų naudojimo privalumai

#### 1. **Patikrintos konfigūracijos**
- Išbandyti optimizavimo nustatymai konkretiems modeliams
- Vengia bandymų ir klaidų ieškant optimalių parametrų

#### 2. **Aparatinės įrangos specifinis derinimas**
- Iš anksto optimizuota skirtingiems vykdymo tiekėjams
- Paruoštos konfigūracijos CPU, GPU ir NPU tikslams

#### 3. **Išsamus aprėptis**
- Palaiko populiariausius atvirojo kodo modelius
- Reguliarūs atnaujinimai su naujais modelių leidimais

#### 4. **Bendruomenės indėlis**
- Bendradarbiavimas su AI bendruomene
- Dalijimasis žiniomis ir geriausiomis praktikomis

### Prisidėjimas prie Olive receptų

Jei optimizavote modelį, kuris nėra įtrauktas į saugyklą:

1. **Fork saugyklą**: Sukurkite savo Olive receptų fork'ą
2. **Sukurkite recepto katalogą**: Pridėkite naują katalogą savo modeliui
3. **Įtraukite konfigūraciją**: Pridėkite olive_config.json ir palaikomuosius failus
4. **Dokumentuokite naudojimą**: Pateikite aiškų README su instrukcijomis
5. **Pateikite Pull Request**: Prisidėkite prie bendruomenės

### Našumo rodikliai

Daugelis receptų apima našumo rodiklius, rodančius:
- **Vėlavimo sumažinimą**: Tipiškai 2-6x greitesnis veikimas nei pradinė versija
- **Atminties sumažinimą**: 50-75% mažesnis atminties naudojimas su kvantizacija
- **Tikslumo išlaikymą**: 95-99% tikslumo išsaugojimas

### Integracija su AI įrankių rinkiniu

Receptai sklandžiai veikia su:
- **VS Code AI Toolkit**: Tiesioginė integracija modelių optimizavimui
- **Azure Machine Learning**: Optimizavimo darbo eigos debesyje
- **ONNX Runtime**: Optimizuotas inferencijos diegimas

## Papildomi ištekliai

### Oficialios nuorodos
- **GitHub saugykla**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive receptų saugykla**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime dokumentacija**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face pavyzdys**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Bendruomenės pavyzdžiai
- **Jupyter užrašų knygelės**: Prieinamos Olive GitHub saugykloje — https://github.com/microsoft/Olive/tree/main/examples

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Dėl svarbios informacijos rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus aiškinimus, atsiradusius naudojant šį vertimą.