<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T16:03:00+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "lt"
}
-->
# 7 skyrius: Qualcomm QNN (Qualcomm Neural Network) optimizavimo rinkinys

## Turinys
1. [Įvadas](../../../Module04)
2. [Kas yra Qualcomm QNN?](../../../Module04)
3. [Įdiegimas](../../../Module04)
4. [Greito starto vadovas](../../../Module04)
5. [Pavyzdys: modelių konvertavimas ir optimizavimas su QNN](../../../Module04)
6. [Išplėstinis naudojimas](../../../Module04)
7. [Geriausia praktika](../../../Module04)
8. [Trikčių šalinimas](../../../Module04)
9. [Papildomi ištekliai](../../../Module04)

## Įvadas

Qualcomm QNN (Qualcomm Neural Network) yra išsamus AI inferencijos pagrindas, sukurtas tam, kad maksimaliai išnaudotų Qualcomm AI aparatūros akceleratorius, įskaitant Hexagon NPU, Adreno GPU ir Kryo CPU. Nesvarbu, ar taikote mobiliesiems įrenginiams, kraštiniams skaičiavimo platformoms, ar automobilių sistemoms, QNN suteikia optimizuotas inferencijos galimybes, kurios pasinaudoja Qualcomm specializuotais AI apdorojimo vienetais, siekiant maksimalaus našumo ir energijos efektyvumo.

## Kas yra Qualcomm QNN?

Qualcomm QNN yra vieningas AI inferencijos pagrindas, leidžiantis kūrėjams efektyviai diegti AI modelius per Qualcomm heterogeninę skaičiavimo architektūrą. Jis suteikia vieningą programavimo sąsają, skirtą pasiekti Hexagon NPU (Neural Processing Unit), Adreno GPU ir Kryo CPU, automatiškai parenkant optimalų apdorojimo vienetą skirtingiems modelio sluoksniams ir operacijoms.

### Pagrindinės savybės

- **Heterogeninis skaičiavimas**: Vieninga prieiga prie NPU, GPU ir CPU su automatiniu darbo krūvio paskirstymu
- **Aparatūrai pritaikyta optimizacija**: Specializuotos optimizacijos Qualcomm Snapdragon platformoms
- **Kvantizacijos palaikymas**: Pažangios INT8, INT16 ir mišrios precizijos kvantizacijos technikos
- **Modelių konvertavimo įrankiai**: Tiesioginis TensorFlow, PyTorch, ONNX ir Caffe modelių palaikymas
- **Optimizuota kraštinei AI**: Specialiai sukurta mobiliesiems ir kraštiniams diegimo scenarijams, orientuojantis į energijos efektyvumą

### Privalumai

- **Maksimalus našumas**: Pasinaudokite specializuota AI aparatūra, kad pasiektumėte iki 15 kartų geresnį našumą
- **Energijos efektyvumas**: Optimizuota mobiliesiems ir baterijomis maitinamiems įrenginiams su išmaniu energijos valdymu
- **Maža delsimo trukmė**: Aparatūros pagreitinta inferencija su minimaliomis sąnaudomis realaus laiko programoms
- **Mastelio pritaikymas**: Nuo išmaniųjų telefonų iki automobilių platformų visoje Qualcomm ekosistemoje
- **Paruošta gamybai**: Patikrintas pagrindas, naudojamas milijonuose diegtų įrenginių

## Įdiegimas

### Būtinos sąlygos

- Qualcomm QNN SDK (reikalinga registracija su Qualcomm)
- Python 3.7 ar naujesnė versija
- Suderinama Qualcomm aparatūra arba simuliatorius
- Android NDK (mobiliesiems diegimams)
- Linux arba Windows kūrimo aplinka

### QNN SDK nustatymas

1. **Registracija ir atsisiuntimas**: Apsilankykite Qualcomm Developer Network, kad užsiregistruotumėte ir atsisiųstumėte QNN SDK
2. **SDK išpakavimas**: Išpakuokite QNN SDK į savo kūrimo katalogą
3. **Aplinkos kintamųjų nustatymas**: Konfigūruokite QNN įrankių ir bibliotekų kelius

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python aplinkos nustatymas

Sukurkite ir aktyvuokite virtualią aplinką:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Įdiekite reikalingus Python paketus:

```bash
pip install numpy tensorflow torch onnx
```

### Įdiegimo patikrinimas

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Jei sėkminga, turėtumėte matyti pagalbos informaciją kiekvienam QNN įrankiui.

## Greito starto vadovas

### Pirmasis modelio konvertavimas

Konvertuokime paprastą PyTorch modelį, kad jis veiktų Qualcomm aparatūroje:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Konvertavimas iš ONNX į QNN formatą

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### QNN modelio bibliotekos generavimas

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Ką daro šis procesas

Optimizavimo darbo eiga apima: originalaus modelio konvertavimą į ONNX formatą, ONNX vertimą į QNN tarpinę reprezentaciją, aparatūrai pritaikytų optimizacijų taikymą ir sukompiliuotos modelio bibliotekos generavimą diegimui.

### Pagrindinių parametrų paaiškinimas

- `--input_network`: Šaltinio ONNX modelio failas
- `--output_path`: Sukurtas C++ šaltinio failas
- `--input_dim`: Įvesties tensoriaus matmenys optimizacijai
- `--quantization_overrides`: Individuali kvantizacijos konfigūracija
- `-t x86_64-linux-clang`: Tikslinė architektūra ir kompiliatorius

## Pavyzdys: modelių konvertavimas ir optimizavimas su QNN

### 1 žingsnis: pažangus modelio konvertavimas su kvantizacija

Štai kaip taikyti individualią kvantizaciją konvertavimo metu:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konvertavimas su individualia kvantizacija:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### 2 žingsnis: daugiaplatforminė optimizacija

Konfigūruokite heterogeninį vykdymą per NPU, GPU ir CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### 3 žingsnis: konteksto binaro kūrimas diegimui

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### 4 žingsnis: inferencija su QNN vykdymo aplinka

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Išvesties struktūra

Po optimizavimo jūsų diegimo kataloge bus:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Išplėstinis naudojimas

### Individuali platformos konfigūracija

Konfigūruokite specifines platformos optimizacijas:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dinaminė kvantizacija

Taikykite kvantizaciją vykdymo metu, kad pasiektumėte geresnį tikslumą:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Našumo profiliavimas

Stebėkite našumą per skirtingas platformas:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatinis platformos pasirinkimas

Įgyvendinkite išmanų platformos pasirinkimą pagal modelio charakteristikas:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Geriausia praktika

### 1. Modelio architektūros optimizavimas
- **Sluoksnių sujungimas**: Sujunkite operacijas, tokias kaip Conv+BatchNorm+ReLU, kad geriau išnaudotumėte NPU
- **Gylio atskiriamos konvoliucijos**: Pirmenybę teikite joms vietoj standartinių konvoliucijų mobiliesiems diegimams
- **Kvantizacijai draugiški dizainai**: Naudokite ReLU aktyvacijas ir venkite operacijų, kurios sunkiai kvantizuojamos

### 2. Kvantizacijos strategija
- **Kvantizacija po mokymo**: Pradėkite nuo šios strategijos greitam diegimui
- **Kalibravimo duomenų rinkinys**: Naudokite reprezentatyvius duomenis, apimančius visas įvesties variacijas
- **Mišri precizija**: Naudokite INT8 daugumai sluoksnių, kritinius sluoksnius palikite aukštesnėje precizijoje

### 3. Platformos pasirinkimo gairės
- **NPU (HTP)**: Geriausia CNN darbo krūviams, kvantizuotiems modeliams ir energijai jautrioms programoms
- **GPU**: Optimalu skaičiavimo intensyvioms operacijoms, didesniems modeliams ir FP16 precizijai
- **CPU**: Atsarginis variantas nepalaikomoms operacijoms ir derinimui

### 4. Našumo optimizavimas
- **Partijos dydis**: Naudokite partijos dydį 1 realaus laiko programoms, didesnes partijas našumui
- **Įvesties išankstinis apdorojimas**: Minimizuokite duomenų kopijavimo ir konvertavimo sąnaudas
- **Konteksto pakartotinis naudojimas**: Iš anksto sukompiliuokite kontekstus, kad išvengtumėte vykdymo laiko kompiliavimo sąnaudų

### 5. Atminties valdymas
- **Tensoriaus paskirstymas**: Naudokite statinį paskirstymą, kai įmanoma, kad išvengtumėte vykdymo laiko sąnaudų
- **Atminties baseinai**: Įgyvendinkite individualius atminties baseinus dažnai paskirstomiems tensoriams
- **Buferio pakartotinis naudojimas**: Pakartotinai naudokite įvesties/išvesties buferius per inferencijos skambučius

### 6. Energijos optimizavimas
- **Našumo režimai**: Naudokite tinkamus našumo režimus pagal šiluminius apribojimus
- **Dinaminis dažnio keitimas**: Leiskite sistemai keisti dažnį pagal darbo krūvį
- **Neaktyvios būsenos valdymas**: Tinkamai atlaisvinkite išteklius, kai jie nenaudojami

## Trikčių šalinimas

### Dažnos problemos

#### 1. SDK įdiegimo problemos
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Modelio konvertavimo klaidos
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Kvantizacijos problemos
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Našumo problemos
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Atminties problemos
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Platformos suderinamumas
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Našumo derinimas

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Pagalbos gavimas

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN dokumentacija**: Prieinama SDK pakete
- **Bendruomenės forumai**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Techninė pagalba**: Per Qualcomm kūrėjų portalą

## Papildomi ištekliai

### Oficialios nuorodos
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon platformos**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Kūrėjų portalas**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI variklis**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Mokymosi ištekliai
- **Pradžios vadovas**: Prieinamas QNN SDK dokumentacijoje
- **Modelių biblioteka**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimizavimo vadovas**: SDK dokumentacija apima išsamias optimizavimo gaires
- **Vaizdo pamokos**: [Qualcomm Developer YouTube kanalas](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integracijos įrankiai
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Iš anksto optimizuoti modeliai Qualcomm aparatūrai
- **Android Neural Networks API**: Integracija su Android NNAPI
- **TensorFlow Lite delegatas**: Qualcomm delegatas TFLite

### Našumo etalonai
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI tyrimai**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Bendruomenės pavyzdžiai
- **Pavyzdinės programos**: Prieinamos QNN SDK pavyzdžių kataloge
- **GitHub saugyklos**: Bendruomenės pateikti pavyzdžiai ir įrankiai
- **Techniniai tinklaraščiai**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Susiję įrankiai
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Pažangios kvantizacijos ir suspaudimo technikos
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Palyginimui ir atsarginiam diegimui
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Kryžminės platformos inferencijos variklis

### Aparatūros specifikacijos
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon platformos**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Kas toliau

Tęskite savo kraštinės AI kelionę, tyrinėdami [5 modulį: SLMOps ir gamybos diegimas](../Module05/README.md), kad sužinotumėte apie mažų kalbos modelių gyvavimo ciklo valdymo operacinius aspektus.

---

**Atsakomybės apribojimas**:  
Šis dokumentas buvo išverstas naudojant AI vertimo paslaugą [Co-op Translator](https://github.com/Azure/co-op-translator). Nors siekiame tikslumo, prašome atkreipti dėmesį, kad automatiniai vertimai gali turėti klaidų ar netikslumų. Originalus dokumentas jo gimtąja kalba turėtų būti laikomas autoritetingu šaltiniu. Dėl svarbios informacijos rekomenduojama profesionali žmogaus vertimo paslauga. Mes neprisiimame atsakomybės už nesusipratimus ar neteisingus aiškinimus, atsiradusius naudojant šį vertimą.