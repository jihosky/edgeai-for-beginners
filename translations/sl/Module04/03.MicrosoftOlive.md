<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T15:01:41+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sl"
}
-->
# Poglavje 3: Microsoft Olive Optimization Suite

## Kazalo
1. [Uvod](../../../Module04)
2. [Kaj je Microsoft Olive?](../../../Module04)
3. [Namestitev](../../../Module04)
4. [Hitri vodič](../../../Module04)
5. [Primer: Pretvorba Qwen3 v ONNX INT4](../../../Module04)
6. [Napredna uporaba](../../../Module04)
7. [Olive Recipes Repository](../../../Module04)
8. [Najboljše prakse](../../../Module04)
9. [Odpravljanje težav](../../../Module04)
10. [Dodatni viri](../../../Module04)

## Uvod

Microsoft Olive je zmogljivo in enostavno za uporabo orodje za optimizacijo modelov, ki je prilagojeno strojni opremi. Poenostavi proces optimizacije modelov strojnega učenja za uporabo na različnih strojnih platformah. Ne glede na to, ali ciljate na CPU-je, GPU-je ali specializirane AI pospeševalnike, Olive pomaga doseči optimalno zmogljivost ob ohranjanju natančnosti modela.

## Kaj je Microsoft Olive?

Olive je enostavno za uporabo orodje za optimizacijo modelov, prilagojeno strojni opremi, ki združuje vodilne tehnike na področju stiskanja modelov, optimizacije in kompilacije. Deluje z ONNX Runtime kot celovita rešitev za optimizacijo inferenčnega procesa.

### Ključne značilnosti

- **Optimizacija prilagojena strojni opremi**: Samodejno izbere najboljše tehnike optimizacije za ciljno strojno opremo
- **40+ vgrajenih komponent za optimizacijo**: Vključuje stiskanje modelov, kvantizacijo, optimizacijo grafov in več
- **Enostaven CLI vmesnik**: Preprosti ukazi za pogoste naloge optimizacije
- **Podpora za več ogrodij**: Deluje s PyTorch, Hugging Face modeli in ONNX
- **Podpora za priljubljene modele**: Olive lahko samodejno optimizira priljubljene arhitekture modelov, kot so Llama, Phi, Qwen, Gemma itd., brez dodatnih nastavitev

### Prednosti

- **Skrajšan čas razvoja**: Ni potrebe po ročnem eksperimentiranju z različnimi tehnikami optimizacije
- **Izboljšana zmogljivost**: Pomembne izboljšave hitrosti (do 6x v nekaterih primerih)
- **Križno-platformska uporaba**: Optimizirani modeli delujejo na različnih strojnih platformah in operacijskih sistemih
- **Ohranjena natančnost**: Optimizacije ohranjajo kakovost modela ob izboljšanju zmogljivosti

## Namestitev

### Predpogoji

- Python 3.8 ali novejši
- Upravljalnik paketov pip
- Virtualno okolje (priporočeno)

### Osnovna namestitev

Ustvarite in aktivirajte virtualno okolje:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Namestite Olive z funkcijami samodejne optimizacije:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Izbirne odvisnosti

Olive ponuja različne izbirne odvisnosti za dodatne funkcije:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Preverjanje namestitve

```bash
olive --help
```

Če je uspešno, bi morali videti sporočilo za pomoč Olive CLI.

## Hitri vodič

### Vaša prva optimizacija

Optimizirajmo majhen jezikovni model z uporabo funkcije samodejne optimizacije Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Kaj ta ukaz naredi

Proces optimizacije vključuje: pridobitev modela iz lokalnega predpomnilnika, zajem ONNX grafa in shranjevanje uteži v ONNX podatkovno datoteko, optimizacijo ONNX grafa ter kvantizacijo modela na int4 z metodo RTN.

### Pojasnilo parametrov ukaza

- `--model_name_or_path`: Identifikator modela Hugging Face ali lokalna pot
- `--output_path`: Mapa, kjer bo shranjen optimizirani model
- `--device`: Ciljna naprava (cpu, gpu)
- `--provider`: Izvajalni ponudnik (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Uporaba ONNX Runtime Generate AI za inferenco
- `--precision`: Kvantizacijska natančnost (int4, int8, fp16)
- `--log_level`: Stopnja podrobnosti dnevnika (0=minimalno, 1=podrobno)

## Primer: Pretvorba Qwen3 v ONNX INT4

Na podlagi podanega primera Hugging Face na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) je tukaj, kako optimizirati model Qwen3:

### Korak 1: Prenos modela (neobvezno)

Za zmanjšanje časa prenosa predpomnite le bistvene datoteke:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Korak 2: Optimizacija modela Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Korak 3: Testiranje optimiziranega modela

Ustvarite preprost Python skript za testiranje vašega optimiziranega modela:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura izhoda

Po optimizaciji bo vaša izhodna mapa vsebovala:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Napredna uporaba

### Konfiguracijske datoteke

Za bolj zapletene delovne procese optimizacije lahko uporabite JSON konfiguracijske datoteke:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Zaženite s konfiguracijo:

```bash
olive run --config config.json
```

### Optimizacija za GPU

Za optimizacijo CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Za DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fino prilagajanje z Olive

Olive podpira tudi fino prilagajanje modelov:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najboljše prakse

### 1. Izbira modela
- Začnite s manjšimi modeli za testiranje (npr. 0.5B-7B parametrov)
- Prepričajte se, da je ciljna arhitektura modela podprta z Olive

### 2. Premisleki o strojni opremi
- Prilagodite ciljno optimizacijo strojni opremi za uporabo
- Uporabite optimizacijo za GPU, če imate strojno opremo, združljivo s CUDA
- Razmislite o DirectML za Windows naprave z integrirano grafiko

### 3. Izbira natančnosti
- **INT4**: Maksimalna stiskanje, rahla izguba natančnosti
- **INT8**: Dobra uravnoteženost med velikostjo in natančnostjo
- **FP16**: Minimalna izguba natančnosti, zmerno zmanjšanje velikosti

### 4. Testiranje in validacija
- Vedno testirajte optimizirane modele z vašimi specifičnimi primeri uporabe
- Primerjajte zmogljivostne metrike (zakasnitev, prepustnost, natančnost)
- Uporabite reprezentativne vhodne podatke za ocenjevanje

### 5. Iterativna optimizacija
- Začnite s samodejno optimizacijo za hitre rezultate
- Uporabite konfiguracijske datoteke za natančno kontrolo
- Eksperimentirajte z različnimi prehodi optimizacije

## Odpravljanje težav

### Pogoste težave

#### 1. Težave z namestitvijo
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Težave s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Težave s pomnilnikom
- Uporabite manjše velikosti serij med optimizacijo
- Poskusite kvantizacijo z višjo natančnostjo (int8 namesto int4)
- Prepričajte se, da imate dovolj prostora na disku za predpomnjenje modelov

#### 4. Napake pri nalaganju modela
- Preverite pot modela in dovoljenja za dostop
- Preverite, ali model zahteva `trust_remote_code=True`
- Prepričajte se, da so prenesene vse potrebne datoteke modela

### Pomoč

- **Dokumentacija**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub težave**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Primeri**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes Repository

### Uvod v Olive Recipes

[Microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) repozitorij dopolnjuje glavno orodje Olive z obsežno zbirko pripravljenih receptov za optimizacijo priljubljenih AI modelov. Ta repozitorij služi kot praktična referenca za optimizacijo javno dostopnih modelov in ustvarjanje delovnih procesov optimizacije za lastniške modele.

### Ključne značilnosti

- **100+ pripravljenih receptov**: Pripravljene konfiguracije optimizacije za priljubljene modele
- **Podpora za več arhitektur**: Vključuje modele transformatorjev, vizualne modele in multimodalne arhitekture
- **Optimizacije prilagojene strojni opremi**: Recepti prilagojeni za CPU, GPU in specializirane pospeševalnike
- **Priljubljene družine modelov**: Vključuje Phi, Llama, Qwen, Gemma, Mistral in mnoge druge

### Podprte družine modelov

Repozitorij vključuje recepte za optimizacijo:

#### Jezikovni modeli
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 serija (0.5B do 14B)
- **Google Gemma**: Različne konfiguracije modelov Gemma
- **Mistral AI**: Serija Mistral-7B
- **DeepSeek**: R1-Distill serija modelov

#### Vizualni in multimodalni modeli
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP modeli**: Različne konfiguracije CLIP-ViT
- **ResNet**: Optimizacije za ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Specializirani modeli
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Osnovne in večjezične različice
- **Sentence Transformers**: all-MiniLM-L6-v2

### Uporaba Olive receptov

#### Metoda 1: Kloniranje specifičnega recepta

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metoda 2: Uporaba recepta kot predloge

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Struktura recepta

Vsaka mapa recepta običajno vsebuje:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Primer: Uporaba recepta Phi-4-mini

Uporabimo recept Phi-4-mini kot primer:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfiguracijska datoteka običajno vključuje:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Prilagajanje receptov

#### Spreminjanje ciljne strojne opreme

Za spremembo ciljne strojne opreme posodobite razdelek `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Prilagajanje parametrov optimizacije

Spremenite razdelek `passes` za različne ravni optimizacije:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Ustvarjanje lastnega recepta

1. **Začnite s podobnim modelom**: Poiščite recept za model s podobno arhitekturo
2. **Posodobite konfiguracijo modela**: Spremenite ime/pot modela v konfiguraciji
3. **Prilagodite parametre**: Po potrebi spremenite parametre optimizacije
4. **Testirajte in validirajte**: Izvedite optimizacijo in preverite rezultate
5. **Prispevajte nazaj**: Razmislite o prispevku svojega recepta v repozitorij

### Prednosti uporabe receptov

#### 1. **Preverjene konfiguracije**
- Testirane nastavitve optimizacije za specifične modele
- Izogibanje poskusom in napakam pri iskanju optimalnih parametrov

#### 2. **Prilagoditev strojni opremi**
- Predhodno optimizirano za različne izvajalne ponudnike
- Pripravljene konfiguracije za cilje CPU, GPU in NPU

#### 3. **Obsežna pokritost**
- Podpora za najbolj priljubljene odprtokodne modele
- Redne posodobitve z novimi izdajami modelov

#### 4. **Prispevki skupnosti**
- Sodelovalni razvoj z AI skupnostjo
- Deljenje znanja in najboljših praks

### Prispevanje k Olive receptom

Če ste optimizirali model, ki ni zajet v repozitoriju:

1. **Forkajte repozitorij**: Ustvarite svojo različico olive-recipes
2. **Ustvarite mapo za recept**: Dodajte novo mapo za svoj model
3. **Dodajte konfiguracijo**: Dodajte olive_config.json in podporne datoteke
4. **Dokumentirajte uporabo**: Pripravite jasen README z navodili
5. **Oddajte Pull Request**: Prispevajte nazaj skupnosti

### Merila zmogljivosti

Veliko receptov vključuje merila zmogljivosti, ki prikazujejo:
- **Izboljšanje zakasnitve**: Običajno 2-6x hitrejše od osnovne različice
- **Zmanjšanje pomnilnika**: 50-75% zmanjšanje uporabe pomnilnika s kvantizacijo
- **Ohranitev natančnosti**: 95-99% ohranitev natančnosti

### Integracija z AI orodji

Recepti se brez težav integrirajo z:
- **VS Code AI Toolkit**: Neposredna integracija za optimizacijo modelov
- **Azure Machine Learning**: Delovni procesi optimizacije v oblaku
- **ONNX Runtime**: Optimizirana uporaba inferenčnega procesa

## Dodatni viri

### Uradne povezave
- **GitHub repozitorij**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime dokumentacija**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face primer**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Primeri skupnosti
- **Jupyter zvezki**: Na voljo v Olive GitHub repozitoriju — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code razširitev**: Pregled AI Toolkit za VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog objave**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Povezana orodja
- **ONNX Runtime**: Visoko zmogljiv inferenčni motor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Vir mnogih združljivih modelov — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Delovni procesi optimizacije v oblaku — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Kaj sledi

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje AI [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije je priporočljivo profesionalno človeško prevajanje. Ne odgovarjamo za morebitne nesporazume ali napačne razlage, ki izhajajo iz uporabe tega prevoda.