<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T16:01:36+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "sl"
}
-->
# Poglavje 7: Qualcomm QNN (Qualcomm Neural Network) Optimizacijski paket

## Kazalo vsebine
1. [Uvod](../../../Module04)
2. [Kaj je Qualcomm QNN?](../../../Module04)
3. [Namestitev](../../../Module04)
4. [Hitri vodič](../../../Module04)
5. [Primer: Pretvorba in optimizacija modelov z QNN](../../../Module04)
6. [Napredna uporaba](../../../Module04)
7. [Najboljše prakse](../../../Module04)
8. [Odpravljanje težav](../../../Module04)
9. [Dodatni viri](../../../Module04)

## Uvod

Qualcomm QNN (Qualcomm Neural Network) je celovit okvir za AI inferenco, zasnovan za izkoriščanje polnega potenciala Qualcommovih AI strojnih pospeševalnikov, vključno s Hexagon NPU, Adreno GPU in Kryo CPU. Ne glede na to, ali ciljate na mobilne naprave, platforme za robno računalništvo ali avtomobilske sisteme, QNN omogoča optimizirano inferenco, ki izkorišča Qualcommove specializirane AI procesne enote za maksimalno zmogljivost in energetsko učinkovitost.

## Kaj je Qualcomm QNN?

Qualcomm QNN je enoten okvir za AI inferenco, ki razvijalcem omogoča učinkovito uvajanje AI modelov na Qualcommovo heterogeno računalniško arhitekturo. Ponuja enoten programski vmesnik za dostop do Hexagon NPU (Neural Processing Unit), Adreno GPU in Kryo CPU ter samodejno izbira optimalno procesno enoto za različne sloje in operacije modela.

### Ključne značilnosti

- **Heterogeno računalništvo**: Enoten dostop do NPU, GPU in CPU s samodejno porazdelitvijo delovne obremenitve
- **Optimizacija glede na strojno opremo**: Specializirane optimizacije za platforme Qualcomm Snapdragon
- **Podpora za kvantizacijo**: Napredne tehnike kvantizacije INT8, INT16 in mešane natančnosti
- **Orodja za pretvorbo modelov**: Neposredna podpora za modele TensorFlow, PyTorch, ONNX in Caffe
- **Optimizirano za robno AI**: Zasnovano posebej za mobilne in robne scenarije z osredotočenostjo na energetsko učinkovitost

### Prednosti

- **Maksimalna zmogljivost**: Izkoristite specializirano AI strojno opremo za do 15-kratne izboljšave zmogljivosti
- **Energetska učinkovitost**: Optimizirano za mobilne naprave in naprave na baterijski pogon z inteligentnim upravljanjem energije
- **Nizka zakasnitev**: Strojno pospešena inferenca z minimalnimi režijskimi stroški za aplikacije v realnem času
- **Razširljiva uvedba**: Od pametnih telefonov do avtomobilskih platform v Qualcommovem ekosistemu
- **Pripravljeno za proizvodnjo**: Preizkušen okvir, uporabljen v milijonih naprav

## Namestitev

### Predpogoji

- Qualcomm QNN SDK (zahteva registracijo pri Qualcommu)
- Python 3.7 ali novejši
- Združljiva Qualcomm strojna oprema ali simulator
- Android NDK (za mobilno uvedbo)
- Razvojno okolje Linux ali Windows

### Nastavitev QNN SDK

1. **Registracija in prenos**: Obiščite Qualcomm Developer Network za registracijo in prenos QNN SDK
2. **Razpakirajte SDK**: Razpakirajte QNN SDK v svojo razvojno mapo
3. **Nastavite okoljske spremenljivke**: Konfigurirajte poti za QNN orodja in knjižnice

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Nastavitev Python okolja

Ustvarite in aktivirajte virtualno okolje:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Namestite potrebne Python pakete:

```bash
pip install numpy tensorflow torch onnx
```

### Preverite namestitev

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Če je uspešno, bi morali videti informacije o pomoči za vsako QNN orodje.

## Hitri vodič

### Vaša prva pretvorba modela

Pretvorimo preprost PyTorch model za delovanje na Qualcomm strojni opremi:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Pretvorba ONNX v QNN format

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Ustvarjanje knjižnice QNN modela

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Kaj ta proces vključuje

Delovni tok optimizacije vključuje: pretvorbo izvirnega modela v ONNX format, prevajanje ONNX v vmesno predstavitev QNN, uporabo optimizacij, specifičnih za strojno opremo, in generiranje prevedene knjižnice modela za uvedbo.

### Pojasnitev ključnih parametrov

- `--input_network`: Izvorna datoteka ONNX modela
- `--output_path`: Generirana C++ izvorna datoteka
- `--input_dim`: Dimenzije vhodnega tenzorja za optimizacijo
- `--quantization_overrides`: Prilagoditev konfiguracije kvantizacije
- `-t x86_64-linux-clang`: Ciljna arhitektura in prevajalnik

## Primer: Pretvorba in optimizacija modelov z QNN

### Korak 1: Napredna pretvorba modela s kvantizacijo

Tukaj je, kako uporabiti prilagojeno kvantizacijo med pretvorbo:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Pretvorba s prilagojeno kvantizacijo:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Korak 2: Optimizacija za več zaledij

Konfiguracija za heterogeno izvajanje na NPU, GPU in CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Korak 3: Ustvarjanje binarne kontekstne datoteke za uvedbo

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Korak 4: Inferenca z QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Struktura izhoda

Po optimizaciji bo vaša mapa za uvedbo vsebovala:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Napredna uporaba

### Prilagoditev konfiguracije zaledja

Konfigurirajte specifične optimizacije zaledja:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dinamična kvantizacija

Uporabite kvantizacijo med izvajanjem za boljšo natančnost:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profiliranje zmogljivosti

Spremljajte zmogljivost na različnih zaledjih:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Samodejna izbira zaledja

Implementirajte inteligentno izbiro zaledja glede na značilnosti modela:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Najboljše prakse

### 1. Optimizacija arhitekture modela
- **Združevanje slojev**: Združite operacije, kot so Conv+BatchNorm+ReLU, za boljšo izrabo NPU
- **Globinsko ločljive konvolucije**: Prednostno uporabite te namesto standardnih konvolucij za mobilno uvedbo
- **Dizajni, prijazni kvantizaciji**: Uporabite ReLU aktivacije in se izogibajte operacijam, ki se težko kvantizirajo

### 2. Strategija kvantizacije
- **Kvantizacija po treningu**: Začnite s tem za hitro uvedbo
- **Kalibracijski podatkovni niz**: Uporabite reprezentativne podatke, ki pokrivajo vse variacije vhodov
- **Mešana natančnost**: Uporabite INT8 za večino slojev, kritične sloje pa ohranite v višji natančnosti

### 3. Smernice za izbiro zaledja
- **NPU (HTP)**: Najboljše za CNN delovne obremenitve, kvantizirane modele in aplikacije, občutljive na energijo
- **GPU**: Optimalno za operacije, ki zahtevajo veliko računske moči, večje modele in FP16 natančnost
- **CPU**: Rezervna možnost za nepodprte operacije in odpravljanje napak

### 4. Optimizacija zmogljivosti
- **Velikost paketa**: Uporabite velikost paketa 1 za aplikacije v realnem času, večje pakete za prepustnost
- **Predhodna obdelava vhodov**: Zmanjšajte režijske stroške kopiranja in pretvorbe podatkov
- **Ponovna uporaba konteksta**: Predhodno prevedite kontekste, da se izognete režijskim stroškom prevajanja med izvajanjem

### 5. Upravljanje pomnilnika
- **Dodelitev tenzorjev**: Uporabite statično dodelitev, kadar je to mogoče, da se izognete režijskim stroškom med izvajanjem
- **Pomnilniški bazeni**: Implementirajte prilagojene pomnilniške bazene za pogosto dodeljene tenzorje
- **Ponovna uporaba medpomnilnikov**: Ponovno uporabite vhodne/izhodne medpomnilnike med klici inferenc

### 6. Optimizacija energije
- **Načini zmogljivosti**: Uporabite ustrezne načine zmogljivosti glede na toplotne omejitve
- **Dinamično skaliranje frekvence**: Dovolite sistemu, da prilagodi frekvenco glede na delovno obremenitev
- **Upravljanje stanja mirovanja**: Pravilno sprostite vire, ko niso v uporabi

## Odpravljanje težav

### Pogoste težave

#### 1. Težave pri namestitvi SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Napake pri pretvorbi modela
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Težave s kvantizacijo
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Težave z zmogljivostjo
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Težave s pomnilnikom
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Združljivost zaledja
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Odpravljanje težav z zmogljivostjo

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Pomoč

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN Dokumentacija**: Na voljo v paketu SDK
- **Forumi skupnosti**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Tehnična podpora**: Prek Qualcommovega portala za razvijalce

## Dodatni viri

### Uradne povezave
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon platforme**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portal za razvijalce**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Viri za učenje
- **Vodič za začetek**: Na voljo v dokumentaciji QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Vodič za optimizacijo**: Dokumentacija SDK vključuje celovite smernice za optimizacijo
- **Video vadnice**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Orodja za integracijo
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Predhodno optimizirani modeli za Qualcomm strojno opremo
- **Android Neural Networks API**: Integracija z Android NNAPI
- **TensorFlow Lite Delegate**: Qualcomm delegat za TFLite

### Merila zmogljivosti
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Primeri skupnosti
- **Vzorčne aplikacije**: Na voljo v primerih QNN SDK
- **GitHub repozitoriji**: Primeri in orodja, ki jih prispeva skupnost
- **Tehnični blogi**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Povezana orodja
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Napredne tehnike kvantizacije in stiskanja
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Za primerjavo in rezervno uvedbo
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Večplatformni inferenčni pogon

### Specifikacije strojne opreme
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon platforme**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Kaj sledi

Nadaljujte svojo pot v robni AI z raziskovanjem [Modula 5: SLMOps in uvedba v proizvodnjo](../Module05/README.md), da se naučite operativnih vidikov upravljanja življenjskega cikla majhnih jezikovnih modelov.

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje z umetno inteligenco [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku naj se šteje za avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne odgovarjamo za morebitne nesporazume ali napačne razlage, ki bi nastale zaradi uporabe tega prevoda.