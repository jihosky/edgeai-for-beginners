<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T14:59:30+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "sl"
}
-->
# AI agenti in majhni jezikovni modeli: Celovit vodnik

## Uvod

V tem priročniku bomo raziskali AI agente in majhne jezikovne modele (SLM) ter njihove napredne strategije implementacije za okolja robnega računalništva. Pokrili bomo temeljne koncepte agentne umetne inteligence, tehnike optimizacije SLM, praktične strategije za implementacijo na napravah z omejenimi viri ter Microsoft Agent Framework za gradnjo produkcijsko pripravljenih agentnih sistemov.

Pokrajina umetne inteligence doživlja paradigmatični premik v letu 2025. Medtem ko je bilo leto 2023 leto chatbotov, leto 2024 pa razcvet kopilotov, leto 2025 pripada AI agentom — inteligentnim sistemom, ki razmišljajo, načrtujejo, uporabljajo orodja in izvajajo naloge z minimalnim človeškim vnosom, vse bolj podprti z učinkovitimi majhnimi jezikovnimi modeli. Microsoft Agent Framework se pojavlja kot vodilna rešitev za gradnjo teh inteligentnih sistemov z zmogljivostmi za delovanje brez povezave na robu.

## Cilji učenja

Do konca tega priročnika boste sposobni:

- 🤖 Razumeti temeljne koncepte AI agentov in agentnih sistemov
- 🔬 Prepoznati prednosti majhnih jezikovnih modelov v primerjavi z velikimi jezikovnimi modeli pri agentnih aplikacijah
- 🚀 Naučiti se naprednih strategij implementacije SLM za okolja robnega računalništva
- 📱 Implementirati praktične agente, podprte z SLM, za resnične aplikacije
- 🏗️ Zgraditi produkcijsko pripravljene agente z uporabo Microsoft Agent Framework
- 🌐 Implementirati agente za delovanje brez povezave z lokalno integracijo LLM in SLM
- 🔧 Integrirati Microsoft Agent Framework z Foundry Local za robno implementacijo

## Razumevanje AI agentov: Osnove in klasifikacije

### Definicija in temeljni koncepti

Umetni inteligentni (AI) agent se nanaša na sistem ali program, ki je sposoben samostojno izvajati naloge v imenu uporabnika ali drugega sistema z oblikovanjem svojega delovnega toka in uporabo razpoložljivih orodij. Za razliko od tradicionalne AI, ki zgolj odgovarja na vaša vprašanja, agent lahko deluje neodvisno za dosego ciljev.

### Okvir klasifikacije agentov

Razumevanje meja agentov pomaga pri izbiri ustreznih vrst agentov za različne scenarije računalništva:

- **🔬 Preprosti refleksni agenti**: Sistemi, ki temeljijo na pravilih in se odzivajo na neposredne zaznave (termostati, osnovna avtomatizacija)
- **📱 Modelno osnovani agenti**: Sistemi, ki vzdržujejo notranje stanje in spomin (robotski sesalniki, navigacijski sistemi)
- **⚖️ Ciljno usmerjeni agenti**: Sistemi, ki načrtujejo in izvajajo zaporedja za dosego ciljev (načrtovalci poti, razporejevalniki nalog)
- **🧠 Učeči se agenti**: Prilagodljivi sistemi, ki izboljšujejo zmogljivost skozi čas (sistemi priporočil, personalizirani asistenti)

### Ključne prednosti AI agentov

AI agenti ponujajo več temeljnih prednosti, zaradi katerih so idealni za aplikacije robnega računalništva:

**Operativna avtonomija**: Agenti omogočajo neodvisno izvajanje nalog brez stalnega človeškega nadzora, kar jih naredi idealne za aplikacije v realnem času. Zahtevajo minimalen nadzor, hkrati pa ohranjajo prilagodljivo vedenje, kar omogoča implementacijo na napravah z omejenimi viri in zmanjšano operativno obremenitev.

**Prilagodljivost implementacije**: Ti sistemi omogočajo zmogljivosti AI na napravi brez zahtev po internetni povezavi, izboljšujejo zasebnost in varnost z lokalno obdelavo, jih je mogoče prilagoditi za aplikacije specifične za določeno področje in so primerni za različna okolja robnega računalništva.

**Učinkovitost stroškov**: Agentni sistemi ponujajo stroškovno učinkovito implementacijo v primerjavi z rešitvami, ki temeljijo na oblaku, z zmanjšanimi operativnimi stroški in nižjimi zahtevami po pasovni širini za aplikacije na robu.

## Napredne strategije za majhne jezikovne modele

### Osnove SLM (Small Language Model)

Majhen jezikovni model (SLM) je jezikovni model, ki se lahko prilega na običajno potrošniško elektronsko napravo in izvaja sklepanje z dovolj nizko zakasnitvijo, da je praktičen za obravnavo agentnih zahtev enega uporabnika. V praktičnem smislu so SLM običajno modeli z manj kot 10 milijardami parametrov.

**Značilnosti odkrivanja formatov**: SLM ponujajo napredno podporo za različne ravni kvantizacije, združljivost med platformami, optimizacijo zmogljivosti v realnem času in zmogljivosti za implementacijo na robu. Uporabniki lahko dostopajo do izboljšane zasebnosti prek lokalne obdelave in podpore za WebGPU za implementacijo v brskalniku.

**Zbirke ravni kvantizacije**: Priljubljeni formati SLM vključujejo Q4_K_M za uravnoteženo stiskanje v mobilnih aplikacijah, serijo Q5_K_S za implementacijo na robu, osredotočeno na kakovost, Q8_0 za skoraj originalno natančnost na zmogljivih napravah na robu ter eksperimentalne formate, kot je Q2_K za scenarije z ultra nizkimi viri.

### GGUF (General GGML Universal Format) za implementacijo SLM

GGUF služi kot primarni format za implementacijo kvantiziranih SLM na CPU in napravah na robu, posebej optimiziran za agentne aplikacije:

**Funkcije optimizirane za agente**: Format zagotavlja celovite vire za pretvorbo in implementacijo SLM z izboljšano podporo za klicanje orodij, generiranje strukturiranih izhodov in pogovore z več obrati. Združljivost med platformami zagotavlja dosledno vedenje agentov na različnih napravah na robu.

**Optimizacija zmogljivosti**: GGUF omogoča učinkovito uporabo pomnilnika za delovne tokove agentov, podpira dinamično nalaganje modelov za sisteme z več agenti in zagotavlja optimizirano sklepanje za interakcije agentov v realnem času.

### Okviri SLM, optimizirani za rob

#### Optimizacija Llama.cpp za agente

Llama.cpp ponuja najsodobnejše tehnike kvantizacije, posebej optimizirane za implementacijo agentnih SLM:

**Kvantizacija, specifična za agente**: Okvir podpira Q4_0 (optimalno za mobilno implementacijo agentov s 75% zmanjšanjem velikosti), Q5_1 (uravnotežena kakovost-stiskanje za agente na robu) in Q8_0 (skoraj originalna kakovost za produkcijske agentne sisteme). Napredni formati omogočajo ultra stisnjene agente za ekstremne scenarije na robu.

**Prednosti implementacije**: Sklepanje, optimizirano za CPU, s pospeševanjem SIMD zagotavlja učinkovito izvajanje agentov v pomnilniku. Združljivost med platformami na arhitekturah x86, ARM in Apple Silicon omogoča univerzalne zmogljivosti implementacije agentov.

#### Apple MLX Framework za SLM agente

Apple MLX zagotavlja nativno optimizacijo, posebej zasnovano za agente, podprte z SLM, na napravah Apple Silicon:

**Optimizacija agentov na Apple Silicon**: Okvir uporablja arhitekturo enotnega pomnilnika z integracijo Metal Performance Shaders, samodejno mešano natančnost za sklepanje agentov in optimizirano pasovno širino pomnilnika za sisteme z več agenti. Agenti SLM kažejo izjemno zmogljivost na čipih serije M.

**Razvojne funkcije**: Podpora za Python in Swift API z optimizacijami, specifičnimi za agente, samodejna diferenciacija za učenje agentov in brezhibna integracija z razvojnimi orodji Apple zagotavljajo celovita razvojna okolja za agente.

#### ONNX Runtime za agente SLM na več platformah

ONNX Runtime zagotavlja univerzalni sklepni stroj, ki omogoča agentom SLM dosledno delovanje na različnih strojnih platformah in operacijskih sistemih:

**Univerzalna implementacija**: ONNX Runtime zagotavlja dosledno vedenje agentov SLM na platformah Windows, Linux, macOS, iOS in Android. Ta združljivost med platformami omogoča razvijalcem, da napišejo enkrat in implementirajo povsod, kar bistveno zmanjša stroške razvoja in vzdrževanja za aplikacije na več platformah.

**Možnosti strojnega pospeševanja**: Okvir zagotavlja optimizirane izvajalne ponudnike za različne strojne konfiguracije, vključno s CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) in specializiranimi pospeševalniki (Intel VPU, Qualcomm NPU). Agenti SLM lahko samodejno izkoristijo najboljšo razpoložljivo strojno opremo brez sprememb kode.

**Funkcije, pripravljene za produkcijo**: ONNX Runtime ponuja funkcije na ravni podjetja, ki so bistvene za implementacijo agentov v produkciji, vključno z optimizacijo grafov za hitrejše sklepanje, upravljanje pomnilnika za okolja z omejenimi viri in celovita orodja za profiliranje za analizo zmogljivosti. Okvir podpira tako Python kot C++ API za prilagodljivo integracijo.

## SLM proti LLM v agentnih sistemih: Napredna primerjava

### Prednosti SLM v aplikacijah za agente

**Operativna učinkovitost**: SLM zagotavljajo 10-30× zmanjšanje stroškov v primerjavi z LLM za naloge agentov, kar omogoča odzive agentov v realnem času na velikem obsegu. Ponujajo hitrejše čase sklepanja zaradi zmanjšane računske kompleksnosti, kar jih naredi idealne za interaktivne aplikacije agentov.

**Zmogljivosti implementacije na robu**: SLM omogočajo izvajanje agentov na napravi brez odvisnosti od interneta, izboljšano zasebnost prek lokalne obdelave in prilagoditev za aplikacije, specifične za določeno področje, primerne za različna okolja robnega računalništva.

**Optimizacija, specifična za agente**: SLM so odlični pri klicanju orodij, generiranju strukturiranih izhodov in rutinskih delovnih tokov odločanja, ki predstavljajo 70-80% tipičnih nalog agentov.

### Kdaj uporabiti SLM proti LLM v agentnih sistemih

**Idealno za SLM**:
- **Ponavljajoče se naloge agentov**: Vnos podatkov, izpolnjevanje obrazcev, rutinski klici API
- **Integracija orodij**: Poizvedbe v podatkovnih bazah, operacije z datotekami, interakcije s sistemom
- **Strukturirani delovni tokovi**: Sledenje vnaprej določenim procesom agentov
- **Agenti, specifični za področje**: Pomoč strankam, razporejanje, osnovna analiza
- **Lokalna obdelava**: Operacije agentov, občutljive na zasebnost

**Bolje za LLM**:
- **Kompleksno razmišljanje**: Reševanje novih problemov, strateško načrtovanje
- **Odprti pogovori**: Splošni klepet, ustvarjalne razprave
- **Naloge z obsežnim znanjem**: Raziskave, ki zahtevajo obsežno splošno znanje
- **Nove situacije**: Obvladovanje popolnoma novih scenarijev agentov

### Hibridna arhitektura agentov

Optimalen pristop združuje SLM in LLM v heterogenih agentnih sistemih:

**Pametna orkestracija agentov**:
1. **SLM kot primarni**: Obdelava 70-80% rutinskih nalog agentov lokalno
2. **LLM po potrebi**: Usmerjanje kompleksnih poizvedb v večje modele v oblaku
3. **Specializirani SLM**: Različni majhni modeli za različna področja agentov
4. **Optimizacija stroškov**: Zmanjšanje dragih klicev LLM z inteligentnim usmerjanjem

## Strategije za produkcijsko implementacijo agentov SLM

### Foundry Local: Robno AI okolje na ravni podjetja

Foundry Local (https://github.com/microsoft/foundry-local) služi kot vodilna rešitev Microsofta za implementacijo majhnih jezikovnih modelov v produkcijskih robnih okoljih. Ponuja popolno okolje za izvajanje, posebej zasnovano za agente, podprte z SLM, z značilnostmi na ravni podjetja in brezhibnimi integracijskimi zmogljivostmi.

**Osnovna arhitektura in funkcije**:
- **Združljiv API z OpenAI**: Popolna združljivost z OpenAI SDK in integracijami Agent Framework
- **Samodejna optimizacija strojne opreme**: Inteligentna izbira različic modelov glede na razpoložljivo strojno opremo (CUDA GPU, Qualcomm NPU, CPU)
- **Upravljanje modelov**: Samodejno nalaganje, predpomnjenje in upravljanje življenjskega cikla modelov SLM
- **Odkritje storitev**: Zaznavanje storitev brez konfiguracije za okvire agentov
- **Optimizacija virov**: Inteligentno upravljanje pomnilnika in energetska učinkovitost za implementacijo na robu

#### Namestitev in nastavitev

**Namestitev na več platformah**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Hiter začetek za razvoj agentov**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integracija z Agent Framework

**Integracija Foundry Local SDK**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Samodejna izbira modela in optimizacija strojne opreme**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Vzorci produkcijske implementacije

**Produkcijska nastavitev enega agenta**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orkestracija več agentov v produkciji**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Funkcije na ravni podjetja in spremljanje

**Spremljanje zdravja in opazljivost**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Upravljanje virov in samodejno skaliranje**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Napredna konfiguracija in optimizacija

**Prilagoditev konfiguracije modela**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Kontrolni seznam za produkcijsko implementacijo**:

✅ **Konfiguracija storitev**:
- Konfigurirajte ustrezne vzdevke modelov za primere uporabe
- Nastavite omejitve virov in prage spremljanja
- Omogočite preverjanje zdravja in zbiranje metrik
- Konfigurirajte samodejni ponovni zagon in preklop

✅ **Varnostna nastavitev**:
- Omogočite lokalni dostop do API (brez zunanje izpostavljenosti)
- Konfigurirajte ustrezno upravljanje ključev API
- Nastavite dnevnik revizij za interakcije agentov
- Implementirajte omejevanje hitrosti za produkcijsko uporabo

✅ **Optimizacija zmogljivosti**:
- Preizkusite zmogljivost modela pod pričakovano obremenitvijo
- Konfigurirajte ustrezne ravni kvantizacije
- Nastavite strategije predpomnjenja in ogrevanja modela
- Spremljajte vzorce uporabe pomnilnika in CPU

✅ **Testiranje integracije**:
- Preizkusite integracijo okvira agentov
- Preverite zmogljivosti delovanja brez povezave
- Preizkusite scenarije preklopa in obnovitve
- Validirajte delovne tokove agentov od začetka do konca

### Ollama: Poenostavljena implementacija agentov SLM

### Ollama: Implementacija agentov SLM, osredotočena na skupnost

Ollama ponuja pristop, ki ga vodi skupnost, k implementaciji agentov SLM s poudarkom na enostavnosti, obsežnem ekosistemu modelov in razvijalcem prijaznih delovnih tokov. Medtem ko se Foundry Local osredotoča na funkcije na ravni podjetja, Ollama izstopa pri hitrem prototipiranju, dostopu do modelov skupnosti in poenostavljenih scenarijih implementacije.

**Osnovna arhitektura in funkcije**:
- **Združljiv API z OpenAI**: Popolna združljivost REST API za brezhibno integracijo okvira agentov
- **Obsežna knjižnica modelov**: Dostop do stotin modelov, ki jih prispeva skupnost, in uradnih modelov
- **En
- Preizkusite integracijo Microsoft Agent Framework
- Preverite zmogljivosti delovanja brez povezave
- Preizkusite scenarije preklopa in obravnavo napak
- Validirajte delovne tokove agentov od začetka do konca

**Primerjava s Foundry Local**:

| Funkcija | Foundry Local | Ollama |
|----------|---------------|--------|
| **Ciljni primer uporabe** | Proizvodnja v podjetjih | Razvoj in skupnost |
| **Ekosistem modelov** | Microsoft-kuriran | Obsežna skupnost |
| **Optimizacija strojne opreme** | Samodejna (CUDA/NPU/CPU) | Ročna konfiguracija |
| **Funkcije za podjetja** | Vgrajeno spremljanje, varnost | Orodja skupnosti |
| **Kompleksnost uvajanja** | Enostavno (namestitev z winget) | Enostavno (namestitev s curl) |
| **Združljivost API** | OpenAI + razširitve | Standard OpenAI |
| **Podpora** | Uradna Microsoftova | Skupnostno vodena |
| **Najboljše za** | Proizvodni agenti | Prototipiranje, raziskave |

**Kdaj izbrati Ollama**:
- **Razvoj in prototipiranje**: Hitro eksperimentiranje z različnimi modeli
- **Skupnostni modeli**: Dostop do najnovejših modelov, ki jih prispeva skupnost
- **Izobraževalna uporaba**: Učenje in poučevanje razvoja AI agentov
- **Raziskovalni projekti**: Akademske raziskave, ki zahtevajo raznolik dostop do modelov
- **Prilagojeni modeli**: Gradnja in testiranje prilagojenih modelov s fino nastavitvijo

### VLLM: Visoko zmogljivo sklepanje SLM agentov

VLLM (sklepanje zelo velikih jezikovnih modelov) zagotavlja visoko zmogljiv, pomnilniško učinkovit sklepni motor, posebej optimiziran za proizvodne SLM implementacije v velikem obsegu. Medtem ko se Foundry Local osredotoča na enostavnost uporabe in Ollama poudarja skupnostne modele, VLLM izstopa v scenarijih visoke zmogljivosti, ki zahtevajo največji pretok in učinkovito uporabo virov.

**Osnovna arhitektura in funkcije**:
- **PagedAttention**: Revolucionarno upravljanje pomnilnika za učinkovito računalniško obdelavo pozornosti
- **Dinamično združevanje**: Inteligentno združevanje zahtev za optimalen pretok
- **Optimizacija GPU**: Napredna podpora za CUDA jedra in paralelizem tenzorjev
- **Združljivost z OpenAI**: Popolna združljivost API za brezhibno integracijo
- **Spekulativno dekodiranje**: Napredne tehnike pospeševanja sklepanja
- **Podpora za kvantizacijo**: Kvantizacija INT4, INT8 in FP16 za pomnilniško učinkovitost

#### Namestitev in nastavitev

**Možnosti namestitve**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**Hiter začetek za razvoj agentov**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integracija z Agent Framework

**VLLM z Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**Visoko zmogljiva nastavitev za več agentov**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### Vzorci proizvodne implementacije

**Proizvodna storitev VLLM za podjetja**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### Funkcije za podjetja in spremljanje

**Napredno spremljanje zmogljivosti VLLM**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### Napredna konfiguracija in optimizacija

**Predloge konfiguracije VLLM za proizvodnjo**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**Kontrolni seznam za proizvodno implementacijo VLLM**:

✅ **Optimizacija strojne opreme**:
- Konfigurirajte paralelizem tenzorjev za več-GPU nastavitve
- Omogočite kvantizacijo (AWQ/GPTQ) za pomnilniško učinkovitost
- Nastavite optimalno uporabo GPU pomnilnika (85-95%)
- Konfigurirajte ustrezne velikosti paketov za pretok

✅ **Izboljšanje zmogljivosti**:
- Omogočite predpomnjenje predpon za ponavljajoče se poizvedbe
- Konfigurirajte razdeljeno predpolnitev za dolge sekvence
- Nastavite spekulativno dekodiranje za hitrejše sklepanje
- Optimizirajte max_num_seqs glede na strojno opremo

✅ **Funkcije za proizvodnjo**:
- Nastavite spremljanje zdravja in zbiranje metrik
- Konfigurirajte samodejni ponovni zagon in preklop
- Implementirajte čakalne vrste zahtev in uravnoteženje obremenitve
- Nastavite celovito beleženje in opozarjanje

✅ **Varnost in zanesljivost**:
- Konfigurirajte pravila požarnega zidu in nadzor dostopa
- Nastavite omejevanje hitrosti API in avtentikacijo
- Implementirajte postopno zaustavitev in čiščenje
- Konfigurirajte varnostno kopiranje in obnovitev ob nesrečah

✅ **Testiranje integracije**:
- Preizkusite integracijo Microsoft Agent Framework
- Validirajte scenarije visoke zmogljivosti
- Preizkusite postopke preklopa in obnovitve
- Primerjajte zmogljivost pod obremenitvijo

**Primerjava z drugimi rešitvami**:

| Funkcija | VLLM | Foundry Local | Ollama |
|----------|------|---------------|--------|
| **Ciljni primer uporabe** | Visoko zmogljiva proizvodnja | Enostavnost uporabe v podjetjih | Razvoj in skupnost |
| **Zmogljivost** | Največji pretok | Uravnoteženo | Dobro |
| **Pomnilniška učinkovitost** | Optimizacija PagedAttention | Samodejna optimizacija | Standard |
| **Kompleksnost nastavitve** | Visoka (veliko parametrov) | Nizka (samodejna) | Nizka (enostavna) |
| **Razširljivost** | Odlična (paralelizem tenzorjev/cevovodov) | Dobra | Omejena |
| **Kvantizacija** | Napredna (AWQ, GPTQ, FP8) | Samodejna | Standard GGUF |
| **Funkcije za podjetja** | Potrebna prilagojena implementacija | Vgrajene | Orodja skupnosti |
| **Najboljše za** | Agenti za proizvodnjo v velikem obsegu | Proizvodnja v podjetjih | Razvoj |

**Kdaj izbrati VLLM**:
- **Zahteve po visoki zmogljivosti**: Obdelava stotin zahtev na sekundo
- **Implementacije v velikem obsegu**: Več-GPU, več-vozliščne implementacije
- **Kritična zmogljivost**: Odzivni časi pod sekundo v velikem obsegu
- **Napredna optimizacija**: Potreba po prilagojeni kvantizaciji in združevanju
- **Učinkovitost virov**: Maksimalna uporaba drage strojne opreme GPU

## Resnične aplikacije SLM agentov

### SLM agenti za podporo strankam
- **SLM zmogljivosti**: Iskanje računov, ponastavitve gesel, preverjanje stanja naročil
- **Stroškovne koristi**: 10-kratno zmanjšanje stroškov sklepanja v primerjavi z LLM agenti
- **Zmogljivost**: Hitrejši odzivni časi z dosledno kakovostjo za rutinske poizvedbe

### SLM agenti za poslovne procese
- **Agenti za obdelavo računov**: Izvleček podatkov, validacija informacij, usmerjanje za odobritev
- **Agenti za upravljanje e-pošte**: Samodejno kategoriziranje, določanje prioritet, priprava odgovorov
- **Agenti za načrtovanje**: Koordinacija sestankov, upravljanje koledarjev, pošiljanje opomnikov

### Osebni digitalni asistenti SLM
- **Agenti za upravljanje nalog**: Ustvarjanje, posodabljanje, organizacija seznamov opravil
- **Agenti za zbiranje informacij**: Raziskovanje tem, povzemanje ugotovitev lokalno
- **Agenti za komunikacijo**: Priprava e-pošte, sporočil, objav na družbenih omrežjih zasebno

### SLM agenti za trgovanje in finance
- **Agenti za spremljanje trga**: Sledenje cenam, prepoznavanje trendov v realnem času
- **Agenti za generiranje poročil**: Samodejno ustvarjanje dnevnih/tedenskih povzetkov
- **Agenti za oceno tveganja**: Ocena pozicij portfelja z uporabo lokalnih podatkov

### SLM agenti za podporo v zdravstvu
- **Agenti za načrtovanje pacientov**: Koordinacija terminov, pošiljanje samodejnih opomnikov
- **Agenti za dokumentacijo**: Lokalno generiranje medicinskih povzetkov, poročil
- **Agenti za upravljanje receptov**: Sledenje ponovnim polnjenjem, preverjanje interakcij zasebno

## Microsoft Agent Framework: Razvoj agentov pripravljenih za proizvodnjo

### Pregled in arhitektura

Microsoft Agent Framework zagotavlja celovito, podjetniško platformo za gradnjo, uvajanje in upravljanje AI agentov, ki lahko delujejo tako v oblaku kot v lokalnih okoljih. Okvir je posebej zasnovan za brezhibno delovanje z majhnimi jezikovnimi modeli in scenariji robnega računalništva, kar ga naredi idealnega za zasebnostno občutljive in z viri omejene implementacije.

**Osnovne komponente okvira**:
- **Agent Runtime**: Lahkotno okolje za izvajanje, optimizirano za robne naprave
- **Sistem za integracijo orodij**: Razširljiva arhitektura vtičnikov za povezovanje zunanjih storitev in API-jev
- **Upravljanje stanja**: Trajen pomnilnik agentov in upravljanje konteksta med sejami
- **Varnostna plast**: Vgrajeni varnostni nadzori za podjetniško implementacijo
- **Orkestracijski motor**: Koordinacija več agentov in upravljanje delovnih tokov

### Ključne funkcije za robne implementacije

**Arhitektura "najprej brez povezave"**: Microsoft Agent Framework je zasnovan z načeli "najprej brez povezave", kar omogoča agentom učinkovito delovanje brez stalne internetne povezave. To vključuje lokalno sklepanje modelov, predpomnjene baze znanja, izvajanje orodij brez povezave in postopno zmanjševanje zmogljivosti, ko storitve v oblaku niso na voljo.

**Optimizacija virov**: Okvir zagotavlja inteligentno upravljanje virov s samodejno optimizacijo pomnilnika za SLM-je, uravnoteženjem obremenitve CPU/GPU za robne naprave, prilagodljivo izbiro modelov glede na razpoložljive vire in energetsko učinkovite vzorce sklepanja za mobilne implementacije.

**Varnost in zasebnost**: Funkcije varnosti na ravni podjetja vključujejo lokalno obdelavo podatkov za ohranjanje zasebnosti, šifrirane komunikacijske kanale agentov, nadzor dostopa na podlagi vlog za zmogljivosti agentov in beleženje revizij za zahteve skladnosti.

### Integracija s Foundry Local

Microsoft Agent Framework se brezhibno integrira s Foundry Local za zagotavljanje celovite rešitve AI na robu:

**Samodejno odkrivanje modelov**: Okvir samodejno zazna in se poveže z instancami Foundry Local, odkrije razpoložljive SLM modele in izbere optimalne modele glede na zahteve agentov in zmogljivosti strojne opreme.

**Dinamično nalaganje modelov**: Agenti lahko dinamično nalagajo različne SLM-je za specifične naloge, kar omogoča sisteme agentov z več modeli, kjer različni modeli obravnavajo različne vrste zahtev, in samodejno preklapljanje med modeli glede na razpoložljivost in zmogljivost.

**Optimizacija zmogljivosti**: Integrirani mehanizmi predpomnjenja zmanjšujejo čase nalaganja modelov, združevanje povezav optimizira klice API-jev na Foundry Local, inteligentno združevanje pa izboljša pretok za več zahtev agentov.

### Gradnja agentov z Microsoft Agent Framework

#### Definicija in konfiguracija agentov

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integracija orodij za robne scenarije

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orkestracija več agentov

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### Napredni vzorci implementacije na robu

#### Hierarhična arhitektura agentov

**Lokalni grozdi agentov**: Implementirajte več specializiranih SLM agentov na robnih napravah, vsak optimiziran za specifične naloge. Uporabite lahke modele, kot je Qwen2.5-0.5B za enostavno usmerjanje in načrtovanje, srednje modele, kot je Phi-4-Mini za podporo strankam in dokumentacijo, ter večje modele za kompleksno sklepanje, ko viri to omogočajo.

**Koordinacija rob-oblaka**: Implementirajte inteligentne vzorce eskalacije, kjer lokalni agenti obravnavajo rutinske naloge, oblačni agenti zagotavljajo kompleksno sklepanje, ko je povezljivost omogočena, in brezhiben prenos med obdelavo na robu in v oblaku ohranja kontinuiteto.

#### Konfiguracije implementacije

**Implementacija na eni napravi**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**Porazdeljena implementacija na robu**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Optimizacija zmogljivosti za robne agente

#### Strategije izbire modelov

**Dodelitev modelov na podlagi nalog**: Microsoft Agent Framework omogoča inteligentno izbiro modelov glede na kompleksnost naloge in zahteve:

- **Enostavne naloge** (Q&A, usmerjanje): Qwen2.5-0.5B (500MB, <100ms odziv)
- **Srednje zahtevne naloge** (podpora strankam, načrtovanje): Phi-4-Mini (2.4GB, 200-500ms odziv)
- **Kompleksne naloge** (tehnične analize, načrtovanje): Phi-4 (7GB, 1-3s odziv, ko viri to omogočajo)

**Dinamično preklapljanje modelov**: Agenti lahko preklapljajo med modeli glede na trenutno obremenitev sistema, oceno kompleksnosti naloge, prioritete uporabnika in razpoložljive strojne vire.

#### Upravljanje pomnilnika in virov

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### Vzorci integracije v podjetju

#### Varnost in skladnost

**Lokalna obdelava podatkov**: Vsa obdelava agentov poteka lokalno, kar zagotavlja, da občutljivi podatki nikoli ne zapustijo robne naprave. To vključuje zaščito informacij o strankah, skladnost s HIPAA za zdravstvene agente, varnost finančnih podatkov za bančne agente in skladnost z GDPR za evropske implementacije.

**Nadzor dostopa**: Dovoljenja na podlagi vlog nadzorujejo, katera orodja lahko agenti uporabljajo, avtentikacija uporabnikov za interakcije z agenti in revizijske sledi za vse akcije in odločitve agentov.

#### Spremljanje in opazovanje

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### Resnični primeri implementacije

#### Sistem robnih agentov za maloprodajo

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Agent za podporo v zdravstvu

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### Najboljše prakse za Microsoft Agent Framework

#### Smernice za razvoj

1. **Začnite enostavno**: Začnite s scenariji enega agenta, preden zgradite kompleksne sisteme z več agenti
2. **Pravilna velikost modela**: Izberite najmanjši model, ki ustreza vašim zahtevam po natančnosti
3. **Oblikovanje orodij**: Ustvarite osredotočena, enonamenska orodja namesto kompleksnih večnamenskih orodij
4. **Obravnava napak**: Implementirajte postopno zmanjševanje zmogljivosti za scenarije brez povezave in okvare modelov
5. **Testiranje**: Obsežno testirajte agente v pogojih brez povezave in z omejenimi viri

#### Najboljše prakse za implementacijo

1. **Postopno uvajanje**: Najprej uvedite majhnim skupinam uporabnikov, pozorno spremljajte zmogljivostne metrike
2. **Spremljanje virov**: Nastavite opozorila za pomnilnik, CPU in odzivne čase
3. **Strategije za izpad**: Vedno
**Izbira okvira za namestitev agentov**: Izberite optimizacijske okvire glede na ciljno strojno opremo in zahteve agenta. Uporabite Llama.cpp za namestitev agentov optimiziranih za CPU, Apple MLX za aplikacije agentov na Apple Silicon in ONNX za združljivost agentov na različnih platformah.

## Praktična pretvorba SLM agentov in primeri uporabe

### Scenariji namestitve agentov v resničnem svetu

**Mobilne aplikacije agentov**: Formati Q4_K so odlični za aplikacije agentov na pametnih telefonih z minimalno porabo pomnilnika, medtem ko Q8_0 zagotavlja uravnoteženo zmogljivost za sisteme agentov na tabličnih računalnikih. Formati Q5_K ponujajo vrhunsko kakovost za mobilne produktivne agente.

**Namizno in robno računalništvo agentov**: Q5_K zagotavlja optimalno zmogljivost za aplikacije agentov na namiznih računalnikih, Q8_0 omogoča visokokakovostno sklepanje za delovne postaje, Q4_K pa omogoča učinkovito obdelavo na napravah z robnimi agenti.

**Raziskovalni in eksperimentalni agenti**: Napredni kvantizacijski formati omogočajo raziskovanje ultra nizkonatančnega sklepanja agentov za akademske raziskave in aplikacije dokazovanja koncepta, ki zahtevajo izjemno omejene vire.

### Merila zmogljivosti SLM agentov

**Hitrost sklepanja agentov**: Q4_K dosega najhitrejše odzivne čase agentov na mobilnih CPU-jih, Q5_K zagotavlja uravnoteženo razmerje med hitrostjo in kakovostjo za splošne aplikacije agentov, Q8_0 ponuja vrhunsko kakovost za kompleksne naloge agentov, eksperimentalni formati pa omogočajo največjo prepustnost za specializirano strojno opremo agentov.

**Zahteve glede pomnilnika agentov**: Kvantizacijske ravni za agente segajo od Q2_K (manj kot 500 MB za majhne modele agentov) do Q8_0 (približno 50 % prvotne velikosti), pri čemer eksperimentalne konfiguracije dosežejo največjo kompresijo za okolja agentov z omejenimi viri.

## Izzivi in premisleki za SLM agente

### Kompromisi zmogljivosti v sistemih agentov

Namestitev SLM agentov zahteva skrbno razmislek o kompromisih med velikostjo modela, hitrostjo odziva agenta in kakovostjo izhoda. Medtem ko Q4_K ponuja izjemno hitrost in učinkovitost za mobilne agente, Q8_0 zagotavlja vrhunsko kakovost za kompleksne naloge agentov. Q5_K predstavlja srednjo pot, primerno za večino splošnih aplikacij agentov.

### Združljivost strojne opreme za SLM agente

Različne robne naprave imajo različne zmogljivosti za namestitev SLM agentov. Q4_K deluje učinkovito na osnovnih procesorjih za preproste agente, Q5_K zahteva zmerne računalniške vire za uravnoteženo zmogljivost agentov, Q8_0 pa koristi zmogljivejšo strojno opremo za napredne zmogljivosti agentov.

### Varnost in zasebnost v sistemih SLM agentov

Medtem ko SLM agenti omogočajo lokalno obdelavo za izboljšano zasebnost, je treba uvesti ustrezne varnostne ukrepe za zaščito modelov agentov in podatkov v robnih okoljih. To je še posebej pomembno pri nameščanju agentov z visoko natančnostjo v poslovnih okoljih ali stisnjenih formatov agentov v aplikacijah, ki obravnavajo občutljive podatke.

## Prihodnji trendi v razvoju SLM agentov

Pokrajina SLM agentov se še naprej razvija z napredkom v tehnikah kompresije, metodah optimizacije in strategijah namestitve na robu. Prihodnji razvoj vključuje učinkovitejše algoritme kvantizacije za modele agentov, izboljšane metode kompresije za delovne tokove agentov in boljšo integracijo s strojno opremo za pospeševanje obdelave agentov na robu.

**Napovedi trga za SLM agente**: Po nedavnih raziskavah bi avtomatizacija, ki jo poganjajo agenti, lahko do leta 2027 odpravila 40–60 % ponavljajočih se kognitivnih nalog v poslovnih delovnih tokovih, pri čemer bodo SLM-i vodili to preobrazbo zaradi svoje stroškovne učinkovitosti in prilagodljivosti pri namestitvi.

**Tehnološki trendi v SLM agentih**:
- **Specializirani SLM agenti**: Modeli, usposobljeni za specifične naloge agentov in industrije
- **Robno računalništvo agentov**: Izboljšane zmogljivosti agentov na napravi z izboljšano zasebnostjo in zmanjšano zakasnitvijo
- **Orkestracija agentov**: Boljša koordinacija med več SLM agenti z dinamičnim usmerjanjem in uravnavanjem obremenitve
- **Demokratizacija**: Prilagodljivost SLM omogoča širšo udeležbo pri razvoju agentov v organizacijah

## Začetek dela s SLM agenti

### Korak 1: Nastavite okolje Microsoft Agent Framework

**Namestite odvisnosti**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicializirajte Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Korak 2: Izberite svoj SLM za aplikacije agentov
Priljubljene možnosti za Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Odličen za splošne naloge agentov z uravnoteženo zmogljivostjo
- **Qwen2.5-0.5B (0.5B)**: Izjemno učinkovit za preproste agente za usmerjanje in razvrščanje
- **Qwen2.5-Coder-0.5B (0.5B)**: Specializiran za naloge agentov, povezane s kodo
- **Phi-4 (7B)**: Napredno sklepanje za kompleksne scenarije na robu, kadar so na voljo viri

### Korak 3: Ustvarite svojega prvega agenta z Microsoft Agent Framework

**Osnovna nastavitev agenta**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Korak 4: Določite obseg in zahteve agenta
Začnite z osredotočenimi, dobro opredeljenimi aplikacijami agentov z Microsoft Agent Framework:
- **Agenti za eno področje**: Pomoč strankam ALI načrtovanje ALI raziskave
- **Jasni cilji agenta**: Specifični, merljivi cilji za zmogljivost agenta
- **Omejena integracija orodij**: Največ 3–5 orodij za začetno namestitev agenta
- **Določene meje agenta**: Jasne poti za eskalacijo pri kompleksnih scenarijih
- **Oblikovanje, osredotočeno na rob**: Prednostna funkcionalnost brez povezave in lokalna obdelava

### Korak 5: Izvedite namestitev na robu z Microsoft Agent Framework

**Konfiguracija virov**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Namestite varnostne ukrepe za robne agente**:
- **Lokalna validacija vnosa**: Preverite zahteve brez odvisnosti od oblaka
- **Filtriranje izhodov brez povezave**: Zagotovite, da odgovori lokalno ustrezajo standardom kakovosti
- **Varnostni nadzor na robu**: Uvedite varnost brez potrebe po internetni povezavi
- **Lokalno spremljanje**: Spremljajte zmogljivost in označite težave z uporabo telemetrije na robu

### Korak 6: Merite in optimizirajte zmogljivost robnih agentov
- **Stopnje dokončanja nalog agenta**: Spremljajte stopnje uspešnosti v scenarijih brez povezave
- **Časi odziva agenta**: Zagotovite odzivne čase pod sekundo za namestitev na robu
- **Poraba virov**: Spremljajte porabo pomnilnika, CPU-ja in baterije na robnih napravah
- **Stroškovna učinkovitost**: Primerjajte stroške namestitve na robu z alternativami v oblaku
- **Zanesljivost brez povezave**: Merite zmogljivost agenta med izpadi omrežja

## Ključne točke za implementacijo SLM agentov

1. **SLM-i so zadostni za agente**: Za večino nalog agentov majhni modeli delujejo enako dobro kot veliki, hkrati pa ponujajo pomembne prednosti
2. **Stroškovna učinkovitost pri agentih**: 10–30x cenejši za delovanje SLM agentov, kar jih naredi ekonomsko izvedljive za široko uporabo
3. **Specializacija deluje za agente**: Fino uglašeni SLM-i pogosto prekašajo splošne LLM-e v specifičnih aplikacijah agentov
4. **Hibridna arhitektura agentov**: Uporabite SLM-e za rutinske naloge agentov, LLM-e za kompleksno sklepanje, kadar je potrebno
5. **Microsoft Agent Framework omogoča produkcijsko namestitev**: Ponuja orodja na ravni podjetja za gradnjo, namestitev in upravljanje robnih agentov
6. **Načela oblikovanja, osredotočena na rob**: Agenti, ki delujejo brez povezave z lokalno obdelavo, zagotavljajo zasebnost in zanesljivost
7. **Integracija Foundry Local**: Brezhibna povezava med Microsoft Agent Framework in lokalnim sklepanjem modelov
8. **Prihodnost so SLM agenti**: Majhni jezikovni modeli s produkcijskimi okviri so prihodnost agentne umetne inteligence, ki omogoča demokratizirano in učinkovito namestitev agentov

## Reference in dodatno branje

### Temeljni raziskovalni članki in publikacije

#### AI agenti in agentni sistemi
- **"Language Agents as Optimizable Graphs"** (2024) - Temeljne raziskave o arhitekturi agentov in strategijah optimizacije
  - Avtorji: Wenyue Hua, Lishan Yang, et al.
  - Povezava: https://arxiv.org/abs/2402.16823
  - Ključni vpogledi: Oblikovanje agentov na osnovi grafov in strategije optimizacije

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Avtorji: Zhiheng Xi, Wenxiang Chen, et al.
  - Povezava: https://arxiv.org/abs/2309.07864
  - Ključni vpogledi: Celovit pregled zmogljivosti in aplikacij agentov na osnovi LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Avtorji: Theodore Sumers, Shunyu Yao, et al.
  - Povezava: https://arxiv.org/abs/2309.02427
  - Ključni vpogledi: Kognitivni okviri za oblikovanje inteligentnih agentov

#### Majhni jezikovni modeli in optimizacija
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Avtorji: Microsoft Research Team
  - Povezava: https://arxiv.org/abs/2404.14219
  - Ključni vpogledi: Načela oblikovanja SLM in strategije mobilne namestitve

- **"Qwen2.5 Technical Report"** (2024)
  - Avtorji: Alibaba Cloud Team
  - Povezava: https://arxiv.org/abs/2407.10671
  - Ključni vpogledi: Napredne tehnike usposabljanja SLM in optimizacija zmogljivosti

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Avtorji: Peiyuan Zhang, Guangtao Zeng, et al.
  - Povezava: https://arxiv.org/abs/2401.02385
  - Ključni vpogledi: Ultra-kompaktno oblikovanje modelov in učinkovitost usposabljanja

### Uradna dokumentacija in okviri

#### Microsoft Agent Framework
- **Uradna dokumentacija**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub repozitorij**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Primarni repozitorij**: https://github.com/microsoft/foundry-local
- **Dokumentacija**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Glavni repozitorij**: https://github.com/vllm-project/vllm
- **Dokumentacija**: https://docs.vllm.ai/


#### Ollama
- **Uradna spletna stran**: https://ollama.ai/
- **GitHub repozitorij**: https://github.com/ollama/ollama

### Okviri za optimizacijo modelov

#### Llama.cpp
- **Repozitorij**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentacija**: https://microsoft.github.io/Olive/
- **GitHub repozitorij**: https://github.com/microsoft/Olive

#### OpenVINO
- **Uradna stran**: https://docs.openvino.ai/

#### Apple MLX
- **Repozitorij**: https://github.com/ml-explore/mlx

### Industrijska poročila in analiza trga

#### Raziskave trga AI agentov
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Povezava: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Ključni vpogledi: Tržni trendi in vzorci sprejemanja v podjetjih

#### Tehnična merila

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Povezava: https://mlcommons.org/en/inference-edge/
  - Ključni vpogledi: Standardizirani kazalniki zmogljivosti za namestitev na robu

### Standardi in specifikacije

#### Formati modelov in standardi
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Format modela za interoperabilnost na različnih platformah
- **GGUF specifikacija**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantiziran format modela za sklepanje na CPU
- **OpenAI API specifikacija**: https://platform.openai.com/docs/api-reference
  - Standardni API format za integracijo jezikovnih modelov

#### Varnost in skladnost
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: Okvir za sisteme AI in varnost
- **IEEE standardi za AI**: https://standards.ieee.org/industry-connections/ai/

Premik k agentom, ki jih poganjajo SLM-i, predstavlja temeljno spremembo v pristopu k namestitvi AI. Microsoft Agent Framework, v kombinaciji z lokalnimi platformami in učinkovitimi majhnimi jezikovnimi modeli, ponuja celovito rešitev za gradnjo produkcijsko pripravljenih agentov, ki učinkovito delujejo v robnih okoljih. S poudarkom na učinkovitosti, specializaciji in praktični uporabnosti ta tehnološki sklop omogoča, da so AI agenti bolj dostopni, cenovno ugodni in učinkoviti za resnične aplikacije v vseh industrijah in okoljih robnega računalništva.

Ko napredujemo proti letu 2025, bo kombinacija vse bolj zmogljivih majhnih modelov, sofisticiranih okvirov agentov, kot je Microsoft Agent Framework, in robustnih platform za namestitev na robu odprla nove možnosti za avtonomne sisteme, ki lahko učinkovito delujejo na robnih napravah, hkrati pa ohranjajo zasebnost, zmanjšujejo stroške in zagotavljajo izjemne uporabniške izkušnje.

**Naslednji koraki za implementacijo**:
1. **Raziskujte funkcijsko klicanje**: Naučite se, kako SLM-i obravnavajo integracijo orodij in strukturirane izhode
2. **Obvladujte protokol konteksta modela (MCP)**: Razumite napredne vzorce komunikacije agentov
3. **Gradite produkcijske agente**: Uporabite Microsoft Agent Framework za namestitve na ravni podjetja
4. **Optimizirajte za rob**: Uporabite napredne tehnike optimizacije za okolja z omejenimi viri

## ➡️ Kaj sledi

- [02: Funkcijs

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje AI [Co-op Translator](https://github.com/Azure/co-op-translator). Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatski prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije priporočamo profesionalni človeški prevod. Ne odgovarjamo za morebitne nesporazume ali napačne razlage, ki bi nastale zaradi uporabe tega prevoda.