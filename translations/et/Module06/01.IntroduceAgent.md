<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T15:39:25+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "et"
}
-->
# AI-agendid ja väikemudelid: põhjalik juhend

## Sissejuhatus

Selles juhendis uurime AI-agente ja väikemudeleid (SLM) ning nende täiustatud rakendusstrateegiaid serva-arvutuskeskkondades. Käsitleme agentliku AI põhikontseptsioone, SLM optimeerimistehnikaid, praktilisi juurutusstrateegiaid ressursipiirangutega seadmetele ja Microsoft Agent Frameworki tootmisvalmis agentide süsteemide loomiseks.

Tehisintellekti maastik kogeb 2025. aastal paradigmatilist muutust. Kui 2023. aasta oli vestlusrobotite ja 2024. aasta kaaspilootide buumi aasta, siis 2025 kuulub AI-agentidele – intelligentsetele süsteemidele, mis mõtlevad, arutlevad, planeerivad, kasutavad tööriistu ja täidavad ülesandeid minimaalse inimsekkumisega, tuginedes üha enam tõhusatele väikemudelitele. Microsoft Agent Framework tõuseb juhtivaks lahenduseks nende intelligentsete süsteemide loomiseks, pakkudes offline servapõhiseid võimalusi.

## Õpieesmärgid

Selle juhendi lõpuks oskate:

- 🤖 Mõista AI-agentide ja agentlike süsteemide põhikontseptsioone
- 🔬 Tuvastada väikemudelite eelised suurte mudelite ees agentlikes rakendustes
- 🚀 Õppida täiustatud SLM juurutusstrateegiaid serva-arvutuskeskkondades
- 📱 Rakendada praktilisi SLM-põhiseid agente reaalses maailmas
- 🏗️ Luua tootmisvalmis agente Microsoft Agent Frameworki abil
- 🌐 Juurutada offline servapõhiseid agente kohaliku LLM-i ja SLM-i integreerimisega
- 🔧 Integreerida Microsoft Agent Framework Foundry Localiga serva juurutamiseks

## AI-agentide mõistmine: alused ja klassifikatsioon

### Definitsioon ja põhikontseptsioonid

Tehisintellekti (AI) agent viitab süsteemile või programmile, mis suudab autonoomselt täita ülesandeid kasutaja või teise süsteemi nimel, kujundades oma töövoogu ja kasutades olemasolevaid tööriistu. Erinevalt traditsioonilisest AI-st, mis lihtsalt vastab teie küsimustele, suudab agent iseseisvalt tegutseda eesmärkide saavutamiseks.

### Agentide klassifikatsiooniraamistik

Agentide piiride mõistmine aitab valida sobivaid agentide tüüpe erinevate arvutuskeskkondade jaoks:

- **🔬 Lihtsad refleksagendid**: Reeglipõhised süsteemid, mis reageerivad koheselt tajudele (termostaadid, lihtne automatiseerimine)
- **📱 Mudelipõhised agendid**: Süsteemid, mis säilitavad sisemist olekut ja mälu (robot-tolmuimejad, navigatsioonisüsteemid)
- **⚖️ Eesmärgipõhised agendid**: Süsteemid, mis planeerivad ja täidavad järjestusi eesmärkide saavutamiseks (marsruudi planeerijad, ülesannete ajastajad)
- **🧠 Õppivad agendid**: Kohanduvad süsteemid, mis aja jooksul parandavad oma jõudlust (soovitussüsteemid, isikupärastatud assistendid)

### AI-agentide peamised eelised

AI-agendid pakuvad mitmeid põhilisi eeliseid, mis muudavad nad ideaalseks serva-arvutuskeskkondade rakendustes:

**Operatiivne autonoomia**: Agendid võimaldavad iseseisvat ülesannete täitmist ilma pideva inimjärelevalveta, muutes nad ideaalseks reaalajas rakendustes. Nad vajavad minimaalset järelevalvet, säilitades samas kohanemisvõime, võimaldades juurutamist ressursipiirangutega seadmetel vähendatud operatiivkuludega.

**Juurutamise paindlikkus**: Need süsteemid võimaldavad seadmesisesi AI-võimekust ilma internetiühenduse vajaduseta, parandavad privaatsust ja turvalisust kohaliku töötlemise kaudu, neid saab kohandada valdkonnaspetsiifiliste rakenduste jaoks ja need sobivad erinevatesse serva-arvutuskeskkondadesse.

**Kulutõhusus**: Agentide süsteemid pakuvad kulutõhusat juurutamist võrreldes pilvepõhiste lahendustega, vähendades operatiivkulusid ja madalamaid ribalaiuse nõudeid servarakenduste jaoks.

## Täiustatud väikemudelite strateegiad

### SLM (väikemudeli) alused

Väikemudel (SLM) on keelemudel, mis mahub tavalisele tarbijaseadmele ja suudab teha järeldusi piisavalt madala latentsusega, et olla praktiline ühe kasutaja agentlike päringute teenindamisel. Praktilises mõttes on SLM-id tavaliselt mudelid, millel on vähem kui 10 miljardit parameetrit.

**Formaatide avastamise funktsioonid**: SLM-id pakuvad täiustatud tuge erinevatele kvantiseerimistasemetele, platvormidevahelist ühilduvust, reaalajas jõudluse optimeerimist ja serva juurutamise võimalusi. Kasutajad saavad juurdepääsu täiustatud privaatsusele kohaliku töötlemise kaudu ja WebGPU toele brauseripõhise juurutamise jaoks.

**Kvantiseerimistasemete kogud**: Populaarsed SLM-formaadid hõlmavad Q4_K_M tasakaalustatud kompressiooni jaoks mobiilirakendustes, Q5_K_S seeria kvaliteedile keskendunud serva juurutamiseks, Q8_0 peaaegu originaalse täpsuse jaoks võimsatel servaseadmetel ja eksperimentaalsed formaadid nagu Q2_K ultra-madalate ressursside stsenaariumide jaoks.

### GGUF (General GGML Universal Format) SLM-i juurutamiseks

GGUF toimib peamise formaadina kvantiseeritud SLM-ide juurutamiseks CPU ja servaseadmetel, olles spetsiaalselt optimeeritud agentlike rakenduste jaoks:

**Agentide optimeeritud funktsioonid**: Formaat pakub ulatuslikke ressursse SLM-i konverteerimiseks ja juurutamiseks, pakkudes täiustatud tuge tööriistade kasutamiseks, struktureeritud väljundite genereerimiseks ja mitme pöördega vestlusteks. Platvormidevaheline ühilduvus tagab agentide järjepideva käitumise erinevatel servaseadmetel.

**Jõudluse optimeerimine**: GGUF võimaldab tõhusat mälukasutust agentide töövoogude jaoks, toetab dünaamilist mudelite laadimist mitme agendi süsteemide jaoks ja pakub optimeeritud järeldusi reaalajas agentide interaktsioonide jaoks.

### Serva optimeeritud SLM-raamistikud

#### Llama.cpp optimeerimine agentide jaoks

Llama.cpp pakub tipptasemel kvantiseerimistehnikaid, mis on spetsiaalselt optimeeritud agentlike SLM-i juurutamiseks:

**Agentide spetsiifiline kvantiseerimine**: Raamistik toetab Q4_0 (optimaalne mobiilsete agentide juurutamiseks 75% suuruse vähendamisega), Q5_1 (tasakaalustatud kvaliteedi-kompressiooni jaoks serva järeldusagentidele) ja Q8_0 (peaaegu originaalse kvaliteedi jaoks tootmisagentide süsteemides). Täiustatud formaadid võimaldavad ultra-kompressitud agente äärmuslike serva stsenaariumide jaoks.

**Rakenduse eelised**: CPU-optimeeritud järeldus SIMD kiirendusega pakub mälutõhusat agentide täitmist. Platvormidevaheline ühilduvus x86, ARM ja Apple Silicon arhitektuuride vahel võimaldab universaalseid agentide juurutamise võimalusi.

#### Apple MLX raamistik SLM-agentide jaoks

Apple MLX pakub natiivset optimeerimist, mis on spetsiaalselt loodud SLM-põhiste agentide jaoks Apple Silicon seadmetel:

**Apple Silicon agentide optimeerimine**: Raamistik kasutab ühtset mälustruktuuri Metal Performance Shaders integratsiooniga, automaatset segatud täpsust agentide järeldamiseks ja optimeeritud mäluriba mitme agendi süsteemide jaoks. SLM-agendid näitavad erakordset jõudlust M-seeria kiipidel.

**Arenduse funktsioonid**: Python ja Swift API tugi agentide spetsiifiliste optimeerimistega, automaatne diferentseerimine agentide õppimiseks ja sujuv integreerimine Apple'i arendustööriistadega pakuvad terviklikke agentide arenduskeskkondi.

#### ONNX Runtime platvormidevaheliste SLM-agentide jaoks

ONNX Runtime pakub universaalset järeldusmootorit, mis võimaldab SLM-agentidel töötada järjepidevalt erinevatel riistvaraplatvormidel ja operatsioonisüsteemidel:

**Universaalne juurutamine**: ONNX Runtime tagab SLM-agentide järjepideva käitumise Windowsi, Linuxi, macOS-i, iOS-i ja Androidi platvormidel. See platvormidevaheline ühilduvus võimaldab arendajatel kirjutada üks kord ja juurutada kõikjal, vähendades oluliselt arenduse ja hoolduse koormust mitme platvormi rakenduste jaoks.

**Riistvara kiirenduse valikud**: Raamistik pakub optimeeritud täitmisvõimalusi erinevate riistvarakonfiguratsioonide jaoks, sealhulgas CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) ja spetsialiseeritud kiirendid (Intel VPU, Qualcomm NPU). SLM-agendid saavad automaatselt kasutada parimat saadaolevat riistvara ilma koodimuudatusteta.

**Tootmisvalmis funktsioonid**: ONNX Runtime pakub ettevõtte tasemel funktsioone, mis on olulised tootmisagentide juurutamiseks, sealhulgas graafiku optimeerimine kiiremaks järeldamiseks, mäluhaldus ressursipiirangutega keskkondade jaoks ja põhjalikud profiilimisvahendid jõudluse analüüsiks. Raamistik toetab nii Python kui C++ API-sid paindlikuks integreerimiseks.

## SLM vs LLM agentlikes süsteemides: täiustatud võrdlus

### SLM-i eelised agentide rakendustes

**Operatiivne tõhusus**: SLM-id pakuvad 10-30× kulude vähendamist võrreldes LLM-idega agentide ülesannete jaoks, võimaldades reaalajas agentlike vastuseid suurel skaalal. Nad pakuvad kiiremaid järeldusaegu tänu vähendatud arvutuslikule keerukusele, muutes nad ideaalseks interaktiivsete agentide rakendustes.

**Serva juurutamise võimalused**: SLM-id võimaldavad seadmesisesi agentide täitmist ilma interneti sõltuvuseta, parandatud privaatsust kohaliku agentide töötlemise kaudu ja kohandamist valdkonnaspetsiifiliste agentide rakenduste jaoks, mis sobivad erinevatesse serva-arvutuskeskkondadesse.

**Agentide spetsiifiline optimeerimine**: SLM-id paistavad silma tööriistade kasutamises, struktureeritud väljundite genereerimises ja rutiinsete otsustusprotsesside töövoogudes, mis moodustavad 70-80% tüüpilistest agentide ülesannetest.

### Millal kasutada SLM-i vs LLM-i agentide süsteemides

**Ideaalne SLM-idele**:
- **Korduvad agentide ülesanded**: Andmesisestus, vormide täitmine, rutiinsed API-kutsed
- **Tööriistade integreerimine**: Andmebaasi päringud, failitoimingud, süsteemi interaktsioonid
- **Struktureeritud töövood**: Eelnevalt määratletud agentide protsesside järgimine
- **Valdkonnaspetsiifilised agendid**: Klienditeenindus, ajastamine, põhianalüüs
- **Kohalik töötlemine**: Privaatsustundlikud agentide operatsioonid

**Parem LLM-idele**:
- **Kompleksne arutlemine**: Uute probleemide lahendamine, strateegiline planeerimine
- **Avatud vestlused**: Üldine vestlus, loovad arutelud
- **Lai teadmiste ulatus**: Uuringud, mis nõuavad ulatuslikke üldteadmisi
- **Uued olukorrad**: Täiesti uute agentide stsenaariumide käsitlemine

### Hübriidne agentide arhitektuur

Optimaalne lähenemine ühendab SLM-id ja LLM-id heterogeensetes agentlikes süsteemides:

**Nutikas agentide orkestreerimine**:
1. **SLM esmaseks**: Käsitle 70-80% rutiinsetest agentide ülesannetest kohapeal
2. **LLM vajadusel**: Suuna keerulised päringud pilvepõhistele suurematele mudelitele
3. **Spetsialiseeritud SLM-id**: Erinevad väikemudelid erinevate agentide valdkondade jaoks
4. **Kulude optimeerimine**: Minimeeri kallid LLM-kutsed intelligentse suunamise kaudu

## Tootmisvalmis SLM-agentide juurutusstrateegiad

### Foundry Local: ettevõtte tasemel serva AI runtime

Foundry Local (https://github.com/microsoft/foundry-local) on Microsofti lipulaev lahendus väikemudelite juurutamiseks tootmise servakeskkondades. See pakub täielikku runtime-keskkonda, mis on spetsiaalselt loodud SLM-põhiste agentide jaoks, pakkudes ettevõtte tasemel funktsioone ja sujuvaid integreerimisvõimalusi.

**Põhiarhitektuur ja funktsioonid**:
- **OpenAI-ühilduv API**: Täielik ühilduvus OpenAI SDK ja Agent Frameworki integreerimistega
- **Automaatne riistvara optimeerimine**: Mudelivariantide intelligentne valik vastavalt saadaolevale riistvarale (CUDA GPU, Qualcomm NPU, CPU)
- **Mudelihaldus**: Automaatne mudelite allalaadimine, vahemällu salvestamine ja elutsükli haldamine
- **Teenuse avastamine**: Nullkonfiguratsiooniga teenuse tuvastamine agentide raamistikeks
- **Ressursside optimeerimine**: Intelligentne mäluhaldus ja energiatõhusus serva juurutamiseks

#### Paigaldamine ja seadistamine

**Platvormidevaheline paigaldamine**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Kiire algus agentide arendamiseks**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Agent Frameworki integreerimine

**Foundry Local SDK integreerimine**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automaatne mudeli valik ja riistvara optimeerimine**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Tootmise juurutusmustrid

**Ühe agendi tootmise seadistus**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Mitme agendi tootmise orkestreerimine**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Ettevõtte funktsioonid ja jälgimine

**Tervise jälgimine ja nähtavus**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Ressursside haldamine ja automaatne skaleerimine**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Täiustatud konfiguratsioon ja optimeerimine

**Kohandatud mudeli konfiguratsioon**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Tootmise juurutamise kontrollnimekiri**:

✅ **Teenuse konfiguratsioon**:
- Konfigureeri sobivad mudeli aliased kasutusjuhtumite jaoks
- Sea ressursipiirangud ja jälgimisläved
- Luba tervisekontrollid ja metrikate kogumine
- Konfigureeri automaatne taaskäivitamine ja tõrkeotsing

✅ **Turvalisuse seadistamine**:
- Luba ainult kohalik API-juurdepääs (välise juurdepääsu puudumine)
- Konfigureeri sobiv API-võtmete haldamine
- Sea üles agentide interaktsioonide auditeerimislogid
- Rakenda tootmise kasutuse jaoks kiiruse piiramine

✅ **Jõudluse optimeerimine**:
- Testi mudeli jõudlust eeldatava koormuse all
- Konfigureeri sobivad kvantiseerimistasemed
- Sea üles mudeli vahemällu salvestamise ja soojendamise strateegiad
- Jälgi mälu ja CPU kasutuse mustreid

✅ **Integreerimise testimine**:
- Testi agentide raamistikku integreerimist
- Kinnita offline töövõimekused
- Testi tõrkeotsingu ja taastamise stsenaariume
- Kinnita otsast lõpuni agentide töövood

### Ollama: lihtsustatud SLM-agentide juurutamine

### Ollama: kogukonnale keskendunud SLM-agentide juurutamine

Ollama pakub kogukonna juhitud lähenemist SLM-agentide juurut
- Testi Microsoft Agent Frameworki integreerimist  
- Kontrolli võrguühenduseta töövõimekust  
- Testi tõrkesiirde stsenaariume ja veakäsitlust  
- Kinnita agentide töövoogude terviklikkus  

**Võrdlus Foundry Localiga**:  

| Funktsioon | Foundry Local | Ollama |  
|------------|---------------|--------|  
| **Sihtkasutus** | Ettevõtte tootmine | Arendus ja kogukond |  
| **Mudeliekosüsteem** | Microsofti kureeritud | Ulatuslik kogukond |  
| **Riistvara optimeerimine** | Automaatne (CUDA/NPU/CPU) | Käsitsi seadistamine |  
| **Ettevõtte funktsioonid** | Sisseehitatud jälgimine, turvalisus | Kogukonna tööriistad |  
| **Paigaldamise keerukus** | Lihtne (winget install) | Lihtne (curl install) |  
| **API ühilduvus** | OpenAI + laiendused | OpenAI standard |  
| **Tugi** | Microsofti ametlik | Kogukonna juhitud |  
| **Parim kasutus** | Tootmisagendid | Prototüüpimine, uurimistöö |  

**Millal valida Ollama**:  
- **Arendus ja prototüüpimine**: Kiire katsetamine erinevate mudelitega  
- **Kogukonna mudelid**: Juurdepääs uusimatele kogukonna loodud mudelitele  
- **Hariduslik kasutus**: AI agentide arendamise õppimine ja õpetamine  
- **Uurimisprojektid**: Akadeemilised uuringud, mis vajavad mitmekesist mudelivalikut  
- **Kohandatud mudelid**: Kohandatud mudelite loomine ja testimine  

### VLLM: Kõrge jõudlusega SLM agentide järeldamine  

VLLM (Very Large Language Model inference) pakub suure läbilaskevõimega ja mälusäästlikku järeldusmootorit, mis on spetsiaalselt optimeeritud tootmise SLM-i juurutamiseks suurel skaalal. Kui Foundry Local keskendub kasutusmugavusele ja Ollama kogukonna mudelitele, siis VLLM paistab silma kõrge jõudlusega stsenaariumides, mis nõuavad maksimaalset läbilaskevõimet ja tõhusat ressursside kasutamist.  

**Põhiarhitektuur ja funktsioonid**:  
- **PagedAttention**: Revolutsiooniline mäluhaldus tõhusaks tähelepanu arvutamiseks  
- **Dünaamiline rühmitamine**: Nutikas päringute rühmitamine optimaalse läbilaskevõime saavutamiseks  
- **GPU optimeerimine**: Täiustatud CUDA tuumad ja tensorite paralleelsuse tugi  
- **OpenAI ühilduvus**: Täielik API ühilduvus sujuvaks integreerimiseks  
- **Spekulatiivne dekodeerimine**: Täiustatud järelduse kiirendamise tehnikad  
- **Kvantiseerimise tugi**: INT4, INT8 ja FP16 kvantiseerimine mälusäästlikkuseks  

#### Paigaldamine ja seadistamine  

**Paigaldamisvõimalused**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Kiire algus agentide arendamiseks**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  
#### Agent Frameworki integreerimine  

**VLLM koos Microsoft Agent Frameworkiga**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**Kõrge läbilaskevõimega multi-agent süsteem**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  
#### Tootmise juurutamise mustrid  

**Ettevõtte VLLM tootmisteenus**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  
#### Ettevõtte funktsioonid ja jälgimine  

**Täiustatud VLLM jõudluse jälgimine**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  
#### Täiustatud konfiguratsioon ja optimeerimine  

**Tootmise VLLM konfiguratsiooni mallid**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Tootmise juurutamise kontrollnimekiri VLLM jaoks**:  

✅ **Riistvara optimeerimine**:  
- Seadista tensorite paralleelsus mitme GPU jaoks  
- Luba kvantiseerimine (AWQ/GPTQ) mälusäästlikkuseks  
- Määra optimaalne GPU mälu kasutus (85-95%)  
- Seadista sobivad rühma suurused läbilaskevõime jaoks  

✅ **Jõudluse häälestamine**:  
- Luba eelfikseerimine korduvate päringute jaoks  
- Seadista tükeldatud eeltäitmine pikkade järjestuste jaoks  
- Luba spekulatiivne dekodeerimine kiiremaks järeldamiseks  
- Optimeeri max_num_seqs vastavalt riistvarale  

✅ **Tootmise funktsioonid**:  
- Seadista tervise jälgimine ja mõõdikute kogumine  
- Konfigureeri automaatne taaskäivitamine ja tõrkesiire  
- Rakenda päringute järjekorda ja koormuse tasakaalustamist  
- Seadista põhjalik logimine ja hoiatused  

✅ **Turvalisus ja töökindlus**:  
- Konfigureeri tulemüüri reeglid ja juurdepääsukontrollid  
- Seadista API kiiruse piiramine ja autentimine  
- Rakenda sujuv sulgemine ja puhastamine  
- Konfigureeri varundamine ja katastroofide taastamine  

✅ **Integreerimise testimine**:  
- Testi Microsoft Agent Frameworki integreerimist  
- Kinnita kõrge läbilaskevõimega stsenaariumid  
- Testi tõrkesiiret ja taastumisprotseduure  
- Võrdle jõudlust koormuse all  

**Võrdlus teiste lahendustega**:  

| Funktsioon | VLLM | Foundry Local | Ollama |  
|------------|------|---------------|--------|  
| **Sihtkasutus** | Kõrge läbilaskevõimega tootmine | Ettevõtte kasutusmugavus | Arendus ja kogukond |  
| **Jõudlus** | Maksimaalne läbilaskevõime | Tasakaalustatud | Hea |  
| **Mälusäästlikkus** | PagedAttention optimeerimine | Automaatne optimeerimine | Standardne |  
| **Seadistamise keerukus** | Kõrge (palju parameetreid) | Madal (automaatne) | Madal (lihtne) |  
| **Mastaapsus** | Suurepärane (tensor/pipeline paralleelsus) | Hea | Piiratud |  
| **Kvantiseerimine** | Täiustatud (AWQ, GPTQ, FP8) | Automaatne | Standard GGUF |  
| **Ettevõtte funktsioonid** | Vajab kohandatud rakendust | Sisseehitatud | Kogukonna tööriistad |  
| **Parim kasutus** | Suuremahulised tootmisagendid | Ettevõtte tootmine | Arendus |  

**Millal valida VLLM**:  
- **Kõrge läbilaskevõime nõuded**: Sajad päringud sekundis  
- **Suuremahulised juurutused**: Mitme GPU ja mitme sõlmega juurutused  
- **Jõudluskriitiline**: Alla sekundi vastuseajad suurel skaalal  
- **Täiustatud optimeerimine**: Vajadus kohandatud kvantiseerimise ja rühmitamise järele  
- **Ressursisäästlikkus**: Maksimaalne kallite GPU riistvara kasutus  

## Reaalsed SLM agentide rakendused  

### Klienditeeninduse SLM agendid  
- **SLM võimekus**: Kontootsingud, paroolide lähtestamine, tellimuste oleku kontroll  
- **Kuluefektiivsus**: 10-kordne järelduskulude vähendamine võrreldes LLM agentidega  
- **Jõudlus**: Kiiremad vastuseajad ja järjepidev kvaliteet rutiinsete päringute jaoks  

### Ärijuhtimise SLM agendid  
- **Arve töötlemise agendid**: Andmete väljavõtmine, teabe valideerimine, suunamine kinnitamiseks  
- **E-posti haldamise agendid**: Kategooriate määramine, prioriteetide seadmine, automaatsete vastuste koostamine  
- **Ajastamise agendid**: Kohtumiste koordineerimine, kalendrite haldamine, meeldetuletuste saatmine  

### Isiklikud SLM digitaalsed assistendid  
- **Ülesannete haldamise agendid**: Tõhusalt loovad, uuendavad ja korraldavad ülesannete nimekirju  
- **Teabe kogumise agendid**: Uurivad teemasid, koostavad kokkuvõtteid kohapeal  
- **Suhtlusagendid**: Koostavad e-kirju, sõnumeid, sotsiaalmeedia postitusi privaatselt  

### Kauplemise ja finantsvaldkonna SLM agendid  
- **Turujälgimise agendid**: Jälgivad hindu, tuvastavad trende reaalajas  
- **Aruannete koostamise agendid**: Loovad automaatselt igapäevaseid/nädalaseid kokkuvõtteid  
- **Riskihindamise agendid**: Hindavad portfelli positsioone kohalike andmete abil  

### Tervishoiu tugiteenuste SLM agendid  
- **Patsiendi ajastamise agendid**: Koordineerivad kohtumisi, saadavad automaatseid meeldetuletusi  
- **Dokumentatsiooni agendid**: Loovad meditsiinilisi kokkuvõtteid ja aruandeid kohapeal  
- **Retsepti haldamise agendid**: Jälgivad täiendusi, kontrollivad koostoimeid privaatselt  

## Microsoft Agent Framework: Tootmisvalmis agentide arendus  

### Ülevaade ja arhitektuur  

Microsoft Agent Framework pakub terviklikku, ettevõtte tasemel platvormi AI agentide loomiseks, juurutamiseks ja haldamiseks, mis suudavad töötada nii pilves kui ka võrguühenduseta servakeskkondades. Raamistik on spetsiaalselt loodud töötama sujuvalt väikeste keelemudelite ja servaarvutuse stsenaariumidega, muutes selle ideaalseks privaatsustundlike ja ressursipiirangutega juurutuste jaoks.  

**Põhikomponendid**:  
- **Agendi käituskeskkond**: Kergekaaluline täitmiskeskkond, optimeeritud servaseadmete jaoks  
- **Tööriistade integreerimissüsteem**: Laiendatav pistikprogrammide arhitektuur väliste teenuste ja API-de ühendamiseks  
- **Oleku haldamine**: Püsiv agendi mälu ja konteksti käsitlus seansside vahel  
- **Turvalisuskiht**: Sisseehitatud turvakontrollid ettevõtte juurutamiseks  
- **Orkestreerimismootor**: Mitme agendi koordineerimine ja töövoogude haldamine  

### Servajuurutuse võtmefunktsioonid  

**Võrguühenduseta esmane arhitektuur**: Microsoft Agent Framework on loodud võrguühenduseta esmaste põhimõtete järgi, võimaldades agentidel tõhusalt töötada ilma pideva internetiühenduseta. See hõlmab kohalikku mudelite järeldamist, vahemällu salvestatud teadmiste baase, võrguühenduseta tööriistade täitmist ja sujuvat degradeerumist, kui pilveteenused pole saadaval.  

**Ressursside optimeerimine**: Raamistik pakub nutikat ressursside haldamist automaatse mälusäästlikkusega SLM-ide jaoks, CPU/GPU koormuse tasakaalustamist servaseadmete jaoks, kohanduvat mudelivalikut vastavalt saadavatele ressurssidele ja energiasäästlikke järeldusmustreid mobiilseks juurutamiseks.  

**Turvalisus ja privaatsus**: Ettevõtte tasemel turvafunktsioonid hõlmavad kohalikku andmetöötlust privaatsuse säilitamiseks, krüpteeritud agendi suhtluskanaleid, rollipõhiseid juurdepääsukontrolle agendi võimekuste jaoks ja auditeerimislogisid vastavusnõuete täitmiseks.  

### Integreerimine Foundry Localiga  

Microsoft Agent Framework integreerub sujuvalt Foundry Localiga, pakkudes täielikku serva AI lahendust:  

**Automaatne mudelite avastamine**: Raamistik tuvastab ja ühendub automaatselt Foundry Locali instantsidega, avastab saadaval olevad SLM mudelid ja valib optimaalsed mudelid vastavalt agendi nõuetele ja riistvara võimekusele.  

**Dünaamiline mudelite laadimine**: Agendid saavad dünaamiliselt laadida erinevaid SLM-e konkreetsete ülesannete jaoks, võimaldades mitme mudeliga agendisüsteeme, kus erinevad mudelid käsitlevad erinevat tüüpi päringuid, ja automaatset tõrkesiiret mudelite vahel vastavalt saadavusele ja jõudlusele.  

**Jõudluse optimeerimine**: Integreeritud vahemälu mehhanismid vähendavad mudelite laadimisaega, ühenduste jagamine optimeerib API-kõnesid Foundry Localile ja nutikas rühmitamine parandab läbilaskevõimet mitme agendi päringute jaoks.  

### Agentide loomine Microsoft Agent Frameworkiga  

#### Agendi määratlemine ja konfiguratsioon  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### Tööriistade integreerimine servastsenaariumide jaoks  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### Mitme agendi orkestreerimine  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  
### Täiustatud servajuurutuse mustrid  

#### Hierarhiline agendi arhitektuur  

**Kohalikud agendiklastrid**: Juurutage mitu spetsialiseeritud SLM agenti servaseadmetele, igaüks optimeeritud konkreetsete ülesannete jaoks. Kasutage kergeid mudeleid nagu Qwen2.5-0.5B lihtsate suunamis- ja ajastamisülesannete jaoks, keskmise suurusega mudeleid nagu Phi-4-Mini klienditeeninduse ja dokumentatsiooni jaoks ning suuremaid mudeleid keerukate arutluste jaoks, kui ressursid seda võimaldavad.  

**Serva-pilve koordineerimine**: Rakendage nutikaid eskalatsioonimustreid, kus kohalikud agendid käsitlevad rutiinseid ülesandeid, pilveagendid pakuvad keerulist arutlust, kui ühenduvus seda võimaldab, ja sujuv üleminek serva ja pilve töötlemise vahel säilitab järjepidevuse.  

#### Juurutuse konfiguratsioonid  

**Ühe seadme juurutus**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**Jaotatud servajuurutus**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  
### Jõudluse optimeerimine servaagentide jaoks  

#### Mudelivaliku strateegiad  

**Ülesandepõhine mudeli määramine**: Microsoft Agent Framework võimaldab nutikat mudelivalikut vastavalt ülesande keerukusele ja nõuetele:  

- **Lihtsad ülesanded** (küsimused ja vastused, suunamine): Qwen2.5-0.5B (500MB, <100ms vastus)  
- **Keskmised ülesanded** (klienditeenindus, ajastamine): Phi-4-Mini (2.4GB, 200-500ms vastus)  
- **Keerukad ülesanded** (tehniline analüüs, planeerimine): Phi-4 (7GB, 1-3s vastus, kui ressursid seda võimaldavad)  

**Dünaamiline mudelite vahetamine**: Agendid saavad vahetada mudeleid vastavalt süsteemi koormusele, ülesande keerukuse hindamisele, kasutaja prioriteetidele ja saadavatele riistvararesurssidele.  

#### Mälu ja ressursside haldamine  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  
### Ettevõtte integreerimise mustrid  

#### Turvalisus ja vastavus  

**Kohalik andmetöötlus**: Kõik agendi töötlemine toimub kohapeal, tagades tundlike andmete servaseadmest lahkumise vältimise. See hõlmab kliendiandmete kaitset, HIPAA vastavust tervishoiuagentide jaoks, finantsandmete turvalisust pangandusagentide jaoks ja GDPR vastavust Euroopa juurutuste jaoks.  

**Juurdepääsukontroll**: Rollipõhised õigused kontrollivad, milliseid tööriistu agendid saavad kasutada, kasutaja autentimine agendi interaktsioonide jaoks ja auditeerimisjäljed kõigi agendi toimingute ja otsuste jaoks.  

#### Jälgimine ja jälgitavus  

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```
  
### Reaalsed rakenduse näited  

#### Jaemüügi servaagendi süsteem  

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```
  
#### Tervishoiu tugiteenuste agent  

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```
  
### Parimad tavad Microsoft Agent Frameworki jaoks  

#### Arenduse
**Raamistiku valik agentide juurutamiseks**: Valige optimeerimisraamistikud vastavalt sihtseadmetele ja agentide nõuetele. Kasutage Llama.cpp CPU optimeeritud agentide juurutamiseks, Apple MLX Apple Siliconi agentide rakenduste jaoks ja ONNX platvormidevahelise ühilduvuse tagamiseks.

## Praktiline SLM agentide konverteerimine ja kasutusjuhtumid

### Agentide juurutamise stsenaariumid päriselus

**Mobiilsed agentide rakendused**: Q4_K formaadid sobivad suurepäraselt nutitelefonide agentide rakendusteks, kuna neil on minimaalne mälukasutus, samas kui Q8_0 pakub tasakaalustatud jõudlust tahvelarvutite agentide süsteemide jaoks. Q5_K formaadid tagavad kõrge kvaliteedi mobiilsete produktiivsusagentide jaoks.

**Lauaarvutite ja servaagentide arvutamine**: Q5_K tagab optimaalse jõudluse lauaarvutite agentide rakendustes, Q8_0 pakub kõrgekvaliteedilist järeldust tööjaamade agentide keskkondades ja Q4_K võimaldab tõhusat töötlemist servaagentide seadmetes.

**Teadus- ja eksperimentaalsed agendid**: Täiustatud kvantiseerimisformaadid võimaldavad uurida ultra-madala täpsusega agentide järeldusi akadeemilisteks uuringuteks ja kontseptsioonide tõestamiseks, mis vajavad äärmiselt piiratud ressursse.

### SLM agentide jõudluse võrdlused

**Agentide järeldamise kiirus**: Q4_K saavutab mobiilsetel protsessoritel kõige kiiremad agentide vastamisajad, Q5_K pakub tasakaalustatud kiiruse ja kvaliteedi suhet üldiste agentide rakenduste jaoks, Q8_0 tagab keerukate agentide ülesannete jaoks ülimalt kõrge kvaliteedi ja eksperimentaalsed formaadid maksimaalse läbilaskevõime spetsialiseeritud agentide riistvarale.

**Agentide mälunõuded**: Agentide kvantiseerimistasemed ulatuvad Q2_K-st (alla 500 MB väikeste agentide mudelite jaoks) kuni Q8_0-ni (umbes 50% algsest suurusest), kusjuures eksperimentaalsed konfiguratsioonid saavutavad maksimaalse tihenduse ressursside piiratud agentide keskkondades.

## Väljakutsed ja kaalutlused SLM agentide jaoks

### Jõudluse kompromissid agentide süsteemides

SLM agentide juurutamine nõuab hoolikat kaalumist mudeli suuruse, agentide vastamiskiiruse ja väljundi kvaliteedi vahel. Kuigi Q4_K pakub erakordset kiirust ja tõhusust mobiilsete agentide jaoks, tagab Q8_0 keerukate agentide ülesannete jaoks ülimalt kõrge kvaliteedi. Q5_K on tasakaalustatud valik, mis sobib enamiku üldiste agentide rakenduste jaoks.

### Riistvara ühilduvus SLM agentide jaoks

Erinevatel servaseadmetel on erinevad võimed SLM agentide juurutamiseks. Q4_K töötab tõhusalt lihtsamatel protsessoritel lihtsate agentide jaoks, Q5_K vajab mõõdukaid arvutusressursse tasakaalustatud agentide jõudluse jaoks ja Q8_0 kasutab ära kõrgema klassi riistvara arenenud agentide võimekuse jaoks.

### Turvalisus ja privaatsus SLM agentide süsteemides

Kuigi SLM agendid võimaldavad kohalikku töötlemist privaatsuse parandamiseks, tuleb rakendada asjakohaseid turvameetmeid, et kaitsta agentide mudeleid ja andmeid servakeskkondades. See on eriti oluline, kui juurutatakse kõrge täpsusega agentide formaate ettevõtete keskkondades või tihendatud agentide formaate rakendustes, mis käsitlevad tundlikke andmeid.

## Tulevikutrendid SLM agentide arenduses

SLM agentide maastik areneb pidevalt koos edusammudega tihendustehnikates, optimeerimismeetodites ja serva juurutamise strateegiates. Tulevased arengud hõlmavad tõhusamaid kvantiseerimisalgoritme agentide mudelite jaoks, paremaid tihendusmeetodeid agentide töövoogude jaoks ja paremat integreerimist serva riistvara kiirenditega agentide töötlemiseks.

**SLM agentide turu prognoosid**: Hiljutiste uuringute kohaselt võib agentide juhitud automatiseerimine kõrvaldada 40–60% korduvatest kognitiivsetest ülesannetest ettevõtete töövoogudes aastaks 2027, kusjuures SLM-id juhivad seda transformatsiooni tänu nende kulutõhususele ja juurutamise paindlikkusele.

**Tehnoloogilised suundumused SLM agentides**:
- **Spetsialiseeritud SLM agendid**: Valdkonnaspetsiifilised mudelid, mis on koolitatud konkreetsete agentide ülesannete ja tööstusharude jaoks
- **Servaagentide arvutamine**: Täiustatud seadmesisesed agentide võimekused parema privaatsuse ja vähendatud latentsusega
- **Agentide orkestreerimine**: Parem koordineerimine mitme SLM agendi vahel dünaamilise suunamise ja koormuse tasakaalustamisega
- **Demokratiseerimine**: SLM-i paindlikkus võimaldab laiemat osalust agentide arendamisel erinevates organisatsioonides

## SLM agentidega alustamine

### Samm 1: Microsoft Agent Frameworki keskkonna seadistamine

**Paigalda sõltuvused**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicialiseeri Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Samm 2: Valige oma SLM agentide rakenduste jaoks
Populaarsed valikud Microsoft Agent Frameworki jaoks:
- **Microsoft Phi-4 Mini (3.8B)**: Suurepärane üldiste agentide ülesannete jaoks tasakaalustatud jõudlusega
- **Qwen2.5-0.5B (0.5B)**: Ülitäpne lihtsate suunamis- ja klassifitseerimisagentide jaoks
- **Qwen2.5-Coder-0.5B (0.5B)**: Spetsialiseerunud koodiga seotud agentide ülesannete jaoks
- **Phi-4 (7B)**: Täiustatud põhjendamine keerukate serva stsenaariumide jaoks, kui ressursid seda võimaldavad

### Samm 3: Looge oma esimene agent Microsoft Agent Frameworkiga

**Põhiline agendi seadistamine**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Samm 4: Määratlege agendi ulatus ja nõuded
Alustage keskendunud, hästi määratletud agentide rakendustega, kasutades Microsoft Agent Frameworki:
- **Ühe valdkonna agendid**: Klienditeenindus VÕI ajakava koostamine VÕI uurimistöö
- **Selged agendi eesmärgid**: Konkreetsed, mõõdetavad eesmärgid agendi jõudluse jaoks
- **Piiratud tööriistade integreerimine**: Maksimaalselt 3–5 tööriista esialgse agendi juurutamise jaoks
- **Määratletud agendi piirid**: Selged eskaleerimisteed keerukate stsenaariumide jaoks
- **Serva-esimene disain**: Eelistage võrguühenduseta funktsionaalsust ja kohalikku töötlemist

### Samm 5: Rakendage serva juurutamine Microsoft Agent Frameworkiga

**Ressursside konfiguratsioon**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Rakendage turvameetmed servaagentide jaoks**:
- **Kohalik sisendi valideerimine**: Kontrollige päringuid ilma pilve sõltuvuseta
- **Võrguühenduseta väljundi filtreerimine**: Veenduge, et vastused vastavad kvaliteedistandarditele kohapeal
- **Serva turvakontrollid**: Rakendage turvameetmeid ilma internetiühenduse vajaduseta
- **Kohalik jälgimine**: Jälgige jõudlust ja tuvastage probleeme serva telemeetria abil

### Samm 6: Mõõtke ja optimeerige servaagentide jõudlust
- **Agendi ülesannete täitmise määrad**: Jälgige edukuse määrasid võrguühenduseta stsenaariumides
- **Agendi vastamisajad**: Tagage serva juurutamise jaoks alla sekundi vastamisajad
- **Ressursside kasutamine**: Jälgige servaseadmete mälu, CPU ja aku kasutust
- **Kulutõhusus**: Võrrelge serva juurutamise kulusid pilvepõhiste alternatiividega
- **Võrguühenduseta töökindlus**: Mõõtke agendi jõudlust võrguühenduse katkestuste ajal

## Olulised punktid SLM agentide rakendamiseks

1. **SLM-id on agentide jaoks piisavad**: Enamiku agentide ülesannete jaoks toimivad väikesed mudelid sama hästi kui suured, pakkudes samal ajal märkimisväärseid eeliseid
2. **Agentide kulutõhusus**: SLM agentide käitamine on 10–30 korda odavam, muutes need majanduslikult elujõuliseks laialdaseks juurutamiseks
3. **Spetsialiseerumine töötab agentide jaoks**: Täpselt häälestatud SLM-id ületavad sageli üldotstarbelisi LLM-e konkreetsetes agentide rakendustes
4. **Hübriidne agendi arhitektuur**: Kasutage SLM-e rutiinsete agentide ülesannete jaoks, LLM-e keerukate põhjenduste jaoks, kui vaja
5. **Microsoft Agent Framework võimaldab tootmisjuurutamist**: Pakub ettevõtte tasemel tööriistu servaagentide loomiseks, juurutamiseks ja haldamiseks
6. **Serva-esimese disaini põhimõtted**: Võrguühenduseta võimekusega agendid, millel on kohalik töötlemine, tagavad privaatsuse ja töökindluse
7. **Foundry Local integratsioon**: Sujuv ühendus Microsoft Agent Frameworki ja kohaliku mudeli järeldamise vahel
8. **Tulevik kuulub SLM agentidele**: Väikesed keelemudelid koos tootmisraamistikega on agentse AI tulevik, võimaldades demokratiseeritud ja tõhusat agentide juurutamist

## Viited ja lisalugemine

### Põhiuuringud ja publikatsioonid

#### AI agendid ja agentide süsteemid
- **"Language Agents as Optimizable Graphs"** (2024) - Põhiuuringud agentide arhitektuuri ja optimeerimise strateegiate kohta
  - Autorid: Wenyue Hua, Lishan Yang jt.
  - Link: https://arxiv.org/abs/2402.16823
  - Olulised tähelepanekud: Graafikupõhine agentide disain ja optimeerimisstrateegiad

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Autorid: Zhiheng Xi, Wenxiang Chen jt.
  - Link: https://arxiv.org/abs/2309.07864
  - Olulised tähelepanekud: LLM-põhiste agentide võimekuse ja rakenduste põhjalik ülevaade

- **"Cognitive Architectures for Language Agents"** (2024)
  - Autorid: Theodore Sumers, Shunyu Yao jt.
  - Link: https://arxiv.org/abs/2309.02427
  - Olulised tähelepanekud: Kognitiivsed raamistikud intelligentsete agentide disainimiseks

#### Väikesed keelemudelid ja optimeerimine
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Autorid: Microsoft Research Team
  - Link: https://arxiv.org/abs/2404.14219
  - Olulised tähelepanekud: SLM-i disainipõhimõtted ja mobiilne juurutamine

- **"Qwen2.5 Technical Report"** (2024)
  - Autorid: Alibaba Cloud Team
  - Link: https://arxiv.org/abs/2407.10671
  - Olulised tähelepanekud: Täiustatud SLM-i treenimistehnikad ja jõudluse optimeerimine

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Autorid: Peiyuan Zhang, Guangtao Zeng jt.
  - Link: https://arxiv.org/abs/2401.02385
  - Olulised tähelepanekud: Ülitäpsete mudelite disain ja treenimise efektiivsus

### Ametlik dokumentatsioon ja raamistikud

#### Microsoft Agent Framework
- **Ametlik dokumentatsioon**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHubi repositoorium**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Peamine repositoorium**: https://github.com/microsoft/foundry-local
- **Dokumentatsioon**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Peamine repositoorium**: https://github.com/vllm-project/vllm
- **Dokumentatsioon**: https://docs.vllm.ai/


#### Ollama
- **Ametlik veebisait**: https://ollama.ai/
- **GitHubi repositoorium**: https://github.com/ollama/ollama

### Mudelite optimeerimise raamistikud

#### Llama.cpp
- **Repositoorium**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentatsioon**: https://microsoft.github.io/Olive/
- **GitHubi repositoorium**: https://github.com/microsoft/Olive

#### OpenVINO
- **Ametlik veebisait**: https://docs.openvino.ai/

#### Apple MLX
- **Repositoorium**: https://github.com/ml-explore/mlx

### Tööstusaruanded ja turuanalüüs

#### AI agentide turu-uuring
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Link: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Olulised tähelepanekud: Turusuundumused ja ettevõtete kasutuselevõtu mustrid

#### Tehnilised võrdlused

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Link: https://mlcommons.org/en/inference-edge/
  - Olulised tähelepanekud: Standardiseeritud jõudlusmõõdikud serva juurutamiseks

### Standardid ja spetsifikatsioonid

#### Mudelite formaadid ja standardid
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Platvormidevaheline mudeli formaat ühilduvuse tagamiseks
- **GGUF spetsifikatsioon**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantiseeritud mudeli formaat CPU järeldamiseks
- **OpenAI API spetsifikatsioon**: https://platform.openai.com/docs/api-reference
  - Standardne API formaat keelemudeli integreerimiseks

#### Turvalisus ja vastavus
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI süsteemid**: Raamistik AI süsteemide ja ohutuse jaoks
- **IEEE standardid AI jaoks**: https://standards.ieee.org/industry-connections/ai/

Üleminek SLM-põhistele agentidele esindab fundamentaalset muutust AI juurutamise lähenemises. Microsoft Agent Framework koos kohalike platvormide ja tõhusate väikeste keelemudelitega pakub täielikku lahendust tootmisvalmis agentide loomiseks, mis töötavad tõhusalt servakeskkondades. Keskendudes tõhususele, spetsialiseerumisele ja praktilisele kasutatavusele, muudab see tehnoloogiline komplekt AI agendid kättesaadavamaks, taskukohasemaks ja tõhusamaks reaalse maailma rakendustes igas tööstusharus ja serva arvutuskeskkonnas.

Edasi liikudes aastasse 2025 avab üha võimekamate väikeste mudelite, keerukate agentide raamistikute nagu Microsoft Agent Framework ja tugevate serva juurutamise platvormide kombinatsioon uusi võimalusi autonoomsetele süsteemidele, mis suudavad tõhusalt töötada servaseadmetel, säilitades samal ajal privaatsuse, vähendades kulusid ja pakkudes erakordseid kasutajakogemusi.

**Järgmised sammud rakendamiseks**:
1. **Uurige funktsioonide kutsumist**: Õppige, kuidas SLM-id integreerivad tööriistu ja struktureeritud väljundeid
2. **Valdage Model Context Protocol (MCP)**: Mõistke arenenud agentide suhtlusmustreid

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta arusaamatuste või valesti tõlgenduste eest, mis võivad tekkida selle tõlke kasutamise tõttu.