<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T15:41:37+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "et"
}
-->
# Sektsioon 3: Microsoft Olive Optimeerimiskomplekt

## Sisukord
1. [Sissejuhatus](../../../Module04)
2. [Mis on Microsoft Olive?](../../../Module04)
3. [Paigaldamine](../../../Module04)
4. [Kiire alustamise juhend](../../../Module04)
5. [Näide: Qwen3 teisendamine ONNX INT4-ks](../../../Module04)
6. [Täiustatud kasutamine](../../../Module04)
7. [Olive retseptide hoidla](../../../Module04)
8. [Parimad praktikad](../../../Module04)
9. [Tõrkeotsing](../../../Module04)
10. [Täiendavad ressursid](../../../Module04)

## Sissejuhatus

Microsoft Olive on võimas ja lihtsasti kasutatav riistvarateadlik mudelite optimeerimise tööriistakomplekt, mis lihtsustab masinõppe mudelite optimeerimist erinevate riistvaraplatvormide jaoks. Olgu sihtmärgiks CPU-d, GPU-d või spetsiaalsed AI kiirendid, Olive aitab saavutada parimat jõudlust, säilitades samal ajal mudeli täpsuse.

## Mis on Microsoft Olive?

Olive on lihtsasti kasutatav riistvarateadlik mudelite optimeerimise tööriist, mis ühendab juhtivaid tehnikaid mudelite tihendamise, optimeerimise ja kompileerimise valdkonnas. See töötab koos ONNX Runtime'iga kui E2E järelduste optimeerimise lahendus.

### Põhifunktsioonid

- **Riistvarateadlik optimeerimine**: Valib automaatselt parimad optimeerimistehnikad sihtriistvara jaoks
- **40+ sisseehitatud optimeerimiskomponenti**: Hõlmab mudelite tihendamist, kvantiseerimist, graafiku optimeerimist ja palju muud
- **Lihtne CLI-liides**: Lihtsad käsud tavaliste optimeerimisülesannete jaoks
- **Mitme raamistiku tugi**: Ühildub PyTorch, Hugging Face mudelite ja ONNX-iga
- **Populaarsete mudelite tugi**: Olive suudab automaatselt optimeerida populaarseid mudeliarhitektuure nagu Llama, Phi, Qwen, Gemma jne otse karbist välja

### Eelised

- **Vähenenud arendusaeg**: Ei ole vaja käsitsi katsetada erinevaid optimeerimistehnikaid
- **Jõudluse paranemine**: Märkimisväärsed kiiruse parandused (mõnel juhul kuni 6x)
- **Platvormidevaheline kasutamine**: Optimeeritud mudelid töötavad erinevatel riistvaradel ja operatsioonisüsteemidel
- **Säilinud täpsus**: Optimeerimised säilitavad mudeli kvaliteedi, parandades samal ajal jõudlust

## Paigaldamine

### Eeltingimused

- Python 3.8 või uuem
- pip pakettihaldur
- Virtuaalne keskkond (soovitatav)

### Põhipaigaldus

Loo ja aktiveeri virtuaalne keskkond:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Paigalda Olive automaatse optimeerimise funktsioonidega:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valikulised sõltuvused

Olive pakub mitmesuguseid valikulisi sõltuvusi lisafunktsioonide jaoks:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Paigalduse kontrollimine

```bash
olive --help
```

Kui paigaldus õnnestus, peaksite nägema Olive CLI abi sõnumit.

## Kiire alustamise juhend

### Esimene optimeerimine

Optimeerime väikese keelemudeli, kasutades Olive automaatse optimeerimise funktsiooni:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mida see käsk teeb

Optimeerimisprotsess hõlmab: mudeli hankimist kohalikust vahemälust, ONNX graafiku jäädvustamist ja kaalude salvestamist ONNX andmefaili, ONNX graafiku optimeerimist ja mudeli kvantiseerimist int4-ks RTN meetodi abil.

### Käskude parameetrite selgitus

- `--model_name_or_path`: Hugging Face mudeli identifikaator või kohalik tee
- `--output_path`: Kataloog, kuhu optimeeritud mudel salvestatakse
- `--device`: Sihtseade (cpu, gpu)
- `--provider`: Täitmise pakkuja (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Kasuta ONNX Runtime Generate AI-d järelduste jaoks
- `--precision`: Kvantiseerimise täpsus (int4, int8, fp16)
- `--log_level`: Logimise detailsus (0=minimaalne, 1=detailne)

## Näide: Qwen3 teisendamine ONNX INT4-ks

Tuginedes Hugging Face näitele [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), optimeerime Qwen3 mudeli järgmiselt:

### Samm 1: Mudeli allalaadimine (valikuline)

Allalaadimisaja vähendamiseks vahemälu ainult olulised failid:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Samm 2: Qwen3 mudeli optimeerimine

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Samm 3: Optimeeritud mudeli testimine

Loo lihtne Python skript, et testida optimeeritud mudelit:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Väljundi struktuur

Pärast optimeerimist sisaldab teie väljundkataloog:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Täiustatud kasutamine

### Konfiguratsioonifailid

Keerukamate optimeerimisvoogude jaoks saate kasutada JSON konfiguratsioonifaile:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Käivita konfiguratsiooniga:

```bash
olive run --config config.json
```

### GPU optimeerimine

CUDA GPU optimeerimiseks:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML jaoks (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mudelite peenhäälestamine Olive'iga

Olive toetab ka mudelite peenhäälestamist:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Parimad praktikad

### 1. Mudeli valik
- Alusta väiksemate mudelitega testimiseks (nt 0.5B-7B parameetrid)
- Veendu, et sihtmudeli arhitektuur on Olive poolt toetatud

### 2. Riistvara kaalutlused
- Sobita optimeerimise siht oma kasutatava riistvaraga
- Kasuta GPU optimeerimist, kui sul on CUDA-ühilduv riistvara
- Kaalu DirectML-i Windowsi masinate jaoks, millel on integreeritud graafika

### 3. Täpsuse valik
- **INT4**: Maksimaalne tihendus, kerge täpsuse kaotus
- **INT8**: Hea tasakaal suuruse ja täpsuse vahel
- **FP16**: Minimaalne täpsuse kaotus, mõõdukas suuruse vähendamine

### 4. Testimine ja valideerimine
- Testi alati optimeeritud mudeleid oma konkreetsete kasutusjuhtumitega
- Võrdle jõudlusmõõdikuid (latentsus, läbilaskevõime, täpsus)
- Kasuta esinduslikke sisendandmeid hindamiseks

### 5. Iteratiivne optimeerimine
- Alusta automaatse optimeerimisega kiirete tulemuste saamiseks
- Kasuta konfiguratsioonifaile täpsema kontrolli jaoks
- Katseta erinevaid optimeerimispasse

## Tõrkeotsing

### Levinud probleemid

#### 1. Paigaldusprobleemid
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU probleemid
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Mälu probleemid
- Kasuta optimeerimise ajal väiksemaid partiisuurusi
- Proovi kvantiseerimist kõrgema täpsusega (int8 asemel int4)
- Veendu, et mudeli vahemälu jaoks on piisavalt kettaruumi

#### 4. Mudeli laadimise vead
- Kontrolli mudeli teed ja juurdepääsuõigusi
- Veendu, et mudel vajab `trust_remote_code=True`
- Kontrolli, kas kõik vajalikud mudelifailid on alla laaditud

### Abi saamine

- **Dokumentatsioon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub probleemid**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Näited**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive retseptide hoidla

### Sissejuhatus Olive retseptidesse

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) hoidla täiendab Olive tööriistakomplekti, pakkudes ulatuslikku kogumikku valmis optimeerimisretsepte populaarsete AI mudelite jaoks. See hoidla toimib praktilise viitena nii avalikult kättesaadavate mudelite optimeerimiseks kui ka optimeerimisvoogude loomiseks privaatsete mudelite jaoks.

### Põhifunktsioonid

- **100+ eelvalmistatud retsepti**: Valmis optimeerimiskonfiguratsioonid populaarsete mudelite jaoks
- **Mitme arhitektuuri tugi**: Hõlmab transformer mudeleid, visioonimudeleid ja multimodaalseid arhitektuure
- **Riistvaraspetsiifilised optimeerimised**: Retseptid kohandatud CPU, GPU ja spetsiaalsete kiirendite jaoks
- **Populaarsed mudeliperekonnad**: Sisaldab Phi, Llama, Qwen, Gemma, Mistral ja palju muud

### Toetatud mudeliperekonnad

Hoidla sisaldab optimeerimisretsepte:

#### Keelemudelid
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 seeria (0.5B kuni 14B)
- **Google Gemma**: Erinevad Gemma mudelikonfiguratsioonid
- **Mistral AI**: Mistral-7B seeria
- **DeepSeek**: R1-Distill seeria mudelid

#### Visiooni- ja multimodaalsed mudelid
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP mudelid**: Erinevad CLIP-ViT konfiguratsioonid
- **ResNet**: ResNet-50 optimeerimised
- **Visiooni transformerid**: ViT-base-patch16-224

#### Spetsialiseeritud mudelid
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Baas- ja mitmekeelne variant
- **Lause transformerid**: all-MiniLM-L6-v2

### Olive retseptide kasutamine

#### Meetod 1: Konkreetse retsepti kloonimine

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Meetod 2: Retsepti kasutamine mallina

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Retsepti struktuur

Iga retsepti kataloog sisaldab tavaliselt:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Näide: Phi-4-mini retsepti kasutamine

Kasutame Phi-4-mini retsepti näitena:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfiguratsioonifail sisaldab tavaliselt:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Retseptide kohandamine

#### Sihtriistvara muutmine

Sihtriistvara muutmiseks uuenda `systems` sektsiooni:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimeerimisparameetrite kohandamine

Muuda `passes` sektsiooni erinevate optimeerimistasemete jaoks:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Oma retsepti loomine

1. **Alusta sarnase mudeliga**: Leia retsept mudelile, mille arhitektuur on sarnane
2. **Uuenda mudeli konfiguratsiooni**: Muuda konfiguratsioonis mudeli nime/teed
3. **Kohanda parameetreid**: Muuda optimeerimisparameetreid vastavalt vajadusele
4. **Testi ja valideeri**: Käivita optimeerimine ja valideeri tulemused
5. **Panusta tagasi**: Kaalu oma retsepti panustamist hoidlasse

### Retseptide kasutamise eelised

#### 1. **Tõestatud konfiguratsioonid**
- Testitud optimeerimisseaded konkreetsete mudelite jaoks
- Väldib katse-eksituse meetodit optimaalseid parameetreid otsides

#### 2. **Riistvaraspetsiifiline häälestamine**
- Eeloptimeeritud erinevate täitmise pakkujate jaoks
- Valmis konfiguratsioonid CPU, GPU ja NPU sihtmärkide jaoks

#### 3. **Ulatuslik katvus**
- Toetab populaarsemaid avatud lähtekoodiga mudeleid
- Regulaarne uuendamine uute mudeliväljalasetega

#### 4. **Kogukonna panused**
- Koostööl põhinev arendus AI kogukonnaga
- Jagatud teadmised ja parimad praktikad

### Panustamine Olive retseptidesse

Kui olete optimeerinud mudeli, mida hoidlas ei ole:

1. **Klooni hoidla**: Loo oma Olive retseptide hoidla kloon
2. **Loo retsepti kataloog**: Lisa uus kataloog oma mudeli jaoks
3. **Lisa konfiguratsioon**: Lisa olive_config.json ja toetavad failid
4. **Dokumenteeri kasutamine**: Paku selge README juhistega
5. **Esita Pull Request**: Panusta tagasi kogukonda

### Jõudluse võrdlused

Paljud retseptid sisaldavad jõudluse võrdlusi, mis näitavad:
- **Latentsuse paranemist**: Tüüpiline 2-6x kiirendus võrreldes algse mudeliga
- **Mälu vähendamist**: 50-75% väiksem mälukasutus kvantiseerimisega
- **Täpsuse säilitamist**: 95-99% täpsuse säilitamine

### Integratsioon AI tööriistakomplektiga

Retseptid töötavad sujuvalt koos:
- **VS Code AI Toolkit**: Otsene integratsioon mudelite optimeerimiseks
- **Azure Machine Learning**: Pilvepõhised optimeerimisvood
- **ONNX Runtime**: Optimeeritud järelduste kasutuselevõtt

## Täiendavad ressursid

### Ametlikud lingid
- **GitHub hoidla**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive retseptide hoidla**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime dokumentatsioon**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face näide**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Kogukonna näited
- **Jupyter Notebookid**: Saadaval Olive GitHub hoidlas — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code laiendus**: AI Toolkit for VS Code ülevaade — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogipostitused**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Seotud tööriistad
- **ONNX Runtime**: Kõrge jõudlusega järeldusmootor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Paljude ühilduvate mudelite allikas — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Pilvepõhised optimeerimisvood — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Mis edasi

- [04: OpenVINO Toolkit Optimeerimiskomplekt](./04.openvino.md)

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta selle tõlke kasutamisest tulenevate arusaamatuste või valesti tõlgenduste eest.