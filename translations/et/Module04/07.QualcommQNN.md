<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T16:03:55+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "et"
}
-->
# Sektsioon 7: Qualcomm QNN (Qualcomm Neural Network) Optimeerimiskomplekt

## Sisukord
1. [Sissejuhatus](../../../Module04)
2. [Mis on Qualcomm QNN?](../../../Module04)
3. [Paigaldamine](../../../Module04)
4. [Kiire alustamise juhend](../../../Module04)
5. [Näide: Mudelite konverteerimine ja optimeerimine QNN-iga](../../../Module04)
6. [Täiustatud kasutamine](../../../Module04)
7. [Parimad praktikad](../../../Module04)
8. [Tõrkeotsing](../../../Module04)
9. [Täiendavad ressursid](../../../Module04)

## Sissejuhatus

Qualcomm QNN (Qualcomm Neural Network) on terviklik AI järeldusraamistik, mis on loodud Qualcommi AI riistvarakiirendite, sealhulgas Hexagon NPU, Adreno GPU ja Kryo CPU, täieliku potentsiaali avamiseks. Olgu sihtmärgiks mobiilseadmed, serva arvutiplatvormid või autotööstuse süsteemid, QNN pakub optimeeritud järeldusvõimalusi, mis kasutavad Qualcommi spetsialiseeritud AI töötlemisüksusi maksimaalse jõudluse ja energiatõhususe saavutamiseks.

## Mis on Qualcomm QNN?

Qualcomm QNN on ühtne AI järeldusraamistik, mis võimaldab arendajatel AI mudeleid tõhusalt juurutada Qualcommi heterogeense arvutusarhitektuuri kaudu. See pakub ühtset programmeerimisliidest Hexagon NPU (Neural Processing Unit), Adreno GPU ja Kryo CPU kasutamiseks, valides automaatselt erinevate mudelikihide ja operatsioonide jaoks optimaalse töötlemisüksuse.

### Põhifunktsioonid

- **Heterogeenne arvutamine**: Ühtne juurdepääs NPU-le, GPU-le ja CPU-le automaatse töökoormuse jaotusega
- **Riistvarateadlik optimeerimine**: Spetsiaalsed optimeerimised Qualcomm Snapdragon platvormidele
- **Kvantiseerimise tugi**: Täiustatud INT8, INT16 ja segatäpsusega kvantiseerimistehnikad
- **Mudelikonverteerimise tööriistad**: Otsene tugi TensorFlow, PyTorch, ONNX ja Caffe mudelitele
- **Serva AI optimeerimine**: Spetsiaalselt loodud mobiilsete ja serva juurutamise stsenaariumide jaoks, keskendudes energiatõhususele

### Eelised

- **Maksimaalne jõudlus**: Kasutage spetsialiseeritud AI riistvara kuni 15-kordse jõudluse parandamiseks
- **Energiasäästlikkus**: Optimeeritud mobiilsete ja akutoitega seadmete jaoks intelligentse energiakasutuse juhtimisega
- **Madal latentsus**: Riistvarakiirendusega järeldus minimaalsete kuludega reaalajas rakenduste jaoks
- **Mastaapsus**: Nutitelefonidest autotööstuse platvormideni Qualcommi ökosüsteemis
- **Valmis tootmiseks**: Tõestatud raamistik, mida kasutatakse miljonites juurutatud seadmetes

## Paigaldamine

### Eeltingimused

- Qualcomm QNN SDK (nõuab registreerimist Qualcommis)
- Python 3.7 või uuem
- Ühilduv Qualcommi riistvara või simulaator
- Android NDK (mobiilseks juurutamiseks)
- Linuxi või Windowsi arenduskeskkond

### QNN SDK seadistamine

1. **Registreeri ja laadi alla**: Külastage Qualcomm Developer Networki, et registreeruda ja alla laadida QNN SDK
2. **Paki SDK lahti**: Paki QNN SDK lahti oma arenduskataloogi
3. **Määra keskkonnamuutujad**: Konfigureeri QNN tööriistade ja teekide teed

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Pythoni keskkonna seadistamine

Loo ja aktiveeri virtuaalne keskkond:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Paigalda vajalikud Pythoni paketid:

```bash
pip install numpy tensorflow torch onnx
```

### Paigalduse kontrollimine

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Kui edukas, peaksid nägema abiinfot iga QNN tööriista kohta.

## Kiire alustamise juhend

### Esimese mudeli konverteerimine

Konverteerime lihtsa PyTorch mudeli Qualcommi riistvaral töötamiseks:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### ONNX-i konverteerimine QNN formaati

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### QNN mudeliteegi genereerimine

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Mida see protsess teeb

Optimeerimise töövoog hõlmab: algse mudeli konverteerimist ONNX formaati, ONNX-i tõlkimist QNN vahepealsesse esitusviisi, riistvaraspetsiifiliste optimeerimiste rakendamist ja kompileeritud mudeliteegi genereerimist juurutamiseks.

### Oluliste parameetrite selgitus

- `--input_network`: Algne ONNX mudelifail
- `--output_path`: Genereeritud C++ lähtefail
- `--input_dim`: Sisendtensori mõõtmed optimeerimiseks
- `--quantization_overrides`: Kohandatud kvantiseerimise konfiguratsioon
- `-t x86_64-linux-clang`: Sihtarhitektuur ja kompilaator

## Näide: Mudelite konverteerimine ja optimeerimine QNN-iga

### Samm 1: Täiustatud mudeli konverteerimine kvantiseerimisega

Siin on, kuidas rakendada kohandatud kvantiseerimist konverteerimise ajal:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konverteerimine kohandatud kvantiseerimisega:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Samm 2: Mitme taustaprogrammi optimeerimine

Konfigureeri heterogeenseks täitmiseks NPU, GPU ja CPU vahel:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Samm 3: Konteksti binaarfaili loomine juurutamiseks

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Samm 4: Järeldus QNN Runtime'iga

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Väljundi struktuur

Pärast optimeerimist sisaldab sinu juurutamise kataloog:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Täiustatud kasutamine

### Kohandatud taustaprogrammi konfiguratsioon

Konfigureeri spetsiifilised taustaprogrammi optimeerimised:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dünaamiline kvantiseerimine

Rakenda kvantiseerimist jooksvalt parema täpsuse saavutamiseks:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Jõudluse profiilimine

Jälgi jõudlust erinevate taustaprogrammide vahel:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automaatne taustaprogrammi valik

Rakenda intelligentset taustaprogrammi valikut mudeli omaduste põhjal:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Parimad praktikad

### 1. Mudeli arhitektuuri optimeerimine
- **Kihide ühendamine**: Kombineeri operatsioonid nagu Conv+BatchNorm+ReLU parema NPU kasutamise jaoks
- **Sügavuselt eraldatud konvolutsioonid**: Eelista neid tavaliste konvolutsioonide asemel mobiilseks juurutamiseks
- **Kvantiseerimise-sõbralikud disainid**: Kasuta ReLU aktivatsioone ja väldi operatsioone, mis ei kvantiseeru hästi

### 2. Kvantiseerimise strateegia
- **Järeltreeningu kvantiseerimine**: Alusta sellest kiireks juurutamiseks
- **Kalibreerimise andmestik**: Kasuta esinduslikku andmestikku, mis katab kõik sisendvariatsioonid
- **Segatäpsus**: Kasuta INT8 enamikus kihtides, hoia kriitilised kihid kõrgemas täpsuses

### 3. Taustaprogrammi valiku juhised
- **NPU (HTP)**: Parim CNN töökoormuste, kvantiseeritud mudelite ja energiatundlike rakenduste jaoks
- **GPU**: Optimaalne arvutusmahukate operatsioonide, suuremate mudelite ja FP16 täpsuse jaoks
- **CPU**: Tagavaravariant toetamatute operatsioonide ja silumise jaoks

### 4. Jõudluse optimeerimine
- **Partii suurus**: Kasuta partii suurust 1 reaalajas rakenduste jaoks, suuremaid partiisid läbilaskevõime jaoks
- **Sisendi eeltöötlus**: Minimeeri andmete kopeerimise ja konverteerimise kulud
- **Konteksti taaskasutus**: Eelkompileeri kontekstid, et vältida jooksva kompileerimise kulusid

### 5. Mälu haldamine
- **Tensori jaotus**: Kasuta staatilist jaotust, kui võimalik, et vältida jooksvaid kulusid
- **Mälu basseinid**: Rakenda kohandatud mälu basseinid sageli jaotatud tensorite jaoks
- **Puhvrite taaskasutus**: Taaskasuta sisend/väljund puhvrit järelduskutsete vahel

### 6. Energiatõhusus
- **Jõudlusrežiimid**: Kasuta sobivaid jõudlusrežiime vastavalt termilistele piirangutele
- **Dünaamiline sageduse reguleerimine**: Luba süsteemil reguleerida sagedust töökoormuse põhjal
- **Ooterežiimi haldamine**: Vabasta ressursid korralikult, kui neid ei kasutata

## Tõrkeotsing

### Levinud probleemid

#### 1. SDK paigaldamise probleemid
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Mudeli konverteerimise vead
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Kvantiseerimise probleemid
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Jõudluse probleemid
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Mälu probleemid
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Taustaprogrammi ühilduvus
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Jõudluse silumine

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Abi saamine

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN dokumentatsioon**: Saadaval SDK paketis
- **Kogukonna foorumid**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Tehniline tugi**: Qualcommi arendajaportaali kaudu

## Täiendavad ressursid

### Ametlikud lingid
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon platvormid**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Arendajaportaali**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI mootor**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Õpperessursid
- **Alustamise juhend**: Saadaval QNN SDK dokumentatsioonis
- **Mudelite kogu**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimeerimise juhend**: SDK dokumentatsioon sisaldab põhjalikke optimeerimisjuhiseid
- **Videotutvustused**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integreerimistööriistad
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Eeloptimeeritud mudelid Qualcommi riistvarale
- **Android Neural Networks API**: Integreerimine Android NNAPI-ga
- **TensorFlow Lite Delegate**: Qualcommi delegaat TFLite jaoks

### Jõudluse võrdlused
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Kogukonna näited
- **Näidisrakendused**: Saadaval QNN SDK näidiste kataloogis
- **GitHubi repositooriumid**: Kogukonna loodud näited ja tööriistad
- **Tehnilised blogid**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Seotud tööriistad
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Täiustatud kvantiseerimise ja tihendamise tehnikad
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Võrdluseks ja varuvariandiks juurutamisel
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Platvormidevaheline järeldusmootor

### Riistvara spetsifikatsioonid
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon platvormid**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Mis edasi

Jätka oma serva AI teekonda, uurides [Moodul 5: SLMOps ja tootmise juurutamine](../Module05/README.md), et õppida väikeste keelemudelite elutsükli haldamise operatiivseid aspekte.

---

**Lahtiütlus**:  
See dokument on tõlgitud AI tõlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi püüame tagada täpsust, palume arvestada, et automaatsed tõlked võivad sisaldada vigu või ebatäpsusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtõlget. Me ei vastuta arusaamatuste või valesti tõlgenduste eest, mis tulenevad selle tõlke kasutamisest.