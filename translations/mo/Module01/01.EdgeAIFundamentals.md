<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:34:41+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "mo"
}
-->
# 第一章：EdgeAI 基礎知識

EdgeAI 代表人工智慧部署的一個新範式，將 AI 能力直接帶到邊緣設備，而不僅僅依賴於基於雲端的處理。了解 EdgeAI 如何在資源有限的設備上進行本地 AI 處理，同時保持合理的性能並解決隱私、延遲和離線功能等挑戰是非常重要的。

## 簡介

在本課程中，我們將探討 EdgeAI 及其基本概念。我們將涵蓋傳統 AI 計算範式、邊緣計算的挑戰、支持 EdgeAI 的關鍵技術，以及在各行業中的實際應用。

## 學習目標

完成本課程後，您將能夠：

- 理解傳統基於雲端的 AI 方法與 EdgeAI 方法的差異。
- 識別支持邊緣設備上 AI 處理的關鍵技術。
- 認識 EdgeAI 實施的優勢和限制。
- 將 EdgeAI 的知識應用於真實場景和使用案例。

## 理解傳統 AI 計算範式

傳統上，生成式 AI 應用依賴高性能計算基礎設施來有效運行大型語言模型（LLMs）。組織通常將這些模型部署在雲端環境中的 GPU 集群上，通過 API 介面訪問其功能。

這種集中式模型適用於許多應用，但在邊緣計算場景中存在固有的限制。傳統方法涉及將用戶查詢發送到遠程伺服器，使用強大的硬件進行處理，並通過互聯網返回結果。雖然此方法提供了最先進的模型，但它在需要傳輸敏感數據到外部伺服器時，會產生對互聯網連接的依賴性、引入延遲問題，並引發隱私方面的考量。

在使用傳統 AI 計算範式時，我們需要理解一些核心概念：

- **☁️ 基於雲端的處理**：AI 模型運行在具有高計算資源的強大伺服器基礎設施上。
- **🔌 基於 API 的訪問**：應用通過遠程 API 調用訪問 AI 功能，而非本地處理。
- **🎛️ 集中式模型管理**：模型集中維護和更新，確保一致性，但需要網絡連接。
- **📈 資源可擴展性**：雲端基礎設施可以動態擴展以應對不同的計算需求。

## 邊緣計算的挑戰

邊緣設備如筆記型電腦、手機以及物聯網（IoT）設備如 Raspberry Pi 和 NVIDIA Orin Nano 具有獨特的計算限制。與數據中心基礎設施相比，這些設備通常具有有限的處理能力、內存和能源資源。

由於硬件限制，在這些設備上運行傳統的 LLMs 一直是個挑戰。然而，在某些場景中，邊緣 AI 處理的需求變得越來越重要。考慮以下情況：例如在網絡連接不可靠或不可用的地方，如偏遠的工業場地、行駛中的車輛或網絡覆蓋不佳的地區。此外，對於需要高安全標準的應用，如醫療設備、金融系統或政府應用，可能需要在本地處理敏感數據以保持隱私和合規性。

### 邊緣計算的主要限制

邊緣計算環境面臨一些傳統基於雲端的 AI 解決方案所不會遇到的基本限制：

- **有限的處理能力**：邊緣設備通常具有較少的 CPU 核心和較低的時鐘速度，與伺服器級硬件相比。
- **內存限制**：邊緣設備上的可用 RAM 和存儲容量顯著減少。
- **能源限制**：電池供電的設備必須在性能和能源消耗之間取得平衡，以延長運行時間。
- **熱管理**：緊湊的外形限制了冷卻能力，影響了負載下的持續性能。

## 什麼是 EdgeAI？

### 概念：EdgeAI 的定義

EdgeAI 是指直接在邊緣設備上部署和執行人工智慧算法——這些設備是位於網絡“邊緣”，靠近數據生成和收集的物理硬件。這些設備包括智能手機、IoT 感測器、智能攝像頭、自動駕駛車輛、可穿戴設備和工業設備。與依賴雲伺服器進行處理的傳統 AI 系統不同，EdgeAI 將智能直接帶到數據源。

從本質上講，EdgeAI 是關於去中心化 AI 處理，將其從集中式數據中心移到我們數字生態系統中的大量設備上。這代表了 AI 系統設計和部署方式的根本性架構轉變。

EdgeAI 的關鍵概念支柱包括：

- **近距離處理**：計算在數據來源附近物理發生。
- **分散式智能**：決策能力分布在多個設備之間。
- **數據主權**：信息保持在本地控制之下，通常不會離開設備。
- **自主運行**：設備可以在不需要持續連接的情況下智能運行。
- **嵌入式 AI**：智能成為日常設備的內在能力。

### EdgeAI 架構可視化

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI 代表人工智慧部署的一個新範式，將 AI 能力直接帶到邊緣設備，而不僅僅依賴於基於雲端的處理。這種方法使 AI 模型能夠在計算資源有限的設備上本地運行，提供即時推理能力，而無需持續的互聯網連接。

EdgeAI 包括各種技術和方法，旨在使 AI 模型更高效，並適合在資源有限的設備上部署。目標是在顯著降低 AI 模型的計算和內存需求的同時保持合理的性能。

讓我們來看看支持 EdgeAI 在不同設備類型和使用案例中實施的基本方法。

### EdgeAI 的核心原則

EdgeAI 建立在幾個基礎原則之上，這些原則使其與傳統基於雲端的 AI 不同：

- **本地處理**：AI 推理直接在邊緣設備上進行，無需外部連接。
- **資源優化**：模型專門針對目標設備的硬件限制進行優化。
- **即時性能**：處理以最小的延遲進行，適用於時間敏感的應用。
- **隱私設計**：敏感數據保留在設備上，增強安全性和合規性。

## 支持 EdgeAI 的關鍵技術

### 模型量化

模型量化是 EdgeAI 中最重要的技術之一。此過程涉及降低模型參數的精度，通常從 32 位浮點數減少到 8 位整數甚至更低的精度格式。雖然精度的降低可能令人擔憂，但研究表明，許多 AI 模型即使在精度顯著降低的情況下仍能保持其性能。

量化通過將浮點值的範圍映射到一組較小的離散值來工作。例如，量化可能僅使用 8 位來表示每個參數，而不是 32 位，從而減少 4 倍的內存需求，並通常導致更快的推理速度。

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

不同的量化技術包括：

- **後訓練量化（PTQ）**：在模型訓練後應用，無需重新訓練。
- **量化感知訓練（QAT）**：在訓練期間納入量化影響以提高準確性。
- **動態量化**：將權重量化為 int8，但動態計算激活。
- **靜態量化**：預先計算所有權重和激活的量化參數。

對於 EdgeAI 部署，選擇適當的量化策略取決於目標設備的特定模型架構、性能需求和硬件能力。

### 模型壓縮與優化

除了量化之外，各種壓縮技術有助於減少模型大小和計算需求，包括：

**剪枝**：此技術移除神經網絡中不必要的連接或神經元。通過識別並消除對模型性能貢獻較少的參數，剪枝可以顯著減少模型大小，同時保持準確性。

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**知識蒸餾**：此方法涉及訓練一個較小的“學生”模型來模仿較大的“教師”模型的行為。學生模型學習近似教師的輸出，通常以顯著較少的參數實現類似的性能。

**模型架構優化**：研究人員開發了專門為邊緣部署設計的架構，例如 MobileNets、EfficientNets 和其他平衡性能與計算效率的輕量級架構。

### 小型語言模型（SLMs）

EdgeAI 的一個新興趨勢是小型語言模型（SLMs）的開發。這些模型從一開始就被設計為緊湊且高效，同時仍提供有意義的自然語言能力。SLMs 通過精心的架構選擇、高效的訓練技術以及專注於特定領域或任務的訓練來實現。

與傳統方法壓縮大型模型不同，SLMs 通常使用較小的數據集和專門設計的架構進行訓練，專門針對邊緣部署進行優化。這種方法可以產生不僅更小且更高效的模型，適合特定的使用案例。

## EdgeAI 的硬件加速

現代邊緣設備越來越多地包括專門設計用於加速 AI 工作負載的硬件：

### 神經處理單元（NPUs）

NPUs 是專門設計用於神經網絡計算的處理器。這些芯片可以比傳統 CPU 更高效地執行 AI 推理任務，通常具有更低的功耗。許多現代智能手機、筆記型電腦和 IoT 設備現在都配備了 NPU，以支持設備上的 AI 處理。

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

配備 NPU 的設備包括：

- **Apple**：搭載 Neural Engine 的 A 系列和 M 系列芯片
- **Qualcomm**：搭載 Hexagon DSP/NPU 的 Snapdragon 處理器
- **Samsung**：搭載 NPU 的 Exynos 處理器
- **Intel**：Movidius VPU 和 Habana Labs 加速器
- **Microsoft**：配備 NPU 的 Windows Copilot+ PC

### 🎮 GPU 加速

雖然邊緣設備可能沒有數據中心中的強大 GPU，但許多仍包括集成或獨立 GPU，可以加速 AI 工作負載。現代移動 GPU 和集成圖形處理器可以為 AI 推理任務提供顯著的性能提升。

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU 優化

即使是僅配備 CPU 的設備也可以通過優化實現 EdgeAI。現代 CPU 包括專門針對 AI 工作負載的指令，並且已開發出軟件框架以最大化 CPU 在 AI 推理中的性能。

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

對於從事 EdgeAI 的軟件工程師來說，了解如何利用這些硬件加速選項對於優化目標設備上的推理性能和能源效率至關重要。

## EdgeAI 的優勢

### 隱私和安全

EdgeAI 的一大優勢是增強的隱私和安全性。通過在設備本地處理數據，敏感信息不會離開用戶的控制範圍。這對於處理個人數據、醫療信息或機密商業數據的應用尤為重要。

### 降低延遲

EdgeAI 消除了將數據發送到遠程伺服器進行處理的需求，顯著降低了延遲。這對於需要即時響應的應用至關重要，例如自動駕駛車輛、工業自動化或交互式應用。

### 離線功能

EdgeAI 即使在沒有網絡連接的情況下也能實現 AI 功能。這對於偏遠地區、旅行期間或網絡可靠性成問題的情況下的應用非常有價值。

### 成本效益

通過減少對雲端 AI 服務的依賴，EdgeAI 可以幫助降低運營成本，特別是對於使用量大的應用。組織可以避免持續的 API 成本並減少帶寬需求。

### 可擴展性

EdgeAI 將計算負載分散到邊緣設備上，而不是集中在數據中心。這有助於降低基礎設施成本並提高整體系統的可擴展性。

## EdgeAI 的應用

### 智能設備和物聯網

EdgeAI 為許多智能設備功能提供支持，從可以本地處理命令的語音助手到可以識別物體和人物的智能攝像頭。IoT 設備使用 EdgeAI 進行預測性維護、環境監測和自動化決策。

### 移動應用

智能手機和平板電腦使用 EdgeAI 提供各種功能，包括照片增強、即時翻譯、擴增實境和個性化推薦。這些應用受益於本地處理的低延遲和隱私優勢。

### 工業應用

製造和工業環境使用 EdgeAI 進行質量控制、預測性維護和流程優化。這些應用通常需要即時處理，並可能在連接有限的環境中運行。

### 醫療保健

醫療設備和醫療應用使用 EdgeAI 進行患者監測、診斷輔助和治療建議。本地處理的隱私和安全性優勢在醫療應用中特別重要。

## 挑戰和限制

### 性能取捨

EdgeAI 通常涉及模型大小、計算效率和性能之間的取捨。雖然像量化和剪枝這樣的技術可以顯著減少資源需求，但它們也可能影響模型的準確性或能力。

### 開發複雜性

開發 EdgeAI 應用需要專業知識和工具。開發人員必須了解優化技術、硬件能力和部署限制，這可能會增加開發的複雜性。

### 硬件限制

儘管邊緣硬件有所進步，但這些設備與數據中心基礎設施相比仍有顯著限制。並非所有 AI 應用都能有效地部署在邊緣設備上，有些可能需要採用混合方法。

### 模型更新與維護

更新部署在邊緣設備上的 AI 模型可能具有挑戰性，特別是對於連接或存儲容量有限的設備。組織必須制定模型版本管理、更新和維護的策略。

## EdgeAI 的未來

EdgeAI 領域正在快速發展，硬件、軟件和技術方面的持續進步不斷涌現。未來的趨勢包括更多專門的邊緣 AI 芯片、改進的優化技術以及更好的 EdgeAI 開發和部署工具。

隨著 5G 網絡的普及，我們可能會看到結合邊緣處理和雲端能力的混合方法，從而實現更複雜的 AI 應用，同時保持本地處理的優勢。

EdgeAI 代表了一種更分散、更高效、更注重隱私的 AI 系統的根本轉變。隨著技術的不斷成熟，我們可以期待 EdgeAI 在啟用各種應用和設備的 AI 功能方面變得越來越重要。

通過 EdgeAI 的 AI 民主化開啟了創新的新可能性，使開發者能夠創建可靠運行於多樣化環境中的 AI 驅動應用，同時尊重用戶隱私並提供響應迅速的即時體驗。理解 EdgeAI 對於任何從事 AI 技術的人來說都越來越重要，因為它代表了 AI 部署和日常生活中體驗的未來。

## ➡️ 下一步
- [02: EdgeAI 應用](02.RealWorldCaseStudies.md)

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵資訊，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。