<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:56:07+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "he"
}
-->
# סעיף 7: Qualcomm QNN (סוויטת אופטימיזציה של רשתות עצביות של Qualcomm)

## תוכן עניינים
1. [מבוא](../../../Module04)
2. [מה זה Qualcomm QNN?](../../../Module04)
3. [התקנה](../../../Module04)
4. [מדריך התחלה מהירה](../../../Module04)
5. [דוגמה: המרה ואופטימיזציה של מודלים עם QNN](../../../Module04)
6. [שימוש מתקדם](../../../Module04)
7. [שיטות עבודה מומלצות](../../../Module04)
8. [פתרון בעיות](../../../Module04)
9. [משאבים נוספים](../../../Module04)

## מבוא

Qualcomm QNN (רשת עצבית של Qualcomm) הוא מסגרת מקיפה להסקת מסקנות AI, שנועדה למצות את הפוטנציאל המלא של מאיצי החומרה של Qualcomm, כולל Hexagon NPU, Adreno GPU ו-Kryo CPU. בין אם אתם מכוונים למכשירים ניידים, פלטפורמות מחשוב בקצה או מערכות רכב, QNN מספק יכולות הסקה אופטימליות שמנצלות את יחידות העיבוד המיוחדות של Qualcomm לביצועים מקסימליים ויעילות אנרגטית.

## מה זה Qualcomm QNN?

Qualcomm QNN הוא מסגרת הסקה מאוחדת ל-AI שמאפשרת למפתחים לפרוס מודלים של AI בצורה יעילה בארכיטקטורת המחשוב ההטרוגנית של Qualcomm. הוא מספק ממשק תכנות מאוחד לגישה ל-Hexagon NPU (יחידת עיבוד עצבית), Adreno GPU ו-Kryo CPU, ובוחר באופן אוטומטי את יחידת העיבוד האופטימלית עבור שכבות מודל ופעולות שונות.

### תכונות עיקריות

- **מחשוב הטרוגני**: גישה מאוחדת ל-NPU, GPU ו-CPU עם חלוקת עומס אוטומטית
- **אופטימיזציה מודעת חומרה**: אופטימיזציות מיוחדות לפלטפורמות Snapdragon של Qualcomm
- **תמיכה בכימות**: טכניקות כימות מתקדמות של INT8, INT16 ודיוק מעורב
- **כלי המרת מודלים**: תמיכה ישירה במודלים של TensorFlow, PyTorch, ONNX ו-Caffe
- **אופטימיזציה ל-AI בקצה**: מיועד במיוחד לתרחישי פריסה ניידים ובקצה עם דגש על יעילות אנרגטית

### יתרונות

- **ביצועים מקסימליים**: ניצול חומרת AI מיוחדת לשיפור ביצועים עד פי 15
- **יעילות אנרגטית**: אופטימיזציה למכשירים ניידים ולמכשירים המופעלים על סוללה עם ניהול אנרגיה חכם
- **זמן תגובה נמוך**: הסקה מואצת חומרה עם תקורה מינימלית ליישומים בזמן אמת
- **פריסה בקנה מידה רחב**: ממכשירי סמארטפון ועד פלטפורמות רכב ברחבי האקוסיסטם של Qualcomm
- **מוכן לייצור**: מסגרת מוכחת בשטח, בשימוש במיליוני מכשירים פרוסים

## התקנה

### דרישות מוקדמות

- Qualcomm QNN SDK (דורש הרשמה ל-Qualcomm)
- Python 3.7 ומעלה
- חומרה תואמת של Qualcomm או סימולטור
- Android NDK (לפריסה ניידת)
- סביבת פיתוח Linux או Windows

### הגדרת QNN SDK

1. **הרשמה והורדה**: בקרו ב-Qualcomm Developer Network כדי להירשם ולהוריד את QNN SDK
2. **חילוץ SDK**: חלצו את QNN SDK לתיקיית הפיתוח שלכם
3. **הגדרת משתני סביבה**: הגדירו נתיבים לכלי QNN ולספריות

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### הגדרת סביבת Python

צרו והפעילו סביבה וירטואלית:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

התקינו חבילות Python נדרשות:

```bash
pip install numpy tensorflow torch onnx
```

### אימות התקנה

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

אם ההתקנה הצליחה, תראו מידע עזרה עבור כל כלי QNN.

## מדריך התחלה מהירה

### המרת המודל הראשון שלכם

בואו נמיר מודל PyTorch פשוט להפעלה על חומרת Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### המרת ONNX לפורמט QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### יצירת ספריית מודל QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### מה התהליך הזה עושה

תהליך האופטימיזציה כולל: המרת המודל המקורי לפורמט ONNX, תרגום ONNX לייצוג ביניים של QNN, יישום אופטימיזציות ספציפיות לחומרה, ויצירת ספריית מודל מקומפלת לפריסה.

### הסבר על פרמטרים עיקריים

- `--input_network`: קובץ מודל ONNX מקור
- `--output_path`: קובץ מקור C++ שנוצר
- `--input_dim`: ממדי טנסור הקלט לאופטימיזציה
- `--quantization_overrides`: תצורת כימות מותאמת אישית
- `-t x86_64-linux-clang`: ארכיטקטורת יעד ומהדר

## דוגמה: המרה ואופטימיזציה של מודלים עם QNN

### שלב 1: המרת מודל מתקדמת עם כימות

כך תיישמו כימות מותאם אישית במהלך ההמרה:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

המרה עם כימות מותאם אישית:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### שלב 2: אופטימיזציה רב-Backend

הגדירו ביצוע הטרוגני בין NPU, GPU ו-CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### שלב 3: יצירת קובץ בינארי להקשר לפריסה

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### שלב 4: הסקה עם QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### מבנה הפלט

לאחר האופטימיזציה, תיקיית הפריסה שלכם תכיל:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## שימוש מתקדם

### תצורת Backend מותאמת אישית

הגדירו אופטימיזציות Backend ספציפיות:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### כימות דינמי

יישמו כימות בזמן ריצה לשיפור דיוק:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### פרופיל ביצועים

עקבו אחר ביצועים בין Backends שונים:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### בחירת Backend אוטומטית

יישמו בחירת Backend חכמה בהתבסס על מאפייני המודל:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## שיטות עבודה מומלצות

### 1. אופטימיזציה של ארכיטקטורת מודל
- **מיזוג שכבות**: שלבו פעולות כמו Conv+BatchNorm+ReLU לניצול טוב יותר של NPU
- **קונבולוציות נפרדות לעומק**: העדיפו אותן על פני קונבולוציות רגילות לפריסה ניידת
- **עיצובים ידידותיים לכימות**: השתמשו באקטיבציות ReLU והימנעו מפעולות שלא מתאימות לכימות

### 2. אסטרטגיית כימות
- **כימות לאחר אימון**: התחילו עם זה לפריסה מהירה
- **מערך נתוני כיול**: השתמשו בנתונים מייצגים המכסים את כל השונות בקלט
- **דיוק מעורב**: השתמשו ב-INT8 עבור רוב השכבות, שמרו שכבות קריטיות בדיוק גבוה יותר

### 3. הנחיות לבחירת Backend
- **NPU (HTP)**: הטוב ביותר לעומסי עבודה של CNN, מודלים מכומתים ויישומים רגישים לאנרגיה
- **GPU**: אופטימלי לפעולות עתירות חישוב, מודלים גדולים ודיוק FP16
- **CPU**: חלופה לפעולות לא נתמכות ולדיבוג

### 4. אופטימיזציית ביצועים
- **גודל אצווה**: השתמשו בגודל אצווה 1 ליישומים בזמן אמת, אצוות גדולות יותר לתפוקה
- **עיבוד מקדים של קלט**: צמצמו את תקורת העתקת הנתונים והמרתם
- **שימוש חוזר בהקשר**: קומפלו הקשרים מראש כדי להימנע מתקורת קומפילציה בזמן ריצה

### 5. ניהול זיכרון
- **הקצאת טנסורים**: השתמשו בהקצאה סטטית כשאפשר כדי להימנע מתקורת זמן ריצה
- **מאגרי זיכרון**: יישמו מאגרי זיכרון מותאמים אישית לטנסורים שמוקצים לעיתים קרובות
- **שימוש חוזר במאגרי קלט/פלט**: השתמשו מחדש במאגרי קלט/פלט בין קריאות הסקה

### 6. אופטימיזציית אנרגיה
- **מצבי ביצועים**: השתמשו במצבי ביצועים מתאימים בהתבסס על מגבלות תרמיות
- **שינוי תדר דינמי**: אפשרו למערכת לשנות תדר בהתבסס על עומס עבודה
- **ניהול מצב סרק**: שחררו משאבים כראוי כאשר אינם בשימוש

## פתרון בעיות

### בעיות נפוצות

#### 1. בעיות התקנת SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. שגיאות המרת מודל
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. בעיות כימות
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. בעיות ביצועים
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. בעיות זיכרון
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. תאימות Backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### דיבוג ביצועים

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### קבלת עזרה

- **רשת המפתחים של Qualcomm**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **תיעוד QNN**: זמין בחבילת SDK
- **פורומים קהילתיים**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **תמיכה טכנית**: דרך פורטל המפתחים של Qualcomm

## משאבים נוספים

### קישורים רשמיים
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **פלטפורמות Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **פורטל מפתחים**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **מנוע AI**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### משאבי למידה
- **מדריך התחלה**: זמין בתיעוד QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **מדריך אופטימיזציה**: תיעוד SDK כולל הנחיות אופטימיזציה מקיפות
- **מדריכי וידאו**: [ערוץ YouTube של Qualcomm Developer](https://www.youtube.com/c/QualcommDeveloperNetwork)

### כלי אינטגרציה
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: מודלים מותאמים מראש לחומרה של Qualcomm
- **Android Neural Networks API**: אינטגרציה עם NNAPI של Android
- **TensorFlow Lite Delegate**: נציג Qualcomm עבור TFLite

### מדדי ביצועים
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **מחקר AI של Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### דוגמאות קהילתיות
- **יישומים לדוגמה**: זמינים בתיקיית הדוגמאות של QNN SDK
- **מאגרי GitHub**: דוגמאות וכלים שתרמו חברי הקהילה
- **בלוגים טכניים**: [בלוג המפתחים של Qualcomm](https://developer.qualcomm.com/blog)

### כלים קשורים
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - טכניקות כימות ודחיסה מתקדמות
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - להשוואה ולפריסה חלופית
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - מנוע הסקה חוצה פלטפורמות

### מפרטי חומרה
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **פלטפורמות Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ מה הלאה

המשיכו במסע ה-AI בקצה שלכם על ידי חקר [מודול 5: SLMOps ופריסה בייצור](../Module05/README.md) כדי ללמוד על היבטים תפעוליים של ניהול מחזור חיים של מודלים קטנים.

---

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירות תרגום AI [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי אנושי. איננו אחראים לאי הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.