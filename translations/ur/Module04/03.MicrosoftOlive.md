<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T11:06:18+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ur"
}
-->
# سیکشن 3: مائیکروسافٹ اولیو آپٹیمائزیشن سوٹ

## فہرست مواد
1. [تعارف](../../../Module04)
2. [مائیکروسافٹ اولیو کیا ہے؟](../../../Module04)
3. [انسٹالیشن](../../../Module04)
4. [جلدی شروع کرنے کی گائیڈ](../../../Module04)
5. [مثال: Qwen3 کو ONNX INT4 میں تبدیل کرنا](../../../Module04)
6. [جدید استعمال](../../../Module04)
7. [اولیو ریسیپیز ریپوزٹری](../../../Module04)
8. [بہترین طریقے](../../../Module04)
9. [مسائل کا حل](../../../Module04)
10. [اضافی وسائل](../../../Module04)

## تعارف

مائیکروسافٹ اولیو ایک طاقتور، آسان استعمال کرنے والا ہارڈویئر-آگاہ ماڈل آپٹیمائزیشن ٹول کٹ ہے جو مشین لرننگ ماڈلز کو مختلف ہارڈویئر پلیٹ فارمز پر ڈپلائمنٹ کے لیے آپٹیمائز کرنے کے عمل کو آسان بناتا ہے۔ چاہے آپ CPUs، GPUs، یا خاص AI ایکسیلیریٹرز کو ہدف بنا رہے ہوں، اولیو آپ کو بہترین کارکردگی حاصل کرنے میں مدد دیتا ہے جبکہ ماڈل کی درستگی کو برقرار رکھتا ہے۔

## مائیکروسافٹ اولیو کیا ہے؟

اولیو ایک آسان استعمال کرنے والا ہارڈویئر-آگاہ ماڈل آپٹیمائزیشن ٹول ہے جو ماڈل کمپریشن، آپٹیمائزیشن، اور کمپائلیشن کے شعبے میں صنعت کے بہترین تکنیکوں کو یکجا کرتا ہے۔ یہ ONNX Runtime کے ساتھ ایک E2E انفرنس آپٹیمائزیشن حل کے طور پر کام کرتا ہے۔

### اہم خصوصیات

- **ہارڈویئر-آگاہ آپٹیمائزیشن**: آپ کے ہدف ہارڈویئر کے لیے بہترین آپٹیمائزیشن تکنیکوں کا خودکار انتخاب
- **40+ بلٹ ان آپٹیمائزیشن کمپوننٹس**: ماڈل کمپریشن، کوانٹائزیشن، گراف آپٹیمائزیشن، اور مزید شامل ہیں
- **آسان CLI انٹرفیس**: عام آپٹیمائزیشن کاموں کے لیے سادہ کمانڈز
- **ملٹی-فریم ورک سپورٹ**: PyTorch، Hugging Face ماڈلز، اور ONNX کے ساتھ کام کرتا ہے
- **مشہور ماڈل سپورٹ**: اولیو خودکار طور پر مشہور ماڈل آرکیٹیکچرز جیسے Llama، Phi، Qwen، Gemma وغیرہ کو آؤٹ آف دی باکس آپٹیمائز کر سکتا ہے

### فوائد

- **ترقی کا وقت کم کریں**: مختلف آپٹیمائزیشن تکنیکوں کے ساتھ دستی تجربہ کرنے کی ضرورت نہیں
- **کارکردگی میں اضافہ**: نمایاں رفتار میں بہتری (کچھ معاملات میں 6x تک)
- **کراس-پلیٹ فارم ڈپلائمنٹ**: آپٹیمائزڈ ماڈلز مختلف ہارڈویئر اور آپریٹنگ سسٹمز پر کام کرتے ہیں
- **درستگی برقرار رکھنا**: آپٹیمائزیشنز ماڈل کی کوالٹی کو برقرار رکھتے ہوئے کارکردگی کو بہتر بناتے ہیں

## انسٹالیشن

### ضروریات

- Python 3.8 یا اس سے زیادہ
- pip پیکیج مینیجر
- ورچوئل ماحول (تجویز کردہ)

### بنیادی انسٹالیشن

ورچوئل ماحول بنائیں اور فعال کریں:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

اولیو کو آٹو آپٹیمائزیشن خصوصیات کے ساتھ انسٹال کریں:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### اختیاری ڈیپینڈنسز

اولیو اضافی خصوصیات کے لیے مختلف اختیاری ڈیپینڈنسز پیش کرتا ہے:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### انسٹالیشن کی تصدیق کریں

```bash
olive --help
```

اگر کامیاب ہو، تو آپ کو اولیو CLI مدد کا پیغام نظر آنا چاہیے۔

## جلدی شروع کرنے کی گائیڈ

### آپ کی پہلی آپٹیمائزیشن

آئیے اولیو کی آٹو آپٹیمائزیشن خصوصیت کا استعمال کرتے ہوئے ایک چھوٹے لینگویج ماڈل کو آپٹیمائز کریں:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### یہ کمانڈ کیا کرتی ہے

آپٹیمائزیشن کے عمل میں شامل ہیں: ماڈل کو لوکل کیش سے حاصل کرنا، ONNX گراف کو کیپچر کرنا اور ویٹس کو ONNX ڈیٹا فائل میں اسٹور کرنا، ONNX گراف کو آپٹیمائز کرنا، اور RTN طریقہ استعمال کرتے ہوئے ماڈل کو int4 میں کوانٹائز کرنا۔

### کمانڈ پیرامیٹرز کی وضاحت

- `--model_name_or_path`: Hugging Face ماڈل کی شناخت یا لوکل راستہ
- `--output_path`: ڈائریکٹری جہاں آپٹیمائزڈ ماڈل محفوظ کیا جائے گا
- `--device`: ہدف ڈیوائس (cpu, gpu)
- `--provider`: ایکسی کیوشن پرووائیڈر (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: انفرنس کے لیے ONNX Runtime Generate AI استعمال کریں
- `--precision`: کوانٹائزیشن پریسیژن (int4, int8, fp16)
- `--log_level`: لاگنگ کی تفصیل (0=minimal, 1=verbose)

## مثال: Qwen3 کو ONNX INT4 میں تبدیل کرنا

Hugging Face کی فراہم کردہ مثال [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) کی بنیاد پر، یہاں Qwen3 ماڈل کو آپٹیمائز کرنے کا طریقہ ہے:

### مرحلہ 1: ماڈل ڈاؤنلوڈ کریں (اختیاری)

ڈاؤنلوڈ کے وقت کو کم کرنے کے لیے صرف ضروری فائلیں کیش کریں:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### مرحلہ 2: Qwen3 ماڈل کو آپٹیمائز کریں

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### مرحلہ 3: آپٹیمائزڈ ماڈل کی جانچ کریں

آپٹیمائزڈ ماڈل کی جانچ کے لیے ایک سادہ Python اسکرپٹ بنائیں:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### آؤٹ پٹ اسٹرکچر

آپٹیمائزیشن کے بعد، آپ کی آؤٹ پٹ ڈائریکٹری میں شامل ہوں گے:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## جدید استعمال

### کنفیگریشن فائلز

زیادہ پیچیدہ آپٹیمائزیشن ورک فلو کے لیے، آپ JSON کنفیگریشن فائلز استعمال کر سکتے ہیں:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

کنفیگریشن کے ساتھ چلائیں:

```bash
olive run --config config.json
```

### GPU آپٹیمائزیشن

CUDA GPU آپٹیمائزیشن کے لیے:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) کے لیے:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### اولیو کے ساتھ فائن ٹیوننگ

اولیو ماڈلز کو فائن ٹیوننگ کی بھی حمایت کرتا ہے:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## بہترین طریقے

### 1. ماڈل کا انتخاب
- ٹیسٹنگ کے لیے چھوٹے ماڈلز سے شروع کریں (مثلاً، 0.5B-7B پیرامیٹرز)
- یقینی بنائیں کہ آپ کا ہدف ماڈل آرکیٹیکچر اولیو کے ذریعے سپورٹ کیا گیا ہے

### 2. ہارڈویئر کے خیالات
- آپٹیمائزیشن ہدف کو آپ کے ڈپلائمنٹ ہارڈویئر سے ملائیں
- اگر آپ کے پاس CUDA-کمپیٹیبل ہارڈویئر ہے تو GPU آپٹیمائزیشن استعمال کریں
- Windows مشینوں کے لیے DirectML پر غور کریں جن میں انٹیگریٹڈ گرافکس ہیں

### 3. پریسیژن کا انتخاب
- **INT4**: زیادہ سے زیادہ کمپریشن، معمولی درستگی کا نقصان
- **INT8**: سائز اور درستگی کا اچھا توازن
- **FP16**: کم سے کم درستگی کا نقصان، درمیانی سائز میں کمی

### 4. ٹیسٹنگ اور ویلیڈیشن
- ہمیشہ آپٹیمائزڈ ماڈلز کو اپنے مخصوص استعمال کے کیسز کے ساتھ ٹیسٹ کریں
- کارکردگی کے میٹرکس کا موازنہ کریں (لیٹنسی، تھروپٹ، درستگی)
- ایویلیویشن کے لیے نمائندہ ان پٹ ڈیٹا استعمال کریں

### 5. تکراری آپٹیمائزیشن
- جلدی نتائج کے لیے آٹو آپٹیمائزیشن سے شروع کریں
- تفصیلی کنٹرول کے لیے کنفیگریشن فائلز استعمال کریں
- مختلف آپٹیمائزیشن پاسز کے ساتھ تجربہ کریں

## مسائل کا حل

### عام مسائل

#### 1. انسٹالیشن کے مسائل
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU کے مسائل
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. میموری کے مسائل
- آپٹیمائزیشن کے دوران چھوٹے بیچ سائز استعمال کریں
- پہلے زیادہ پریسیژن کے ساتھ کوانٹائزیشن آزمائیں (int8 بجائے int4)
- ماڈل کیشنگ کے لیے کافی ڈسک اسپیس یقینی بنائیں

#### 4. ماڈل لوڈنگ کی غلطیاں
- ماڈل کے راستے اور رسائی کی اجازتوں کی تصدیق کریں
- چیک کریں کہ آیا ماڈل کو `trust_remote_code=True` کی ضرورت ہے
- یقینی بنائیں کہ تمام مطلوبہ ماڈل فائلز ڈاؤنلوڈ ہو چکی ہیں

### مدد حاصل کرنا

- **دستاویزات**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub مسائل**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **مثالیں**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## اولیو ریسیپیز ریپوزٹری

### اولیو ریسیپیز کا تعارف

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) ریپوزٹری اولیو ٹول کٹ کے ساتھ ایک جامع مجموعہ فراہم کرتی ہے جو مشہور AI ماڈلز کے لیے تیار استعمال آپٹیمائزیشن ریسیپیز پر مشتمل ہے۔ یہ ریپوزٹری عوامی دستیاب ماڈلز کو آپٹیمائز کرنے اور ملکیتی ماڈلز کے لیے آپٹیمائزیشن ورک فلو بنانے کے لیے ایک عملی حوالہ کے طور پر کام کرتی ہے۔

### اہم خصوصیات

- **100+ پری بلٹ ریسیپیز**: مشہور ماڈلز کے لیے تیار استعمال آپٹیمائزیشن کنفیگریشنز
- **ملٹی-آرکیٹیکچر سپورٹ**: ٹرانسفارمر ماڈلز، وژن ماڈلز، اور ملٹی موڈل آرکیٹیکچرز کا احاطہ کرتا ہے
- **ہارڈویئر-خصوصی آپٹیمائزیشنز**: CPU، GPU، اور خاص ایکسیلیریٹرز کے لیے تیار ریسیپیز
- **مشہور ماڈل فیملیز**: Phi، Llama، Qwen، Gemma، Mistral، اور بہت کچھ شامل ہیں

### سپورٹڈ ماڈل فیملیز

ریپوزٹری میں آپٹیمائزیشن ریسیپیز شامل ہیں:

#### لینگویج ماڈلز
- **مائیکروسافٹ Phi**: Phi-3-mini، Phi-3.5-mini، Phi-4-mini، Phi-4-reasoning
- **Meta Llama**: Llama-2-7b، Llama-3.1-8B، Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B، Qwen2-7B، Qwen2.5 سیریز (0.5B سے 14B)
- **Google Gemma**: مختلف Gemma ماڈل کنفیگریشنز
- **Mistral AI**: Mistral-7B سیریز
- **DeepSeek**: R1-Distill سیریز ماڈلز

#### وژن اور ملٹی موڈل ماڈلز
- **Stable Diffusion**: v1.4، XL-base-1.0
- **CLIP ماڈلز**: مختلف CLIP-ViT کنفیگریشنز
- **ResNet**: ResNet-50 آپٹیمائزیشنز
- **ویژن ٹرانسفارمرز**: ViT-base-patch16-224

#### خاص ماڈلز
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: بیس اور ملٹی لنگوئل ویریئنٹس
- **سینٹنس ٹرانسفارمرز**: all-MiniLM-L6-v2

### اولیو ریسیپیز کا استعمال

#### طریقہ 1: مخصوص ریسیپی کلون کریں

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### طریقہ 2: ریسیپی کو ٹیمپلیٹ کے طور پر استعمال کریں

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### ریسیپی اسٹرکچر

ہر ریسیپی ڈائریکٹری عام طور پر شامل ہوتی ہے:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### مثال: Phi-4-mini ریسیپی کا استعمال

آئیے Phi-4-mini ریسیپی کو ایک مثال کے طور پر استعمال کریں:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

کنفیگریشن فائل عام طور پر شامل ہوتی ہے:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### ریسیپیز کو حسب ضرورت بنانا

#### ہدف ہارڈویئر کو تبدیل کرنا

ہدف ہارڈویئر کو تبدیل کرنے کے لیے، `systems` سیکشن کو اپ ڈیٹ کریں:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### آپٹیمائزیشن پیرامیٹرز کو ایڈجسٹ کرنا

مختلف آپٹیمائزیشن لیولز کے لیے `passes` سیکشن کو تبدیل کریں:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### اپنی ریسیپی بنانا

1. **مشابہ ماڈل سے شروع کریں**: ایسے ماڈل کے لیے ریسیپی تلاش کریں جس کی آرکیٹیکچر مشابہ ہو
2. **ماڈل کنفیگریشن کو اپ ڈیٹ کریں**: کنفیگریشن میں ماڈل کا نام/راستہ تبدیل کریں
3. **پیرامیٹرز کو ایڈجسٹ کریں**: ضرورت کے مطابق آپٹیمائزیشن پیرامیٹرز کو تبدیل کریں
4. **ٹیسٹ اور ویلیڈیٹ کریں**: آپٹیمائزیشن چلائیں اور نتائج کی تصدیق کریں
5. **واپس تعاون کریں**: اپنی ریسیپی کو ریپوزٹری میں تعاون کرنے پر غور کریں

### ریسیپیز کے استعمال کے فوائد

#### 1. **ثابت شدہ کنفیگریشنز**
- مخصوص ماڈلز کے لیے آزمائے گئے آپٹیمائزیشن سیٹنگز
- بہترین پیرامیٹرز تلاش کرنے میں آزمائش اور غلطی سے بچتا ہے

#### 2. **ہارڈویئر-خصوصی ٹیوننگ**
- مختلف ایکسی کیوشن پرووائیڈرز کے لیے پہلے سے آپٹیمائزڈ
- CPU، GPU، اور NPU اہداف کے لیے تیار استعمال کنفیگریشنز

#### 3. **جامع کوریج**
- سب سے مشہور اوپن سورس ماڈلز کو سپورٹ کرتا ہے
- نئے ماڈل ریلیزز کے ساتھ باقاعدہ اپ ڈیٹس

#### 4. **کمیونٹی تعاون**
- AI کمیونٹی کے ساتھ تعاون پر مبنی ترقی
- مشترکہ علم اور بہترین طریقے

### اولیو ریسیپیز میں تعاون کرنا

اگر آپ نے کسی ماڈل کو آپٹیمائز کیا ہے جو ریپوزٹری میں شامل نہیں ہے:

1. **ریپوزٹری کو فورک کریں**: اپنی ریپوزٹری کا فورک بنائیں
2. **ریسیپی ڈائریکٹری بنائیں**: اپنے ماڈل کے لیے نئی ڈائریکٹری شامل کریں
3. **کنفیگریشن شامل کریں**: olive_config.json اور معاون فائلز شامل کریں
4. **استعمال کی دستاویز کریں**: واضح README کے ساتھ ہدایات فراہم کریں
5. **پُل ریکویسٹ جمع کریں**: کمیونٹی میں واپس تعاون کریں

### کارکردگی کے بینچ مارکس

بہت سی ریسیپیز میں کارکردگی کے بینچ مارکس شامل ہیں جو دکھاتے ہیں:
- **لیٹنسی میں بہتری**: بیس لائن کے مقابلے میں عام طور پر 2-6x رفتار میں اضافہ
- **میموری میں کمی**: کوانٹائزیشن کے ساتھ 50-75% میموری کے استعمال میں کمی
- **درستگی کا تحفظ**: 95-99% درستگی برقرار رکھنا

### AI ٹول کٹ کے ساتھ انٹیگریشن

ریسیپیز بغیر کسی رکاوٹ کے کام کرتی ہیں:
- **VS کوڈ AI ٹول کٹ**: ماڈل آپٹیمائزیشن کے لیے براہ راست انٹیگریشن
- **Azure مشین لرننگ**: کلاؤڈ پر مبنی آپٹیمائزیشن ورک فلو
- **ONNX Runtime**: آپٹیمائزڈ انفرنس ڈپلائمنٹ

## اضافی وسائل

### آفیشل لنکس
- **GitHub ریپوزٹری**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **اولیو ریسیپیز ریپوزٹری**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime دستاویزات**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face مثال**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔