<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "e8d157e0a282083a1e1c7bb5dda28646",
  "translation_date": "2025-10-30T11:06:49+00:00",
  "source_file": "Module04/README.md",
  "language_code": "ur"
}
-->
# باب 04: ماڈل فارمیٹ کنورژن اور کوانٹائزیشن - باب کا جائزہ

ایج اے آئی کی ترقی نے ماڈل فارمیٹ کنورژن اور کوانٹائزیشن کو ان اہم ٹیکنالوجیز میں شامل کر دیا ہے جو محدود وسائل والے آلات پر جدید مشین لرننگ کی صلاحیتوں کو نافذ کرنے کے لیے ضروری ہیں۔ یہ جامع باب ایج ڈپلائمنٹ کے منظرناموں کے لیے ماڈلز کو سمجھنے، نافذ کرنے اور بہتر بنانے کے لیے مکمل رہنمائی فراہم کرتا ہے۔

## 📚 باب کی ساخت اور سیکھنے کا راستہ

یہ باب سات ترقی پسند حصوں میں منظم ہے، ہر ایک پچھلے حصے پر تعمیر کرتا ہے تاکہ ایج کمپیوٹنگ کے لیے ماڈل کی اصلاح کو مکمل طور پر سمجھا جا سکے:

---

## [حصہ 1: ماڈل فارمیٹ کنورژن اور کوانٹائزیشن کی بنیادیں](./01.Introduce.md)

### 🎯 جائزہ
یہ بنیادی حصہ ایج کمپیوٹنگ ماحول میں ماڈل کی اصلاح کے لیے نظریاتی فریم ورک قائم کرتا ہے، جس میں 1-بٹ سے 8-بٹ تک کی درستگی کی سطحوں اور کلیدی فارمیٹ کنورژن حکمت عملیوں کا احاطہ کیا گیا ہے۔

**اہم موضوعات:**
- درستگی کی درجہ بندی کا فریم ورک (انتہائی کم، کم، درمیانی درستگی)
- GGUF اور ONNX فارمیٹ کے فوائد اور استعمال کے معاملات
- آپریشنل کارکردگی اور ڈپلائمنٹ کی لچک کے لیے کوانٹائزیشن کے فوائد
- کارکردگی کے بینچ مارکس اور میموری کے استعمال کے موازنہ

**سیکھنے کے نتائج:**
- کوانٹائزیشن کی حدود اور درجہ بندی کو سمجھیں
- مناسب فارمیٹ کنورژن تکنیکوں کی شناخت کریں
- ایج ڈپلائمنٹ کے لیے جدید اصلاحی حکمت عملی سیکھیں

---

## [حصہ 2: Llama.cpp نفاذ کی رہنمائی](./02.Llamacpp.md)

### 🎯 جائزہ
Llama.cpp کے نفاذ کے لیے ایک جامع ٹیوٹوریل، ایک طاقتور C++ فریم ورک جو مختلف ہارڈویئر کنفیگریشنز میں کم سے کم سیٹ اپ کے ساتھ موثر بڑے زبان ماڈل کی انفرنس کو فعال کرتا ہے۔

**اہم موضوعات:**
- ونڈوز، میک او ایس، اور لینکس پلیٹ فارمز پر انسٹالیشن
- GGUF فارمیٹ کنورژن اور مختلف کوانٹائزیشن لیولز (Q2_K سے Q8_0)
- CUDA، Metal، OpenCL، اور Vulkan کے ساتھ ہارڈویئر ایکسیلیریشن
- Python انٹیگریشن اور پروڈکشن ڈپلائمنٹ کی حکمت عملی

**سیکھنے کے نتائج:**
- کراس پلیٹ فارم انسٹالیشن اور سورس سے بلڈنگ میں مہارت حاصل کریں
- ماڈل کوانٹائزیشن اور اصلاحی تکنیکوں کو نافذ کریں
- REST API انٹیگریشن کے ساتھ سرور موڈ میں ماڈلز کو ڈپلائے کریں

---

## [حصہ 3: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

### 🎯 جائزہ
Microsoft Olive کا جائزہ، ایک ہارڈویئر سے آگاہ ماڈل اصلاحی ٹول کٹ جس میں 40+ بلٹ ان اصلاحی اجزاء شامل ہیں، جو مختلف ہارڈویئر پلیٹ فارمز پر انٹرپرائز گریڈ ماڈل ڈپلائمنٹ کے لیے ڈیزائن کیا گیا ہے۔

**اہم موضوعات:**
- متحرک اور جامد کوانٹائزیشن کے ساتھ آٹو اصلاحی خصوصیات
- CPU، GPU، اور NPU ڈپلائمنٹ کے لیے ہارڈویئر سے آگاہ ذہانت
- مشہور ماڈل کی سپورٹ (Llama، Phi، Qwen، Gemma) آؤٹ آف دی باکس
- Azure ML اور پروڈکشن ورک فلو کے ساتھ انٹرپرائز انٹیگریشن

**سیکھنے کے نتائج:**
- مختلف ماڈل آرکیٹیکچرز کے لیے خودکار اصلاحی حکمت عملیوں کا فائدہ اٹھائیں
- کراس پلیٹ فارم ڈپلائمنٹ کی حکمت عملیوں کو نافذ کریں
- انٹرپرائز کے لیے تیار اصلاحی پائپ لائنز قائم کریں

---

## [حصہ 4: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

### 🎯 جائزہ
Intel کے OpenVINO ٹول کٹ کا جامع جائزہ، ایک اوپن سورس پلیٹ فارم جو کلاؤڈ، آن پریمیس، اور ایج ماحول میں اعلیٰ کارکردگی والے AI حلوں کو ڈپلائے کرنے کے لیے جدید Neural Network Compression Framework (NNCF) صلاحیتوں کے ساتھ ہے۔

**اہم موضوعات:**
- ہارڈویئر ایکسیلیریشن کے ساتھ کراس پلیٹ فارم ڈپلائمنٹ (CPU، GPU، VPU، AI ایکسیلیریٹرز)
- Neural Network Compression Framework (NNCF) کے لیے جدید کوانٹائزیشن اور پروننگ
- OpenVINO GenAI بڑے زبان ماڈل کی اصلاح اور ڈپلائمنٹ کے لیے
- انٹرپرائز گریڈ ماڈل سرور کی صلاحیتیں اور قابل پیمائش ڈپلائمنٹ کی حکمت عملی

**سیکھنے کے نتائج:**
- OpenVINO ماڈل کنورژن اور اصلاحی ورک فلو میں مہارت حاصل کریں
- NNCF کے ساتھ جدید کوانٹائزیشن تکنیکوں کو نافذ کریں
- مختلف ہارڈویئر پلیٹ فارمز پر ماڈلز کو ڈپلائے کریں

---

## [حصہ 5: Apple MLX Framework Deep Dive](./05.AppleMLX.md)

### 🎯 جائزہ
Apple MLX کا جامع احاطہ، ایک انقلابی فریم ورک جو خاص طور پر Apple Silicon پر موثر مشین لرننگ کے لیے ڈیزائن کیا گیا ہے، جس میں بڑے زبان ماڈل کی صلاحیتوں اور مقامی ڈپلائمنٹ پر زور دیا گیا ہے۔

**اہم موضوعات:**
- متحد میموری آرکیٹیکچر کے فوائد اور Metal Performance Shaders
- LLaMA، Mistral، Phi-3، Qwen، اور Code Llama ماڈلز کے لیے سپورٹ
- LoRA فائن ٹیوننگ کے لیے موثر ماڈل حسب ضرورت
- Hugging Face انٹیگریشن اور کوانٹائزیشن سپورٹ (4-بٹ اور 8-بٹ)

**سیکھنے کے نتائج:**
- Apple Silicon کی اصلاح کے لیے LLM ڈپلائمنٹ میں مہارت حاصل کریں
- فائن ٹیوننگ اور ماڈل حسب ضرورت تکنیکوں کو نافذ کریں
- بہتر پرائیویسی خصوصیات کے ساتھ انٹرپرائز AI ایپلیکیشنز بنائیں

---

## [حصہ 6: Edge AI Development Workflow Synthesis](./06.workflow-synthesis.md)

### 🎯 جائزہ
تمام اصلاحی فریم ورک کو متحد ورک فلو، فیصلہ میٹرکس، اور بہترین طریقوں میں جامع طور پر یکجا کرنے کا جائزہ، جو موبائل، ڈیسک ٹاپ، اور کلاؤڈ ماحول سمیت مختلف پلیٹ فارمز اور استعمال کے معاملات کے لیے پروڈکشن کے لیے تیار ایج AI ڈپلائمنٹ کے لیے ہیں۔

**اہم موضوعات:**
- متعدد اصلاحی فریم ورک کو مربوط کرنے کے لیے متحد ورک فلو آرکیٹیکچر
- فریم ورک کے انتخاب کے فیصلہ درخت اور کارکردگی کے تجزیے
- پروڈکشن کی تیاری کی توثیق اور جامع ڈپلائمنٹ کی حکمت عملی
- ابھرتے ہوئے ہارڈویئر اور ماڈل آرکیٹیکچرز کے لیے مستقبل کی حکمت عملی

**سیکھنے کے نتائج:**
- ضروریات اور پابندیوں کی بنیاد پر منظم فریم ورک کے انتخاب میں مہارت حاصل کریں
- جامع نگرانی کے ساتھ پروڈکشن گریڈ ایج AI پائپ لائنز کو نافذ کریں
- ابھرتی ہوئی ٹیکنالوجیز اور ضروریات کے ساتھ ترقی پذیر ورک فلو ڈیزائن کریں

---

## [حصہ 7: Qualcomm QNN Optimization Suite](./07.QualcommQNN.md)

### 🎯 جائزہ
Qualcomm QNN (Qualcomm Neural Network) کا جامع جائزہ، ایک متحد AI انفرنس فریم ورک جو Qualcomm کے ہائٹروجینیس کمپیوٹنگ آرکیٹیکچر کو زیادہ سے زیادہ کارکردگی اور موبائل اور ایج ڈیوائسز پر توانائی کی کارکردگی کے لیے استعمال کرتا ہے، جس میں Hexagon NPU، Adreno GPU، اور Kryo CPU شامل ہیں۔

**اہم موضوعات:**
- NPU، GPU، اور CPU تک متحد رسائی کے ساتھ ہائٹروجینیس کمپیوٹنگ
- Snapdragon پلیٹ فارمز کے لیے ہارڈویئر سے آگاہ اصلاحی حکمت عملیوں کے ساتھ ذہین ورک لوڈ تقسیم
- موبائل ڈپلائمنٹ کے لیے جدید کوانٹائزیشن تکنیکیں (INT8، INT16، مکسڈ پریسیشن)
- بیٹری سے چلنے والے آلات اور ریئل ٹائم ایپلیکیشنز کے لیے توانائی کی موثر انفرنس

**سیکھنے کے نتائج:**
- موبائل AI ڈپلائمنٹ کے لیے Qualcomm ہارڈویئر ایکسیلیریشن میں مہارت حاصل کریں
- ایج کمپیوٹنگ کے لیے توانائی کی موثر اصلاحی حکمت عملیوں کو نافذ کریں
- Qualcomm کے ایکو سسٹم میں پروڈکشن کے لیے تیار ماڈلز کو زیادہ سے زیادہ کارکردگی کے ساتھ ڈپلائے کریں

---

## 🎯 باب کے سیکھنے کے نتائج

اس جامع باب کو مکمل کرنے کے بعد، قارئین درج ذیل حاصل کریں گے:

### **تکنیکی مہارت**
- کوانٹائزیشن کی حدود اور عملی اطلاق کی گہری سمجھ
- متعدد اصلاحی فریم ورک کے ساتھ عملی تجربہ
- ایج کمپیوٹنگ ماحول کے لیے پروڈکشن ڈپلائمنٹ کی مہارت

### **اسٹریٹجک سمجھ**
- ہارڈویئر سے آگاہ اصلاحی انتخاب کی صلاحیتیں
- کارکردگی کے تجارتی معاہدوں پر باخبر فیصلہ سازی
- انٹرپرائز کے لیے تیار ڈپلائمنٹ اور نگرانی کی حکمت عملی

### **کارکردگی کے بینچ مارکس**

| فریم ورک | کوانٹائزیشن | میموری کا استعمال | رفتار میں بہتری | استعمال کا معاملہ |
|-----------|-------------|--------------------|------------------|--------------------|
| Llama.cpp | Q4_K_M | ~4GB | 2-3x | کراس پلیٹ فارم ڈپلائمنٹ |
| Olive | INT4 | 60-75% کمی | 2-6x | انٹرپرائز ورک فلو |
| OpenVINO | INT8/INT4 | 50-75% کمی | 2-5x | Intel ہارڈویئر اصلاح |
| QNN | INT8/INT4 | 50-80% کمی | 5-15x | Qualcomm موبائل/ایج |
| MLX | 4-bit | ~4GB | 2-4x | Apple Silicon اصلاح |

## 🚀 اگلے مراحل اور جدید اطلاقات

یہ باب مکمل بنیاد فراہم کرتا ہے:
- مخصوص ڈومینز کے لیے حسب ضرورت ماڈل کی ترقی
- ایج AI اصلاح میں تحقیق
- تجارتی AI ایپلیکیشن کی ترقی
- بڑے پیمانے پر انٹرپرائز ایج AI ڈپلائمنٹ

ان سات حصوں سے حاصل کردہ علم ایج AI ماڈل کی اصلاح اور ڈپلائمنٹ کے تیزی سے ترقی پذیر منظرنامے کو نیویگیٹ کرنے کے لیے ایک جامع ٹول کٹ فراہم کرتا ہے۔

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔