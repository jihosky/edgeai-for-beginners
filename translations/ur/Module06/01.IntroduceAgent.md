<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T11:04:24+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "ur"
}
-->
# اے آئی ایجنٹس اور چھوٹے لینگویج ماڈلز: ایک جامع رہنما

## تعارف

اس ٹیوٹوریل میں، ہم اے آئی ایجنٹس اور چھوٹے لینگویج ماڈلز (SLMs) کے جدید نفاذ کی حکمت عملیوں کو ایج کمپیوٹنگ ماحول کے لیے دریافت کریں گے۔ ہم ایجنٹک اے آئی کے بنیادی تصورات، SLM کی اصلاحی تکنیک، وسائل کی محدود ڈیوائسز کے لیے عملی تعیناتی کی حکمت عملیوں، اور پروڈکشن کے لیے تیار ایجنٹ سسٹمز بنانے کے لیے مائیکروسافٹ ایجنٹ فریم ورک کا احاطہ کریں گے۔

مصنوعی ذہانت کا منظرنامہ 2025 میں ایک بنیادی تبدیلی کا سامنا کر رہا ہے۔ 2023 چیٹ بوٹس کا سال تھا اور 2024 میں کوپائلٹس کا عروج دیکھا گیا، جبکہ 2025 اے آئی ایجنٹس کا سال ہے — ذہین نظام جو سوچتے ہیں، منصوبہ بندی کرتے ہیں، ٹولز استعمال کرتے ہیں، اور کم سے کم انسانی مداخلت کے ساتھ کام انجام دیتے ہیں، جو زیادہ تر مؤثر چھوٹے لینگویج ماڈلز کے ذریعے چلائے جاتے ہیں۔ مائیکروسافٹ ایجنٹ فریم ورک آف لائن ایج پر مبنی صلاحیتوں کے ساتھ ان ذہین نظاموں کو بنانے کے لیے ایک اہم حل کے طور پر ابھرتا ہے۔

## سیکھنے کے مقاصد

اس ٹیوٹوریل کے اختتام تک، آپ قابل ہوں گے:

- 🤖 اے آئی ایجنٹس اور ایجنٹک سسٹمز کے بنیادی تصورات کو سمجھیں
- 🔬 ایجنٹک ایپلیکیشنز میں بڑے لینگویج ماڈلز کے مقابلے میں چھوٹے لینگویج ماڈلز کے فوائد کی شناخت کریں
- 🚀 ایج کمپیوٹنگ ماحول کے لیے SLM کی تعیناتی کی جدید حکمت عملیوں کو سیکھیں
- 📱 حقیقی دنیا کی ایپلیکیشنز کے لیے SLM سے چلنے والے ایجنٹس کو نافذ کریں
- 🏗️ مائیکروسافٹ ایجنٹ فریم ورک کا استعمال کرتے ہوئے پروڈکشن کے لیے تیار ایجنٹس بنائیں
- 🌐 مقامی LLM اور SLM انضمام کے ساتھ آف لائن ایج پر مبنی ایجنٹس کو تعینات کریں
- 🔧 مائیکروسافٹ ایجنٹ فریم ورک کو Foundry Local کے ساتھ ایج تعیناتی کے لیے مربوط کریں

## اے آئی ایجنٹس کو سمجھنا: بنیادیں اور درجہ بندی

### تعریف اور بنیادی تصورات

مصنوعی ذہانت (AI) ایجنٹ ایک ایسا نظام یا پروگرام ہے جو صارف یا کسی دوسرے نظام کی جانب سے خود مختار طور پر کام انجام دینے کی صلاحیت رکھتا ہے، اپنے ورک فلو کو ڈیزائن کرتا ہے اور دستیاب ٹولز کا استعمال کرتا ہے۔ روایتی اے آئی کے برعکس جو صرف آپ کے سوالات کا جواب دیتا ہے، ایک ایجنٹ آزادانہ طور پر اہداف حاصل کرنے کے لیے کام کر سکتا ہے۔

### ایجنٹ کی درجہ بندی کا فریم ورک

ایجنٹ کی حدود کو سمجھنا مختلف کمپیوٹنگ منظرناموں کے لیے مناسب ایجنٹ کی اقسام کا انتخاب کرنے میں مدد کرتا ہے:

- **🔬 سادہ ریفلیکس ایجنٹس**: اصول پر مبنی نظام جو فوری تاثرات پر ردعمل ظاہر کرتے ہیں (تھرموسٹیٹس، بنیادی آٹومیشن)
- **📱 ماڈل پر مبنی ایجنٹس**: نظام جو اندرونی حالت اور یادداشت کو برقرار رکھتے ہیں (روبوٹ ویکیومز، نیویگیشن سسٹمز)
- **⚖️ مقصد پر مبنی ایجنٹس**: نظام جو مقاصد حاصل کرنے کے لیے ترتیب وار منصوبہ بندی اور عمل کرتے ہیں (روٹ پلانرز، ٹاسک شیڈولرز)
- **🧠 سیکھنے والے ایجنٹس**: موافقت پذیر نظام جو وقت کے ساتھ کارکردگی کو بہتر بناتے ہیں (تجویز کردہ نظام، ذاتی معاونین)

### اے آئی ایجنٹس کے کلیدی فوائد

اے آئی ایجنٹس کئی بنیادی فوائد پیش کرتے ہیں جو انہیں ایج کمپیوٹنگ ایپلیکیشنز کے لیے مثالی بناتے ہیں:

**عملی خود مختاری**: ایجنٹس آزادانہ طور پر کام انجام دیتے ہیں بغیر مسلسل انسانی نگرانی کے، جو انہیں حقیقی وقت کی ایپلیکیشنز کے لیے مثالی بناتا ہے۔ وہ کم نگرانی کے ساتھ موافقت پذیر رویہ برقرار رکھتے ہیں، جس سے وسائل کی محدود ڈیوائسز پر تعیناتی ممکن ہوتی ہے۔

**تعیناتی کی لچک**: یہ نظام انٹرنیٹ کنیکٹیویٹی کی ضرورت کے بغیر ڈیوائس پر اے آئی صلاحیتوں کو فعال کرتے ہیں، مقامی پروسیسنگ کے ذریعے پرائیویسی اور سیکیورٹی کو بڑھاتے ہیں، ڈومین مخصوص ایپلیکیشنز کے لیے حسب ضرورت بنائے جا سکتے ہیں، اور مختلف ایج کمپیوٹنگ ماحول کے لیے موزوں ہیں۔

**لاگت کی مؤثریت**: ایجنٹ سسٹمز کلاؤڈ پر مبنی حل کے مقابلے میں لاگت مؤثر تعیناتی پیش کرتے ہیں، ایج ایپلیکیشنز کے لیے کم آپریٹنگ اخراجات اور کم بینڈوڈتھ کی ضروریات کے ساتھ۔

## چھوٹے لینگویج ماڈلز کی جدید حکمت عملی

### SLM (چھوٹے لینگویج ماڈل) کی بنیادیں

ایک چھوٹا لینگویج ماڈل (SLM) ایک لینگویج ماڈل ہے جو عام صارف الیکٹرانک ڈیوائس پر فٹ ہو سکتا ہے اور ایک صارف کی ایجنٹک درخواستوں کو پورا کرنے کے لیے عملی طور پر کم تاخیر کے ساتھ انفرینس انجام دے سکتا ہے۔ عملی طور پر، SLMs عام طور پر 10 ارب پیرامیٹرز سے کم ماڈلز ہوتے ہیں۔

**فارمیٹ دریافت کی خصوصیات**: SLMs مختلف کوانٹائزیشن لیولز، کراس پلیٹ فارم مطابقت، حقیقی وقت کی کارکردگی کی اصلاح، اور ایج تعیناتی کی صلاحیتوں کے لیے جدید سپورٹ پیش کرتے ہیں۔ صارفین مقامی پروسیسنگ اور WebGPU سپورٹ کے ذریعے براؤزر پر مبنی تعیناتی کے ذریعے بہتر پرائیویسی تک رسائی حاصل کر سکتے ہیں۔

**کوانٹائزیشن لیول کلیکشنز**: مقبول SLM فارمیٹس میں Q4_K_M شامل ہیں جو موبائل ایپلیکیشنز میں متوازن کمپریشن کے لیے، Q5_K_S سیریز جو معیار پر مرکوز ایج تعیناتی کے لیے، Q8_0 جو طاقتور ایج ڈیوائسز پر قریب اصل درستگی کے لیے، اور تجرباتی فارمیٹس جیسے Q2_K انتہائی کم وسائل والے منظرناموں کے لیے۔

### GGUF (جنرل GGML یونیورسل فارمیٹ) SLM تعیناتی کے لیے

GGUF ایجنٹک ایپلیکیشنز کے لیے خاص طور پر بہتر SLMs کو CPU اور ایج ڈیوائسز پر تعینات کرنے کے لیے بنیادی فارمیٹ کے طور پر کام کرتا ہے:

**ایجنٹ کے لیے بہتر خصوصیات**: فارمیٹ SLM کنورژن اور تعیناتی کے لیے جامع وسائل فراہم کرتا ہے، ٹول کالنگ، ساختی آؤٹ پٹ جنریشن، اور ملٹی ٹرن گفتگو کے لیے بہتر سپورٹ کے ساتھ۔ کراس پلیٹ فارم مطابقت مختلف ایج ڈیوائسز پر مستقل ایجنٹ کے رویے کو یقینی بناتی ہے۔

**کارکردگی کی اصلاح**: GGUF ایجنٹ ورک فلو کے لیے مؤثر میموری استعمال کو فعال کرتا ہے، ملٹی ایجنٹ سسٹمز کے لیے متحرک ماڈل لوڈنگ کو سپورٹ کرتا ہے، اور حقیقی وقت کے ایجنٹ تعاملات کے لیے بہتر انفرینس فراہم کرتا ہے۔

### ایج کے لیے بہتر SLM فریم ورک

#### Llama.cpp ایجنٹس کے لیے اصلاح

Llama.cpp ایجنٹک SLM تعیناتی کے لیے خاص طور پر بہتر کوانٹائزیشن تکنیک فراہم کرتا ہے:

**ایجنٹ کے لیے مخصوص کوانٹائزیشن**: فریم ورک Q4_0 (موبائل ایجنٹ تعیناتی کے لیے 75% سائز کی کمی کے ساتھ بہترین)، Q5_1 (ایج انفرینس ایجنٹس کے لیے معیار-کمپریشن کا توازن)، اور Q8_0 (پروڈکشن ایجنٹ سسٹمز کے لیے قریب اصل معیار) کو سپورٹ کرتا ہے۔ جدید فارمیٹس انتہائی کمپریسڈ ایجنٹس کو انتہائی ایج منظرناموں کے لیے فعال کرتے ہیں۔

**نفاذ کے فوائد**: SIMD ایکسیلریشن کے ساتھ CPU کے لیے بہتر انفرینس میموری مؤثر ایجنٹ کے نفاذ کو فراہم کرتا ہے۔ x86، ARM، اور Apple Silicon آرکیٹیکچرز کے درمیان کراس پلیٹ فارم مطابقت عالمی ایجنٹ تعیناتی کی صلاحیتوں کو فعال کرتی ہے۔

#### Apple MLX فریم ورک SLM ایجنٹس کے لیے

Apple MLX ایپل سلیکون ڈیوائسز پر SLM سے چلنے والے ایجنٹس کے لیے خاص طور پر بہتر مقامی اصلاح فراہم کرتا ہے:

**ایپل سلیکون ایجنٹ کی اصلاح**: فریم ورک متحد میموری آرکیٹیکچر کے ساتھ میٹل پرفارمنس شیڈرز انضمام، ایجنٹ انفرینس کے لیے خودکار مکسڈ پریسیشن، اور ملٹی ایجنٹ سسٹمز کے لیے بہتر میموری بینڈوڈتھ کا استعمال کرتا ہے۔ SLM ایجنٹس M سیریز چپس پر غیر معمولی کارکردگی دکھاتے ہیں۔

**ترقی کی خصوصیات**: ایجنٹ کے لیے مخصوص اصلاحات کے ساتھ Python اور Swift API سپورٹ، ایجنٹ لرننگ کے لیے خودکار تفریق، اور ایپل ڈیولپمنٹ ٹولز کے ساتھ ہموار انضمام جامع ایجنٹ ڈیولپمنٹ ماحول فراہم کرتے ہیں۔

#### ONNX Runtime کراس پلیٹ فارم SLM ایجنٹس کے لیے

ONNX Runtime ایک یونیورسل انفرینس انجن فراہم کرتا ہے جو SLM ایجنٹس کو مختلف ہارڈویئر پلیٹ فارمز اور آپریٹنگ سسٹمز پر مستقل طور پر چلانے کے قابل بناتا ہے:

**یونیورسل تعیناتی**: ONNX Runtime مختلف پلیٹ فارمز جیسے ونڈوز، لینکس، میک او ایس، آئی او ایس، اور اینڈرائیڈ پر SLM ایجنٹ کے رویے کو مستقل طور پر یقینی بناتا ہے۔ یہ کراس پلیٹ فارم مطابقت ڈویلپرز کو ایک بار لکھنے اور ہر جگہ تعینات کرنے کے قابل بناتی ہے، ملٹی پلیٹ فارم ایپلیکیشنز کے لیے ترقی اور دیکھ بھال کے اخراجات کو نمایاں طور پر کم کرتی ہے۔

**ہارڈویئر ایکسیلریشن کے اختیارات**: فریم ورک مختلف ہارڈویئر کنفیگریشنز کے لیے بہتر نفاذ فراہم کرتا ہے جن میں CPU (Intel, AMD, ARM)، GPU (NVIDIA CUDA, AMD ROCm)، اور خصوصی ایکسیلریٹرز (Intel VPU, Qualcomm NPU) شامل ہیں۔ SLM ایجنٹس کوڈ میں تبدیلی کے بغیر بہترین دستیاب ہارڈویئر کا خود بخود فائدہ اٹھا سکتے ہیں۔

**پروڈکشن کے لیے تیار خصوصیات**: ONNX Runtime پروڈکشن ایجنٹ تعیناتی کے لیے ضروری انٹرپرائز گریڈ خصوصیات پیش کرتا ہے جن میں تیز تر انفرینس کے لیے گراف کی اصلاح، وسائل کی محدود ماحول کے لیے میموری مینجمنٹ، اور کارکردگی کے تجزیے کے لیے جامع پروفائلنگ ٹولز شامل ہیں۔ فریم ورک Python اور C++ APIs دونوں کے لیے لچکدار انضمام کو سپورٹ کرتا ہے۔
- مائیکروسافٹ ایجنٹ فریم ورک انضمام کی جانچ کریں  
- آف لائن آپریشن کی صلاحیتوں کی تصدیق کریں  
- فیل اوور منظرنامے اور غلطی سے نمٹنے کی جانچ کریں  
- ایجنٹ ورک فلو کی مکمل توثیق کریں  

**فاؤنڈری لوکل کے ساتھ موازنہ**:

| خصوصیت | فاؤنڈری لوکل | اولاما |
|---------|---------------|--------|
| **ہدف استعمال کا کیس** | انٹرپرائز پروڈکشن | ترقی اور کمیونٹی |
| **ماڈل ایکو سسٹم** | مائیکروسافٹ کیوریٹڈ | وسیع کمیونٹی |
| **ہارڈویئر آپٹیمائزیشن** | خودکار (CUDA/NPU/CPU) | دستی ترتیب |
| **انٹرپرائز خصوصیات** | بلٹ ان مانیٹرنگ، سیکیورٹی | کمیونٹی ٹولز |
| **تعیناتی کی پیچیدگی** | سادہ (winget انسٹال) | سادہ (curl انسٹال) |
| **API مطابقت** | OpenAI + توسیعات | OpenAI معیاری |
| **سپورٹ** | مائیکروسافٹ آفیشل | کمیونٹی ڈرائیون |
| **بہترین استعمال کے لیے** | پروڈکشن ایجنٹس | پروٹوٹائپنگ، تحقیق |

**اولاما کو کب منتخب کریں**:  
- **ترقی اور پروٹوٹائپنگ**: مختلف ماڈلز کے ساتھ تیز تجربات  
- **کمیونٹی ماڈلز**: تازہ ترین کمیونٹی تعاون یافتہ ماڈلز تک رسائی  
- **تعلیمی استعمال**: AI ایجنٹ کی ترقی سیکھنے اور سکھانے کے لیے  
- **تحقیقی منصوبے**: متنوع ماڈلز تک رسائی کی ضرورت والے تعلیمی تحقیق  
- **حسب ضرورت ماڈلز**: اپنی مرضی کے مطابق فائن ٹیونڈ ماڈلز بنانا اور جانچنا  

### VLLM: اعلی کارکردگی SLM ایجنٹ انفرنس  

VLLM (بہت بڑے زبان ماڈل انفرنس) ایک اعلی تھروپٹ، میموری موثر انفرنس انجن فراہم کرتا ہے جو خاص طور پر بڑے پیمانے پر پروڈکشن SLM تعیناتیوں کے لیے بہتر بنایا گیا ہے۔ جبکہ فاؤنڈری لوکل استعمال میں آسانی پر توجہ مرکوز کرتا ہے اور اولاما کمیونٹی ماڈلز پر زور دیتا ہے، VLLM ان اعلی کارکردگی والے منظرناموں میں بہترین ہے جن میں زیادہ سے زیادہ تھروپٹ اور وسائل کی موثر استعمال کی ضرورت ہوتی ہے۔  

**بنیادی فن تعمیر اور خصوصیات**:  
- **PagedAttention**: موثر توجہ کی گنتی کے لیے انقلابی میموری مینجمنٹ  
- **Dynamic Batching**: بہترین تھروپٹ کے لیے ذہین درخواست بیچنگ  
- **GPU آپٹیمائزیشن**: جدید CUDA کرنلز اور ٹینسر پیراللزم سپورٹ  
- **OpenAI مطابقت**: ہموار انضمام کے لیے مکمل API مطابقت  
- **Speculative Decoding**: جدید انفرنس ایکسلریشن تکنیک  
- **Quantization سپورٹ**: INT4، INT8، اور FP16 کوانٹائزیشن میموری کی کارکردگی کے لیے  

#### انسٹالیشن اور سیٹ اپ  

**انسٹالیشن کے اختیارات**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**ایجنٹ ترقی کے لیے فوری آغاز**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### ایجنٹ فریم ورک انضمام  

**VLLM مائیکروسافٹ ایجنٹ فریم ورک کے ساتھ**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**اعلی تھروپٹ ملٹی ایجنٹ سیٹ اپ**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### پروڈکشن تعیناتی کے نمونے  

**انٹرپرائز VLLM پروڈکشن سروس**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### انٹرپرائز خصوصیات اور مانیٹرنگ  

**جدید VLLM کارکردگی مانیٹرنگ**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### جدید ترتیب اور آپٹیمائزیشن  

**پروڈکشن VLLM کنفیگریشن ٹیمپلیٹس**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**پروڈکشن تعیناتی چیک لسٹ VLLM کے لیے**:  

✅ **ہارڈویئر آپٹیمائزیشن**:  
- ملٹی-GPU سیٹ اپ کے لیے ٹینسر پیراللزم ترتیب دیں  
- میموری کی کارکردگی کے لیے کوانٹائزیشن (AWQ/GPTQ) فعال کریں  
- GPU میموری کے استعمال کو بہتر بنائیں (85-95%)  
- تھروپٹ کے لیے مناسب بیچ سائز ترتیب دیں  

✅ **کارکردگی کی ٹیوننگ**:  
- بار بار سوالات کے لیے پری فکس کیشنگ فعال کریں  
- طویل سلسلوں کے لیے چنکڈ پری فل ترتیب دیں  
- تیز انفرنس کے لیے speculative decoding سیٹ کریں  
- ہارڈویئر کے مطابق max_num_seqs کو بہتر بنائیں  

✅ **پروڈکشن خصوصیات**:  
- صحت کی نگرانی اور میٹرکس جمع کرنے کا سیٹ اپ کریں  
- خودکار ری اسٹارٹ اور فیل اوور ترتیب دیں  
- درخواست کی قطار بندی اور لوڈ بیلنسنگ نافذ کریں  
- جامع لاگنگ اور الرٹنگ سیٹ اپ کریں  

✅ **سیکیورٹی اور قابل اعتماد**:  
- فائر وال رولز اور رسائی کنٹرول ترتیب دیں  
- API ریٹ لمیٹنگ اور تصدیق سیٹ اپ کریں  
- گریسفل شٹ ڈاؤن اور صفائی نافذ کریں  
- بیک اپ اور آفت سے بحالی ترتیب دیں  

✅ **انضمام کی جانچ**:  
- مائیکروسافٹ ایجنٹ فریم ورک انضمام کی جانچ کریں  
- اعلی تھروپٹ منظرناموں کی توثیق کریں  
- فیل اوور اور بحالی کے طریقہ کار کی جانچ کریں  
- لوڈ کے تحت کارکردگی کا بینچ مارک کریں  

**دیگر حلوں کے ساتھ موازنہ**:

| خصوصیت | VLLM | فاؤنڈری لوکل | اولاما |
|---------|------|---------------|--------|
| **ہدف استعمال کا کیس** | اعلی تھروپٹ پروڈکشن | انٹرپرائز استعمال میں آسانی | ترقی اور کمیونٹی |
| **کارکردگی** | زیادہ سے زیادہ تھروپٹ | متوازن | اچھا |
| **میموری کی کارکردگی** | PagedAttention آپٹیمائزیشن | خودکار آپٹیمائزیشن | معیاری |
| **سیٹ اپ کی پیچیدگی** | زیادہ (کئی پیرامیٹرز) | کم (خودکار) | کم (سادہ) |
| **اسکیل ایبلٹی** | بہترین (ٹینسر/پائپ لائن پیرالل) | اچھا | محدود |
| **کوانٹائزیشن** | جدید (AWQ، GPTQ، FP8) | خودکار | معیاری GGUF |
| **انٹرپرائز خصوصیات** | حسب ضرورت نفاذ کی ضرورت | بلٹ ان | کمیونٹی ٹولز |
| **بہترین استعمال کے لیے** | بڑے پیمانے پر پروڈکشن ایجنٹس | انٹرپرائز پروڈکشن | ترقی |

**VLLM کو کب منتخب کریں**:  
- **اعلی تھروپٹ کی ضروریات**: سیکنڈ میں سینکڑوں درخواستوں کی پروسیسنگ  
- **بڑے پیمانے پر تعیناتی**: ملٹی-GPU، ملٹی-نوڈ تعیناتی  
- **کارکردگی اہم**: بڑے پیمانے پر سب سیکنڈ رسپانس ٹائمز  
- **جدید آپٹیمائزیشن**: حسب ضرورت کوانٹائزیشن اور بیچنگ کی ضرورت  
- **وسائل کی کارکردگی**: مہنگے GPU ہارڈویئر کا زیادہ سے زیادہ استعمال  

## حقیقی دنیا کے SLM ایجنٹ ایپلیکیشنز  

### کسٹمر سروس SLM ایجنٹس  
- **SLM صلاحیتیں**: اکاؤنٹ کی تلاش، پاس ورڈ ری سیٹ، آرڈر کی حیثیت کی جانچ  
- **لاگت کے فوائد**: LLM ایجنٹس کے مقابلے میں انفرنس لاگت میں 10x کمی  
- **کارکردگی**: معمول کے سوالات کے لیے مستقل معیار کے ساتھ تیز ردعمل کا وقت  

### بزنس پروسیس SLM ایجنٹس  
- **انوائس پروسیسنگ ایجنٹس**: ڈیٹا نکالیں، معلومات کی توثیق کریں، منظوری کے لیے روٹ کریں  
- **ای میل مینجمنٹ ایجنٹس**: خود بخود زمرہ بندی کریں، ترجیح دیں، جوابات کا مسودہ تیار کریں  
- **شیڈولنگ ایجنٹس**: ملاقاتوں کو مربوط کریں، کیلنڈرز کا انتظام کریں، یاد دہانیاں بھیجیں  

### ذاتی SLM ڈیجیٹل اسسٹنٹس  
- **ٹاسک مینجمنٹ ایجنٹس**: مؤثر طریقے سے ٹو ڈو لسٹ بنائیں، اپ ڈیٹ کریں، منظم کریں  
- **معلومات جمع کرنے والے ایجنٹس**: موضوعات پر تحقیق کریں، مقامی طور پر نتائج کا خلاصہ کریں  
- **کمیونیکیشن ایجنٹس**: ای میلز، پیغامات، سوشل میڈیا پوسٹس نجی طور پر تیار کریں  

### ٹریڈنگ اور مالیاتی SLM ایجنٹس  
- **مارکیٹ مانیٹرنگ ایجنٹس**: قیمتوں کو ٹریک کریں، حقیقی وقت میں رجحانات کی شناخت کریں  
- **رپورٹ جنریشن ایجنٹس**: خود بخود روزانہ/ہفتہ وار خلاصے بنائیں  
- **رسک اسیسمنٹ ایجنٹس**: مقامی ڈیٹا کا استعمال کرتے ہوئے پورٹ فولیو پوزیشنز کا جائزہ لیں  

### صحت کی دیکھ بھال کے معاون SLM ایجنٹس  
- **مریض شیڈولنگ ایجنٹس**: ملاقاتوں کو مربوط کریں، خودکار یاد دہانیاں بھیجیں  
- **دستاویزات ایجنٹس**: مقامی طور پر طبی خلاصے، رپورٹس تیار کریں  
- **نسخہ مینجمنٹ ایجنٹس**: ریفلز کو ٹریک کریں، نجی طور پر تعاملات کی جانچ کریں  

## مائیکروسافٹ ایجنٹ فریم ورک: پروڈکشن کے لیے تیار ایجنٹ کی ترقی  

### جائزہ اور فن تعمیر  

مائیکروسافٹ ایجنٹ فریم ورک ایک جامع، انٹرپرائز گریڈ پلیٹ فارم فراہم کرتا ہے جو AI ایجنٹس بنانے، تعینات کرنے، اور ان کا انتظام کرنے کے لیے ہے جو کلاؤڈ اور آف لائن ایج ماحول میں کام کر سکتے ہیں۔ فریم ورک خاص طور پر چھوٹے زبان ماڈلز اور ایج کمپیوٹنگ منظرناموں کے ساتھ ہموار کام کرنے کے لیے ڈیزائن کیا گیا ہے، جو پرائیویسی حساس اور وسائل کی محدود تعیناتیوں کے لیے مثالی ہے۔  

**بنیادی فریم ورک کے اجزاء**:  
- **ایجنٹ رن ٹائم**: ایج ڈیوائسز کے لیے بہتر ہلکا پھلکا ایگزیکیوشن ماحول  
- **ٹول انٹیگریشن سسٹم**: بیرونی خدمات اور APIs کو مربوط کرنے کے لیے قابل توسیع پلگ ان فن تعمیر  
- **اسٹیٹ مینجمنٹ**: سیشنز کے دوران مستقل ایجنٹ میموری اور سیاق و سباق کو سنبھالنا  
- **سیکیورٹی لیئر**: انٹرپرائز تعیناتی کے لیے بلٹ ان سیکیورٹی کنٹرولز  
- **آرکسٹریشن انجن**: ملٹی ایجنٹ کوآرڈینیشن اور ورک فلو مینجمنٹ  

### ایج تعیناتی کے لیے کلیدی خصوصیات  

**آف لائن فرسٹ فن تعمیر**: مائیکروسافٹ ایجنٹ فریم ورک آف لائن فرسٹ اصولوں کے ساتھ ڈیزائن کیا گیا ہے، جو ایجنٹس کو مستقل انٹرنیٹ کنیکٹیویٹی کے بغیر مؤثر طریقے سے کام کرنے کے قابل بناتا ہے۔ اس میں مقامی ماڈل انفرنس، کیشڈ نالج بیسز، آف لائن ٹول ایگزیکیوشن، اور کلاؤڈ سروسز کی عدم دستیابی کے وقت گریسفل ڈیگریڈیشن شامل ہے۔  

**وسائل کی آپٹیمائزیشن**: فریم ورک ذہین وسائل کے انتظام فراہم کرتا ہے جس میں SLMs کے لیے خودکار میموری آپٹیمائزیشن، ایج ڈیوائسز کے لیے CPU/GPU لوڈ بیلنسنگ، دستیاب وسائل کی بنیاد پر موافقت پذیر ماڈل کا انتخاب، اور موبائل تعیناتی کے لیے پاور موثر انفرنس پیٹرنز شامل ہیں۔  

**سیکیورٹی اور پرائیویسی**: انٹرپرائز گریڈ سیکیورٹی خصوصیات میں پرائیویسی کو برقرار رکھنے کے لیے مقامی ڈیٹا پروسیسنگ، ایجنٹ کمیونیکیشن چینلز کی انکرپشن، ایجنٹ صلاحیتوں کے لیے رول بیسڈ ایکسیس کنٹرولز، اور تعمیل کی ضروریات کے لیے آڈٹ لاگنگ شامل ہیں۔  

### فاؤنڈری لوکل کے ساتھ انضمام  

مائیکروسافٹ ایجنٹ فریم ورک فاؤنڈری لوکل کے ساتھ ہموار انضمام فراہم کرتا ہے تاکہ مکمل ایج AI حل فراہم کیا جا سکے:  

**خودکار ماڈل دریافت**: فریم ورک خود بخود فاؤنڈری لوکل انسٹینسز کا پتہ لگاتا ہے، دستیاب SLM ماڈلز سے جڑتا ہے، اور ایجنٹ کی ضروریات اور ہارڈویئر کی صلاحیتوں کی بنیاد پر بہترین ماڈلز کا انتخاب کرتا ہے۔  

**ڈائنامک ماڈل لوڈنگ**: ایجنٹس مخصوص کاموں کے لیے مختلف SLMs کو متحرک طور پر لوڈ کر سکتے ہیں، ملٹی ماڈل ایجنٹ سسٹمز کو فعال کرتے ہیں جہاں مختلف ماڈلز مختلف قسم کی درخواستوں کو سنبھالتے ہیں، اور دستیابی اور کارکردگی کی بنیاد پر ماڈلز کے درمیان خودکار فیل اوور۔  

**کارکردگی کی آپٹیمائزیشن**: مربوط کیشنگ میکانزم ماڈل لوڈنگ کے وقت کو کم کرتے ہیں، کنکشن پولنگ فاؤنڈری لوکل کے API کالز کو بہتر بناتی ہے، اور ذہین بیچنگ متعدد ایجنٹ درخواستوں کے لیے تھروپٹ کو بہتر بناتی ہے۔  

### مائیکروسافٹ ایجنٹ فریم ورک کے ساتھ ایجنٹس بنانا  

#### ایجنٹ کی تعریف اور ترتیب  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### ایج منظرناموں کے لیے ٹول انضمام  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### ملٹی ایجنٹ آرکسٹریشن  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### ایج تعیناتی کے جدید نمونے  

#### درجہ بندی ایجنٹ فن تعمیر  

**مقامی ایجنٹ کلسٹرز**: ایج ڈیوائسز پر متعدد خصوصی SLM ایجنٹس کو تعینات کریں، ہر ایک مخصوص کاموں کے لیے بہتر بنایا گیا ہے۔ سادہ روٹنگ اور شیڈولنگ کے لیے Qwen2.5-0.5B جیسے ہلکے ماڈلز استعمال کریں، کسٹمر سروس اور دستاویزات کے لیے درمیانے ماڈلز جیسے Phi-4-Mini، اور وسائل کی اجازت کے وقت پیچیدہ استدلال کے لیے بڑے ماڈلز۔  

**ایج سے کلاؤڈ کوآرڈینیشن**: ذہین اسکیلیشن پیٹرنز نافذ کریں جہاں مقامی ایجنٹس معمول کے کاموں کو سنبھالتے ہیں، کلاؤڈ ایجنٹس کنیکٹیویٹی کی اجازت کے وقت پیچیدہ استدلال فراہم کرتے ہیں، اور ایج اور کلاؤڈ پروسیسنگ کے درمیان ہموار ہینڈ آف تسلسل کو برقرار رکھتا ہے۔  

#### تعیناتی کی ترتیب  

**سنگل ڈیوائس تعیناتی**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**تقسیم شدہ ایج تعیناتی**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### ایج ایجنٹس کے لیے کارکردگی کی آپٹیمائزیشن  

#### ماڈل انتخاب کی حکمت عملی  

**ٹاسک پر مبنی ماڈل اسائنمنٹ**: مائیکروسافٹ ایجنٹ فریم ورک کام کی پیچیدگی اور ضروریات کی بنیاد پر ذہین ماڈل انتخاب کو فعال کرتا ہے:  

- **سادہ کام** (Q&A، روٹنگ): Qwen2.5-0.5B (500MB، <100ms رسپانس)  
- **درمیانے کام** (کسٹمر سروس، شیڈولنگ): Phi-4-Mini (2.4GB، 200-500ms رسپانس)  
- **پیچیدہ کام** (تکنیکی تجزیہ، منصوبہ بندی): Phi-4 (7GB، 1-3s رسپانس جب وسائل اجازت دیں)  

**ڈائنامک ماڈل سوئچنگ**: ایجنٹس موجودہ سسٹم لوڈ، کام کی پیچیدگی کی تشخیص، صارف کی ترجیحی سطح، اور دستیاب ہارڈویئر وسائل کی بنیاد پر ماڈلز کے درمیان سوئچ کر سکتے ہیں۔  

#### میموری اور وسائل کا انتظام  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  

### انٹرپرائز انضمام کے نمونے  

#### سیکیورٹی اور تعمیل  

**مقامی ڈیٹا پروسیسنگ**: تمام ایجنٹ پروسیسنگ مقامی طور پر ہوتی ہے، اس بات کو یقینی بناتے ہوئے کہ حساس ڈیٹا کبھی بھی ایج ڈیوائس سے باہر نہ جائے۔ اس میں کسٹمر معلومات کا تحفظ، صحت کی دیکھ بھال کے ایجنٹس کے لیے HIPAA تعمیل، بینکنگ ایجنٹس کے لیے مالیاتی ڈیٹا کی سیکیورٹی، اور یورپی تعیناتیوں کے لیے GDPR تعمیل شامل ہے۔  

**رسائی کنٹرول**: رول پر مبنی اجازتیں کنٹرول کرتی ہیں کہ کون سے ٹولز ایجنٹس تک رسائی حاصل کر سکتے ہیں، ایجنٹ تعاملات
**ایجنٹ کی تعیناتی کے لیے فریم ورک کا انتخاب**: ہدف ہارڈویئر اور ایجنٹ کی ضروریات کے مطابق آپٹیمائزیشن فریم ورک کا انتخاب کریں۔ CPU-optimized ایجنٹ کی تعیناتی کے لیے Llama.cpp استعمال کریں، Apple Silicon ایجنٹ ایپلیکیشنز کے لیے Apple MLX، اور کراس پلیٹ فارم ایجنٹ مطابقت کے لیے ONNX۔

## عملی SLM ایجنٹ کی تبدیلی اور استعمال کے کیسز

### حقیقی دنیا کے ایجنٹ تعیناتی کے منظرنامے

**موبائل ایجنٹ ایپلیکیشنز**: Q4_K فارمیٹس اسمارٹ فون ایجنٹ ایپلیکیشنز میں کم میموری استعمال کے ساتھ بہترین ہیں، جبکہ Q8_0 ٹیبلٹ پر مبنی ایجنٹ سسٹمز کے لیے متوازن کارکردگی فراہم کرتا ہے۔ Q5_K فارمیٹس موبائل پروڈکٹیویٹی ایجنٹس کے لیے اعلیٰ معیار فراہم کرتے ہیں۔

**ڈیسک ٹاپ اور ایج ایجنٹ کمپیوٹنگ**: Q5_K ڈیسک ٹاپ ایجنٹ ایپلیکیشنز کے لیے بہترین کارکردگی فراہم کرتا ہے، Q8_0 ورک سٹیشن ایجنٹ ماحول کے لیے اعلیٰ معیار کی انفرنس فراہم کرتا ہے، اور Q4_K ایج ایجنٹ ڈیوائسز پر مؤثر پروسیسنگ کو ممکن بناتا ہے۔

**تحقیق اور تجرباتی ایجنٹس**: جدید کوانٹائزیشن فارمیٹس انتہائی کم پریسیژن ایجنٹ انفرنس کی تحقیق اور پروف آف کانسیپٹ ایجنٹ ایپلیکیشنز کے لیے جو انتہائی وسائل کی پابندیوں کی ضرورت ہوتی ہے، کو ممکن بناتے ہیں۔

### SLM ایجنٹ کی کارکردگی کے بینچ مارکس

**ایجنٹ انفرنس کی رفتار**: Q4_K موبائل CPUs پر سب سے تیز ایجنٹ کے ردعمل کا وقت حاصل کرتا ہے، Q5_K عمومی ایجنٹ ایپلیکیشنز کے لیے رفتار اور معیار کا متوازن تناسب فراہم کرتا ہے، Q8_0 پیچیدہ ایجنٹ کاموں کے لیے اعلیٰ معیار فراہم کرتا ہے، اور تجرباتی فارمیٹس خصوصی ایجنٹ ہارڈویئر کے لیے زیادہ سے زیادہ تھروپٹ فراہم کرتے ہیں۔

**ایجنٹ میموری کی ضروریات**: ایجنٹس کے لیے کوانٹائزیشن لیولز Q2_K (چھوٹے ایجنٹ ماڈلز کے لیے 500MB سے کم) سے لے کر Q8_0 (اصل سائز کا تقریباً 50%) تک ہوتے ہیں، جبکہ تجرباتی کنفیگریشنز وسائل کی پابندی والے ایجنٹ ماحول کے لیے زیادہ سے زیادہ کمپریشن حاصل کرتے ہیں۔

## SLM ایجنٹس کے لیے چیلنجز اور غور و فکر

### ایجنٹ سسٹمز میں کارکردگی کے سمجھوتے

SLM ایجنٹ کی تعیناتی میں ماڈل کے سائز، ایجنٹ کے ردعمل کی رفتار، اور آؤٹ پٹ کے معیار کے درمیان سمجھوتے پر غور کرنا شامل ہے۔ جہاں Q4_K موبائل ایجنٹس کے لیے غیر معمولی رفتار اور کارکردگی فراہم کرتا ہے، وہیں Q8_0 پیچیدہ ایجنٹ کاموں کے لیے اعلیٰ معیار فراہم کرتا ہے۔ Q5_K زیادہ تر عمومی ایجنٹ ایپلیکیشنز کے لیے ایک درمیانی راستہ فراہم کرتا ہے۔

### SLM ایجنٹس کے لیے ہارڈویئر مطابقت

مختلف ایج ڈیوائسز SLM ایجنٹ کی تعیناتی کے لیے مختلف صلاحیتیں رکھتی ہیں۔ Q4_K سادہ ایجنٹس کے لیے بنیادی پروسیسرز پر مؤثر طریقے سے چلتا ہے، Q5_K متوازن ایجنٹ کی کارکردگی کے لیے معتدل کمپیوٹیشنل وسائل کی ضرورت ہوتی ہے، اور Q8_0 اعلیٰ درجے کے ہارڈویئر سے فائدہ اٹھاتا ہے تاکہ جدید ایجنٹ کی صلاحیتوں کو ممکن بنایا جا سکے۔

### SLM ایجنٹ سسٹمز میں سیکیورٹی اور پرائیویسی

جہاں SLM ایجنٹس بہتر پرائیویسی کے لیے مقامی پروسیسنگ کو ممکن بناتے ہیں، وہاں ایجنٹ ماڈلز اور ڈیٹا کو ایج ماحول میں محفوظ رکھنے کے لیے مناسب سیکیورٹی اقدامات نافذ کرنا ضروری ہے۔ یہ خاص طور پر اہم ہے جب انٹرپرائز ماحول میں اعلیٰ پریسیژن ایجنٹ فارمیٹس یا حساس ڈیٹا کو ہینڈل کرنے والی ایپلیکیشنز میں کمپریسڈ ایجنٹ فارمیٹس کو تعینات کیا جائے۔

## SLM ایجنٹ کی ترقی میں مستقبل کے رجحانات

SLM ایجنٹ کا منظر نامہ کمپریشن تکنیک، آپٹیمائزیشن کے طریقے، اور ایج تعیناتی کی حکمت عملیوں میں ترقی کے ساتھ مسلسل ترقی کر رہا ہے۔ مستقبل کی ترقی میں ایجنٹ ماڈلز کے لیے زیادہ مؤثر کوانٹائزیشن الگورتھمز، ایجنٹ ورک فلو کے لیے بہتر کمپریشن کے طریقے، اور ایجنٹ پروسیسنگ کے لیے ایج ہارڈویئر ایکسیلیریٹرز کے ساتھ بہتر انضمام شامل ہیں۔

**SLM ایجنٹس کے لیے مارکیٹ کی پیش گوئیاں**: حالیہ تحقیق کے مطابق، ایجنٹ سے چلنے والی آٹومیشن 2027 تک انٹرپرائز ورک فلو میں 40–60% دہرائے جانے والے علمی کاموں کو ختم کر سکتی ہے، SLMs اس تبدیلی کی قیادت کریں گے کیونکہ وہ لاگت کی کارکردگی اور تعیناتی کی لچک فراہم کرتے ہیں۔

**SLM ایجنٹس میں ٹیکنالوجی کے رجحانات**:
- **خصوصی SLM ایجنٹس**: مخصوص ایجنٹ کاموں اور صنعتوں کے لیے تربیت یافتہ ڈومین مخصوص ماڈلز
- **ایج ایجنٹ کمپیوٹنگ**: بہتر پرائیویسی اور کم لیٹنسی کے ساتھ آن ڈیوائس ایجنٹ کی صلاحیتیں
- **ایجنٹ آرکسٹریشن**: متعدد SLM ایجنٹس کے درمیان بہتر ہم آہنگی، متحرک روٹنگ اور لوڈ بیلنسنگ کے ساتھ
- **جمہوریت پسندی**: SLM کی لچک تنظیموں میں ایجنٹ کی ترقی میں وسیع شرکت کو ممکن بناتی ہے

## SLM ایجنٹس کے ساتھ شروعات

### مرحلہ 1: Microsoft Agent Framework ماحول ترتیب دیں

**Dependencies انسٹال کریں**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Foundry Local کو انیشیئلائز کریں**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### مرحلہ 2: ایجنٹ ایپلیکیشنز کے لیے اپنا SLM منتخب کریں
Microsoft Agent Framework کے لیے مشہور اختیارات:
- **Microsoft Phi-4 Mini (3.8B)**: عمومی ایجنٹ کاموں کے لیے بہترین، متوازن کارکردگی کے ساتھ
- **Qwen2.5-0.5B (0.5B)**: سادہ روٹنگ اور کلاسیفیکیشن ایجنٹس کے لیے انتہائی مؤثر
- **Qwen2.5-Coder-0.5B (0.5B)**: کوڈ سے متعلق ایجنٹ کاموں کے لیے خصوصی
- **Phi-4 (7B)**: وسائل کی اجازت ہونے پر پیچیدہ ایج منظرناموں کے لیے اعلیٰ استدلال

### مرحلہ 3: Microsoft Agent Framework کے ساتھ اپنا پہلا ایجنٹ بنائیں

**بنیادی ایجنٹ سیٹ اپ**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### مرحلہ 4: ایجنٹ کے دائرہ کار اور ضروریات کی وضاحت کریں
Microsoft Agent Framework کا استعمال کرتے ہوئے مرکوز، اچھی طرح سے وضاحت شدہ ایجنٹ ایپلیکیشنز کے ساتھ شروع کریں:
- **سنگل ڈومین ایجنٹس**: کسٹمر سروس یا شیڈولنگ یا تحقیق
- **ایجنٹ کے واضح مقاصد**: ایجنٹ کی کارکردگی کے لیے مخصوص، قابل پیمائش اہداف
- **محدود ٹول انضمام**: ابتدائی ایجنٹ کی تعیناتی کے لیے زیادہ سے زیادہ 3-5 ٹولز
- **ایجنٹ کی واضح حدود**: پیچیدہ منظرناموں کے لیے واضح اسکیلیشن راستے
- **ایج-فرسٹ ڈیزائن**: آف لائن فعالیت اور مقامی پروسیسنگ کو ترجیح دیں

### مرحلہ 5: Microsoft Agent Framework کے ساتھ ایج تعیناتی نافذ کریں

**وسائل کی ترتیب**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**ایج ایجنٹس کے لیے حفاظتی اقدامات تعینات کریں**:
- **مقامی ان پٹ کی توثیق**: کلاؤڈ پر انحصار کیے بغیر درخواستوں کی جانچ کریں
- **آف لائن آؤٹ پٹ فلٹرنگ**: یقینی بنائیں کہ جوابات مقامی طور پر معیار کے معیار پر پورا اترتے ہیں
- **ایج سیکیورٹی کنٹرولز**: انٹرنیٹ کنیکٹیویٹی کی ضرورت کے بغیر سیکیورٹی نافذ کریں
- **مقامی نگرانی**: کارکردگی کو ٹریک کریں اور ایج ٹیلیمیٹری کا استعمال کرتے ہوئے مسائل کو نشان زد کریں

### مرحلہ 6: ایج ایجنٹ کی کارکردگی کی پیمائش اور آپٹیمائز کریں
- **ایجنٹ کے کام مکمل کرنے کی شرحیں**: آف لائن منظرناموں میں کامیابی کی شرحوں کی نگرانی کریں
- **ایجنٹ کے ردعمل کے اوقات**: ایج تعیناتی کے لیے سب سیکنڈ ردعمل کے اوقات کو یقینی بنائیں
- **وسائل کا استعمال**: ایج ڈیوائسز پر میموری، CPU، اور بیٹری کے استعمال کو ٹریک کریں
- **لاگت کی کارکردگی**: ایج تعیناتی کے اخراجات کو کلاؤڈ پر مبنی متبادلات سے موازنہ کریں
- **آف لائن قابل اعتماد**: نیٹ ورک کی خرابی کے دوران ایجنٹ کی کارکردگی کی پیمائش کریں

## SLM ایجنٹ کے نفاذ کے لیے اہم نکات

1. **ایجنٹس کے لیے SLM کافی ہیں**: زیادہ تر ایجنٹ کاموں کے لیے، چھوٹے ماڈلز بڑے ماڈلز کی طرح کارکردگی دکھاتے ہیں جبکہ اہم فوائد پیش کرتے ہیں
2. **ایجنٹس میں لاگت کی کارکردگی**: SLM ایجنٹس کو چلانا 10-30x سستا ہے، جو انہیں وسیع پیمانے پر تعیناتی کے لیے اقتصادی طور پر قابل عمل بناتا ہے
3. **ایجنٹس کے لیے تخصص کام کرتا ہے**: مخصوص ایجنٹ ایپلیکیشنز میں فائن ٹیونڈ SLMs اکثر عمومی مقصد کے LLMs سے بہتر کارکردگی دکھاتے ہیں
4. **ہائبرڈ ایجنٹ آرکیٹیکچر**: معمول کے ایجنٹ کاموں کے لیے SLMs کا استعمال کریں، پیچیدہ استدلال کے لیے LLMs جب ضروری ہو
5. **Microsoft Agent Framework پیداوار کی تعیناتی کو ممکن بناتا ہے**: ایج ایجنٹس کی تعمیر، تعیناتی، اور انتظام کے لیے انٹرپرائز گریڈ ٹولز فراہم کرتا ہے
6. **ایج-فرسٹ ڈیزائن اصول**: آف لائن قابل ایجنٹس مقامی پروسیسنگ کے ساتھ پرائیویسی اور قابل اعتماد کو یقینی بناتے ہیں
7. **Foundry Local انضمام**: Microsoft Agent Framework اور مقامی ماڈل انفرنس کے درمیان ہموار کنکشن
8. **مستقبل SLM ایجنٹس ہیں**: پروڈکشن فریم ورک کے ساتھ چھوٹے زبان کے ماڈلز ایجنٹک AI کا مستقبل ہیں، جو جمہوری اور مؤثر ایجنٹ کی تعیناتی کو ممکن بناتے ہیں

## حوالہ جات اور مزید مطالعہ

### بنیادی تحقیق کے مقالے اور اشاعتیں

#### AI ایجنٹس اور ایجنٹک سسٹمز
- **"Language Agents as Optimizable Graphs"** (2024) - ایجنٹ آرکیٹیکچر اور آپٹیمائزیشن پر بنیادی تحقیق
  - مصنفین: Wenyue Hua, Lishan Yang, وغیرہ
  - لنک: https://arxiv.org/abs/2402.16823
  - اہم نکات: گراف پر مبنی ایجنٹ ڈیزائن اور آپٹیمائزیشن کی حکمت عملی

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - مصنفین: Zhiheng Xi, Wenxiang Chen, وغیرہ
  - لنک: https://arxiv.org/abs/2309.07864
  - اہم نکات: LLM پر مبنی ایجنٹ کی صلاحیتوں اور ایپلیکیشنز کا جامع سروے

- **"Cognitive Architectures for Language Agents"** (2024)
  - مصنفین: Theodore Sumers, Shunyu Yao, وغیرہ
  - لنک: https://arxiv.org/abs/2309.02427
  - اہم نکات: ذہین ایجنٹس کے ڈیزائن کے لیے علمی فریم ورک

#### چھوٹے زبان کے ماڈلز اور آپٹیمائزیشن
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - مصنفین: Microsoft Research Team
  - لنک: https://arxiv.org/abs/2404.14219
  - اہم نکات: SLM ڈیزائن اصول اور موبائل تعیناتی کی حکمت عملی

- **"Qwen2.5 Technical Report"** (2024)
  - مصنفین: Alibaba Cloud Team
  - لنک: https://arxiv.org/abs/2407.10671
  - اہم نکات: جدید SLM تربیتی تکنیک اور کارکردگی کی آپٹیمائزیشن

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - مصنفین: Peiyuan Zhang, Guangtao Zeng, وغیرہ
  - لنک: https://arxiv.org/abs/2401.02385
  - اہم نکات: انتہائی کمپیکٹ ماڈل ڈیزائن اور تربیتی کارکردگی

### آفیشل دستاویزات اور فریم ورک

#### Microsoft Agent Framework
- **آفیشل دستاویزات**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub ریپوزیٹری**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **پرائمری ریپوزیٹری**: https://github.com/microsoft/foundry-local
- **دستاویزات**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **مین ریپوزیٹری**: https://github.com/vllm-project/vllm
- **دستاویزات**: https://docs.vllm.ai/


#### Ollama
- **آفیشل ویب سائٹ**: https://ollama.ai/
- **GitHub ریپوزیٹری**: https://github.com/ollama/ollama

### ماڈل آپٹیمائزیشن فریم ورک

#### Llama.cpp
- **ریپوزیٹری**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **دستاویزات**: https://microsoft.github.io/Olive/
- **GitHub ریپوزیٹری**: https://github.com/microsoft/Olive

#### OpenVINO
- **آفیشل سائٹ**: https://docs.openvino.ai/

#### Apple MLX
- **ریپوزیٹری**: https://github.com/ml-explore/mlx

### انڈسٹری رپورٹس اور مارکیٹ تجزیہ

#### AI ایجنٹ مارکیٹ ریسرچ
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - لنک: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - اہم نکات: مارکیٹ کے رجحانات اور انٹرپرائز اپنانے کے نمونے

#### تکنیکی بینچ مارکس

- **"Edge AI Inference Benchmarks"** - MLPerf
  - لنک: https://mlcommons.org/en/inference-edge/
  - اہم نکات: ایج تعیناتی کے لیے معیاری کارکردگی کے میٹرکس

### معیارات اور وضاحتیں

#### ماڈل فارمیٹس اور معیارات
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - انٹرآپریبلٹی کے لیے کراس پلیٹ فارم ماڈل فارمیٹ
- **GGUF Specification**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - CPU انفرنس کے لیے کوانٹائزڈ ماڈل فارمیٹ
- **OpenAI API Specification**: https://platform.openai.com/docs/api-reference
  - زبان ماڈل انضمام کے لیے معیاری API فارمیٹ

#### سیکیورٹی اور تعمیل
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: AI سسٹمز اور حفاظت کے لیے فریم ورک
- **IEEE Standards for AI**: https://standards.ieee.org/industry-connections/ai/

SLM سے چلنے والے ایجنٹس کی طرف منتقلی AI تعیناتی کے طریقے میں ایک بنیادی تبدیلی کی نمائندگی کرتی ہے۔ Microsoft Agent Framework، مقامی پلیٹ فارمز اور مؤثر Small Language Models کے ساتھ مل کر، ایج ماحول میں مؤثر طریقے سے کام کرنے والے پروڈکشن ریڈی ایجنٹس کی تعمیر کے لیے ایک مکمل حل فراہم کرتا ہے۔ کارکردگی، تخصص، اور عملی افادیت پر توجہ مرکوز کرتے ہوئے، یہ ٹیکنالوجی اسٹیک AI ایجنٹس کو ہر صنعت اور ایج کمپیوٹنگ ماحول میں حقیقی دنیا کی ایپلیکیشنز کے لیے زیادہ قابل رسائی، سستی، اور مؤثر بناتا ہے۔

2025 تک، چھوٹے ماڈلز، Microsoft Agent Framework جیسے پیچیدہ ایجنٹ فریم ورک، اور مضبوط ایج تعیناتی پلیٹ فارمز کے امتزاج کے ساتھ، خود مختار سسٹمز کے لیے نئے امکانات کھ

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔