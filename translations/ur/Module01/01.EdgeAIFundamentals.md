<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:33:42+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "ur"
}
-->
# سیکشن 1: ایج اے آئی کے بنیادی اصول

ایج اے آئی مصنوعی ذہانت کی تعیناتی میں ایک نیا رجحان ہے، جو اے آئی کی صلاحیتوں کو براہ راست ایج ڈیوائسز پر لے آتا ہے، بجائے اس کے کہ صرف کلاؤڈ پر مبنی پروسیسنگ پر انحصار کیا جائے۔ یہ سمجھنا ضروری ہے کہ ایج اے آئی کس طرح محدود وسائل والے ڈیوائسز پر مقامی اے آئی پروسیسنگ کو ممکن بناتا ہے، جبکہ کارکردگی کو مناسب سطح پر برقرار رکھتا ہے اور پرائیویسی، تاخیر، اور آف لائن صلاحیتوں جیسے چیلنجز کو حل کرتا ہے۔

## تعارف

اس سبق میں، ہم ایج اے آئی اور اس کے بنیادی تصورات کا جائزہ لیں گے۔ ہم روایتی اے آئی کمپیوٹنگ کے طریقہ کار، ایج کمپیوٹنگ کے چیلنجز، ایج اے آئی کو ممکن بنانے والی کلیدی ٹیکنالوجیز، اور مختلف صنعتوں میں عملی اطلاقات پر بات کریں گے۔

## سیکھنے کے مقاصد

اس سبق کے اختتام تک، آپ:

- روایتی کلاؤڈ پر مبنی اے آئی اور ایج اے آئی کے طریقوں کے درمیان فرق کو سمجھ سکیں گے۔
- ان کلیدی ٹیکنالوجیز کی شناخت کر سکیں گے جو ایج ڈیوائسز پر اے آئی پروسیسنگ کو ممکن بناتی ہیں۔
- ایج اے آئی کے نفاذ کے فوائد اور حدود کو پہچان سکیں گے۔
- ایج اے آئی کے علم کو حقیقی دنیا کے منظرناموں اور استعمال کے معاملات میں لاگو کر سکیں گے۔

## روایتی اے آئی کمپیوٹنگ کے طریقہ کار کو سمجھنا

روایتی طور پر، جنریٹو اے آئی ایپلیکیشنز بڑے زبان ماڈلز (LLMs) کو مؤثر طریقے سے چلانے کے لیے اعلیٰ کارکردگی والے کمپیوٹنگ انفراسٹرکچر پر انحصار کرتی ہیں۔ تنظیمیں عام طور پر ان ماڈلز کو کلاؤڈ ماحول میں GPU کلسٹرز پر تعینات کرتی ہیں اور API انٹرفیس کے ذریعے ان کی صلاحیتوں تک رسائی حاصل کرتی ہیں۔

یہ مرکزی ماڈل بہت سی ایپلیکیشنز کے لیے اچھا کام کرتا ہے لیکن ایج کمپیوٹنگ کے منظرناموں میں اندرونی حدود رکھتا ہے۔ روایتی طریقہ کار میں صارف کے سوالات کو دور دراز سرورز پر بھیجنا، طاقتور ہارڈویئر کا استعمال کرتے ہوئے ان پر عمل کرنا، اور انٹرنیٹ کے ذریعے نتائج واپس کرنا شامل ہے۔ اگرچہ یہ طریقہ جدید ترین ماڈلز تک رسائی فراہم کرتا ہے، لیکن یہ انٹرنیٹ کنیکٹیویٹی پر انحصار پیدا کرتا ہے، تاخیر کے خدشات کو جنم دیتا ہے، اور حساس ڈیٹا کو بیرونی سرورز پر منتقل کرنے پر پرائیویسی کے مسائل پیدا کرتا ہے۔

روایتی اے آئی کمپیوٹنگ کے طریقہ کار کے ساتھ کام کرتے وقت ہمیں کچھ بنیادی تصورات کو سمجھنے کی ضرورت ہے، یعنی:

- **☁️ کلاؤڈ پر مبنی پروسیسنگ**: اے آئی ماڈلز طاقتور سرور انفراسٹرکچر پر چلتے ہیں جن میں اعلیٰ کمپیوٹیشنل وسائل ہوتے ہیں۔
- **🔌 API پر مبنی رسائی**: ایپلیکیشنز مقامی پروسیسنگ کے بجائے ریموٹ API کالز کے ذریعے اے آئی صلاحیتوں تک رسائی حاصل کرتی ہیں۔
- **🎛️ مرکزی ماڈل مینجمنٹ**: ماڈلز کو مرکزی طور پر برقرار رکھا اور اپ ڈیٹ کیا جاتا ہے، جس سے مستقل مزاجی یقینی ہوتی ہے لیکن نیٹ ورک کنیکٹیویٹی کی ضرورت ہوتی ہے۔
- **📈 وسائل کی توسیع پذیری**: کلاؤڈ انفراسٹرکچر مختلف کمپیوٹیشنل مطالبات کو پورا کرنے کے لیے متحرک طور پر توسیع کر سکتا ہے۔

## ایج کمپیوٹنگ کا چیلنج

ایج ڈیوائسز جیسے لیپ ٹاپ، موبائل فونز، اور انٹرنیٹ آف تھنگز (IoT) ڈیوائسز جیسے Raspberry Pi اور NVIDIA Orin Nano منفرد کمپیوٹیشنل پابندیوں کا سامنا کرتے ہیں۔ ان ڈیوائسز میں عام طور پر ڈیٹا سینٹر انفراسٹرکچر کے مقابلے میں محدود پروسیسنگ پاور، میموری، اور توانائی کے وسائل ہوتے ہیں۔

ایسے ڈیوائسز پر روایتی LLMs چلانا تاریخی طور پر ان ہارڈویئر کی حدود کی وجہ سے مشکل رہا ہے۔ تاہم، مختلف منظرناموں میں ایج اے آئی پروسیسنگ کی ضرورت دن بدن بڑھ رہی ہے۔ ان حالات پر غور کریں جہاں انٹرنیٹ کنیکٹیویٹی ناقابل اعتماد یا دستیاب نہیں ہے، جیسے دور دراز صنعتی مقامات، سفر میں موجود گاڑیاں، یا کمزور نیٹ ورک کوریج والے علاقے۔ اس کے علاوہ، وہ ایپلیکیشنز جو اعلیٰ سیکیورٹی معیارات کی ضرورت رکھتی ہیں، جیسے طبی آلات، مالیاتی نظام، یا حکومتی ایپلیکیشنز، حساس ڈیٹا کو مقامی طور پر پروسیس کرنے کی ضرورت ہو سکتی ہے تاکہ پرائیویسی اور تعمیل کی ضروریات کو برقرار رکھا جا سکے۔

### ایج کمپیوٹنگ کی کلیدی پابندیاں

ایج کمپیوٹنگ کے ماحول کو کئی بنیادی پابندیوں کا سامنا کرنا پڑتا ہے جو روایتی کلاؤڈ پر مبنی اے آئی حلوں کو نہیں ہوتا:

- **محدود پروسیسنگ پاور**: ایج ڈیوائسز میں عام طور پر سرور گریڈ ہارڈویئر کے مقابلے میں کم CPU کورز اور کم کلاک اسپیڈز ہوتی ہیں۔
- **میموری کی پابندیاں**: ایج ڈیوائسز پر دستیاب RAM اور اسٹوریج کی گنجائش نمایاں طور پر کم ہوتی ہے۔
- **توانائی کی حدود**: بیٹری سے چلنے والے ڈیوائسز کو طویل آپریشن کے لیے کارکردگی اور توانائی کی کھپت کے درمیان توازن قائم کرنا ہوتا ہے۔
- **تھرمل مینجمنٹ**: کمپیکٹ فارم فیکٹرز کولنگ کی صلاحیتوں کو محدود کرتے ہیں، جو لوڈ کے تحت مسلسل کارکردگی کو متاثر کرتے ہیں۔

## ایج اے آئی کیا ہے؟

### تصور: ایج اے آئی کی تعریف

ایج اے آئی سے مراد مصنوعی ذہانت کے الگورتھم کو براہ راست ایج ڈیوائسز پر تعینات کرنا اور ان پر عمل درآمد کرنا ہے—وہ فزیکل ہارڈویئر جو نیٹ ورک کے "ایج" پر موجود ہوتا ہے، جہاں ڈیٹا پیدا اور جمع کیا جاتا ہے۔ ان ڈیوائسز میں اسمارٹ فونز، IoT سینسرز، اسمارٹ کیمرے، خود مختار گاڑیاں، پہننے کے قابل آلات، اور صنعتی آلات شامل ہیں۔ روایتی اے آئی سسٹمز کے برعکس جو پروسیسنگ کے لیے کلاؤڈ سرورز پر انحصار کرتے ہیں، ایج اے آئی ذہانت کو براہ راست ڈیٹا کے منبع تک لے آتا ہے۔

ایج اے آئی بنیادی طور پر اے آئی پروسیسنگ کو غیر مرکزی بنانے کے بارے میں ہے، اسے مرکزی ڈیٹا سینٹرز سے دور لے جا کر ان گنت ڈیوائسز کے نیٹ ورک میں تقسیم کرنا جو ہمارے ڈیجیٹل ماحولیاتی نظام کو تشکیل دیتے ہیں۔ یہ اس بات میں ایک بنیادی معماری تبدیلی کی نمائندگی کرتا ہے کہ اے آئی سسٹمز کو کیسے ڈیزائن اور تعینات کیا جاتا ہے۔

ایج اے آئی کے کلیدی تصورات میں شامل ہیں:

- **قربت پروسیسنگ**: کمپیوٹیشن جسمانی طور پر اس جگہ کے قریب ہوتی ہے جہاں سے ڈیٹا پیدا ہوتا ہے۔
- **غیر مرکزی ذہانت**: فیصلہ سازی کی صلاحیتیں متعدد ڈیوائسز میں تقسیم ہوتی ہیں۔
- **ڈیٹا کی خود مختاری**: معلومات مقامی کنٹرول میں رہتی ہیں، اکثر کبھی بھی ڈیوائس سے باہر نہیں جاتی۔
- **خود مختار آپریشن**: ڈیوائسز مستقل کنیکٹیویٹی کی ضرورت کے بغیر ذہین طور پر کام کر سکتی ہیں۔
- **ایمبیڈڈ اے آئی**: ذہانت روزمرہ کے آلات کی ایک اندرونی صلاحیت بن جاتی ہے۔

### ایج اے آئی آرکیٹیکچر کی بصری وضاحت

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

ایج اے آئی مصنوعی ذہانت کی تعیناتی میں ایک نیا رجحان ہے، جو اے آئی کی صلاحیتوں کو براہ راست ایج ڈیوائسز پر لے آتا ہے، بجائے اس کے کہ صرف کلاؤڈ پر مبنی پروسیسنگ پر انحصار کیا جائے۔ یہ طریقہ محدود کمپیوٹیشنل وسائل والے ڈیوائسز پر اے آئی ماڈلز کو مقامی طور پر چلانے کے قابل بناتا ہے، بغیر مستقل انٹرنیٹ کنیکٹیویٹی کی ضرورت کے حقیقی وقت میں انفرینس کی صلاحیت فراہم کرتا ہے۔

ایج اے آئی مختلف ٹیکنالوجیز اور تکنیکوں کو شامل کرتا ہے جو اے آئی ماڈلز کو زیادہ موثر اور محدود وسائل والے ڈیوائسز پر تعیناتی کے لیے موزوں بناتی ہیں۔ مقصد یہ ہے کہ کارکردگی کو مناسب سطح پر برقرار رکھتے ہوئے اے آئی ماڈلز کی کمپیوٹیشنل اور میموری کی ضروریات کو نمایاں طور پر کم کیا جائے۔

آئیے ان بنیادی طریقوں پر نظر ڈالیں جو مختلف ڈیوائس کی اقسام اور استعمال کے معاملات میں ایج اے آئی کے نفاذ کو ممکن بناتے ہیں۔

### ایج اے آئی کے بنیادی اصول

ایج اے آئی کئی بنیادی اصولوں پر مبنی ہے جو اسے روایتی کلاؤڈ پر مبنی اے آئی سے ممتاز کرتے ہیں:

- **مقامی پروسیسنگ**: اے آئی انفرینس براہ راست ایج ڈیوائس پر ہوتا ہے، بغیر بیرونی کنیکٹیویٹی کی ضرورت کے۔
- **وسائل کی اصلاح**: ماڈلز کو ہدف ڈیوائسز کی ہارڈویئر کی حدود کے لیے خاص طور پر بہتر بنایا جاتا ہے۔
- **حقیقی وقت کی کارکردگی**: وقت کے حساس ایپلیکیشنز کے لیے کم سے کم تاخیر کے ساتھ پروسیسنگ ہوتی ہے۔
- **پرائیویسی بائی ڈیزائن**: حساس ڈیٹا ڈیوائس پر ہی رہتا ہے، جس سے سیکیورٹی اور تعمیل میں اضافہ ہوتا ہے۔

## ایج اے آئی کو ممکن بنانے والی کلیدی ٹیکنالوجیز

### ماڈل کوانٹائزیشن

ایج اے آئی میں سب سے اہم تکنیکوں میں سے ایک ماڈل کوانٹائزیشن ہے۔ اس عمل میں ماڈل پیرامیٹرز کی درستگی کو کم کرنا شامل ہے، عام طور پر 32-بٹ فلوٹنگ پوائنٹ نمبروں سے 8-بٹ انٹیجرز یا اس سے بھی کم درستگی فارمیٹس تک۔ اگرچہ یہ درستگی میں کمی پریشان کن لگ سکتی ہے، تحقیق سے پتہ چلا ہے کہ بہت سے اے آئی ماڈلز اپنی کارکردگی کو برقرار رکھ سکتے ہیں، چاہے درستگی میں نمایاں کمی ہو۔

کوانٹائزیشن فلوٹنگ پوائنٹ ویلیوز کی رینج کو چھوٹے سیٹ کے ڈسکریٹ ویلیوز میں میپ کرکے کام کرتا ہے۔ مثال کے طور پر، ہر پیرامیٹر کی نمائندگی کے لیے 32 بٹس استعمال کرنے کے بجائے، کوانٹائزیشن صرف 8 بٹس استعمال کر سکتا ہے، جس کے نتیجے میں میموری کی ضروریات میں 4 گنا کمی اور اکثر تیز تر انفرینس وقت ہوتا ہے۔

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

مختلف کوانٹائزیشن تکنیکوں میں شامل ہیں:

- **پوسٹ ٹریننگ کوانٹائزیشن (PTQ)**: ماڈل کی تربیت کے بعد لاگو کیا جاتا ہے، دوبارہ تربیت کی ضرورت نہیں ہوتی۔
- **کوانٹائزیشن-آویر ٹریننگ (QAT)**: تربیت کے دوران کوانٹائزیشن اثرات کو شامل کرتا ہے تاکہ بہتر درستگی حاصل کی جا سکے۔
- **ڈائنامک کوانٹائزیشن**: ویٹس کو int8 میں کوانٹائز کرتا ہے لیکن ایکٹیویشنز کو ڈائنامک طور پر حساب کرتا ہے۔
- **اسٹیٹک کوانٹائزیشن**: ویٹس اور ایکٹیویشنز دونوں کے لیے تمام کوانٹائزیشن پیرامیٹرز کو پہلے سے حساب کرتا ہے۔

ایج اے آئی کے نفاذ کے لیے، مناسب کوانٹائزیشن حکمت عملی کا انتخاب ہدف ڈیوائس کی مخصوص ماڈل آرکیٹیکچر، کارکردگی کی ضروریات، اور ہارڈویئر کی صلاحیتوں پر منحصر ہوتا ہے۔

### ماڈل کمپریشن اور اصلاح

کوانٹائزیشن کے علاوہ، مختلف کمپریشن تکنیکیں ماڈل کے سائز اور کمپیوٹیشنل ضروریات کو کم کرنے میں مدد کرتی ہیں۔ ان میں شامل ہیں:

**پروننگ**: یہ تکنیک نیورل نیٹ ورکس سے غیر ضروری کنکشنز یا نیورونز کو ہٹاتی ہے۔ ان پیرامیٹرز کی شناخت اور ختم کرکے جو ماڈل کی کارکردگی میں کم حصہ ڈالتے ہیں، پروننگ ماڈل کے سائز کو نمایاں طور پر کم کر سکتی ہے جبکہ درستگی کو برقرار رکھتی ہے۔

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**نالج ڈسٹلیشن**: یہ طریقہ ایک چھوٹے "اسٹوڈنٹ" ماڈل کو ایک بڑے "ٹیچر" ماڈل کے رویے کی نقل کرنے کے لیے تربیت دینے پر مشتمل ہے۔ اسٹوڈنٹ ماڈل ٹیچر کے آؤٹ پٹس کو تقریباً اسی کارکردگی کے ساتھ سیکھتا ہے، لیکن نمایاں طور پر کم پیرامیٹرز کے ساتھ۔

**ماڈل آرکیٹیکچر کی اصلاح**: محققین نے خاص طور پر ایج تعیناتی کے لیے ڈیزائن کردہ خصوصی آرکیٹیکچرز تیار کیے ہیں، جیسے MobileNets، EfficientNets، اور دیگر ہلکے آرکیٹیکچرز جو کارکردگی اور کمپیوٹیشنل کارکردگی کے درمیان توازن قائم کرتے ہیں۔

### چھوٹے زبان ماڈلز (SLMs)

ایج اے آئی میں ایک ابھرتا ہوا رجحان چھوٹے زبان ماڈلز (SLMs) کی ترقی ہے۔ یہ ماڈلز شروع سے ہی کمپیکٹ اور موثر ہونے کے لیے ڈیزائن کیے گئے ہیں، جبکہ اب بھی معنی خیز قدرتی زبان کی صلاحیتیں فراہم کرتے ہیں۔ SLMs یہ حاصل کرتے ہیں محتاط آرکیٹیکچرل انتخاب، موثر تربیتی تکنیکوں، اور مخصوص ڈومینز یا کاموں پر مرکوز تربیت کے ذریعے۔

روایتی طریقوں کے برعکس جو بڑے ماڈلز کو کمپریس کرنے پر مشتمل ہیں، SLMs اکثر چھوٹے ڈیٹا سیٹس اور خاص طور پر ایج تعیناتی کے لیے ڈیزائن کردہ بہتر آرکیٹیکچرز کے ساتھ تربیت یافتہ ہوتے ہیں۔ یہ طریقہ ایسے ماڈلز کا نتیجہ دے سکتا ہے جو نہ صرف چھوٹے ہیں بلکہ مخصوص استعمال کے معاملات کے لیے زیادہ موثر ہیں۔

## ایج اے آئی کے لیے ہارڈویئر کی تیز رفتاری

جدید ایج ڈیوائسز میں تیزی سے خصوصی ہارڈویئر شامل ہوتا ہے جو اے آئی ورک لوڈز کو تیز کرنے کے لیے ڈیزائن کیا گیا ہے:

### نیورل پروسیسنگ یونٹس (NPUs)

NPUs خاص طور پر نیورل نیٹ ورک کی کمپیوٹیشنز کے لیے ڈیزائن کیے گئے پروسیسرز ہیں۔ یہ چپس روایتی CPUs کے مقابلے میں اے آئی انفرینس کے کاموں کو زیادہ مؤثر طریقے سے انجام دے سکتی ہیں، اکثر کم توانائی کی کھپت کے ساتھ۔ بہت سے جدید اسمارٹ فونز، لیپ ٹاپس، اور IoT ڈیوائسز اب NPUs شامل کرتے ہیں تاکہ ڈیوائس پر اے آئی پروسیسنگ کو ممکن بنایا جا سکے۔

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

NPUs والے ڈیوائسز میں شامل ہیں:

- **ایپل**: A-series اور M-series چپس نیورل انجن کے ساتھ
- **کوالکوم**: Snapdragon پروسیسرز Hexagon DSP/NPU کے ساتھ
- **سام سنگ**: Exynos پروسیسرز NPU کے ساتھ
- **انٹیل**: Movidius VPUs اور Habana Labs ایکسیلریٹرز
- **مائیکروسافٹ**: Windows Copilot+ PCs NPUs کے ساتھ

### 🎮 GPU کی تیز رفتاری

اگرچہ ایج ڈیوائسز میں ڈیٹا سینٹرز میں موجود طاقتور GPUs نہیں ہوتے، لیکن بہت سے اب بھی مربوط یا الگ GPUs شامل کرتے ہیں جو اے آئی ورک لوڈز کو تیز کر سکتے ہیں۔ جدید موبائل GPUs اور مربوط گرافکس پروسیسرز اے آئی انفرینس کے کاموں کے لیے نمایاں کارکردگی میں بہتری فراہم کر سکتے ہیں۔

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU کی اصلاح

یہاں تک کہ صرف CPU والے ڈیوائسز بھی ایج اے آئی کے ذریعے بہتر نفاذ سے فائدہ اٹھا سکتے ہیں۔ جدید CPUs میں اے آئی ورک لوڈز کے لیے خصوصی ہدایات شامل ہیں، اور سافٹ ویئر فریم ورک تیار کیے گئے ہیں تاکہ ہدف ڈیوائسز پر اے آئی انفرینس کی کارکردگی اور توانائی کی کارکردگی کو زیادہ سے زیادہ کیا جا سکے۔

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

ایج اے آئی کے ساتھ کام کرنے والے سافٹ ویئر انجینئرز کے لیے، ان ہارڈویئر کی تیز رفتاری کے اختیارات کو استعمال کرنے کا طریقہ سمجھنا ہدف ڈیوائسز پر انفرینس کی کارکردگی اور توانائی کی کارکردگی کو بہتر بنانے کے لیے اہم ہے۔

## ایج اے آئی کے فوائد

### پرائیویسی اور سیکیورٹی

ایج اے آئی کا سب سے اہم فائدہ بہتر پرائیویسی اور سیکیورٹی ہے۔ ڈیٹا کو مقامی طور پر ڈیوائس پر پروسیس کرکے، حساس معلومات کبھی بھی صارف کے کنٹرول سے باہر نہیں جاتی۔ یہ خاص طور پر ذاتی ڈیٹا، طبی معلومات، یا خفیہ کاروباری ڈیٹا کو سنبھالنے والی ایپلیکیشنز کے لیے اہم ہے۔

### تاخیر میں کمی

ایج اے آئی ڈیٹا کو پروسیسنگ کے لیے دور دراز سرورز پر بھیجنے کی ضرورت کو ختم کرتا ہے، تاخیر کو نمایاں طور پر کم کرتا ہے۔ یہ خود مختار گاڑیوں، صنعتی
- [02: ایج اے آئی ایپلیکیشنز](02.RealWorldCaseStudies.md)

---

**ڈسکلیمر**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔