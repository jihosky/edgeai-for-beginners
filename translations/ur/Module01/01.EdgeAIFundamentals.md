<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "76b134be93f45283a2df8f8a93717d06",
  "translation_date": "2025-10-17T09:12:57+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "ur"
}
-->
# سیکشن 1: ایج اے آئی کے بنیادی اصول

ایج اے آئی مصنوعی ذہانت کی تعیناتی میں ایک نیا رجحان ہے، جو اے آئی کی صلاحیتوں کو براہ راست ایج ڈیوائسز پر لاتا ہے بجائے اس کے کہ صرف کلاؤڈ پر مبنی پروسیسنگ پر انحصار کیا جائے۔ یہ سمجھنا ضروری ہے کہ ایج اے آئی کس طرح محدود وسائل والے ڈیوائسز پر مقامی اے آئی پروسیسنگ کو ممکن بناتا ہے، جبکہ کارکردگی کو برقرار رکھتا ہے اور پرائیویسی، لیٹنسی، اور آف لائن صلاحیتوں جیسے چیلنجز کو حل کرتا ہے۔

## تعارف

اس سبق میں، ہم ایج اے آئی اور اس کے بنیادی تصورات کا جائزہ لیں گے۔ ہم روایتی اے آئی کمپیوٹنگ کے طریقے، ایج کمپیوٹنگ کے چیلنجز، ایج اے آئی کو ممکن بنانے والی کلیدی ٹیکنالوجیز، اور مختلف صنعتوں میں عملی اطلاقات کا احاطہ کریں گے۔

## سیکھنے کے مقاصد

اس سبق کے اختتام تک، آپ:

- روایتی کلاؤڈ پر مبنی اے آئی اور ایج اے آئی کے طریقوں کے فرق کو سمجھ سکیں گے۔
- ان کلیدی ٹیکنالوجیز کی شناخت کر سکیں گے جو ایج ڈیوائسز پر اے آئی پروسیسنگ کو ممکن بناتی ہیں۔
- ایج اے آئی کے فوائد اور حدود کو پہچان سکیں گے۔
- ایج اے آئی کے علم کو حقیقی دنیا کے منظرناموں اور استعمال کے معاملات میں لاگو کر سکیں گے۔

## روایتی اے آئی کمپیوٹنگ کے طریقے کو سمجھنا

روایتی طور پر، جنریٹو اے آئی ایپلیکیشنز بڑے لینگویج ماڈلز (LLMs) کو مؤثر طریقے سے چلانے کے لیے ہائی پرفارمنس کمپیوٹنگ انفراسٹرکچر پر انحصار کرتی ہیں۔ تنظیمیں عام طور پر ان ماڈلز کو کلاؤڈ ماحول میں GPU کلسٹرز پر تعینات کرتی ہیں اور API انٹرفیس کے ذریعے ان کی صلاحیتوں تک رسائی حاصل کرتی ہیں۔

یہ مرکزی ماڈل بہت سی ایپلیکیشنز کے لیے اچھا کام کرتا ہے لیکن ایج کمپیوٹنگ کے منظرناموں میں اندرونی حدود رکھتا ہے۔ روایتی طریقہ کار میں صارف کے سوالات کو دور دراز سرورز پر بھیجنا، انہیں طاقتور ہارڈویئر کے ذریعے پروسیس کرنا، اور انٹرنیٹ کے ذریعے نتائج واپس کرنا شامل ہے۔ اگرچہ یہ طریقہ جدید ترین ماڈلز تک رسائی فراہم کرتا ہے، یہ انٹرنیٹ کنیکٹیویٹی پر انحصار پیدا کرتا ہے، لیٹنسی کے خدشات کو جنم دیتا ہے، اور حساس ڈیٹا کو بیرونی سرورز پر منتقل کرنے کی ضرورت کے وقت پرائیویسی کے مسائل پیدا کرتا ہے۔

روایتی اے آئی کمپیوٹنگ کے طریقے کے ساتھ کام کرتے وقت ہمیں کچھ بنیادی تصورات کو سمجھنے کی ضرورت ہے، یعنی:

- **☁️ کلاؤڈ پر مبنی پروسیسنگ**: اے آئی ماڈلز طاقتور سرور انفراسٹرکچر پر چلتے ہیں جن میں اعلیٰ کمپیوٹیشنل وسائل ہوتے ہیں۔
- **🔌 API پر مبنی رسائی**: ایپلیکیشنز مقامی پروسیسنگ کے بجائے ریموٹ API کالز کے ذریعے اے آئی صلاحیتوں تک رسائی حاصل کرتی ہیں۔
- **🎛️ مرکزی ماڈل مینجمنٹ**: ماڈلز کو مرکزی طور پر برقرار رکھا اور اپ ڈیٹ کیا جاتا ہے، جس سے مستقل مزاجی یقینی ہوتی ہے لیکن نیٹ ورک کنیکٹیویٹی کی ضرورت ہوتی ہے۔
- **📈 وسائل کی توسیع پذیری**: کلاؤڈ انفراسٹرکچر مختلف کمپیوٹیشنل مطالبات کو پورا کرنے کے لیے متحرک طور پر توسیع پذیر ہو سکتا ہے۔

## ایج کمپیوٹنگ کا چیلنج

ایج ڈیوائسز جیسے لیپ ٹاپ، موبائل فونز، اور انٹرنیٹ آف تھنگز (IoT) ڈیوائسز جیسے Raspberry Pi اور NVIDIA Orin Nano منفرد کمپیوٹیشنل حدود پیش کرتے ہیں۔ ان ڈیوائسز میں عام طور پر ڈیٹا سینٹر انفراسٹرکچر کے مقابلے میں محدود پروسیسنگ پاور، میموری، اور توانائی کے وسائل ہوتے ہیں۔

ایسے ڈیوائسز پر روایتی LLMs چلانا تاریخی طور پر ان ہارڈویئر کی حدود کی وجہ سے مشکل رہا ہے۔ تاہم، مختلف منظرناموں میں ایج اے آئی پروسیسنگ کی ضرورت بڑھتی جا رہی ہے۔ انٹرنیٹ کنیکٹیویٹی ناقابل اعتماد یا دستیاب نہ ہونے کی صورت میں، جیسے دور دراز صنعتی مقامات، سفر میں موجود گاڑیاں، یا کمزور نیٹ ورک کوریج والے علاقے، ایج اے آئی کی اہمیت بڑھ جاتی ہے۔ مزید برآں، اعلیٰ سیکیورٹی معیارات کی ضرورت والے ایپلیکیشنز، جیسے طبی آلات، مالیاتی نظام، یا حکومتی ایپلیکیشنز، حساس ڈیٹا کو مقامی طور پر پروسیس کرنے کی ضرورت ہو سکتی ہے تاکہ پرائیویسی اور تعمیل کی ضروریات کو برقرار رکھا جا سکے۔

### ایج کمپیوٹنگ کی کلیدی حدود

ایج کمپیوٹنگ کے ماحول کو کئی بنیادی حدود کا سامنا کرنا پڑتا ہے جو روایتی کلاؤڈ پر مبنی اے آئی حلوں کو نہیں ہوتا:

- **محدود پروسیسنگ پاور**: ایج ڈیوائسز میں عام طور پر سرور گریڈ ہارڈویئر کے مقابلے میں کم CPU کورز اور کم کلاک اسپیڈز ہوتی ہیں۔
- **میموری کی حدود**: ایج ڈیوائسز پر دستیاب RAM اور اسٹوریج کی گنجائش نمایاں طور پر کم ہوتی ہے۔
- **توانائی کی حدود**: بیٹری سے چلنے والے ڈیوائسز کو طویل آپریشن کے لیے کارکردگی اور توانائی کی کھپت کے درمیان توازن قائم کرنا ہوتا ہے۔
- **تھرمل مینجمنٹ**: کمپیکٹ فارم فیکٹرز کولنگ کی صلاحیتوں کو محدود کرتے ہیں، جو لوڈ کے تحت مسلسل کارکردگی کو متاثر کرتے ہیں۔

## ایج اے آئی کیا ہے؟

### تصور: ایج اے آئی کی تعریف

ایج اے آئی سے مراد مصنوعی ذہانت کے الگورتھمز کی تعیناتی اور عملدرآمد براہ راست ایج ڈیوائسز پر ہے—وہ جسمانی ہارڈویئر جو نیٹ ورک کے "ایج" پر موجود ہوتا ہے، جہاں ڈیٹا پیدا اور جمع کیا جاتا ہے۔ ان ڈیوائسز میں اسمارٹ فونز، IoT سینسرز، اسمارٹ کیمرے، خود مختار گاڑیاں، پہننے کے قابل آلات، اور صنعتی سامان شامل ہیں۔ روایتی اے آئی سسٹمز کے برعکس جو پروسیسنگ کے لیے کلاؤڈ سرورز پر انحصار کرتے ہیں، ایج اے آئی ذہانت کو براہ راست ڈیٹا کے ماخذ تک لے آتا ہے۔

ایج اے آئی بنیادی طور پر اے آئی پروسیسنگ کو مرکزیت سے دور کرنے کے بارے میں ہے، اسے مرکزی ڈیٹا سینٹرز سے دور لے جا کر ان ڈیوائسز کے وسیع نیٹ ورک میں تقسیم کرنا جو ہمارے ڈیجیٹل ماحولیاتی نظام کو تشکیل دیتے ہیں۔ یہ اے آئی سسٹمز کے ڈیزائن اور تعیناتی میں ایک بنیادی آرکیٹیکچرل تبدیلی کی نمائندگی کرتا ہے۔

ایج اے آئی کے کلیدی تصوری ستون شامل ہیں:

- **قربت پروسیسنگ**: کمپیوٹیشن جسمانی طور پر اس جگہ کے قریب ہوتی ہے جہاں ڈیٹا پیدا ہوتا ہے۔
- **غیر مرکزی ذہانت**: فیصلہ سازی کی صلاحیتیں متعدد ڈیوائسز میں تقسیم ہوتی ہیں۔
- **ڈیٹا کی خود مختاری**: معلومات مقامی کنٹرول میں رہتی ہیں، اکثر کبھی بھی ڈیوائس سے باہر نہیں جاتی۔
- **خود مختار آپریشن**: ڈیوائسز مستقل کنیکٹیویٹی کی ضرورت کے بغیر ذہانت سے کام کر سکتی ہیں۔
- **ایمبیڈڈ اے آئی**: ذہانت روزمرہ کے آلات کی ایک اندرونی صلاحیت بن جاتی ہے۔

### ایج اے آئی آرکیٹیکچر کی بصری وضاحت

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

ایج اے آئی مصنوعی ذہانت کی تعیناتی میں ایک نیا رجحان ہے، جو اے آئی کی صلاحیتوں کو براہ راست ایج ڈیوائسز پر لاتا ہے بجائے اس کے کہ صرف کلاؤڈ پر مبنی پروسیسنگ پر انحصار کیا جائے۔ یہ طریقہ محدود کمپیوٹیشنل وسائل والے ڈیوائسز پر اے آئی ماڈلز کو مقامی طور پر چلانے کے قابل بناتا ہے، مستقل انٹرنیٹ کنیکٹیویٹی کی ضرورت کے بغیر ریئل ٹائم انفرنس کی صلاحیتیں فراہم کرتا ہے۔

ایج اے آئی مختلف ٹیکنالوجیز اور تکنیکوں کو شامل کرتا ہے جو اے آئی ماڈلز کو زیادہ موثر اور محدود وسائل والے ڈیوائسز پر تعیناتی کے لیے موزوں بنانے کے لیے ڈیزائن کی گئی ہیں۔ مقصد یہ ہے کہ کارکردگی کو برقرار رکھتے ہوئے اے آئی ماڈلز کی کمپیوٹیشنل اور میموری کی ضروریات کو نمایاں طور پر کم کیا جائے۔

آئیے ان بنیادی طریقوں پر نظر ڈالیں جو مختلف ڈیوائس کی اقسام اور استعمال کے معاملات میں ایج اے آئی کی تعیناتی کو ممکن بناتے ہیں۔

### ایج اے آئی کے بنیادی اصول

ایج اے آئی کئی بنیادی اصولوں پر مبنی ہے جو اسے روایتی کلاؤڈ پر مبنی اے آئی سے ممتاز کرتے ہیں:

- **مقامی پروسیسنگ**: اے آئی انفرنس براہ راست ایج ڈیوائس پر ہوتی ہے، بیرونی کنیکٹیویٹی کی ضرورت کے بغیر۔
- **وسائل کی اصلاح**: ماڈلز کو ہدف ڈیوائسز کی ہارڈویئر کی حدود کے لیے خاص طور پر بہتر بنایا جاتا ہے۔
- **ریئل ٹائم کارکردگی**: وقت کے حساس ایپلیکیشنز کے لیے کم سے کم لیٹنسی کے ساتھ پروسیسنگ ہوتی ہے۔
- **پرائیویسی بائی ڈیزائن**: حساس ڈیٹا ڈیوائس پر ہی رہتا ہے، سیکیورٹی اور تعمیل کو بڑھاتا ہے۔

## ایج اے آئی کو ممکن بنانے والی کلیدی ٹیکنالوجیز

### ماڈل کوانٹائزیشن

ایج اے آئی میں سب سے اہم تکنیکوں میں سے ایک ماڈل کوانٹائزیشن ہے۔ اس عمل میں ماڈل پیرامیٹرز کی درستگی کو کم کرنا شامل ہے، عام طور پر 32 بٹ فلوٹنگ پوائنٹ نمبرز سے 8 بٹ انٹیجرز یا اس سے بھی کم درستگی فارمیٹس تک۔ اگرچہ یہ درستگی میں کمی پریشان کن لگ سکتی ہے، تحقیق سے پتہ چلا ہے کہ بہت سے اے آئی ماڈلز اپنی کارکردگی کو برقرار رکھ سکتے ہیں یہاں تک کہ نمایاں طور پر کم درستگی کے ساتھ۔

کوانٹائزیشن فلوٹنگ پوائنٹ ویلیوز کی رینج کو چھوٹے سیٹ کے ڈسکریٹ ویلیوز میں میپ کرکے کام کرتا ہے۔ مثال کے طور پر، ہر پیرامیٹر کی نمائندگی کے لیے 32 بٹس استعمال کرنے کے بجائے، کوانٹائزیشن صرف 8 بٹس استعمال کر سکتا ہے، جس کے نتیجے میں میموری کی ضروریات میں 4x کمی اور اکثر تیز انفرنس کے اوقات ہوتے ہیں۔

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

مختلف کوانٹائزیشن تکنیکیں شامل ہیں:

- **پوسٹ ٹریننگ کوانٹائزیشن (PTQ)**: ماڈل کی تربیت کے بعد لاگو کیا جاتا ہے، دوبارہ تربیت کی ضرورت کے بغیر۔
- **کوانٹائزیشن-آگاہ تربیت (QAT)**: بہتر درستگی کے لیے تربیت کے دوران کوانٹائزیشن اثرات کو شامل کرتا ہے۔
- **ڈائنامک کوانٹائزیشن**: ویٹس کو int8 میں کوانٹائز کرتا ہے لیکن ایکٹیویشنز کو متحرک طور پر حساب کرتا ہے۔
- **اسٹیک کوانٹائزیشن**: ویٹس اور ایکٹیویشنز کے لیے تمام کوانٹائزیشن پیرامیٹرز کو پہلے سے حساب کرتا ہے۔

ایج اے آئی کی تعیناتی کے لیے، مناسب کوانٹائزیشن حکمت عملی کا انتخاب مخصوص ماڈل آرکیٹیکچر، کارکردگی کی ضروریات، اور ہدف ڈیوائس کی ہارڈویئر کی صلاحیتوں پر منحصر ہے۔

### ماڈل کمپریشن اور اصلاح

کوانٹائزیشن کے علاوہ، مختلف کمپریشن تکنیکیں ماڈل کے سائز اور کمپیوٹیشنل ضروریات کو کم کرنے میں مدد کرتی ہیں۔ ان میں شامل ہیں:

**پروننگ**: یہ تکنیک نیورل نیٹ ورکس سے غیر ضروری کنکشنز یا نیورونز کو ہٹاتی ہے۔ ان پیرامیٹرز کی شناخت اور ختم کرکے جو ماڈل کی کارکردگی میں کم حصہ ڈالتے ہیں، پروننگ ماڈل کے سائز کو نمایاں طور پر کم کر سکتی ہے جبکہ درستگی کو برقرار رکھتی ہے۔

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**نالج ڈسٹلیشن**: اس طریقہ کار میں ایک چھوٹے "اسٹوڈنٹ" ماڈل کو ایک بڑے "ٹیچر" ماڈل کے رویے کی نقل کرنے کے لیے تربیت دی جاتی ہے۔ اسٹوڈنٹ ماڈل ٹیچر کے آؤٹ پٹس کو تقریباً حاصل کرنے کے لیے سیکھتا ہے، اکثر کم پیرامیٹرز کے ساتھ اسی طرح کی کارکردگی حاصل کرتا ہے۔

**ماڈل آرکیٹیکچر کی اصلاح**: محققین نے خاص طور پر ایج تعیناتی کے لیے ڈیزائن کردہ خصوصی آرکیٹیکچرز تیار کیے ہیں، جیسے MobileNets، EfficientNets، اور دیگر ہلکے آرکیٹیکچرز جو کارکردگی کو کمپیوٹیشنل کارکردگی کے ساتھ متوازن کرتے ہیں۔

### چھوٹے لینگویج ماڈلز (SLMs)

ایج اے آئی میں ایک ابھرتا ہوا رجحان چھوٹے لینگویج ماڈلز (SLMs) کی ترقی ہے۔ یہ ماڈلز شروع سے ہی کمپیکٹ اور موثر ہونے کے لیے ڈیزائن کیے گئے ہیں جبکہ اب بھی معنی خیز قدرتی زبان کی صلاحیتیں فراہم کرتے ہیں۔ SLMs یہ حاصل کرتے ہیں محتاط آرکیٹیکچرل انتخاب، موثر تربیتی تکنیکوں، اور مخصوص ڈومینز یا کاموں پر مرکوز تربیت کے ذریعے۔

روایتی طریقوں کے برعکس جو بڑے ماڈلز کو کمپریس کرنے میں شامل ہوتے ہیں، SLMs اکثر چھوٹے ڈیٹا سیٹس اور خاص طور پر ایج تعیناتی کے لیے ڈیزائن کردہ بہتر آرکیٹیکچرز کے ساتھ تربیت یافتہ ہوتے ہیں۔ یہ طریقہ ایسے ماڈلز کا نتیجہ دے سکتا ہے جو نہ صرف چھوٹے ہوں بلکہ مخصوص استعمال کے معاملات کے لیے زیادہ موثر ہوں۔

## ایج اے آئی کے لیے ہارڈویئر ایکسیلیریشن

جدید ایج ڈیوائسز میں تیزی سے خصوصی ہارڈویئر شامل ہوتا ہے جو اے آئی ورک لوڈز کو تیز کرنے کے لیے ڈیزائن کیا گیا ہے:

### نیورل پروسیسنگ یونٹس (NPUs)

NPUs خاص طور پر نیورل نیٹ ورک کمپیوٹیشنز کے لیے ڈیزائن کردہ پروسیسرز ہیں۔ یہ چپس روایتی CPUs کے مقابلے میں اے آئی انفرنس کے کاموں کو زیادہ مؤثر طریقے سے انجام دے سکتی ہیں، اکثر کم توانائی کی کھپت کے ساتھ۔ بہت سے جدید اسمارٹ فونز، لیپ ٹاپ، اور IoT ڈیوائسز اب NPUs شامل کرتے ہیں تاکہ ڈیوائس پر اے آئی پروسیسنگ کو ممکن بنایا جا سکے۔

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

NPUs والے ڈیوائسز شامل ہیں:

- **ایپل**: A-series اور M-series چپس نیورل انجن کے ساتھ
- **کوالکوم**: Snapdragon پروسیسرز Hexagon DSP/NPU کے ساتھ
- **سامسنگ**: Exynos پروسیسرز NPU کے ساتھ
- **انٹیل**: Movidius VPUs اور Habana Labs ایکسیلیریٹرز
- **مائیکروسافٹ**: Windows Copilot+ PCs NPUs کے ساتھ

### 🎮 GPU ایکسیلیریشن

اگرچہ ایج ڈیوائسز میں ڈیٹا سینٹرز میں پائے جانے والے طاقتور GPUs نہیں ہوتے، بہت سے اب بھی مربوط یا الگ GPUs شامل کرتے ہیں جو اے آئی ورک لوڈز کو تیز کر سکتے ہیں۔ جدید موبائل GPUs اور مربوط گرافکس پروسیسرز اے آئی انفرنس کے کاموں کے لیے نمایاں کارکردگی میں بہتری فراہم کر سکتے ہیں۔

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU کی اصلاح

یہاں تک کہ صرف CPU والے ڈیوائسز بھی ایج اے آئی کے ذریعے بہتر نفاذ سے فائدہ اٹھا سکتے ہیں۔ جدید CPUs میں اے آئی ورک لوڈز کے لیے خصوصی ہدایات شامل ہیں، اور سافٹ ویئر فریم ورک تیار کیے گئے ہیں تاکہ اے آئی انفرنس کے لیے CPU کی کارکردگی کو زیادہ سے زیادہ کیا جا سکے۔

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

ایج اے آئی کے ساتھ کام کرنے والے سافٹ ویئر انجینئرز کے لیے، ان ہارڈویئر ایکسیلیریشن کے اختیارات کو استعمال کرنے کا طریقہ سمجھنا ہدف ڈیوائسز پر انفرنس کی کارکردگی اور توانائی کی کارکردگی کو بہتر بنانے کے لیے اہم ہے۔

## ایج اے آئی کے فوائد

### پرائیویسی اور سیکیورٹی

ایج اے آئی کا سب سے اہم فائدہ بہتر پرائیویسی اور سیکیورٹی ہے۔ ڈیٹا کو مقامی طور پر ڈیوائس پر پروسیس کرکے، حساس معلومات کبھی بھی صارف کے کنٹرول سے باہر نہیں جاتی۔ یہ خاص طور پر ذاتی ڈیٹا، طبی معلومات، یا خفیہ کاروباری ڈیٹا کو سنبھالنے والی ایپلیکیشنز کے لیے اہم ہے۔

### لیٹنسی میں کمی

ایج اے آئی ڈیٹا کو پروسیسنگ کے لیے دور دراز سرورز پر بھیجنے کی ضرورت کو ختم کرتا ہے، لیٹنسی کو نمایاں طور پر کم کرتا ہے۔ یہ خود مختار گاڑیاں، صنعتی آٹومیشن، یا انٹرایکٹو ایپلیکیشنز جیسے ریئل ٹائم ایپلیکیشنز کے لیے اہم ہے جہاں فوری ردعمل کی ضرورت ہوتی ہے۔

### آف لائن صلاحیت

ایج اے آئی
- [02: ایج اے آئی ایپلیکیشنز](02.RealWorldCaseStudies.md)

---

**اعلانِ لاتعلقی**:  
یہ دستاویز AI ترجمہ سروس [Co-op Translator](https://github.com/Azure/co-op-translator) کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے لیے ہم ذمہ دار نہیں ہیں۔