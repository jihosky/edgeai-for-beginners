<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T12:57:43+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "th"
}
-->
# ส่วนที่ 03 - การผสานรวม Model Context Protocol (MCP)

## แนะนำ MCP (Model Context Protocol)

Model Context Protocol (MCP) เป็นมาตรฐานโอเพ่นซอร์สสำหรับการเชื่อมต่อแอปพลิเคชัน AI กับระบบภายนอก โดยใช้ MCP แอปพลิเคชัน AI เช่น Claude หรือ ChatGPT สามารถเชื่อมต่อกับแหล่งข้อมูล (เช่น ไฟล์ในเครื่อง ฐานข้อมูล) เครื่องมือ (เช่น เครื่องมือค้นหา เครื่องคิดเลข) และเวิร์กโฟลว์ (เช่น คำสั่งพิเศษ) เพื่อให้เข้าถึงข้อมูลสำคัญและดำเนินการต่าง ๆ ได้

ลองนึกถึง MCP เหมือนกับ **พอร์ต USB-C สำหรับแอปพลิเคชัน AI** เช่นเดียวกับที่ USB-C ให้วิธีการเชื่อมต่ออุปกรณ์อิเล็กทรอนิกส์แบบมาตรฐาน MCP ก็ให้วิธีการเชื่อมต่อแอปพลิเคชัน AI กับระบบภายนอกแบบมาตรฐานเช่นกัน

### MCP สามารถทำอะไรได้บ้าง?

MCP ปลดล็อกความสามารถที่ทรงพลังสำหรับแอปพลิเคชัน AI:

- **ผู้ช่วย AI ส่วนบุคคล**: เอเจนต์สามารถเข้าถึง Google Calendar และ Notion ของคุณ ทำหน้าที่เป็นผู้ช่วย AI ที่ปรับแต่งได้มากขึ้น
- **การสร้างโค้ดขั้นสูง**: Claude Code สามารถสร้างเว็บแอปทั้งหมดโดยใช้การออกแบบจาก Figma
- **การผสานรวมข้อมูลระดับองค์กร**: แชทบอทองค์กรสามารถเชื่อมต่อกับฐานข้อมูลหลายแห่งในองค์กร ช่วยให้ผู้ใช้วิเคราะห์ข้อมูลผ่านการแชท
- **เวิร์กโฟลว์สร้างสรรค์**: โมเดล AI สามารถสร้างการออกแบบ 3D บน Blender และพิมพ์ออกมาโดยใช้เครื่องพิมพ์ 3D
- **การเข้าถึงข้อมูลแบบเรียลไทม์**: เชื่อมต่อกับแหล่งข้อมูลภายนอกเพื่อข้อมูลที่ทันสมัย
- **การดำเนินการหลายขั้นตอนที่ซับซ้อน**: ดำเนินการเวิร์กโฟลว์ที่ซับซ้อนโดยรวมเครื่องมือและระบบหลายตัว

### ทำไม MCP ถึงสำคัญ?

MCP ให้ประโยชน์ในทุกส่วนของระบบนิเวศ:

**สำหรับนักพัฒนา**: MCP ลดเวลาและความซับซ้อนในการพัฒนาเมื่อสร้างหรือผสานรวมกับแอปพลิเคชัน AI หรือเอเจนต์

**สำหรับแอปพลิเคชัน AI**: MCP ให้การเข้าถึงระบบนิเวศของแหล่งข้อมูล เครื่องมือ และแอปพลิเคชันที่ช่วยเพิ่มความสามารถและปรับปรุงประสบการณ์ของผู้ใช้

**สำหรับผู้ใช้ปลายทาง**: MCP ส่งผลให้แอปพลิเคชัน AI หรือเอเจนต์มีความสามารถมากขึ้น สามารถเข้าถึงข้อมูลของคุณและดำเนินการแทนคุณเมื่อจำเป็น

## Small Language Models (SLMs) ใน MCP

Small Language Models เป็นวิธีการที่มีประสิทธิภาพในการใช้งาน AI โดยมีข้อดีหลายประการ:

### ข้อดีของ SLMs
- **ประหยัดทรัพยากร**: ต้องการทรัพยากรคอมพิวเตอร์น้อยลง
- **ตอบสนองเร็วขึ้น**: ลดความล่าช้าสำหรับแอปพลิเคชันแบบเรียลไทม์  
- **คุ้มค่า**: ต้องการโครงสร้างพื้นฐานน้อย
- **ความเป็นส่วนตัว**: สามารถทำงานในเครื่องโดยไม่ต้องส่งข้อมูล
- **ปรับแต่งได้**: ปรับแต่งได้ง่ายสำหรับโดเมนเฉพาะ

### ทำไม SLMs ถึงทำงานได้ดีร่วมกับ MCP

SLMs ที่จับคู่กับ MCP สร้างการผสมผสานที่ทรงพลัง โดยความสามารถในการให้เหตุผลของโมเดลได้รับการเสริมด้วยเครื่องมือภายนอก ชดเชยจำนวนพารามิเตอร์ที่น้อยลงด้วยฟังก์ชันที่เพิ่มขึ้น

## ภาพรวม Python MCP SDK

Python MCP SDK เป็นพื้นฐานสำหรับการสร้างแอปพลิเคชันที่รองรับ MCP SDK รวมถึง:

- **ไลบรารีไคลเอนต์**: สำหรับการเชื่อมต่อกับเซิร์ฟเวอร์ MCP
- **เฟรมเวิร์กเซิร์ฟเวอร์**: สำหรับการสร้างเซิร์ฟเวอร์ MCP แบบกำหนดเอง
- **ตัวจัดการโปรโตคอล**: สำหรับการจัดการการสื่อสาร
- **การผสานรวมเครื่องมือ**: สำหรับการดำเนินการฟังก์ชันภายนอก

## การใช้งานจริง: Phi-4 MCP Client

มาสำรวจการใช้งานจริงโดยใช้โมเดลขนาดเล็ก Phi-4 ของ Microsoft ที่ผสานรวมความสามารถ MCP

### ภาพรวมสถาปัตยกรรม MCP

MCP ใช้ **สถาปัตยกรรมไคลเอนต์-เซิร์ฟเวอร์** โดยที่โฮสต์ MCP (แอปพลิเคชัน AI เช่น Claude Code หรือ Claude Desktop) สร้างการเชื่อมต่อกับเซิร์ฟเวอร์ MCP หนึ่งตัวหรือมากกว่า โฮสต์ MCP ทำสิ่งนี้โดยสร้างไคลเอนต์ MCP หนึ่งตัวสำหรับแต่ละเซิร์ฟเวอร์ MCP

#### ผู้เข้าร่วมหลัก

- **โฮสต์ MCP**: แอปพลิเคชัน AI ที่ประสานงานและจัดการไคลเอนต์ MCP หนึ่งตัวหรือหลายตัว
- **ไคลเอนต์ MCP**: ส่วนประกอบที่รักษาการเชื่อมต่อกับเซิร์ฟเวอร์ MCP และรับบริบทจากเซิร์ฟเวอร์ MCP เพื่อให้โฮสต์ MCP ใช้
- **เซิร์ฟเวอร์ MCP**: โปรแกรมที่ให้บริบทแก่ไคลเอนต์ MCP

#### สถาปัตยกรรมสองชั้น

MCP ประกอบด้วยสองชั้นที่แตกต่างกัน:

**ชั้นข้อมูล**: กำหนดโปรโตคอล JSON-RPC สำหรับการสื่อสารระหว่างไคลเอนต์และเซิร์ฟเวอร์ รวมถึง:
- การจัดการวงจรชีวิต (การเริ่มต้นการเชื่อมต่อ การเจรจาความสามารถ)
- องค์ประกอบหลัก (เครื่องมือ ทรัพยากร คำสั่ง)
- คุณสมบัติไคลเอนต์ (การสุ่มตัวอย่าง การรวบรวมข้อมูล การบันทึก)
- คุณสมบัติยูทิลิตี้ (การแจ้งเตือน การติดตามความคืบหน้า)

**ชั้นการขนส่ง**: กำหนดกลไกและช่องทางการสื่อสาร:
- **STDIO Transport**: ใช้สตรีมอินพุต/เอาต์พุตมาตรฐานสำหรับกระบวนการในเครื่อง (ประสิทธิภาพสูงสุด ไม่มีค่าใช้จ่ายเครือข่าย)
- **Streamable HTTP Transport**: ใช้ HTTP POST พร้อม Server-Sent Events สำหรับเซิร์ฟเวอร์ระยะไกล (รองรับการตรวจสอบสิทธิ์ HTTP มาตรฐาน)

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### องค์ประกอบหลักของ MCP

MCP กำหนดองค์ประกอบที่ระบุประเภทของข้อมูลบริบทที่สามารถแชร์กับแอปพลิเคชัน AI และช่วงของการดำเนินการที่สามารถทำได้

#### องค์ประกอบเซิร์ฟเวอร์

MCP กำหนดองค์ประกอบหลักสามอย่างที่เซิร์ฟเวอร์สามารถเปิดเผยได้:

**เครื่องมือ**: ฟังก์ชันที่สามารถดำเนินการได้ที่แอปพลิเคชัน AI สามารถเรียกใช้เพื่อดำเนินการ
- ตัวอย่าง: การดำเนินการไฟล์ การเรียก API การสืบค้นฐานข้อมูล
- วิธีการ: `tools/list`, `tools/call`
- รองรับการค้นพบและการดำเนินการแบบไดนามิก

**ทรัพยากร**: แหล่งข้อมูลที่ให้ข้อมูลบริบทแก่แอปพลิเคชัน AI
- ตัวอย่าง: เนื้อหาไฟล์ บันทึกฐานข้อมูล การตอบสนอง API
- วิธีการ: `resources/list`, `resources/read`
- ช่วยให้เข้าถึงข้อมูลที่มีโครงสร้าง

**คำสั่ง**: เทมเพลตที่นำกลับมาใช้ใหม่ได้ที่ช่วยจัดโครงสร้างการโต้ตอบกับโมเดลภาษา
- ตัวอย่าง: คำสั่งระบบ ตัวอย่าง few-shot
- วิธีการ: `prompts/list`, `prompts/get`
- ทำให้รูปแบบการโต้ตอบ AI เป็นมาตรฐาน

#### องค์ประกอบไคลเอนต์

MCP ยังกำหนดองค์ประกอบที่ไคลเอนต์สามารถเปิดเผยเพื่อให้การโต้ตอบสมบูรณ์ยิ่งขึ้น:

**การสุ่มตัวอย่าง**: อนุญาตให้เซิร์ฟเวอร์ร้องขอการเติมเต็มโมเดลภาษาจากแอปพลิเคชัน AI ของไคลเอนต์
- วิธีการ: `sampling/complete`
- ช่วยให้การพัฒนาเซิร์ฟเวอร์ไม่ขึ้นกับโมเดล
- ให้การเข้าถึงโมเดลภาษาของโฮสต์

**การรวบรวมข้อมูล**: อนุญาตให้เซิร์ฟเวอร์ร้องขอข้อมูลเพิ่มเติมจากผู้ใช้
- วิธีการ: `elicitation/request`
- ช่วยให้การโต้ตอบและการยืนยันของผู้ใช้
- รองรับการรวบรวมข้อมูลแบบไดนามิก

**การบันทึก**: อนุญาตให้เซิร์ฟเวอร์ส่งข้อความบันทึกไปยังไคลเอนต์
- ใช้สำหรับการดีบักและการตรวจสอบ
- ให้การมองเห็นการดำเนินการของเซิร์ฟเวอร์

### วงจรโปรโตคอล MCP

#### การเริ่มต้นและการเจรจาความสามารถ

MCP เป็นโปรโตคอลที่มีสถานะซึ่งต้องการการจัดการวงจรชีวิต กระบวนการเริ่มต้นมีวัตถุประสงค์สำคัญหลายประการ:

1. **การเจรจาเวอร์ชันโปรโตคอล**: ตรวจสอบให้แน่ใจว่าไคลเอนต์และเซิร์ฟเวอร์ใช้เวอร์ชันโปรโตคอลที่เข้ากันได้ (เช่น "2025-06-18")
2. **การค้นพบความสามารถ**: แต่ละฝ่ายประกาศคุณสมบัติและองค์ประกอบที่รองรับ
3. **การแลกเปลี่ยนตัวตน**: ให้ข้อมูลการระบุตัวตนและเวอร์ชัน

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### การค้นหาและการดำเนินการเครื่องมือ

หลังจากการเริ่มต้น ไคลเอนต์สามารถค้นหาและดำเนินการเครื่องมือ:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### การแจ้งเตือนแบบเรียลไทม์

MCP รองรับการแจ้งเตือนแบบเรียลไทม์สำหรับการอัปเดตแบบไดนามิก:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## เริ่มต้นใช้งาน: คู่มือทีละขั้นตอน

### ขั้นตอนที่ 1: การตั้งค่าสภาพแวดล้อม

ติดตั้งการพึ่งพาที่จำเป็น:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### ขั้นตอนที่ 2: การกำหนดค่าพื้นฐาน

ตั้งค่าตัวแปรสภาพแวดล้อมของคุณ:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### ขั้นตอนที่ 3: การเรียกใช้ไคลเอนต์ MCP ครั้งแรกของคุณ

**การตั้งค่า Ollama พื้นฐาน:**
```bash
python ghmodel_mcp_demo.py
```

**การใช้ vLLM Backend:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**การเชื่อมต่อ Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**เซิร์ฟเวอร์ MCP แบบกำหนดเอง:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### ขั้นตอนที่ 4: การใช้งานเชิงโปรแกรม

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## ฟีเจอร์ขั้นสูง

### รองรับหลาย Backend

การใช้งานรองรับทั้ง Ollama และ vLLM backend ช่วยให้คุณเลือกได้ตามความต้องการ:

- **Ollama**: เหมาะสำหรับการพัฒนาและทดสอบในเครื่อง
- **vLLM**: ปรับให้เหมาะสำหรับการใช้งานในระบบผลิตและสถานการณ์ที่มีปริมาณงานสูง

### โปรโตคอลการเชื่อมต่อที่ยืดหยุ่น

รองรับโหมดการเชื่อมต่อสองแบบ:

**โหมด STDIO**: การสื่อสารกระบวนการโดยตรง
- ความล่าช้าต่ำกว่า
- เหมาะสำหรับเครื่องมือในเครื่อง
- การตั้งค่าง่าย

**โหมด SSE**: การสตรีมแบบ HTTP
- รองรับเครือข่าย
- เหมาะสำหรับระบบกระจาย
- การอัปเดตแบบเรียลไทม์

### ความสามารถในการผสานรวมเครื่องมือ

ระบบสามารถผสานรวมกับเครื่องมือต่าง ๆ:
- ระบบอัตโนมัติบนเว็บ (Playwright)
- การดำเนินการไฟล์
- การโต้ตอบ API
- คำสั่งระบบ
- ฟังก์ชันแบบกำหนดเอง

## การจัดการข้อผิดพลาดและแนวทางปฏิบัติที่ดีที่สุด

### การจัดการข้อผิดพลาดอย่างครอบคลุม

การใช้งานรวมถึงการจัดการข้อผิดพลาดที่แข็งแกร่งสำหรับ:

**ข้อผิดพลาดการเชื่อมต่อ:**
- เซิร์ฟเวอร์ MCP ล้มเหลว
- การหมดเวลาของเครือข่าย
- ปัญหาการเชื่อมต่อ

**ข้อผิดพลาดการดำเนินการเครื่องมือ:**
- เครื่องมือที่ขาดหายไป
- การตรวจสอบพารามิเตอร์
- ความล้มเหลวในการดำเนินการ

**ข้อผิดพลาดการประมวลผลการตอบสนอง:**
- ปัญหาการแยกวิเคราะห์ JSON
- ความไม่สอดคล้องของรูปแบบ
- ความผิดปกติของการตอบสนอง LLM

### แนวทางปฏิบัติที่ดีที่สุด

1. **การจัดการทรัพยากร**: ใช้ตัวจัดการบริบทแบบอะซิงโครนัส
2. **การจัดการข้อผิดพลาด**: ใช้บล็อก try-catch อย่างครอบคลุม
3. **การบันทึก**: เปิดใช้งานระดับการบันทึกที่เหมาะสม
4. **ความปลอดภัย**: ตรวจสอบความถูกต้องของอินพุตและทำความสะอาดเอาต์พุต
5. **ประสิทธิภาพ**: ใช้การรวมการเชื่อมต่อและการแคช

## การใช้งานจริง

### ระบบอัตโนมัติบนเว็บ
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### การประมวลผลข้อมูล
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### การผสานรวม API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## การปรับปรุงประสิทธิภาพ

### การจัดการหน่วยความจำ
- การจัดการประวัติข้อความอย่างมีประสิทธิภาพ
- การทำความสะอาดทรัพยากรอย่างเหมาะสม
- การรวมการเชื่อมต่อ

### การปรับปรุงเครือข่าย
- การดำเนินการ HTTP แบบอะซิงโครนัส
- การตั้งค่าการหมดเวลาที่กำหนดค่าได้
- การกู้คืนข้อผิดพลาดอย่างราบรื่น

### การประมวลผลพร้อมกัน
- I/O แบบไม่บล็อก
- การดำเนินการเครื่องมือแบบขนาน
- รูปแบบอะซิงโครนัสที่มีประสิทธิภาพ

## การพิจารณาด้านความปลอดภัย

### การปกป้องข้อมูล
- การจัดการคีย์ API อย่างปลอดภัย
- การตรวจสอบความถูกต้องของอินพุต
- การทำความสะอาดเอาต์พุต

### ความปลอดภัยของเครือข่าย
- รองรับ HTTPS
- ค่าเริ่มต้นของจุดสิ้นสุดในเครื่อง
- การจัดการโทเค็นอย่างปลอดภัย

### ความปลอดภัยในการดำเนินการ
- การกรองเครื่องมือ
- สภาพแวดล้อมที่แยกกัน
- การบันทึกการตรวจสอบ

## ระบบนิเวศ MCP และการพัฒนา

### ขอบเขตโครงการ MCP

ระบบนิเวศ Model Context Protocol รวมถึงองค์ประกอบสำคัญหลายประการ:

- **[MCP Specification](https://modelcontextprotocol.io/specification/latest)**: ข้อกำหนดอย่างเป็นทางการที่ระบุข้อกำหนดการใช้งานสำหรับไคลเอนต์และเซิร์ฟเวอร์
- **[MCP SDKs](https://modelcontextprotocol.io/docs/sdk)**: SDK สำหรับภาษาการเขียนโปรแกรมต่าง ๆ ที่ใช้งาน MCP
- **เครื่องมือพัฒนา MCP**: เครื่องมือสำหรับการพัฒนาเซิร์ฟเวอร์และไคลเอนต์ MCP รวมถึง [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[MCP Reference Server Implementations](https://github.com/modelcontextprotocol/servers)**: การใช้งานเซิร์ฟเวอร์ MCP แบบอ้างอิง

### เริ่มต้นการพัฒนา MCP

เพื่อเริ่มต้นสร้างด้วย MCP:

**สร้างเซิร์ฟเวอร์**: [สร้างเซิร์ฟเวอร์ MCP](https://modelcontextprotocol.io/docs/develop/build-server) เพื่อเปิดเผยข้อมูลและเครื่องมือของคุณ

**สร้างไคลเอนต์**: [พัฒนาแอปพลิเคชัน](https://modelcontextprotocol.io/docs/develop/build-client) ที่เชื่อมต่อกับเซิร์ฟเวอร์ MCP

**เรียนรู้แนวคิด**: [ทำความเข้าใจแนวคิดหลัก](https://modelcontextprotocol.io/docs/learn/architecture) และสถาปัตยกรรมของ MCP

## สรุป

SLMs ที่ผสานรวมกับ MCP เป็นการเปลี่ยนแปลงรูปแบบในพัฒนาการแอปพลิเค
- **[เอกสาร Ollama](https://ollama.ai/docs)** - แพลตฟอร์มการใช้งาน LLM ในเครื่อง
- **[เอกสาร vLLM](https://docs.vllm.ai/)** - การให้บริการ LLM ประสิทธิภาพสูง

### มาตรฐานและโปรโตคอลทางเทคนิค

- **[JSON-RPC 2.0 Specification](https://www.jsonrpc.org/)** - โปรโตคอล RPC ที่ใช้ใน MCP
- **[JSON Schema](https://json-schema.org/)** - มาตรฐานการกำหนด Schema สำหรับเครื่องมือ MCP
- **[OpenAPI Specification](https://swagger.io/specification/)** - มาตรฐานเอกสาร API
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - มาตรฐานเว็บสำหรับการอัปเดตแบบเรียลไทม์

### การพัฒนา AI Agent

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - การพัฒนา Agent ที่พร้อมใช้งานในระดับการผลิต
- **[เอกสาร LangChain](https://docs.langchain.com/)** - เฟรมเวิร์กสำหรับการรวม Agent และเครื่องมือ
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - SDK การจัดการ AI ของ Microsoft

### รายงานและงานวิจัยในอุตสาหกรรม

- **[ประกาศ Model Context Protocol ของ Anthropic](https://www.anthropic.com/news/model-context-protocol)** - การแนะนำ MCP ครั้งแรก
- **[การสำรวจ Small Language Models](https://arxiv.org/abs/2410.20011)** - การสำรวจงานวิจัย SLM ในเชิงวิชาการ
- **[การวิเคราะห์ตลาด Edge AI](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - แนวโน้มและการคาดการณ์ในอุตสาหกรรม
- **[แนวปฏิบัติที่ดีที่สุดในการพัฒนา AI Agent](https://arxiv.org/abs/2309.02427)** - งานวิจัยเกี่ยวกับสถาปัตยกรรม Agent

ส่วนนี้เป็นพื้นฐานสำหรับการสร้างแอปพลิเคชัน MCP ที่ขับเคลื่อนด้วย SLM ของคุณเอง ซึ่งเปิดโอกาสสำหรับการทำงานอัตโนมัติ การประมวลผลข้อมูล และการรวมระบบอัจฉริยะ

## ➡️ สิ่งที่ควรทำต่อไป

- [โมดูล 7. ตัวอย่าง Edge AI](../Module07/README.md)

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลสำคัญ แนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้