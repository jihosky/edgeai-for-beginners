<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:58:10+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "sk"
}
-->
# Sekcia 1: Základy EdgeAI

EdgeAI predstavuje paradigmatickú zmenu v nasadzovaní umelej inteligencie, prinášajúc schopnosti AI priamo na koncové zariadenia namiesto spoliehania sa výlučne na spracovanie v cloude. Je dôležité pochopiť, ako EdgeAI umožňuje lokálne spracovanie AI na zariadeniach s obmedzenými zdrojmi, pričom si zachováva primeraný výkon a rieši výzvy ako súkromie, latencia a offline schopnosti.

## Úvod

V tejto lekcii preskúmame EdgeAI a jeho základné koncepty. Pokryjeme tradičný paradigmat spracovania AI, výzvy edge computingu, kľúčové technológie umožňujúce EdgeAI a praktické aplikácie naprieč rôznymi odvetviami.

## Ciele učenia

Na konci tejto lekcie budete schopní:

- Pochopiť rozdiel medzi tradičným prístupom AI založeným na cloude a EdgeAI.
- Identifikovať kľúčové technológie umožňujúce spracovanie AI na koncových zariadeniach.
- Rozpoznať výhody a obmedzenia implementácií EdgeAI.
- Aplikovať znalosti o EdgeAI na reálne scenáre a prípady použitia.

## Pochopenie tradičného paradigmu spracovania AI

Tradične sa generatívne aplikácie AI spoliehajú na infraštruktúru vysokovýkonného výpočtového spracovania na efektívne prevádzkovanie veľkých jazykových modelov (LLMs). Organizácie zvyčajne nasadzujú tieto modely na GPU klastroch v cloudových prostrediach, pričom k ich schopnostiam pristupujú prostredníctvom API rozhraní.

Tento centralizovaný model funguje dobre pre mnohé aplikácie, ale má inherentné obmedzenia v scenároch edge computingu. Tradičný prístup zahŕňa odosielanie používateľských dotazov na vzdialené servery, ich spracovanie pomocou výkonného hardvéru a návrat výsledkov cez internet. Hoci táto metóda poskytuje prístup k najmodernejším modelom, vytvára závislosť na internetovom pripojení, prináša obavy z latencie a vyvoláva otázky o súkromí, keď sa citlivé údaje musia prenášať na externé servery.

Existuje niekoľko základných konceptov, ktoré musíme pochopiť pri práci s tradičnými paradigmatmi spracovania AI, konkrétne:

- **☁️ Spracovanie založené na cloude**: AI modely bežia na výkonných serverových infraštruktúrach s vysokými výpočtovými zdrojmi.
- **🔌 Prístup založený na API**: Aplikácie pristupujú k schopnostiam AI prostredníctvom vzdialených API volaní namiesto lokálneho spracovania.
- **🎛️ Centralizovaná správa modelov**: Modely sú udržiavané a aktualizované centrálne, čo zaisťuje konzistenciu, ale vyžaduje sieťové pripojenie.
- **📈 Škálovateľnosť zdrojov**: Cloudová infraštruktúra sa môže dynamicky škálovať na zvládnutie rôznych výpočtových požiadaviek.

## Výzvy edge computingu

Koncové zariadenia, ako sú notebooky, mobilné telefóny a zariadenia internetu vecí (IoT), ako Raspberry Pi a NVIDIA Orin Nano, predstavujú jedinečné výpočtové obmedzenia. Tieto zariadenia majú zvyčajne obmedzený výpočtový výkon, pamäť a energetické zdroje v porovnaní s infraštruktúrou dátových centier.

Prevádzkovanie tradičných LLMs na takýchto zariadeniach bolo historicky náročné kvôli týmto hardvérovým obmedzeniam. Avšak potreba spracovania AI na koncových zariadeniach sa stáva čoraz dôležitejšou v rôznych scenároch. Zvážte situácie, kde je internetové pripojenie nespoľahlivé alebo nedostupné, ako napríklad vzdialené priemyselné lokality, vozidlá v pohybe alebo oblasti so slabým pokrytím siete. Okrem toho aplikácie vyžadujúce vysoké bezpečnostné štandardy, ako sú zdravotnícke zariadenia, finančné systémy alebo vládne aplikácie, môžu potrebovať spracovávať citlivé údaje lokálne, aby si zachovali súkromie a splnili požiadavky na súlad.

### Kľúčové obmedzenia edge computingu

Prostredia edge computingu čelia niekoľkým základným obmedzeniam, ktoré tradičné cloudové AI riešenia nepoznajú:

- **Obmedzený výpočtový výkon**: Koncové zariadenia majú zvyčajne menej CPU jadier a nižšie taktovacie frekvencie v porovnaní s hardvérom na úrovni serverov.
- **Pamäťové obmedzenia**: Dostupná RAM a kapacita úložiska sú na koncových zariadeniach výrazne znížené.
- **Energetické obmedzenia**: Zariadenia napájané batériou musia vyvážiť výkon so spotrebou energie pre dlhšiu prevádzku.
- **Tepelné riadenie**: Kompaktné formáty obmedzujú schopnosti chladenia, čo ovplyvňuje udržateľný výkon pri zaťažení.

## Čo je EdgeAI?

### Koncept: Definícia Edge AI

Edge AI sa týka nasadenia a vykonávania algoritmov umelej inteligencie priamo na koncových zariadeniach—fyzickom hardvéri, ktorý existuje na "okraji" siete, blízko miesta, kde sa generujú a zhromažďujú údaje. Tieto zariadenia zahŕňajú smartfóny, IoT senzory, inteligentné kamery, autonómne vozidlá, nositeľné zariadenia a priemyselné vybavenie. Na rozdiel od tradičných AI systémov, ktoré sa spoliehajú na cloudové servery na spracovanie, Edge AI prináša inteligenciu priamo k zdroju údajov.

V jadre Edge AI ide o decentralizáciu spracovania AI, presun od centralizovaných dátových centier a distribúciu naprieč rozsiahlym sieťovým ekosystémom zariadení. To predstavuje zásadnú architektonickú zmenu v tom, ako sú AI systémy navrhnuté a nasadené.

Kľúčové konceptuálne piliere Edge AI zahŕňajú:

- **Spracovanie v blízkosti**: Výpočty prebiehajú fyzicky blízko miesta, kde údaje vznikajú.
- **Decentralizovaná inteligencia**: Schopnosti rozhodovania sú distribuované medzi viaceré zariadenia.
- **Suverenita údajov**: Informácie zostávajú pod lokálnou kontrolou, často nikdy neopúšťajú zariadenie.
- **Autonómna prevádzka**: Zariadenia môžu fungovať inteligentne bez potreby neustáleho pripojenia.
- **Vstavaná AI**: Inteligencia sa stáva vnútornou schopnosťou každodenných zariadení.

### Vizualizácia architektúry Edge AI

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI predstavuje paradigmatickú zmenu v nasadzovaní umelej inteligencie, prinášajúc schopnosti AI priamo na koncové zariadenia namiesto spoliehania sa výlučne na spracovanie v cloude. Tento prístup umožňuje AI modelom bežať lokálne na zariadeniach s obmedzenými výpočtovými zdrojmi, poskytujúc schopnosti inferencie v reálnom čase bez potreby neustáleho internetového pripojenia.

EdgeAI zahŕňa rôzne technológie a techniky navrhnuté na to, aby AI modely boli efektívnejšie a vhodné na nasadenie na zariadeniach s obmedzenými zdrojmi. Cieľom je zachovať primeraný výkon pri výraznom znížení výpočtových a pamäťových požiadaviek AI modelov.

Pozrime sa na základné prístupy, ktoré umožňujú implementácie EdgeAI naprieč rôznymi typmi zariadení a prípadmi použitia.

### Základné princípy EdgeAI

EdgeAI je postavené na niekoľkých základných princípoch, ktoré ho odlišujú od tradičného cloudového AI:

- **Lokálne spracovanie**: Inferencia AI prebieha priamo na koncovom zariadení bez potreby externého pripojenia.
- **Optimalizácia zdrojov**: Modely sú špecificky optimalizované pre hardvérové obmedzenia cieľových zariadení.
- **Výkon v reálnom čase**: Spracovanie prebieha s minimálnou latenciou pre aplikácie citlivé na čas.
- **Súkromie ako základ**: Citlivé údaje zostávajú na zariadení, čím sa zvyšuje bezpečnosť a súlad.

## Kľúčové technológie umožňujúce EdgeAI

### Kvantizácia modelov

Jednou z najdôležitejších techník v EdgeAI je kvantizácia modelov. Tento proces zahŕňa zníženie presnosti parametrov modelu, zvyčajne z 32-bitových čísel s pohyblivou desatinnou čiarkou na 8-bitové celé čísla alebo dokonca formáty s nižšou presnosťou. Hoci sa toto zníženie presnosti môže zdať znepokojujúce, výskum ukázal, že mnohé AI modely si dokážu zachovať svoj výkon aj pri výrazne zníženej presnosti.

Kvantizácia funguje mapovaním rozsahu hodnôt s pohyblivou desatinnou čiarkou na menšiu množinu diskrétnych hodnôt. Napríklad namiesto použitia 32 bitov na reprezentáciu každého parametra môže kvantizácia použiť iba 8 bitov, čo vedie k 4-násobnému zníženiu pamäťových požiadaviek a často k rýchlejším časom inferencie.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Rôzne techniky kvantizácie zahŕňajú:

- **Post-Training Quantization (PTQ)**: Aplikovaná po tréningu modelu bez potreby opätovného tréningu.
- **Quantization-Aware Training (QAT)**: Zahŕňa účinky kvantizácie počas tréningu pre lepšiu presnosť.
- **Dynamická kvantizácia**: Kvantizuje váhy na int8, ale aktivácie počíta dynamicky.
- **Statická kvantizácia**: Predpočítava všetky kvantizačné parametre pre váhy aj aktivácie.

Pre nasadenia EdgeAI výber vhodnej stratégie kvantizácie závisí od konkrétnej architektúry modelu, požiadaviek na výkon a hardvérových schopností cieľového zariadenia.

### Kompresia a optimalizácia modelov

Okrem kvantizácie rôzne techniky kompresie pomáhajú znižovať veľkosť modelu a výpočtové požiadavky. Patria sem:

**Prerezávanie**: Táto technika odstraňuje nepotrebné spojenia alebo neuróny z neurónových sietí. Identifikáciou a elimináciou parametrov, ktoré málo prispievajú k výkonu modelu, môže prerezávanie výrazne znížiť veľkosť modelu pri zachovaní presnosti.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Destilácia znalostí**: Tento prístup zahŕňa tréning menšieho "študentského" modelu na napodobnenie správania väčšieho "učiteľského" modelu. Študentský model sa učí približovať výstupy učiteľa, často dosahujúc podobný výkon s výrazne menším počtom parametrov.

**Optimalizácia architektúry modelu**: Výskumníci vyvinuli špecializované architektúry navrhnuté špecificky pre nasadenie na koncových zariadeniach, ako sú MobileNets, EfficientNets a ďalšie ľahké architektúry, ktoré vyvažujú výkon s výpočtovou efektivitou.

### Malé jazykové modely (SLMs)

Vznikajúcim trendom v EdgeAI je vývoj malých jazykových modelov (SLMs). Tieto modely sú od základu navrhnuté tak, aby boli kompaktné a efektívne, pričom stále poskytujú zmysluplné schopnosti spracovania prirodzeného jazyka. SLMs to dosahujú prostredníctvom premyslených architektonických rozhodnutí, efektívnych tréningových techník a zameraného tréningu na konkrétne oblasti alebo úlohy.

Na rozdiel od tradičných prístupov, ktoré zahŕňajú kompresiu veľkých modelov, SLMs sú často trénované na menších datasetoch a optimalizovaných architektúrach špecificky navrhnutých pre nasadenie na koncových zariadeniach. Tento prístup môže viesť k modelom, ktoré sú nielen menšie, ale aj efektívnejšie pre konkrétne prípady použitia.

## Hardvérová akcelerácia pre EdgeAI

Moderné koncové zariadenia čoraz viac zahŕňajú špecializovaný hardvér navrhnutý na akceleráciu AI úloh:

### Neurónové procesorové jednotky (NPUs)

NPUs sú špecializované procesory navrhnuté špeciálne pre výpočty neurónových sietí. Tieto čipy dokážu vykonávať úlohy inferencie AI oveľa efektívnejšie ako tradičné CPU, často s nižšou spotrebou energie. Mnohé moderné smartfóny, notebooky a IoT zariadenia teraz zahŕňajú NPUs na umožnenie spracovania AI priamo na zariadení.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Zariadenia s NPUs zahŕňajú:

- **Apple**: Čipy série A a M s Neural Engine
- **Qualcomm**: Procesory Snapdragon s Hexagon DSP/NPU
- **Samsung**: Procesory Exynos s NPU
- **Intel**: Movidius VPUs a akcelerátory Habana Labs
- **Microsoft**: Windows Copilot+ PC s NPUs

### 🎮 Akcelerácia GPU

Hoci koncové zariadenia nemusia mať výkonné GPU, ktoré sa nachádzajú v dátových centrách, mnohé stále zahŕňajú integrované alebo diskrétne GPU, ktoré dokážu akcelerovať AI úlohy. Moderné mobilné GPU a integrované grafické procesory môžu poskytnúť významné zlepšenia výkonu pre úlohy inferencie AI.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### Optimalizácia CPU

Dokonca aj zariadenia iba s CPU môžu profitovať z EdgeAI prostredníctvom optimalizovaných implementácií. Moderné CPU zahŕňajú špecializované inštrukcie pre AI úlohy a boli vyvinuté softvérové rámce na maximalizáciu výkonu CPU pre inferenciu AI.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Pre softvérových inžinierov pracujúcich s EdgeAI je kritické pochopiť, ako využiť tieto možnosti hardvérovej akcelerácie na optimalizáciu výkonu inferencie a energetickej efektivity na cieľových zariadeniach.

## Výhody EdgeAI

### Súkromie a bezpečnosť

Jednou z najvýznamnejších výhod EdgeAI je zvýšené súkromie a bezpečnosť. Spracovaním údajov lokálne na zariadení citlivé informácie nikdy neopustia kontrolu používateľa. To je obzvlášť dôležité pre aplikácie, ktoré pracujú s osobnými údajmi, zdravotníckymi informáciami alebo dôvernými obchodnými údajmi.

### Znížená latencia

EdgeAI eliminuje potrebu odosielania údajov na vzdialené servery na spracovanie, čím výrazne znižuje latenciu. To je kľúčové pre aplikácie v reálnom čase, ako sú autonómne vozidlá, priemyselná automatizácia alebo interaktívne aplikácie, kde sú potrebné okamžité reakcie.

### Schopnosť pracovať offline

EdgeAI umožňuje funkčnosť AI aj vtedy, keď nie je dostupné internetové pripojenie. To je cenné pre aplikácie na vzdialených miestach, počas cestovania alebo v situáciách, kde je spoľahlivosť siete otázna.

### Nákladová efektívnosť

Znížením závislosti na cloudových AI službách môže EdgeAI pomôcť znížiť prevádzkové náklady, najmä pre aplikácie s vysokým objemom používania. Organizácie môžu vyhnúť sa neustálym nákladom na API a znížiť požiadavky na šírku pásma.

### Škálovateľnosť

EdgeAI rozdeľuje výpočtovú záťaž medzi koncové zariadenia namiesto jej
- [02: EdgeAI Aplikácie](02.RealWorldCaseStudies.md)

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nenesieme zodpovednosť za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.