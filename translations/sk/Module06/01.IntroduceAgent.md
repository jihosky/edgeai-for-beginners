<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T14:21:25+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "sk"
}
-->
# AI agenti a malé jazykové modely: Komplexný sprievodca

## Úvod

V tomto tutoriáli preskúmame AI agentov a malé jazykové modely (SLM) spolu s ich pokročilými implementačnými stratégiami pre prostredia edge computingu. Pokryjeme základné koncepty agentickej AI, optimalizačné techniky SLM, praktické stratégie nasadenia pre zariadenia s obmedzenými zdrojmi a Microsoft Agent Framework na vytváranie produkčne pripravených agentových systémov.

Krajina umelej inteligencie zažíva paradigmatický posun v roku 2025. Zatiaľ čo rok 2023 bol rokom chatbotov a rok 2024 zaznamenal boom kopilotov, rok 2025 patrí AI agentom — inteligentným systémom, ktoré myslia, uvažujú, plánujú, používajú nástroje a vykonávajú úlohy s minimálnym zásahom človeka, čoraz viac poháňané efektívnymi malými jazykovými modelmi. Microsoft Agent Framework sa objavuje ako vedúce riešenie na vytváranie týchto inteligentných systémov s offline schopnosťami na edge zariadeniach.

## Ciele učenia

Na konci tohto tutoriálu budete schopní:

- 🤖 Pochopiť základné koncepty AI agentov a agentických systémov
- 🔬 Identifikovať výhody malých jazykových modelov oproti veľkým jazykovým modelom v agentických aplikáciách
- 🚀 Naučiť sa pokročilé stratégie nasadenia SLM pre prostredia edge computingu
- 📱 Implementovať praktických agentov poháňaných SLM pre reálne aplikácie
- 🏗️ Vytvoriť produkčne pripravených agentov pomocou Microsoft Agent Framework
- 🌐 Nasadiť offline agentov na edge zariadeniach s integráciou lokálnych LLM a SLM
- 🔧 Integrovať Microsoft Agent Framework s Foundry Local pre nasadenie na edge zariadeniach

## Pochopenie AI agentov: Základy a klasifikácie

### Definícia a základné koncepty

Umelý inteligentný agent (AI agent) označuje systém alebo program, ktorý je schopný autonómne vykonávať úlohy v mene používateľa alebo iného systému tým, že navrhuje svoj pracovný postup a využíva dostupné nástroje. Na rozdiel od tradičnej AI, ktorá len odpovedá na vaše otázky, agent môže konať nezávisle na dosiahnutie cieľov.

### Rámec klasifikácie agentov

Pochopenie hraníc agentov pomáha pri výbere vhodných typov agentov pre rôzne výpočtové scenáre:

- **🔬 Jednoduché reflexné agenty**: Systémy založené na pravidlách, ktoré reagujú na okamžité vnímanie (termostaty, základná automatizácia)
- **📱 Modelovo založené agenty**: Systémy, ktoré udržiavajú vnútorný stav a pamäť (robotické vysávače, navigačné systémy)
- **⚖️ Cieľovo orientované agenty**: Systémy, ktoré plánujú a vykonávajú sekvencie na dosiahnutie cieľov (plánovače trás, plánovače úloh)
- **🧠 Učiace sa agenty**: Adaptívne systémy, ktoré zlepšujú výkon v priebehu času (systémy odporúčaní, personalizovaní asistenti)

### Kľúčové výhody AI agentov

AI agenti ponúkajú niekoľko základných výhod, ktoré ich robia ideálnymi pre aplikácie edge computingu:

**Operačná autonómia**: Agenti poskytujú nezávislé vykonávanie úloh bez neustáleho dohľadu človeka, čo ich robí ideálnymi pre aplikácie v reálnom čase. Vyžadujú minimálny dohľad pri zachovaní adaptívneho správania, čo umožňuje nasadenie na zariadeniach s obmedzenými zdrojmi s nižšími prevádzkovými nákladmi.

**Flexibilita nasadenia**: Tieto systémy umožňujú AI schopnosti na zariadení bez požiadaviek na internetové pripojenie, zvyšujú súkromie a bezpečnosť prostredníctvom lokálneho spracovania, môžu byť prispôsobené pre aplikácie špecifické pre danú oblasť a sú vhodné pre rôzne prostredia edge computingu.

**Nákladová efektívnosť**: Agentové systémy ponúkajú nákladovo efektívne nasadenie v porovnaní s cloudovými riešeniami, s nižšími prevádzkovými nákladmi a nižšími požiadavkami na šírku pásma pre aplikácie na edge zariadeniach.

## Pokročilé stratégie malých jazykových modelov

### Základy SLM (Small Language Model)

Malý jazykový model (SLM) je jazykový model, ktorý sa zmestí na bežné spotrebiteľské elektronické zariadenie a vykonáva inferenciu s latenciou dostatočne nízkou na to, aby bol praktický pri obsluhe agentických požiadaviek jedného používateľa. V praktických termínoch sú SLM typicky modely s menej ako 10 miliardami parametrov.

**Funkcie objavovania formátov**: SLM ponúkajú pokročilú podporu pre rôzne úrovne kvantizácie, kompatibilitu medzi platformami, optimalizáciu výkonu v reálnom čase a schopnosti nasadenia na edge zariadeniach. Používatelia môžu získať zvýšené súkromie prostredníctvom lokálneho spracovania a podpory WebGPU pre nasadenie v prehliadači.

**Zbierky úrovní kvantizácie**: Populárne formáty SLM zahŕňajú Q4_K_M pre vyváženú kompresiu v mobilných aplikáciách, sériu Q5_K_S pre nasadenie zamerané na kvalitu na edge zariadeniach, Q8_0 pre takmer originálnu presnosť na výkonných edge zariadeniach a experimentálne formáty ako Q2_K pre scenáre s ultra nízkymi zdrojmi.

### GGUF (General GGML Universal Format) pre nasadenie SLM

GGUF slúži ako primárny formát pre nasadenie kvantizovaných SLM na CPU a edge zariadeniach, špecificky optimalizovaný pre agentické aplikácie:

**Funkcie optimalizované pre agentov**: Formát poskytuje komplexné zdroje pre konverziu a nasadenie SLM s rozšírenou podporou pre volanie nástrojov, generovanie štruktúrovaných výstupov a viacnásobné konverzácie. Kompatibilita medzi platformami zaisťuje konzistentné správanie agentov na rôznych edge zariadeniach.

**Optimalizácia výkonu**: GGUF umožňuje efektívne využívanie pamäte pre pracovné postupy agentov, podporuje dynamické načítanie modelov pre systémy s viacerými agentmi a poskytuje optimalizovanú inferenciu pre interakcie agentov v reálnom čase.

### Rámce optimalizované pre SLM na edge zariadeniach

#### Optimalizácia Llama.cpp pre agentov

Llama.cpp poskytuje špičkové techniky kvantizácie špecificky optimalizované pre nasadenie agentických SLM:

**Kvantizácia špecifická pre agentov**: Rámec podporuje Q4_0 (optimálne pre nasadenie mobilných agentov s 75% znížením veľkosti), Q5_1 (vyvážená kvalita-kompresia pre agentov na edge zariadeniach) a Q8_0 (takmer originálna kvalita pre produkčné agentové systémy). Pokročilé formáty umožňujú ultra-komprimovaných agentov pre extrémne edge scenáre.

**Výhody implementácie**: Inferencia optimalizovaná pre CPU s akceleráciou SIMD poskytuje pamäťovo efektívne vykonávanie agentov. Kompatibilita medzi platformami na architektúrach x86, ARM a Apple Silicon umožňuje univerzálne schopnosti nasadenia agentov.

#### Apple MLX Framework pre SLM agentov

Apple MLX poskytuje natívnu optimalizáciu špecificky navrhnutú pre agentov poháňaných SLM na zariadeniach Apple Silicon:

**Optimalizácia agentov na Apple Silicon**: Rámec využíva jednotnú pamäťovú architektúru s integráciou Metal Performance Shaders, automatickú zmiešanú presnosť pre inferenciu agentov a optimalizovanú šírku pamäťového pásma pre systémy s viacerými agentmi. Agenti SLM vykazujú výnimočný výkon na čipoch série M.

**Funkcie vývoja**: Podpora API pre Python a Swift s optimalizáciami špecifickými pre agentov, automatická diferenciácia pre učenie agentov a bezproblémová integrácia s vývojovými nástrojmi Apple poskytujú komplexné prostredie pre vývoj agentov.

#### ONNX Runtime pre agentov SLM na rôznych platformách

ONNX Runtime poskytuje univerzálny inferenčný engine, ktorý umožňuje agentom SLM bežať konzistentne na rôznych hardvérových platformách a operačných systémoch:

**Univerzálne nasadenie**: ONNX Runtime zaisťuje konzistentné správanie agentov SLM na platformách Windows, Linux, macOS, iOS a Android. Táto kompatibilita medzi platformami umožňuje vývojárom písať raz a nasadiť všade, čo výrazne znižuje náklady na vývoj a údržbu pre aplikácie na viacerých platformách.

**Možnosti hardvérovej akcelerácie**: Rámec poskytuje optimalizované vykonávacie moduly pre rôzne hardvérové konfigurácie vrátane CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) a špecializovaných akcelerátorov (Intel VPU, Qualcomm NPU). Agenti SLM môžu automaticky využívať najlepší dostupný hardvér bez zmien v kóde.

**Funkcie pripravené na produkciu**: ONNX Runtime ponúka funkcie na podnikovej úrovni, ktoré sú nevyhnutné pre nasadenie agentov v produkcii, vrátane optimalizácie grafov pre rýchlejšiu inferenciu, správy pamäte pre prostredia s obmedzenými zdrojmi a komplexných nástrojov na profilovanie pre analýzu výkonu. Rámec podporuje API pre Python aj C++ pre flexibilnú integráciu.

## SLM vs LLM v agentických systémoch: Pokročilé porovnanie

### Výhody SLM v agentických aplikáciách

**Operačná efektívnosť**: SLM poskytujú 10-30× zníženie nákladov v porovnaní s LLM pre agentické úlohy, čo umožňuje agentické odpovede v reálnom čase vo veľkom rozsahu. Ponúkajú rýchlejšie časy inferencie vďaka zníženej výpočtovej zložitosti, čo ich robí ideálnymi pre interaktívne agentické aplikácie.

**Schopnosti nasadenia na edge zariadeniach**: SLM umožňujú vykonávanie agentov na zariadení bez závislosti na internete, zvýšené súkromie prostredníctvom lokálneho spracovania a prispôsobenie pre aplikácie špecifické pre danú oblasť vhodné pre rôzne prostredia edge computingu.

**Optimalizácia špecifická pre agentov**: SLM vynikajú pri volaní nástrojov, generovaní štruktúrovaných výstupov a rutinných pracovných postupoch rozhodovania, ktoré tvoria 70-80% typických agentických úloh.

### Kedy použiť SLM vs LLM v agentických systémoch

**Ideálne pre SLM**:
- **Opakujúce sa agentické úlohy**: Zadávanie údajov, vyplňovanie formulárov, rutinné volania API
- **Integrácia nástrojov**: Dotazy na databázy, operácie so súbormi, interakcie so systémom
- **Štruktúrované pracovné postupy**: Nasledovanie preddefinovaných procesov agentov
- **Agenti špecifickí pre danú oblasť**: Zákaznícka podpora, plánovanie, základná analýza
- **Lokálne spracovanie**: Operácie agentov citlivé na súkromie

**Lepšie pre LLM**:
- **Komplexné uvažovanie**: Nové riešenie problémov, strategické plánovanie
- **Otvorené konverzácie**: Všeobecný chat, kreatívne diskusie
- **Úlohy s rozsiahlymi znalosťami**: Výskum vyžadujúci rozsiahle všeobecné znalosti
- **Nové situácie**: Riešenie úplne nových scenárov agentov

### Hybridná architektúra agentov

Optimálny prístup kombinuje SLM a LLM v heterogénnych agentických systémoch:

**Inteligentná orchestrácia agentov**:
1. **SLM ako primárny**: Rieši 70-80% rutinných agentických úloh lokálne
2. **LLM podľa potreby**: Presmeruje komplexné dotazy na cloudové väčšie modely
3. **Špecializované SLM**: Rôzne malé modely pre rôzne oblasti agentov
4. **Optimalizácia nákladov**: Minimalizuje drahé volania LLM prostredníctvom inteligentného smerovania

## Produkčné stratégie nasadenia agentov SLM

### Foundry Local: Runtime AI na edge zariadeniach na podnikovej úrovni

Foundry Local (https://github.com/microsoft/foundry-local) slúži ako vlajkové riešenie Microsoftu pre nasadenie malých jazykových modelov v produkčných edge prostrediach. Poskytuje kompletné runtime prostredie špecificky navrhnuté pre agentov poháňaných SLM s funkciami na podnikovej úrovni a bezproblémovými integračnými schopnosťami.

**Základná architektúra a funkcie**:
- **Kompatibilné API s OpenAI**: Plná kompatibilita s OpenAI SDK a integráciami Agent Framework
- **Automatická optimalizácia hardvéru**: Inteligentný výber variantov modelu na základe dostupného hardvéru (CUDA GPU, Qualcomm NPU, CPU)
- **Správa modelov**: Automatické sťahovanie, ukladanie do vyrovnávacej pamäte a správa životného cyklu modelov SLM
- **Objavovanie služieb**: Detekcia služieb bez konfigurácie pre rámce agentov
- **Optimalizácia zdrojov**: Inteligentná správa pamäte a energetická efektívnosť pre nasadenie na edge zariadeniach

#### Inštalácia a nastavenie

**Inštalácia na rôznych platformách**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Rýchly štart pre vývoj agentov**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integrácia Agent Framework

**Integrácia SDK Foundry Local**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automatický výber modelu a optimalizácia hardvéru**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Produkčné vzory nasadenia

**Produkčné nastavenie jedného agenta**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orchestrácia produkcie viacerých agentov**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Funkcie na podnikovej úrovni a monitorovanie

**Monitorovanie zdravia a pozorovateľnosť**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Správa zdrojov a automatické škálovanie**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Pokročilá konfigurácia a optimalizácia

**Konfigurácia vlastného modelu**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Kontrolný zoznam nasadenia do produkcie**:

✅ **Konfigurácia služby**:
- Konfigurujte vhodné aliasy modelov pre prípady použitia
- Nastavte limity zdrojov a prahové hodnoty monitorovania
- Aktivujte kontroly zdravia a zber metrik
- Konfigurujte automatické reštartovanie a zálohovanie

✅ **Nastavenie bezpečnosti**:
- Aktivujte prístup k API iba lokálne (bez externého vystavenia)
- Konfigurujte vhodnú správu API kľúčov
- Nastavte auditovanie interakcií agentov
- Implementujte obmedzenie rýchlosti pre produkčné použitie

✅ **Optimalizácia výkonu**:
- Testujte výkon modelu pod očakávaným zaťažením
- Konfigurujte vhodné úrovne kvantizácie
- Nastavte stratégie ukladania modelu do vyrovnávacej pamäte a zahrievania
- Monitorujte vzory využívania pamäte a CPU

✅ **Testovanie integrácie**:
- Testujte integráciu rámca agentov
- Overte schopnosti offline operácií
- Testujte scenáre zálohovania a obnovy
- Validujte pracovné postupy agentov od začiatku do konca

### Ollama: Jednoduché nasadenie agentov
- Testovanie integrácie Microsoft Agent Framework
- Overenie schopností offline prevádzky
- Testovanie scenárov zlyhania a spracovania chýb
- Validácia pracovných postupov agentov od začiatku do konca

**Porovnanie s Foundry Local**:

| Funkcia | Foundry Local | Ollama |
|---------|---------------|--------|
| **Cieľové použitie** | Produkcia v podnikoch | Vývoj a komunita |
| **Ekosystém modelov** | Kurátorované Microsoftom | Rozsiahla komunita |
| **Optimalizácia hardvéru** | Automatická (CUDA/NPU/CPU) | Manuálna konfigurácia |
| **Funkcie pre podniky** | Zabudované monitorovanie, bezpečnosť | Nástroje komunity |
| **Komplexnosť nasadenia** | Jednoduché (winget install) | Jednoduché (curl install) |
| **Kompatibilita API** | OpenAI + rozšírenia | Štandard OpenAI |
| **Podpora** | Oficiálna podpora Microsoftu | Riadená komunitou |
| **Najlepšie pre** | Produkčné agenty | Prototypovanie, výskum |

**Kedy zvoliť Ollama**:
- **Vývoj a prototypovanie**: Rýchle experimentovanie s rôznymi modelmi
- **Komunitné modely**: Prístup k najnovším modelom prispievaným komunitou
- **Vzdelávacie použitie**: Učenie a výučba vývoja AI agentov
- **Výskumné projekty**: Akademický výskum vyžadujúci prístup k rôznym modelom
- **Vlastné modely**: Vytváranie a testovanie vlastných jemne doladených modelov

### VLLM: Vysokovýkonná inferencia SLM agentov

VLLM (Very Large Language Model inference) poskytuje vysokopriechodový, pamäťovo efektívny inferenčný engine špeciálne optimalizovaný pre produkčné nasadenia SLM vo veľkom rozsahu. Zatiaľ čo Foundry Local sa zameriava na jednoduchosť použitia a Ollama zdôrazňuje komunitné modely, VLLM vyniká vo vysokovýkonných scenároch vyžadujúcich maximálny priechod a efektívne využitie zdrojov.

**Hlavná architektúra a funkcie**:
- **PagedAttention**: Revolučné spravovanie pamäte pre efektívne výpočty pozornosti
- **Dynamické dávkovanie**: Inteligentné dávkovanie požiadaviek pre optimálny priechod
- **Optimalizácia GPU**: Pokročilé CUDA jadrá a podpora paralelizmu tensorov
- **Kompatibilita s OpenAI**: Plná kompatibilita API pre bezproblémovú integráciu
- **Špekulatívne dekódovanie**: Pokročilé techniky zrýchlenia inferencie
- **Podpora kvantizácie**: Kvantizácia INT4, INT8 a FP16 pre efektívnosť pamäte

#### Inštalácia a nastavenie

**Možnosti inštalácie**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**Rýchly štart pre vývoj agentov**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integrácia Agent Framework

**VLLM s Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**Nastavenie vysokopriechodového multi-agentového systému**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### Produkčné vzory nasadenia

**Produkčná služba VLLM pre podniky**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### Funkcie pre podniky a monitorovanie

**Pokročilé monitorovanie výkonu VLLM**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### Pokročilá konfigurácia a optimalizácia

**Šablóny konfigurácie VLLM pre produkciu**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**Kontrolný zoznam nasadenia VLLM pre produkciu**:

✅ **Optimalizácia hardvéru**:
- Konfigurácia paralelizmu tensorov pre multi-GPU nastavenia
- Povolenie kvantizácie (AWQ/GPTQ) pre efektívnosť pamäte
- Nastavenie optimálneho využitia pamäte GPU (85-95%)
- Konfigurácia vhodných veľkostí dávok pre priechod

✅ **Ladenie výkonu**:
- Povolenie prefixového cachovania pre opakované dotazy
- Konfigurácia chunked prefill pre dlhé sekvencie
- Nastavenie špekulatívneho dekódovania pre rýchlejšiu inferenciu
- Optimalizácia max_num_seqs na základe hardvéru

✅ **Funkcie pre produkciu**:
- Nastavenie monitorovania zdravia a zberu metrík
- Konfigurácia automatického reštartu a zlyhania
- Implementácia frontovania požiadaviek a vyvažovania záťaže
- Nastavenie komplexného logovania a upozornení

✅ **Bezpečnosť a spoľahlivosť**:
- Konfigurácia pravidiel firewallu a kontrol prístupu
- Nastavenie obmedzenia rýchlosti API a autentifikácie
- Implementácia plynulého vypnutia a vyčistenia
- Konfigurácia zálohovania a obnovy po havárii

✅ **Testovanie integrácie**:
- Testovanie integrácie Microsoft Agent Framework
- Validácia vysokopriechodových scenárov
- Testovanie postupov zlyhania a obnovy
- Benchmarking výkonu pod záťažou

**Porovnanie s inými riešeniami**:

| Funkcia | VLLM | Foundry Local | Ollama |
|---------|------|---------------|--------|
| **Cieľové použitie** | Produkcia s vysokým priechodom | Jednoduchosť pre podniky | Vývoj a komunita |
| **Výkon** | Maximálny priechod | Vyvážený | Dobrý |
| **Efektívnosť pamäte** | Optimalizácia PagedAttention | Automatická optimalizácia | Štandardná |
| **Komplexnosť nastavenia** | Vysoká (veľa parametrov) | Nízka (automatická) | Nízka (jednoduchá) |
| **Škálovateľnosť** | Výborná (tensor/pipeline paralelizmus) | Dobrá | Obmedzená |
| **Kvantizácia** | Pokročilá (AWQ, GPTQ, FP8) | Automatická | Štandardná GGUF |
| **Funkcie pre podniky** | Potrebná vlastná implementácia | Zabudované | Nástroje komunity |
| **Najlepšie pre** | Produkčné agenty vo veľkom rozsahu | Produkcia v podnikoch | Vývoj |

**Kedy zvoliť VLLM**:
- **Požiadavky na vysoký priechod**: Spracovanie stoviek požiadaviek za sekundu
- **Nasadenia vo veľkom rozsahu**: Multi-GPU, multi-node nasadenia
- **Kritický výkon**: Odozvy pod sekundu vo veľkom rozsahu
- **Pokročilá optimalizácia**: Potreba vlastnej kvantizácie a dávkovania
- **Efektívnosť zdrojov**: Maximálne využitie drahého GPU hardvéru

## Reálne aplikácie SLM agentov

### SLM agenti pre zákaznícky servis
- **Schopnosti SLM**: Vyhľadávanie účtov, resetovanie hesiel, kontrola stavu objednávok
- **Výhody nákladov**: 10-násobné zníženie nákladov na inferenciu v porovnaní s LLM agentmi
- **Výkon**: Rýchlejšie odozvy s konzistentnou kvalitou pre rutinné dotazy

### SLM agenti pre obchodné procesy
- **Agenti na spracovanie faktúr**: Extrakcia údajov, validácia informácií, smerovanie na schválenie
- **Agenti na správu e-mailov**: Kategorizácia, prioritizácia, automatické návrhy odpovedí
- **Agenti na plánovanie**: Koordinácia stretnutí, správa kalendárov, posielanie pripomienok

### Osobní digitálni asistenti SLM
- **Agenti na správu úloh**: Vytváranie, aktualizácia, organizácia zoznamov úloh efektívne
- **Agenti na zhromažďovanie informácií**: Výskum tém, sumarizácia zistení lokálne
- **Komunikační agenti**: Návrhy e-mailov, správ, príspevkov na sociálne siete súkromne

### SLM agenti pre obchodovanie a financie
- **Agenti na monitorovanie trhu**: Sledovanie cien, identifikácia trendov v reálnom čase
- **Agenti na generovanie správ**: Automatické vytváranie denných/týždenných súhrnov
- **Agenti na hodnotenie rizík**: Vyhodnocovanie pozícií portfólia pomocou lokálnych údajov

### SLM agenti na podporu zdravotníctva
- **Agenti na plánovanie pacientov**: Koordinácia termínov, posielanie automatických pripomienok
- **Agenti na dokumentáciu**: Generovanie lekárskych súhrnov, správ lokálne
- **Agenti na správu predpisov**: Sledovanie opakovaní, kontrola interakcií súkromne

## Microsoft Agent Framework: Vývoj agentov pripravených na produkciu

### Prehľad a architektúra

Microsoft Agent Framework poskytuje komplexnú, podnikovo orientovanú platformu na vytváranie, nasadzovanie a správu AI agentov, ktorí môžu fungovať v cloudových aj offline edge prostrediach. Rámec je špeciálne navrhnutý na bezproblémovú prácu s malými jazykovými modelmi a scenármi edge computingu, čo ho robí ideálnym pre nasadenia citlivé na súkromie a obmedzené zdroje.

**Hlavné komponenty rámca**:
- **Runtime agenta**: Ľahké prostredie na vykonávanie optimalizované pre edge zariadenia
- **Systém integrácie nástrojov**: Rozšíriteľná architektúra pluginov na pripojenie externých služieb a API
- **Správa stavu**: Trvalá pamäť agenta a spracovanie kontextu naprieč reláciami
- **Bezpečnostná vrstva**: Zabudované bezpečnostné kontroly pre podnikové nasadenie
- **Orchestračný engine**: Koordinácia viacerých agentov a správa pracovných postupov

### Kľúčové funkcie pre nasadenie na edge

**Architektúra orientovaná na offline**: Microsoft Agent Framework je navrhnutý s princípmi orientovanými na offline, umožňujúc agentom efektívne fungovať bez neustáleho pripojenia na internet. To zahŕňa lokálnu inferenciu modelov, cachované znalostné databázy, offline vykonávanie nástrojov a plynulé zhoršenie funkčnosti, keď cloudové služby nie sú dostupné.

**Optimalizácia zdrojov**: Rámec poskytuje inteligentné spravovanie zdrojov s automatickou optimalizáciou pamäte pre SLM, vyvažovaním záťaže CPU/GPU pre edge zariadenia, adaptívnym výberom modelov na základe dostupných zdrojov a energeticky efektívnymi vzormi inferencie pre mobilné nasadenie.

**Bezpečnosť a súkromie**: Funkcie bezpečnosti na úrovni podniku zahŕňajú lokálne spracovanie údajov na zachovanie súkromia, šifrované komunikačné kanály agentov, kontrolu prístupu na základe rolí pre schopnosti agentov a auditovanie logov pre požiadavky na súlad.

### Integrácia s Foundry Local

Microsoft Agent Framework sa bezproblémovo integruje s Foundry Local, aby poskytol kompletné edge AI riešenie:

**Automatické objavovanie modelov**: Rámec automaticky detekuje a pripája sa k inštanciám Foundry Local, objavuje dostupné SLM modely a vyberá optimálne modely na základe požiadaviek agentov a schopností hardvéru.

**Dynamické načítanie modelov**: Agenti môžu dynamicky načítať rôzne SLM pre konkrétne úlohy, umožňujúc systémy viacerých modelov, kde rôzne modely spracovávajú rôzne typy požiadaviek, a automatické zlyhanie medzi modelmi na základe dostupnosti a výkonu.

**Optimalizácia výkonu**: Integrované mechanizmy cachovania znižujú časy načítania modelov, pooling pripojení optimalizuje API volania na Foundry Local a inteligentné dávkovanie zlepšuje priechod pre viaceré požiadavky agentov.

### Vytváranie agentov s Microsoft Agent Framework

#### Definícia a konfigurácia agenta

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integrácia nástrojov pre edge scenáre

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orchestrácia viacerých agentov

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### Pokročilé vzory nasadenia na edge

#### Hierarchická architektúra agentov

**Lokálne klastre agentov**: Nasadenie viacerých špecializovaných SLM agentov na edge zariadeniach, každý optimalizovaný pre konkrétne úlohy. Použitie ľahkých modelov ako Qwen2.5-0.5B pre jednoduché smerovanie a plánovanie, stredne veľkých modelov ako Phi-4-Mini pre zákaznícky servis a dokumentáciu, a väčších modelov pre komplexné uvažovanie, keď to zdroje umožňujú.

**Koordinácia edge-to-cloud**: Implementácia inteligentných eskalačných vzorov, kde lokálni agenti spracovávajú rutinné úlohy, cloudoví agenti poskytujú komplexné uvažovanie, keď to pripojenie umožňuje, a plynulý prechod medzi edge a cloud spracovaním zachováva kontinuitu.

#### Konfigurácie nasadenia

**Nasadenie na jednom zariadení**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**Distribuované nasadenie na edge**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Optimalizácia výkonu pre edge agentov

#### Stratégie výberu modelov

**Priradenie modelov na základe úloh**: Microsoft Agent Framework umožňuje inteligentný výber modelov na základe zložitosti úlohy a požiadaviek:

- **Jednoduché úlohy** (Q&A, smerovanie): Qwen2.5-0.5B (500MB, <100ms odozva)
- **Stredne náročné úlohy** (zákaznícky servis, plánovanie): Phi-4-Mini (2.4GB, 200-500ms odozva)
- **Komplexné úlohy** (technická analýza, plánovanie): Phi-4 (7GB, 1-3s odozva, keď to zdroje umožňujú)

**Dynamické prepínanie modelov**: Agenti môžu prepínať medzi modelmi na základe aktuálneho zaťaženia systému, hodnotenia zložitosti úlohy, úrovní priority používateľa a dostupných hardvérových zdrojov.

#### Správa pamäte a zdrojov

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### Vzory integrácie pre podniky

#### Bezpečnosť a súlad

**Lokálne spracovanie údajov**: Všetko spracovanie agentov prebieha lokálne, čím sa zabezpečí, že citlivé údaje nikdy neopustia edge zariadenie. To zahŕňa ochranu informácií o zákazníkoch, súlad s HIPAA pre zdravotnícke agenty, bezpečnosť finančných údajov pre bankové agenty a súlad s GDPR pre európske nasadenia.

**Kontrola prístupu**: Oprávnenia na základe rolí kontrolujú, ktoré nástroje môžu agenti používať, autentifikácia používateľov pre interakcie s agentmi a auditné stopy pre všetky akcie a rozhodnutia agentov.

#### Monitorovanie a pozorovateľnosť

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### Reálne príklady implementácie

#### Maloobchodný systém agentov na edge

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Agent na podporu zdravotníctva

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### Najlepšie postupy pre Microsoft Agent Framework

#### Pokyny pre vývoj

1. **Začnite jednoducho**: Začnite s jedným agentom pred vytváraním komplexných systémov viacerých agentov
2. **Správna veľkosť modelu**: Vyberte najmenší model, ktorý spĺňa vaše požiadavky na presnosť
3. **Dizajn nástrojov**: Vytvárajte zamerané, jednoúčelové nástroje namiesto komplexných multifunkčných nástrojov
4. **Spracovanie chýb**: Implementujte plynulé zhoršenie funkčnosti pre offline scenáre a zlyhania modelov
5. **Testovanie**: Testujte agentov dôkladne v offline podmienkach a prostrediach s obmedzenými zdrojmi

#### Najlepšie postupy nasadenia

1. **Postupné zavádzanie**: Nasadzujte naj
**Výber rámca pre nasadenie agentov**: Vyberte optimalizačné rámce na základe cieľového hardvéru a požiadaviek agenta. Použite Llama.cpp pre nasadenie agentov optimalizovaných na CPU, Apple MLX pre aplikácie agentov na Apple Silicon a ONNX pre kompatibilitu agentov naprieč platformami.

## Praktická konverzia SLM agentov a príklady použitia

### Scenáre nasadenia agentov v reálnom svete

**Mobilné aplikácie agentov**: Formáty Q4_K excelujú v aplikáciách agentov na smartfónoch s minimálnou pamäťovou náročnosťou, zatiaľ čo Q8_0 poskytuje vyvážený výkon pre systémy agentov na tabletoch. Formáty Q5_K ponúkajú vynikajúcu kvalitu pre mobilných produktívnych agentov.

**Počítačové a edge výpočty agentov**: Q5_K poskytuje optimálny výkon pre aplikácie agentov na stolných počítačoch, Q8_0 ponúka vysokokvalitné inferencie pre pracovné stanice a Q4_K umožňuje efektívne spracovanie na edge zariadeniach.

**Výskumné a experimentálne agenti**: Pokročilé kvantizačné formáty umožňujú skúmanie ultra-nízkej presnosti inferencie agentov pre akademický výskum a aplikácie agentov typu proof-of-concept vyžadujúce extrémne obmedzené zdroje.

### Výkonnostné benchmarky SLM agentov

**Rýchlosť inferencie agenta**: Q4_K dosahuje najrýchlejšie časy odozvy agenta na mobilných CPU, Q5_K poskytuje vyvážený pomer rýchlosti a kvality pre všeobecné aplikácie agentov, Q8_0 ponúka vynikajúcu kvalitu pre komplexné úlohy agentov a experimentálne formáty dosahujú maximálnu priepustnosť pre špecializovaný hardvér agentov.

**Pamäťové požiadavky agenta**: Úrovne kvantizácie pre agentov sa pohybujú od Q2_K (menej ako 500 MB pre malé modely agentov) po Q8_0 (približne 50 % pôvodnej veľkosti), pričom experimentálne konfigurácie dosahujú maximálnu kompresiu pre prostredia agentov s obmedzenými zdrojmi.

## Výzvy a úvahy pre SLM agentov

### Kompromisy výkonu v systémoch agentov

Nasadenie SLM agentov zahŕňa dôkladné zváženie kompromisov medzi veľkosťou modelu, rýchlosťou odozvy agenta a kvalitou výstupu. Zatiaľ čo Q4_K ponúka výnimočnú rýchlosť a efektivitu pre mobilných agentov, Q8_0 poskytuje vynikajúcu kvalitu pre komplexné úlohy agentov. Q5_K predstavuje strednú cestu vhodnú pre väčšinu všeobecných aplikácií agentov.

### Hardvérová kompatibilita pre SLM agentov

Rôzne edge zariadenia majú rôzne schopnosti pre nasadenie SLM agentov. Q4_K funguje efektívne na základných procesoroch pre jednoduchých agentov, Q5_K vyžaduje stredné výpočtové zdroje pre vyvážený výkon agenta a Q8_0 využíva výhody špičkového hardvéru pre pokročilé schopnosti agentov.

### Bezpečnosť a ochrana súkromia v systémoch SLM agentov

Zatiaľ čo SLM agenti umožňujú lokálne spracovanie pre zvýšenú ochranu súkromia, je potrebné implementovať správne bezpečnostné opatrenia na ochranu modelov agentov a údajov v edge prostrediach. To je obzvlášť dôležité pri nasadzovaní vysokopresných formátov agentov v podnikových prostrediach alebo komprimovaných formátov agentov v aplikáciách, ktoré spracovávajú citlivé údaje.

## Budúce trendy vo vývoji SLM agentov

Prostredie SLM agentov sa neustále vyvíja s pokrokmi v kompresných technikách, optimalizačných metódach a stratégiách nasadenia na edge zariadeniach. Budúci vývoj zahŕňa efektívnejšie algoritmy kvantizácie pre modely agentov, vylepšené metódy kompresie pre pracovné postupy agentov a lepšiu integráciu s hardvérovými akcelerátormi na edge zariadeniach pre spracovanie agentov.

**Predpovede trhu pre SLM agentov**: Podľa nedávneho výskumu by automatizácia poháňaná agentmi mohla do roku 2027 eliminovať 40–60 % opakujúcich sa kognitívnych úloh v podnikových pracovných postupoch, pričom SLM budú viesť túto transformáciu vďaka svojej nákladovej efektívnosti a flexibilite nasadenia.

**Technologické trendy v SLM agentoch**:
- **Špecializovaní SLM agenti**: Modely špecifické pre danú oblasť, trénované na konkrétne úlohy a odvetvia
- **Edge výpočty agentov**: Vylepšené schopnosti agentov na zariadeniach s lepšou ochranou súkromia a zníženou latenciou
- **Orchestrácia agentov**: Lepšia koordinácia medzi viacerými SLM agentmi s dynamickým smerovaním a vyvažovaním záťaže
- **Demokratizácia**: Flexibilita SLM umožňuje širšiu účasť na vývoji agentov naprieč organizáciami

## Začíname so SLM agentmi

### Krok 1: Nastavenie prostredia Microsoft Agent Framework

**Inštalácia závislostí**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicializácia Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Krok 2: Výber SLM pre aplikácie agentov
Populárne možnosti pre Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Vynikajúci pre všeobecné úlohy agentov s vyváženým výkonom
- **Qwen2.5-0.5B (0.5B)**: Ultra-efektívny pre jednoduchých agentov na smerovanie a klasifikáciu
- **Qwen2.5-Coder-0.5B (0.5B)**: Špecializovaný na úlohy agentov súvisiace s kódom
- **Phi-4 (7B)**: Pokročilé uvažovanie pre komplexné edge scenáre, keď to zdroje umožňujú

### Krok 3: Vytvorenie prvého agenta s Microsoft Agent Framework

**Základné nastavenie agenta**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Krok 4: Definovanie rozsahu a požiadaviek agenta
Začnite s aplikáciami agentov s jasným zameraním a dobre definovanými cieľmi pomocou Microsoft Agent Framework:
- **Agenti pre jednu oblasť**: Zákaznícka podpora ALEBO plánovanie ALEBO výskum
- **Jasné ciele agenta**: Konkrétne, merateľné ciele pre výkon agenta
- **Obmedzená integrácia nástrojov**: Maximálne 3–5 nástrojov pre počiatočné nasadenie agenta
- **Definované hranice agenta**: Jasné eskalačné cesty pre komplexné scenáre
- **Edge-first dizajn**: Prioritizujte offline funkčnosť a lokálne spracovanie

### Krok 5: Implementácia nasadenia na edge zariadeniach s Microsoft Agent Framework

**Konfigurácia zdrojov**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Nasadenie bezpečnostných opatrení pre edge agentov**:
- **Lokálna validácia vstupov**: Kontrola požiadaviek bez závislosti na cloude
- **Offline filtrovanie výstupov**: Zabezpečenie kvality odpovedí lokálne
- **Bezpečnostné kontroly na edge zariadeniach**: Implementácia bezpečnosti bez potreby internetového pripojenia
- **Lokálne monitorovanie**: Sledovanie výkonu a označovanie problémov pomocou edge telemetrie

### Krok 6: Meranie a optimalizácia výkonu edge agentov
- **Miera dokončenia úloh agenta**: Monitorovanie úspešnosti v offline scenároch
- **Časy odozvy agenta**: Zabezpečenie odozvy pod jednu sekundu pre nasadenie na edge zariadeniach
- **Využitie zdrojov**: Sledovanie pamäte, CPU a spotreby batérie na edge zariadeniach
- **Nákladová efektívnosť**: Porovnanie nákladov na nasadenie na edge zariadeniach s alternatívami založenými na cloude
- **Spoľahlivosť offline**: Meranie výkonu agenta počas výpadkov siete

## Kľúčové poznatky pre implementáciu SLM agentov

1. **SLM sú dostatočné pre agentov**: Pre väčšinu úloh agentov malé modely fungujú rovnako dobre ako veľké, pričom ponúkajú významné výhody
2. **Nákladová efektívnosť agentov**: Prevádzka SLM agentov je 10–30x lacnejšia, čo ich robí ekonomicky životaschopnými pre široké nasadenie
3. **Špecializácia funguje pre agentov**: Jemne doladené SLM často prekonávajú všeobecné LLM v špecifických aplikáciách agentov
4. **Hybridná architektúra agentov**: Používajte SLM pre rutinné úlohy agentov, LLM pre komplexné uvažovanie, keď je to potrebné
5. **Microsoft Agent Framework umožňuje produkčné nasadenie**: Poskytuje nástroje na podnikovej úrovni na vytváranie, nasadzovanie a správu edge agentov
6. **Edge-first dizajnové princípy**: Agenti schopní offline spracovania s lokálnym spracovaním zabezpečujú ochranu súkromia a spoľahlivosť
7. **Integrácia Foundry Local**: Bezproblémové prepojenie medzi Microsoft Agent Framework a lokálnou inferenciou modelov
8. **Budúcnosť je v SLM agentoch**: Malé jazykové modely s produkčnými rámcami sú budúcnosťou agentickej AI, umožňujúcou demokratizované a efektívne nasadenie agentov

## Referencie a ďalšie čítanie

### Základné výskumné práce a publikácie

#### AI agenti a agentické systémy
- **"Language Agents as Optimizable Graphs"** (2024) - Základný výskum o architektúre agentov a optimalizačných stratégiách
  - Autori: Wenyue Hua, Lishan Yang, et al.
  - Link: https://arxiv.org/abs/2402.16823
  - Kľúčové poznatky: Dizajn agentov založený na grafoch a optimalizačné stratégie

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Autori: Zhiheng Xi, Wenxiang Chen, et al.
  - Link: https://arxiv.org/abs/2309.07864
  - Kľúčové poznatky: Komplexný prehľad schopností a aplikácií agentov založených na LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Autori: Theodore Sumers, Shunyu Yao, et al.
  - Link: https://arxiv.org/abs/2309.02427
  - Kľúčové poznatky: Kognitívne rámce na navrhovanie inteligentných agentov

#### Malé jazykové modely a optimalizácia
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Autori: Microsoft Research Team
  - Link: https://arxiv.org/abs/2404.14219
  - Kľúčové poznatky: Dizajnové princípy SLM a stratégie nasadenia na mobilných zariadeniach

- **"Qwen2.5 Technical Report"** (2024)
  - Autori: Alibaba Cloud Team
  - Link: https://arxiv.org/abs/2407.10671
  - Kľúčové poznatky: Pokročilé techniky trénovania SLM a optimalizácia výkonu

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Autori: Peiyuan Zhang, Guangtao Zeng, et al.
  - Link: https://arxiv.org/abs/2401.02385
  - Kľúčové poznatky: Ultra-kompaktný dizajn modelu a efektivita trénovania

### Oficiálna dokumentácia a rámce

#### Microsoft Agent Framework
- **Oficiálna dokumentácia**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub Repository**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Hlavný repozitár**: https://github.com/microsoft/foundry-local
- **Dokumentácia**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Hlavný repozitár**: https://github.com/vllm-project/vllm
- **Dokumentácia**: https://docs.vllm.ai/


#### Ollama
- **Oficiálna webová stránka**: https://ollama.ai/
- **GitHub Repository**: https://github.com/ollama/ollama

### Rámce optimalizácie modelov

#### Llama.cpp
- **Repozitár**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentácia**: https://microsoft.github.io/Olive/
- **GitHub Repository**: https://github.com/microsoft/Olive

#### OpenVINO
- **Oficiálna stránka**: https://docs.openvino.ai/

#### Apple MLX
- **Repozitár**: https://github.com/ml-explore/mlx

### Priemyselné správy a analýzy trhu

#### Výskum trhu AI agentov
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Link: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Kľúčové poznatky: Trhové trendy a vzory prijatia v podnikoch

#### Technické benchmarky

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Link: https://mlcommons.org/en/inference-edge/
  - Kľúčové poznatky: Štandardizované metriky výkonu pre nasadenie na edge zariadeniach

### Štandardy a špecifikácie

#### Formáty modelov a štandardy
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Formát modelu naprieč platformami pre interoperabilitu
- **GGUF špecifikácia**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantizovaný formát modelu pre inferenciu na CPU
- **OpenAI API špecifikácia**: https://platform.openai.com/docs/api-reference
  - Štandardný formát API pre integráciu jazykových modelov

#### Bezpečnosť a súlad
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI systémy**: Rámec pre AI systémy a bezpečnosť
- **IEEE štandardy pre AI**: https://standards.ieee.org/industry-connections/ai/

Posun smerom k agentom poháňaným SLM predstavuje zásadnú zmenu v prístupe k nasadeniu AI. Microsoft Agent Framework, v kombinácii s lokálnymi platformami a efektívnymi malými jazykovými modelmi, poskytuje kompletné riešenie na vytváranie produkčne pripravených agentov, ktoré efektívne fungujú v edge prostrediach. Zameraním sa na efektivitu, špecializáciu a praktickú užitočnosť robí tento technologický stack AI agentov dostupnejšími, cenovo výhodnejšími a efektívnejšími pre reálne aplikácie naprieč každým odvetvím a edge výpočtovým prostredím.

Ako postupujeme do roku 2025, kombinácia stále schopnejších malých modelov, sofistikovaných rámcov agentov ako Microsoft Agent Framework a robustných platforiem nasadenia na edge zariadeniach odomkne nové možnosti pre autonómne systémy, ktoré môžu efektívne fungovať na edge zariadeniach pri zachovaní ochrany súkromia, znižovaní nákladov a poskytovaní výnimočných používateľských skúseností.

**Ďalšie kroky pre implementáciu**:
1. **Preskúmajte funkčné volania**: Naučte sa, ako SLM spracovávajú integráciu nástrojov a štruktúrované výstupy
2. **O

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nenesieme zodpovednosť za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.