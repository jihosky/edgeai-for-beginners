<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T11:28:43+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ja"
}
-->
# セクション03 - モデルコンテキストプロトコル（MCP）の統合

## MCP（モデルコンテキストプロトコル）の概要

モデルコンテキストプロトコル（MCP）は、AIアプリケーションを外部システムに接続するためのオープンソース標準です。MCPを使用することで、ClaudeやChatGPTのようなAIアプリケーションがデータソース（例：ローカルファイル、データベース）、ツール（例：検索エンジン、計算機）、ワークフロー（例：専門的なプロンプト）に接続し、重要な情報にアクセスしたりタスクを実行したりすることが可能になります。

MCPは、AIアプリケーションのための**USB-Cポート**のようなものと考えることができます。USB-Cが電子機器を接続するための標準化された方法を提供するのと同様に、MCPはAIアプリケーションを外部システムに接続するための標準化された方法を提供します。

### MCPで可能になること

MCPはAIアプリケーションに強力な機能を解放します：

- **パーソナライズされたAIアシスタント**: GoogleカレンダーやNotionにアクセスし、より個人的なAIアシスタントとして機能
- **高度なコード生成**: Claude CodeがFigmaデザインを基にウェブアプリ全体を生成
- **企業データ統合**: 企業のチャットボットが複数のデータベースに接続し、ユーザーがチャットを通じてデータを分析できるようにする
- **クリエイティブなワークフロー**: AIモデルがBlenderで3Dデザインを作成し、3Dプリンターで印刷
- **リアルタイム情報アクセス**: 外部データソースに接続して最新情報を取得
- **複雑なマルチステップ操作**: 複数のツールやシステムを組み合わせた高度なワークフローを実行

### MCPが重要な理由

MCPはエコシステム全体に利益をもたらします：

**開発者向け**: MCPはAIアプリケーションやエージェントの構築や統合時の開発時間と複雑さを軽減します。

**AIアプリケーション向け**: MCPはデータソース、ツール、アプリのエコシステムへのアクセスを提供し、機能を強化し、エンドユーザー体験を向上させます。

**エンドユーザー向け**: MCPにより、必要に応じてデータにアクセスし、代わりに行動を起こすことができる、より能力の高いAIアプリケーションやエージェントが実現します。

## MCPにおける小型言語モデル（SLM）

小型言語モデルは効率的なAI展開のアプローチを提供し、いくつかの利点を持っています：

### SLMの利点
- **リソース効率**: 計算要件が低い
- **応答時間の短縮**: リアルタイムアプリケーションでの遅延を削減  
- **コスト効率**: インフラニーズが最小限
- **プライバシー**: データ送信なしでローカルで実行可能
- **カスタマイズ**: 特定のドメインに合わせて調整が容易

### MCPとの相性が良い理由

SLMとMCPを組み合わせることで、モデルの推論能力が外部ツールによって強化され、パラメータ数が少ないモデルでも機能を補完する強力な組み合わせが実現します。

## Python MCP SDKの概要

Python MCP SDKは、MCP対応アプリケーションを構築するための基盤を提供します。このSDKには以下が含まれます：

- **クライアントライブラリ**: MCPサーバーへの接続用
- **サーバーフレームワーク**: カスタムMCPサーバーの作成用
- **プロトコルハンドラー**: 通信管理用
- **ツール統合**: 外部機能の実行用

## 実践的な実装: Phi-4 MCPクライアント

MicrosoftのPhi-4ミニモデルをMCP機能と統合した実際の実装を見てみましょう。

### MCPアーキテクチャの概要

MCPは**クライアント-サーバーアーキテクチャ**に従い、MCPホスト（Claude CodeやClaude DesktopのようなAIアプリケーション）が1つ以上のMCPサーバーに接続を確立します。MCPホストは、各MCPサーバーに対して1つのMCPクライアントを作成することでこれを実現します。

#### 主な参加者

- **MCPホスト**: 複数のMCPクライアントを調整・管理するAIアプリケーション
- **MCPクライアント**: MCPサーバーとの接続を維持し、MCPホストが使用するためのコンテキストを取得するコンポーネント
- **MCPサーバー**: MCPクライアントにコンテキストを提供するプログラム

#### 二層アーキテクチャ

MCPは2つの異なる層で構成されています：

**データ層**: クライアント-サーバー通信のためのJSON-RPCベースのプロトコルを定義
- ライフサイクル管理（接続初期化、機能交渉）
- コアプリミティブ（ツール、リソース、プロンプト）
- クライアント機能（サンプリング、情報引き出し、ログ記録）
- ユーティリティ機能（通知、進捗追跡）

**トランスポート層**: 通信メカニズムとチャネルを定義
- **STDIOトランスポート**: ローカルプロセス用の標準入力/出力ストリームを使用（最適なパフォーマンス、ネットワークオーバーヘッドなし）
- **ストリーム可能なHTTPトランスポート**: HTTP POSTとオプションのServer-Sent Eventsを使用してリモートサーバーと通信（標準HTTP認証をサポート）

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### MCPコアプリミティブ

MCPは、AIアプリケーションと共有できるコンテキスト情報の種類や実行可能なアクションの範囲を指定するプリミティブを定義します。

#### サーバープリミティブ

MCPはサーバーが公開できる3つのコアプリミティブを定義します：

**ツール**: AIアプリケーションがアクションを実行するために呼び出せる実行可能な関数
- 例: ファイル操作、API呼び出し、データベースクエリ
- メソッド: `tools/list`, `tools/call`
- 動的な発見と実行をサポート

**リソース**: AIアプリケーションにコンテキスト情報を提供するデータソース
- 例: ファイル内容、データベースレコード、APIレスポンス
- メソッド: `resources/list`, `resources/read`
- 構造化データへのアクセスを可能にする

**プロンプト**: 言語モデルとのインタラクションを構造化する再利用可能なテンプレート
- 例: システムプロンプト、少数ショット例
- メソッド: `prompts/list`, `prompts/get`
- AIインタラクションパターンを標準化

#### クライアントプリミティブ

MCPは、より豊かなインタラクションを可能にするためにクライアントが公開できるプリミティブも定義します：

**サンプリング**: サーバーがクライアントのAIアプリケーションから言語モデルの補完を要求できる
- メソッド: `sampling/complete`
- モデルに依存しないサーバー開発を可能にする
- ホストの言語モデルへのアクセスを提供

**情報引き出し**: サーバーがユーザーから追加情報を要求できる
- メソッド: `elicitation/request`
- ユーザーインタラクションと確認を可能にする
- 動的な情報収集をサポート

**ログ記録**: サーバーがクライアントにログメッセージを送信できる
- デバッグと監視目的で使用
- サーバー操作の可視性を提供

### MCPプロトコルライフサイクル

#### 初期化と機能交渉

MCPは状態を持つプロトコルであり、ライフサイクル管理が必要です。初期化プロセスは以下の重要な目的を果たします：

1. **プロトコルバージョンの交渉**: クライアントとサーバーが互換性のあるプロトコルバージョンを使用していることを確認（例: "2025-06-18"）
2. **機能の発見**: 各当事者がサポートする機能とプリミティブを宣言
3. **アイデンティティ交換**: 識別情報とバージョン情報を提供

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### ツールの発見と実行

初期化後、クライアントはツールを発見し実行できます：

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### リアルタイム通知

MCPは動的な更新のためのリアルタイム通知をサポートします：

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## 始め方: ステップバイステップガイド

### ステップ1: 環境設定

必要な依存関係をインストール：
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### ステップ2: 基本設定

環境変数を設定：
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### ステップ3: 初めてのMCPクライアントの実行

**基本的なOllamaセットアップ:**
```bash
python ghmodel_mcp_demo.py
```

**vLLMバックエンドの使用:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Server-Sent Events接続:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**カスタムMCPサーバー:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### ステップ4: プログラムによる使用

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 高度な機能

### マルチバックエンドサポート

この実装はOllamaとvLLMの両方のバックエンドをサポートしており、要件に応じて選択できます：

- **Ollama**: ローカル開発とテストに最適
- **vLLM**: 本番環境と高スループットシナリオに最適化

### 柔軟な接続プロトコル

2つの接続モードがサポートされています：

**STDIOモード**: プロセス間の直接通信
- 低遅延
- ローカルツールに適している
- シンプルなセットアップ

**SSEモード**: HTTPベースのストリーミング
- ネットワーク対応
- 分散システムに適している
- リアルタイム更新

### ツール統合機能

システムはさまざまなツールと統合可能：
- ウェブ自動化（Playwright）
- ファイル操作
- APIインタラクション
- システムコマンド
- カスタム関数

## エラーハンドリングとベストプラクティス

### 包括的なエラー管理

この実装には以下のための堅牢なエラーハンドリングが含まれています：

**接続エラー:**
- MCPサーバーの障害
- ネットワークタイムアウト
- 接続問題

**ツール実行エラー:**
- ツールの欠如
- パラメータ検証
- 実行失敗

**レスポンス処理エラー:**
- JSON解析問題
- フォーマットの不整合
- LLMレスポンスの異常

### ベストプラクティス

1. **リソース管理**: 非同期コンテキストマネージャーを使用
2. **エラーハンドリング**: 包括的なtry-catchブロックを実装
3. **ログ記録**: 適切なログレベルを有効化
4. **セキュリティ**: 入力を検証し、出力をサニタイズ
5. **パフォーマンス**: 接続プールとキャッシュを使用

## 実際のアプリケーション

### ウェブ自動化
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### データ処理
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API統合
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## パフォーマンス最適化

### メモリ管理
- 効率的なメッセージ履歴管理
- 適切なリソースクリーンアップ
- 接続プール

### ネットワーク最適化
- 非同期HTTP操作
- 設定可能なタイムアウト
- 優雅なエラー回復

### 同時処理
- 非ブロッキングI/O
- ツールの並列実行
- 効率的な非同期パターン

## セキュリティの考慮事項

### データ保護
- 安全なAPIキー管理
- 入力検証
- 出力サニタイズ

### ネットワークセキュリティ
- HTTPSサポート
- ローカルエンドポイントのデフォルト
- 安全なトークン管理

### 実行の安全性
- ツールフィルタリング
- サンドボックス環境
- 監査ログ

## MCPエコシステムと開発

### MCPプロジェクトの範囲

モデルコンテキストプロトコルエコシステムには以下の主要コンポーネントが含まれます：

- **[MCP仕様](https://modelcontextprotocol.io/specification/latest)**: クライアントとサーバーの実装要件を概説する公式仕様
- **[MCP SDK](https://modelcontextprotocol.io/docs/sdk)**: MCPを実装するための異なるプログラミング言語向けSDK
- **MCP開発ツール**: MCPサーバーとクライアントを開発するためのツール、[MCP Inspector](https://github.com/modelcontextprotocol/inspector)を含む
- **[MCPリファレンスサーバー実装](https://github.com/modelcontextprotocol/servers)**: MCPサーバーのリファレンス実装

### MCP開発の始め方

MCPを使用して構築を開始するには：

**サーバーを構築**: [MCPサーバーを作成](https://modelcontextprotocol.io/docs/develop/build-server)してデータやツールを公開

**クライアントを構築**: [アプリケーションを開発](https://modelcontextprotocol.io/docs/develop/build-client)してMCPサーバーに接続

**概念を学ぶ**: [MCPのコア概念](https://modelcontextprotocol.io/docs/learn/architecture)とアーキテクチャを理解

## 結論

MCPと統合されたSLMは、AIアプリケーション開発におけるパラダイムシフトを表しています。小型モデルの効率性と外部ツールの力を組み合わせることで、開発者はリソース効率が高く、非常に能力の高いインテリジェントシステムを構築できます。

モデルコンテキストプロトコルは、電子機器のユニバーサル接続標準であるUSB-Cのように、AIアプリケーションを外部システムに接続するための標準化された方法を提供します。この標準化により以下が可能になります：

- **シームレスな統合**: AIモデルを多様なデータソースやツールに接続
- **エコシステムの成長**: 一度構築すれば複数のAIアプリケーションで使用可能
- **機能強化**: SLMを外部機能で補完
- **リアルタイム更新**: 動的で応答性の高いAIアプリケーションをサポート

重要なポイント：
- MCPはAIアプリケーションと外部システムを橋渡しするオープン標準
- プロトコルはツール、リソース、プロンプトをコアプリミティブとしてサポート
- リアルタイム通知により動的で応答性の高いアプリケーションを実現
- 適切なライフサイクル管理とエラーハンドリングは本番環境で不可欠
- エコシステムは包括的なSDKと開発ツールを提供

## 参考文献とさらなる学習
- **[Ollama ドキュメント](https://ollama.ai/docs)** - ローカル LLM デプロイメントプラットフォーム  
- **[vLLM ドキュメント](https://docs.vllm.ai/)** - 高性能 LLM サービング  

### 技術標準とプロトコル  

- **[JSON-RPC 2.0 仕様](https://www.jsonrpc.org/)** - MCP が使用する基盤となる RPC プロトコル  
- **[JSON Schema](https://json-schema.org/)** - MCP ツールのスキーマ定義標準  
- **[OpenAPI 仕様](https://swagger.io/specification/)** - API ドキュメント標準  
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - リアルタイム更新のためのウェブ標準  

### AI エージェント開発  

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - 実用的なエージェント開発  
- **[LangChain ドキュメント](https://docs.langchain.com/)** - エージェントとツール統合フレームワーク  
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - Microsoft の AI オーケストレーション SDK  

### 業界レポートと研究  

- **[Anthropic の Model Context Protocol 発表](https://www.anthropic.com/news/model-context-protocol)** - MCP の初回導入  
- **[小型言語モデル調査](https://arxiv.org/abs/2410.20011)** - SLM 研究の学術調査  
- **[エッジ AI 市場分析](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - 業界動向と予測  
- **[AI エージェント開発のベストプラクティス](https://arxiv.org/abs/2309.02427)** - エージェントアーキテクチャに関する研究  

このセクションは、SLM を活用した MCP アプリケーションを構築するための基盤を提供し、自動化、データ処理、インテリジェントシステム統合の可能性を広げます。  

## ➡️ 次に進む  

- [モジュール 7. エッジ AI サンプル](../Module07/README.md)  

---

**免責事項**:  
この文書はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があります。元の言語で記載された文書を正式な情報源としてお考えください。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当社は責任を負いません。