<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T11:29:15+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ja"
}
-->
# セクション3 : Microsoft Olive Optimization Suite

## 目次
1. [はじめに](../../../Module04)
2. [Microsoft Oliveとは？](../../../Module04)
3. [インストール](../../../Module04)
4. [クイックスタートガイド](../../../Module04)
5. [例: Qwen3をONNX INT4に変換する](../../../Module04)
6. [高度な使用法](../../../Module04)
7. [Olive Recipesリポジトリ](../../../Module04)
8. [ベストプラクティス](../../../Module04)
9. [トラブルシューティング](../../../Module04)
10. [追加リソース](../../../Module04)

## はじめに

Microsoft Oliveは、ハードウェアに対応したモデル最適化ツールキットであり、機械学習モデルをさまざまなハードウェアプラットフォームに展開するための最適化プロセスを簡素化します。CPU、GPU、または専用AIアクセラレータを対象にしている場合でも、Oliveはモデルの精度を維持しながら最適なパフォーマンスを実現するのに役立ちます。

## Microsoft Oliveとは？

Oliveは、モデル圧縮、最適化、コンパイルにおいて業界をリードする技術を組み合わせた、使いやすいハードウェア対応のモデル最適化ツールです。ONNX Runtimeと連携し、E2E推論最適化ソリューションとして機能します。

### 主な特徴

- **ハードウェア対応の最適化**: ターゲットハードウェアに最適な最適化技術を自動選択
- **40以上の組み込み最適化コンポーネント**: モデル圧縮、量子化、グラフ最適化などを網羅
- **簡単なCLIインターフェース**: 一般的な最適化タスクのためのシンプルなコマンド
- **マルチフレームワーク対応**: PyTorch、Hugging Faceモデル、ONNXに対応
- **人気モデル対応**: Llama、Phi、Qwen、Gemmaなどの人気モデルアーキテクチャを自動的に最適化可能

### 利点

- **開発時間の短縮**: 異なる最適化技術を手動で試す必要がない
- **パフォーマンス向上**: 最大6倍の速度向上が可能
- **クロスプラットフォーム展開**: 最適化されたモデルは異なるハードウェアやOSで動作
- **精度の維持**: モデル品質を維持しながらパフォーマンスを向上

## インストール

### 前提条件

- Python 3.8以上
- pipパッケージマネージャー
- 仮想環境（推奨）

### 基本インストール

仮想環境を作成して有効化します:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

自動最適化機能付きのOliveをインストールします:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### オプション依存関係

Oliveは追加機能のためのさまざまなオプション依存関係を提供します:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### インストール確認

```bash
olive --help
```

成功すると、Olive CLIのヘルプメッセージが表示されます。

## クイックスタートガイド

### 初めての最適化

Oliveの自動最適化機能を使用して、小型の言語モデルを最適化してみましょう:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### このコマンドの動作

最適化プロセスには以下が含まれます: ローカルキャッシュからモデルを取得、ONNXグラフをキャプチャしてONNXデータファイルにウェイトを保存、ONNXグラフの最適化、RTNメソッドを使用してモデルをint4に量子化。

### コマンドパラメータの説明

- `--model_name_or_path`: Hugging Faceモデル識別子またはローカルパス
- `--output_path`: 最適化されたモデルを保存するディレクトリ
- `--device`: ターゲットデバイス（cpu, gpu）
- `--provider`: 実行プロバイダー（CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider）
- `--use_ort_genai`: 推論にONNX Runtime Generate AIを使用
- `--precision`: 量子化精度（int4, int8, fp16）
- `--log_level`: ログの詳細度（0=最小, 1=詳細）

## 例: Qwen3をONNX INT4に変換する

Hugging Faceの例 [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) に基づき、Qwen3モデルを最適化する方法を紹介します:

### ステップ1: モデルをダウンロード（オプション）

ダウンロード時間を短縮するため、必要なファイルのみをキャッシュします:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ステップ2: Qwen3モデルを最適化

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ステップ3: 最適化されたモデルをテスト

最適化されたモデルをテストする簡単なPythonスクリプトを作成します:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### 出力構造

最適化後、出力ディレクトリには以下が含まれます:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## 高度な使用法

### 設定ファイル

より複雑な最適化ワークフローにはJSON設定ファイルを使用できます:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

設定を使用して実行:

```bash
olive run --config config.json
```

### GPU最適化

CUDA GPU最適化の場合:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML（Windows）の場合:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Oliveによるファインチューニング

Oliveはモデルのファインチューニングもサポートしています:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## ベストプラクティス

### 1. モデル選択
- テストには小型モデル（例: 0.5B-7Bパラメータ）から始める
- ターゲットモデルアーキテクチャがOliveでサポートされていることを確認

### 2. ハードウェアの考慮
- 最適化ターゲットを展開するハードウェアに合わせる
- CUDA対応ハードウェアがある場合はGPU最適化を使用
- WindowsマシンではDirectMLを検討

### 3. 精度選択
- **INT4**: 最大圧縮、わずかな精度低下
- **INT8**: サイズと精度のバランスが良い
- **FP16**: 精度低下が最小、サイズ削減は中程度

### 4. テストと検証
- 最適化されたモデルを特定のユースケースで必ずテスト
- パフォーマンス指標（レイテンシ、スループット、精度）を比較
- 評価には代表的な入力データを使用

### 5. 繰り返し最適化
- クイック結果には自動最適化から始める
- 設定ファイルを使用して詳細な制御を行う
- 異なる最適化パスを試してみる

## トラブルシューティング

### よくある問題

#### 1. インストールの問題
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPUの問題
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. メモリの問題
- 最適化中にバッチサイズを小さくする
- まず高精度で量子化を試す（int8を使用してint4の代わりに）
- モデルキャッシュに十分なディスクスペースを確保

#### 4. モデル読み込みエラー
- モデルパスとアクセス権を確認
- モデルが`trust_remote_code=True`を必要とするか確認
- 必要なモデルファイルがすべてダウンロードされていることを確認

### ヘルプを得る

- **ドキュメント**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **例**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipesリポジトリ

### Olive Recipesの概要

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) リポジトリは、人気のAIモデルの最適化レシピを網羅した包括的なコレクションを提供し、Oliveツールキットを補完します。このリポジトリは、公開モデルの最適化や独自モデルの最適化ワークフローを作成するための実用的な参考資料として機能します。

### 主な特徴

- **100以上の事前構築レシピ**: 人気モデルの最適化設定がすぐに使用可能
- **マルチアーキテクチャ対応**: トランスフォーマーモデル、ビジョンモデル、マルチモーダルアーキテクチャを網羅
- **ハードウェア特化の最適化**: CPU、GPU、専用アクセラレータ向けのレシピ
- **人気モデルファミリー**: Phi、Llama、Qwen、Gemma、Mistralなどを含む

### サポートされているモデルファミリー

リポジトリには以下のモデルの最適化レシピが含まれています:

#### 言語モデル
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5シリーズ（0.5Bから14B）
- **Google Gemma**: 各種Gemmaモデル設定
- **Mistral AI**: Mistral-7Bシリーズ
- **DeepSeek**: R1-Distillシリーズモデル

#### ビジョンおよびマルチモーダルモデル
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIPモデル**: 各種CLIP-ViT設定
- **ResNet**: ResNet-50の最適化
- **Vision Transformers**: ViT-base-patch16-224

#### 特化型モデル
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: ベースおよび多言語バリアント
- **Sentence Transformers**: all-MiniLM-L6-v2

### Olive Recipesの使用方法

#### 方法1: 特定のレシピをクローン

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### 方法2: レシピをテンプレートとして使用

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### レシピ構造

各レシピディレクトリには通常以下が含まれます:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### 例: Phi-4-miniレシピの使用

Phi-4-miniレシピを例にとってみましょう:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

設定ファイルには通常以下が含まれます:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### レシピのカスタマイズ

#### ターゲットハードウェアの変更

ターゲットハードウェアを変更するには、`systems`セクションを更新します:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### 最適化パラメータの調整

異なる最適化レベルのために`passes`セクションを変更します:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### 独自のレシピを作成

1. **類似モデルから開始**: 類似したアーキテクチャのモデルのレシピを見つける
2. **モデル設定を更新**: 設定内のモデル名/パスを変更
3. **パラメータを調整**: 必要に応じて最適化パラメータを変更
4. **テストと検証**: 最適化を実行し、結果を検証
5. **コミュニティに貢献**: 作成したレシピをリポジトリに寄稿することを検討

### レシピ使用の利点

#### 1. **実証済みの設定**
- 特定モデル向けにテスト済みの最適化設定
- 最適なパラメータを見つける試行錯誤を回避

#### 2. **ハードウェア特化の調整**
- 異なる実行プロバイダー向けに事前最適化
- CPU、GPU、NPUターゲット向けの設定がすぐに使用可能

#### 3. **包括的なカバレッジ**
- 人気のオープンソースモデルをサポート
- 新しいモデルリリースに伴う定期的な更新

#### 4. **コミュニティ貢献**
- AIコミュニティとの共同開発
- 知識とベストプラクティスの共有

### Olive Recipesへの貢献

リポジトリに含まれていないモデルを最適化した場合:

1. **リポジトリをフォーク**: olive-recipesをフォーク
2. **レシピディレクトリを作成**: モデル用の新しいディレクトリを追加
3. **設定を含める**: olive_config.jsonとサポートファイルを追加
4. **使用法を文書化**: 明確な使用説明をREADMEに記載
5. **プルリクエストを送信**: コミュニティに貢献

### パフォーマンスベンチマーク

多くのレシピには以下を示すパフォーマンスベンチマークが含まれています:
- **レイテンシ改善**: ベースラインに比べて通常2-6倍の速度向上
- **メモリ削減**: 量子化によりメモリ使用量が50-75%削減
- **精度保持**: 精度の95-99%を維持

### AIツールキットとの統合

レシピは以下とシームレスに連携します:
- **VS Code AI Toolkit**: モデル最適化の直接統合
- **Azure Machine Learning**: クラウドベースの最適化ワークフロー
- **ONNX Runtime**: 最適化された推論展開

## 追加リソース

### 公式リンク
- **GitHubリポジトリ**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipesリポジトリ**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtimeドキュメント**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face例**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### コミュニティ例
- **Jupyter Notebooks**: Olive GitHubリポジトリで利用可能 — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code拡張機能**: AI Toolkit for VS Code概要 — https://learn.microsoft.com/azure/ai-toolkit/overview
- **ブログ投稿**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### 関連ツール
- **ONNX Runtime**: 高性能推論エンジン — https://onnxruntime.ai/
- **Hugging Face Transformers**: 多くの互換性のあるモデルのソース — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: クラウドベースの最適化ワークフロー — https://learn.microsoft.com/azure/machine-learning/

## ➡️ 次に進む

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**免責事項**:  
この文書はAI翻訳サービス[Co-op Translator](https://github.com/Azure/co-op-translator)を使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確さが含まれる可能性があります。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解について、当社は責任を負いません。