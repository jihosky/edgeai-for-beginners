<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:12:46+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "no"
}
-->
# Seksjon 3: Microsoft Olive Optimaliseringsverktøy

## Innholdsfortegnelse
1. [Introduksjon](../../../Module04)
2. [Hva er Microsoft Olive?](../../../Module04)
3. [Installasjon](../../../Module04)
4. [Hurtigstartveiledning](../../../Module04)
5. [Eksempel: Konvertering av Qwen3 til ONNX INT4](../../../Module04)
6. [Avansert bruk](../../../Module04)
7. [Olive-oppskriftsarkiv](../../../Module04)
8. [Beste praksis](../../../Module04)
9. [Feilsøking](../../../Module04)
10. [Ekstra ressurser](../../../Module04)

## Introduksjon

Microsoft Olive er et kraftig og brukervennlig verktøy for maskinvarebevisst modelloptimalisering som forenkler prosessen med å optimalisere maskinlæringsmodeller for distribusjon på ulike maskinvareplattformer. Enten du sikter mot CPU-er, GPU-er eller spesialiserte AI-akseleratorer, hjelper Olive deg med å oppnå optimal ytelse samtidig som modellens nøyaktighet opprettholdes.

## Hva er Microsoft Olive?

Olive er et brukervennlig verktøy for maskinvarebevisst modelloptimalisering som kombinerer ledende teknikker innen modellkompresjon, optimalisering og kompilering. Det fungerer med ONNX Runtime som en ende-til-ende løsning for inferensoptimalisering.

### Nøkkelfunksjoner

- **Maskinvarebevisst optimalisering**: Velger automatisk de beste optimaliseringsteknikkene for din målmaskinvare
- **40+ innebygde optimaliseringskomponenter**: Dekker modellkompresjon, kvantisering, grafoptimalisering og mer
- **Enkel CLI-grensesnitt**: Enkle kommandoer for vanlige optimaliseringsoppgaver
- **Støtte for flere rammeverk**: Fungerer med PyTorch, Hugging Face-modeller og ONNX
- **Støtte for populære modeller**: Olive kan automatisk optimalisere populære modellarkitekturer som Llama, Phi, Qwen, Gemma, osv. rett ut av boksen

### Fordeler

- **Redusert utviklingstid**: Ingen behov for manuell eksperimentering med ulike optimaliseringsteknikker
- **Ytelsesforbedringer**: Betydelige hastighetsforbedringer (opptil 6x i noen tilfeller)
- **Plattformuavhengig distribusjon**: Optimaliserte modeller fungerer på tvers av ulike maskinvare og operativsystemer
- **Opprettholdt nøyaktighet**: Optimaliseringer bevarer modellkvaliteten samtidig som ytelsen forbedres

## Installasjon

### Forutsetninger

- Python 3.8 eller nyere
- pip-pakkebehandler
- Virtuelt miljø (anbefales)

### Grunnleggende installasjon

Opprett og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installer Olive med funksjoner for automatisk optimalisering:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valgfrie avhengigheter

Olive tilbyr ulike valgfrie avhengigheter for ekstra funksjoner:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifiser installasjonen

```bash
olive --help
```

Hvis installasjonen er vellykket, bør du se Olive CLI-hjelpemeldingen.

## Hurtigstartveiledning

### Din første optimalisering

La oss optimalisere en liten språkmodell ved hjelp av Olives funksjon for automatisk optimalisering:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hva denne kommandoen gjør

Optimaliseringsprosessen innebærer: å hente modellen fra lokal cache, fange ONNX-grafen og lagre vektene i en ONNX-datafil, optimalisere ONNX-grafen og kvantisere modellen til int4 ved hjelp av RTN-metoden.

### Forklaring av kommandoparametere

- `--model_name_or_path`: Hugging Face-modellidentifikator eller lokal sti
- `--output_path`: Katalog der den optimaliserte modellen vil bli lagret
- `--device`: Målmaskinvare (cpu, gpu)
- `--provider`: Utførelsesleverandør (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Bruk ONNX Runtime Generate AI for inferens
- `--precision`: Kvantiseringpresisjon (int4, int8, fp16)
- `--log_level`: Loggnivå (0=minimal, 1=detaljert)

## Eksempel: Konvertering av Qwen3 til ONNX INT4

Basert på det oppgitte Hugging Face-eksempelet på [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), slik optimaliserer du en Qwen3-modell:

### Trinn 1: Last ned modell (valgfritt)

For å minimere nedlastingstid, cache kun nødvendige filer:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Trinn 2: Optimaliser Qwen3-modellen

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Trinn 3: Test den optimaliserte modellen

Opprett et enkelt Python-skript for å teste din optimaliserte modell:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Utgangsstruktur

Etter optimalisering vil utgangskatalogen din inneholde:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Avansert bruk

### Konfigurasjonsfiler

For mer komplekse optimaliseringsarbeidsflyter kan du bruke JSON-konfigurasjonsfiler:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Kjør med konfigurasjon:

```bash
olive run --config config.json
```

### GPU-optimalisering

For CUDA GPU-optimalisering:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

For DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finjustering med Olive

Olive støtter også finjustering av modeller:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Beste praksis

### 1. Modellvalg
- Start med mindre modeller for testing (f.eks. 0.5B-7B parametere)
- Sørg for at målmodellarkitekturen støttes av Olive

### 2. Maskinvarehensyn
- Match optimaliseringsmålet ditt med distribusjonsmaskinvaren
- Bruk GPU-optimalisering hvis du har CUDA-kompatibel maskinvare
- Vurder DirectML for Windows-maskiner med integrert grafikk

### 3. Presisjonsvalg
- **INT4**: Maksimal kompresjon, liten nøyaktighetstap
- **INT8**: God balanse mellom størrelse og nøyaktighet
- **FP16**: Minimalt nøyaktighetstap, moderat størrelsesreduksjon

### 4. Testing og validering
- Test alltid optimaliserte modeller med dine spesifikke bruksområder
- Sammenlign ytelsesmålinger (latens, gjennomstrømning, nøyaktighet)
- Bruk representativ inputdata for evaluering

### 5. Iterativ optimalisering
- Start med automatisk optimalisering for raske resultater
- Bruk konfigurasjonsfiler for finjustert kontroll
- Eksperimenter med ulike optimaliseringspass

## Feilsøking

### Vanlige problemer

#### 1. Installasjonsproblemer
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-problemer
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Minneproblemer
- Bruk mindre batch-størrelser under optimalisering
- Prøv kvantisering med høyere presisjon først (int8 i stedet for int4)
- Sørg for tilstrekkelig diskplass for modellcaching

#### 4. Modellinnlastingsfeil
- Verifiser modellsti og tilgangstillatelser
- Sjekk om modellen krever `trust_remote_code=True`
- Sørg for at alle nødvendige modelfiler er lastet ned

### Få hjelp

- **Dokumentasjon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Eksempler**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive-oppskriftsarkiv

### Introduksjon til Olive-oppskrifter

Arkivet [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) kompletterer hovedverktøyet Olive ved å tilby en omfattende samling av klare optimaliseringsoppskrifter for populære AI-modeller. Dette arkivet fungerer som en praktisk referanse for både optimalisering av offentlige modeller og oppretting av optimaliseringsarbeidsflyter for proprietære modeller.

### Nøkkelfunksjoner

- **100+ ferdige oppskrifter**: Klar-til-bruk optimaliseringskonfigurasjoner for populære modeller
- **Støtte for flere arkitekturer**: Dekker transformermodeller, visuelle modeller og multimodale arkitekturer
- **Maskinvare-spesifikke optimaliseringer**: Oppskrifter tilpasset CPU, GPU og spesialiserte akseleratorer
- **Populære modelfamilier**: Inkluderer Phi, Llama, Qwen, Gemma, Mistral og mange flere

### Støttede modelfamilier

Arkivet inkluderer optimaliseringsoppskrifter for:

#### Språkmodeller
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5-serien (0.5B til 14B)
- **Google Gemma**: Ulike Gemma-modellkonfigurasjoner
- **Mistral AI**: Mistral-7B-serien
- **DeepSeek**: R1-Distill-serien

#### Visuelle og multimodale modeller
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP-modeller**: Ulike CLIP-ViT-konfigurasjoner
- **ResNet**: ResNet-50-optimaliseringer
- **Visuelle transformatorer**: ViT-base-patch16-224

#### Spesialiserte modeller
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Base- og flerspråklige varianter
- **Setningstransformatorer**: all-MiniLM-L6-v2

### Bruk av Olive-oppskrifter

#### Metode 1: Klon spesifikk oppskrift

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metode 2: Bruk oppskrift som mal

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Oppskriftsstruktur

Hver oppskriftskatalog inneholder vanligvis:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Eksempel: Bruk av Phi-4-mini-oppskrift

La oss bruke Phi-4-mini-oppskriften som et eksempel:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfigurasjonsfilen inneholder vanligvis:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Tilpasning av oppskrifter

#### Endring av målmaskinvare

For å endre målmaskinvare, oppdater `systems`-seksjonen:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Justering av optimaliseringsparametere

Endre `passes`-seksjonen for ulike optimaliseringsnivåer:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Oppretting av egen oppskrift

1. **Start med en lignende modell**: Finn en oppskrift for en modell med lignende arkitektur
2. **Oppdater modellkonfigurasjon**: Endre modellnavn/stien i konfigurasjonen
3. **Justér parametere**: Modifiser optimaliseringsparametere etter behov
4. **Test og valider**: Kjør optimaliseringen og valider resultatene
5. **Bidra tilbake**: Vurder å bidra med din oppskrift til arkivet

### Fordeler med oppskrifter

#### 1. **Beviste konfigurasjoner**
- Testede optimaliseringsinnstillinger for spesifikke modeller
- Unngår prøving og feiling for å finne optimale parametere

#### 2. **Maskinvare-spesifikk tuning**
- Forhåndsoptimalisert for ulike utførelsesleverandører
- Klar-til-bruk konfigurasjoner for CPU-, GPU- og NPU-mål

#### 3. **Omfattende dekning**
- Støtter de mest populære open-source-modellene
- Regelmessige oppdateringer med nye modellutgivelser

#### 4. **Fellesskapsbidrag**
- Samarbeidsutvikling med AI-fellesskapet
- Delt kunnskap og beste praksis

### Bidrag til Olive-oppskrifter

Hvis du har optimalisert en modell som ikke dekkes i arkivet:

1. **Fork arkivet**: Opprett din egen fork av olive-recipes
2. **Opprett oppskriftskatalog**: Legg til en ny katalog for din modell
3. **Inkluder konfigurasjon**: Legg til olive_config.json og støttende filer
4. **Dokumenter bruk**: Gi en klar README med instruksjoner
5. **Send inn Pull Request**: Bidra tilbake til fellesskapet

### Ytelsesbenchmarker

Mange oppskrifter inkluderer ytelsesbenchmarker som viser:
- **Latensforbedringer**: Typisk 2-6x hastighetsøkning sammenlignet med baseline
- **Minnereduksjon**: 50-75% reduksjon i minnebruk med kvantisering
- **Nøyaktighetsbevaring**: 95-99% opprettholdelse av nøyaktighet

### Integrasjon med AI-verktøy

Oppskriftene fungerer sømløst med:
- **VS Code AI Toolkit**: Direkte integrasjon for modelloptimalisering
- **Azure Machine Learning**: Skybaserte optimaliseringsarbeidsflyter
- **ONNX Runtime**: Optimalisert inferensdistribusjon

## Ekstra ressurser

### Offisielle lenker
- **GitHub-arkiv**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive-oppskriftsarkiv**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime-dokumentasjon**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face-eksempel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Fellesskapseksempler
- **Jupyter Notebooks**: Tilgjengelig i Olive GitHub-arkivet — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code-utvidelse**: AI Toolkit for VS Code oversikt — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogginnlegg**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Relaterte verktøy
- **ONNX Runtime**: Høyytelses inferensmotor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Kilde til mange kompatible modeller — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Skybaserte optimaliseringsarbeidsflyter — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Hva er neste

- [04: OpenVINO Toolkit Optimaliseringsverktøy](./04.openvino.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på dets opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.