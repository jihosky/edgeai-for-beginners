<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:38:54+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "sv"
}
-->
# Avsnitt 2: Lokal miljöutplacering - Integritetsfokuserade lösningar

Lokal utplacering av små språkmodeller (SLMs) representerar ett paradigmskifte mot integritetsbevarande och kostnadseffektiva AI-lösningar. Denna omfattande guide utforskar två kraftfulla ramverk—Ollama och Microsoft Foundry Local—som gör det möjligt för utvecklare att utnyttja SLMs fulla potential samtidigt som de behåller full kontroll över sin utplaceringsmiljö.

## Introduktion

I denna lektion kommer vi att utforska avancerade utplaceringsstrategier för små språkmodeller i lokala miljöer. Vi kommer att täcka de grundläggande koncepten för lokal AI-utplacering, granska två ledande plattformar (Ollama och Microsoft Foundry Local) och ge praktisk vägledning för produktionsklara lösningar.

## Lärandemål

Efter denna lektion kommer du att kunna:

- Förstå arkitekturen och fördelarna med ramverk för lokal SLM-utplacering.
- Implementera produktionsklara utplaceringar med Ollama och Microsoft Foundry Local.
- Jämföra och välja den lämpligaste plattformen baserat på specifika krav och begränsningar.
- Optimera lokala utplaceringar för prestanda, säkerhet och skalbarhet.

## Förstå lokal SLM-utplaceringsarkitektur

Lokal SLM-utplacering representerar ett grundläggande skifte från molnbaserade AI-tjänster till lokala, integritetsbevarande lösningar. Detta tillvägagångssätt gör det möjligt för organisationer att behålla full kontroll över sin AI-infrastruktur samtidigt som de säkerställer datasuveränitet och operativ självständighet.

### Klassificering av utplaceringsramverk

Att förstå olika utplaceringsmetoder hjälper till att välja rätt strategi för specifika användningsområden:

- **Utvecklingsfokuserad**: Enkel installation för experiment och prototyper
- **Företagsklass**: Produktionsklara lösningar med företagsintegrationsmöjligheter  
- **Plattformsoberoende**: Universell kompatibilitet över olika operativsystem och hårdvara

### Viktiga fördelar med lokal SLM-utplacering

Lokal SLM-utplacering erbjuder flera grundläggande fördelar som gör den idealisk för företags- och integritetskänsliga applikationer:

**Integritet och säkerhet**: Lokal bearbetning säkerställer att känslig data aldrig lämnar organisationens infrastruktur, vilket möjliggör efterlevnad av GDPR, HIPAA och andra regleringskrav. Luftgapade utplaceringar är möjliga för klassificerade miljöer, medan fullständiga granskningsspår bibehåller säkerhetsövervakning.

**Kostnadseffektivitet**: Eliminering av prissättningsmodeller per token minskar driftskostnaderna avsevärt. Lägre bandbreddskrav och minskad molnberoende ger förutsägbara kostnadsstrukturer för företagsbudgetering.

**Prestanda och tillförlitlighet**: Snabbare inferenstider utan nätverksfördröjning möjliggör realtidsapplikationer. Offlinefunktionalitet säkerställer kontinuerlig drift oavsett internetanslutning, medan lokal resursoptimering ger konsekvent prestanda.

## Ollama: Universell plattform för lokal utplacering

### Kärnarkitektur och filosofi

Ollama är utformad som en universell, utvecklarvänlig plattform som demokratiserar lokal LLM-utplacering över olika hårdvarukonfigurationer och operativsystem.

**Teknisk grund**: Byggd på det robusta llama.cpp-ramverket använder Ollama det effektiva GGUF-modellformatet för optimal prestanda. Plattformsoberoende kompatibilitet säkerställer konsekvent beteende över Windows, macOS och Linux-miljöer, medan intelligent resursförvaltning optimerar CPU-, GPU- och minnesanvändning.

**Designfilosofi**: Ollama prioriterar enkelhet utan att kompromissa med funktionalitet, och erbjuder utplacering utan konfiguration för omedelbar produktivitet. Plattformen bibehåller bred modellkompatibilitet samtidigt som den tillhandahåller konsekventa API:er över olika modellarkitekturer.

### Avancerade funktioner och kapaciteter

**Excellens inom modellhantering**: Ollama erbjuder omfattande livscykelhantering av modeller med automatisk hämtning, caching och versionshantering. Plattformen stöder ett omfattande modelekosystem inklusive Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral och specialiserade inbäddningsmodeller.

**Anpassning genom Modelfiles**: Avancerade användare kan skapa anpassade modellkonfigurationer med specifika parametrar, systemprompter och beteendeförändringar. Detta möjliggör domänspecifika optimeringar och specialiserade applikationskrav.

**Prestandaoptimering**: Ollama upptäcker och använder automatiskt tillgänglig hårdvaruacceleration inklusive NVIDIA CUDA, Apple Metal och OpenCL. Intelligent minneshantering säkerställer optimal resursanvändning över olika hårdvarukonfigurationer.

### Produktionsimplementeringsstrategier

**Installation och inställning**: Ollama erbjuder enkel installation över plattformar via inbyggda installationsprogram, pakethanterare (WinGet, Homebrew, APT) och Docker-containrar för containeriserade utplaceringar.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Grundläggande kommandon och operationer**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Avancerad konfiguration**: Modelfiles möjliggör sofistikerad anpassning för företagskrav:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Exempel på utvecklarintegration

**Python API-integration**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript-integration (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API-användning med cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Prestandajustering och optimering

**Minnes- och trådinställningar**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Kvantisering för olika hårdvara**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Företagsplattform för Edge AI

### Företagsklassad arkitektur

Microsoft Foundry Local representerar en omfattande företagslösning som är specifikt utformad för produktionsutplaceringar av edge AI med djup integration i Microsoft-ekosystemet.

**ONNX-baserad grund**: Byggd på den industristandardiserade ONNX Runtime, erbjuder Foundry Local optimerad prestanda över olika hårdvaruarkitekturer. Plattformen utnyttjar Windows ML-integration för inbyggd Windows-optimering samtidigt som den bibehåller plattformsoberoende kompatibilitet.

**Excellens inom hårdvaruacceleration**: Foundry Local har intelligent hårdvarudetektion och optimering över CPU:er, GPU:er och NPU:er. Djupgående samarbete med hårdvaruleverantörer (AMD, Intel, NVIDIA, Qualcomm) säkerställer optimal prestanda på företags hårdvarukonfigurationer.

### Avancerad utvecklarupplevelse

**Multi-gränssnittstillgång**: Foundry Local erbjuder omfattande utvecklingsgränssnitt inklusive ett kraftfullt CLI för modellhantering och utplacering, SDK:er för flera språk (Python, NodeJS) för inbyggd integration och RESTful API:er med OpenAI-kompatibilitet för smidig migrering.

**Integration med Visual Studio**: Plattformen integreras sömlöst med AI Toolkit för VS Code, vilket tillhandahåller verktyg för modellkonvertering, kvantisering och optimering inom utvecklingsmiljön. Denna integration påskyndar utvecklingsflöden och minskar utplaceringskomplexiteten.

**Pipeline för modelloptimering**: Microsoft Olive-integration möjliggör sofistikerade arbetsflöden för modelloptimering inklusive dynamisk kvantisering, grafoptimering och hårdvaruspecifik justering. Molnbaserade konverteringsmöjligheter via Azure ML erbjuder skalbar optimering för stora modeller.

### Produktionsimplementeringsstrategier

**Installation och konfiguration**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Operationer för modellhantering**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Avancerad utplaceringskonfiguration**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Integration i företags-ekosystemet

**Säkerhet och efterlevnad**: Foundry Local erbjuder säkerhetsfunktioner på företagsnivå inklusive rollbaserad åtkomstkontroll, granskningsloggning, efterlevnadsrapportering och krypterad modellförvaring. Integration med Microsofts säkerhetsinfrastruktur säkerställer efterlevnad av företags säkerhetspolicyer.

**Inbyggda AI-tjänster**: Plattformen erbjuder färdiga AI-funktioner inklusive Phi Silica för lokal språkbehandling, AI Imaging för bildförbättring och analys, samt specialiserade API:er för vanliga företags-AI-uppgifter.

## Jämförande analys: Ollama vs Foundry Local

### Jämförelse av teknisk arkitektur

| **Aspekt** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Modellformat** | GGUF (via llama.cpp) | ONNX (via ONNX Runtime) |
| **Plattformsfokus** | Universell plattformsoberoende | Windows/företagsoptimering |
| **Hårdvaruintegration** | Generellt GPU/CPU-stöd | Djup Windows ML, NPU-stöd |
| **Optimering** | llama.cpp kvantisering | Microsoft Olive + ONNX Runtime |
| **Företagsfunktioner** | Community-driven | Företagsklass med SLA:er |

### Prestandaegenskaper

**Ollamas prestandastyrkor**:
- Exceptionell CPU-prestanda genom llama.cpp-optimering
- Konsekvent beteende över olika plattformar och hårdvara
- Effektiv minnesanvändning med intelligent modellhantering
- Snabba kallstartstider för utvecklings- och testscenarier

**Foundry Locals prestandafördelar**:
- Överlägsen NPU-användning på modern Windows-hårdvara
- Optimerad GPU-acceleration genom leverantörspartnerskap
- Prestandaövervakning och optimering på företagsnivå
- Skalbara utplaceringsmöjligheter för produktionsmiljöer

### Analys av utvecklarupplevelse

**Ollamas utvecklarupplevelse**:
- Minimala installationskrav med omedelbar produktivitet
- Intuitivt kommandoradsgränssnitt för alla operationer
- Omfattande community-stöd och dokumentation
- Flexibel anpassning genom Modelfiles

**Foundry Locals utvecklarupplevelse**:
- Omfattande IDE-integration med Visual Studio-ekosystemet
- Företagsutvecklingsarbetsflöden med team-samarbetsfunktioner
- Professionella supportkanaler med Microsofts stöd
- Avancerade felsöknings- och optimeringsverktyg

### Optimering av användningsområden

**Välj Ollama när**:
- Utveckling av plattformsoberoende applikationer kräver konsekvent beteende
- Öppenhet och community-bidrag prioriteras
- Begränsade resurser eller budgetrestriktioner råder
- Experimentella eller forskningsfokuserade applikationer byggs
- Bred modellkompatibilitet över olika arkitekturer krävs

**Välj Foundry Local när**:
- Utplacering av företagsapplikationer med strikta prestandakrav
- Utnyttjande av Windows-specifika hårdvaruoptimeringar (NPU, Windows ML)
- Företagsstöd, SLA:er och efterlevnadsfunktioner krävs
- Byggande av produktionsapplikationer med Microsoft-ekosystemintegration
- Avancerade optimeringsverktyg och professionella utvecklingsarbetsflöden behövs

## Avancerade utplaceringsstrategier

### Containeriserade utplaceringsmönster

**Ollama-containerisering**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local företagsutplacering**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Prestandaoptimeringstekniker

**Ollamas optimeringsstrategier**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local optimering**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Säkerhets- och efterlevnadsöverväganden

### Implementering av företagsäkerhet

**Ollamas bästa säkerhetspraxis**:
- Nätverksisolering med brandväggsregler och VPN-åtkomst
- Autentisering genom omvänd proxyintegration
- Verifiering av modellintegritet och säker modelldistribution
- Granskningsloggning för API-åtkomst och modelloperationer

**Foundry Locals företagsäkerhet**:
- Inbyggd rollbaserad åtkomstkontroll med Active Directory-integration
- Omfattande granskningsspår med efterlevnadsrapportering
- Krypterad modellförvaring och säker modellutplacering
- Integration med Microsofts säkerhetsinfrastruktur

### Efterlevnad och regleringskrav

Båda plattformarna stöder regleringsöverensstämmelse genom:
- Kontroll över dataresidens som säkerställer lokal bearbetning
- Granskningsloggning för regleringsrapportering
- Åtkomstkontroller för hantering av känslig data
- Kryptering vid vila och under överföring för dataskydd

## Bästa praxis för produktionsutplacering

### Övervakning och insyn

**Viktiga mätvärden att övervaka**:
- Modellens inferenslatens och genomströmning
- Resursanvändning (CPU, GPU, minne)
- API-svarstider och felprocent
- Modellens noggrannhet och prestandadrift

**Implementering av övervakning**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Kontinuerlig integration och utplacering

**Integration av CI/CD-pipeline**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Framtida trender och överväganden

### Framväxande teknologier

Landskapet för lokal SLM-utplacering fortsätter att utvecklas med flera viktiga trender:

**Avancerade modellarkitekturer**: Nästa generations SLMs med förbättrad effektivitet och kapacitetsförhållanden dyker upp, inklusive mixture-of-experts-modeller för dynamisk skalning och specialiserade arkitekturer för edge-utplacering.

**Hårdvaruintegration**: Djupare integration med specialiserad AI-hårdvara inklusive NPU:er, anpassad kisel och edge computing-acceleratorer kommer att ge förbättrade prestandakapaciteter.

**Ekosystemutveckling**: Standardiseringsinsatser över utplaceringsplattformar och förbättrad interoperabilitet mellan olika ramverk kommer att förenkla multipla plattformsutplaceringar.

### Mönster för bruksadoption

**Företagsadoption**: Ökad företagsadoption driven av integritetskrav, kostnadsoptimering och regleringsöverensstämmelse. Statliga och försvarssektorer fokuserar särskilt på luftgapade utplaceringar.

**Globala överväganden**: Internationella krav på datasuveränitet driver adoption av lokal utplacering, särskilt i regioner med strikta dataskyddsregler.

## Utmaningar och överväganden

### Tekniska utmaningar

**Infrastrukturkrav**: Lokal utplacering kräver noggrann kapacitetsplanering och hårdvaruval. Organisationer måste balansera prestandakrav med kostnadsbegränsningar samtidigt som de säkerställer skalbarhet för växande arbetsbelastningar.

**🔧 Underhåll och uppdateringar**: Regelbundna modelluppdateringar, säkerhetsfixar och prestandaoptimering kräver dedikerade resurser och expertis. Automatiserade utplaceringspipelines blir avgörande för produktionsmiljöer.

### Säkerhetsöverväganden

**Modellsäkerhet**: Skydda proprietära modeller från obehörig åtkomst eller extraktion kräver omfattande säkerhetsåtgärder inklusive kryptering, åtkomstkontroller och granskningsloggning.

**Dataskydd**: Säkerställa säker datahantering genom hela inferenspipeline samtidigt som prestanda och användbarhetsstandarder bibehålls.

## Praktisk implementeringschecklista

### ✅ Förutplaceringsbedömning

- [ ] Analys av hårdvarukrav och kapacitetsplanering
- [ ] Definition av nätverksarkitektur och säkerhetskrav
- [ ] Modellval och prestandabenchmarking
- [ ] Validering av efterlevnad och regleringskrav

### ✅ Utplaceringsimplementering

- [ ] Plattformval baserat på kravanalys
- [ ] Installation och konfiguration av vald plattform
- [ ] Implementering av modelloptimering och kvantisering
- [ ] Slutförande av API-integration och testning

### ✅ Produktionsberedskap

- [ ] Konfiguration av övervaknings- och varningssystem
- [ ] Etablering av backup- och katastrofåterställningsprocedurer
- [ ] Slutförande av prestandajustering och optimering
- [ ] Utveckling av dokumentation och utbildningsmaterial

## Slutsats

Valet mellan Ollama och Microsoft Foundry Local beror på specifika organisatoriska krav, tekniska begränsningar och strategiska mål. Båda plattformarna erbjuder övertygande fördelar för lokal SLM-utplacering, där Ollama utmärker sig i

---

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiserade översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på dess ursprungliga språk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.