<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:03:30+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "sv"
}
-->
# Avsnitt 3: Microsoft Olive Optimization Suite

## Innehållsförteckning
1. [Introduktion](../../../Module04)
2. [Vad är Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Snabbstartsguide](../../../Module04)
5. [Exempel: Konvertera Qwen3 till ONNX INT4](../../../Module04)
6. [Avancerad användning](../../../Module04)
7. [Olive Recipes Repository](../../../Module04)
8. [Bästa praxis](../../../Module04)
9. [Felsökning](../../../Module04)
10. [Ytterligare resurser](../../../Module04)

## Introduktion

Microsoft Olive är ett kraftfullt och användarvänligt verktyg för hårdvaruanpassad modelloptimering som förenklar processen att optimera maskininlärningsmodeller för distribution på olika hårdvaruplattformar. Oavsett om du riktar dig mot CPU:er, GPU:er eller specialiserade AI-acceleratorer hjälper Olive dig att uppnå optimal prestanda samtidigt som modellens noggrannhet bibehålls.

## Vad är Microsoft Olive?

Olive är ett användarvänligt verktyg för hårdvaruanpassad modelloptimering som kombinerar branschledande tekniker inom modellkomprimering, optimering och kompilering. Det fungerar med ONNX Runtime som en E2E-lösning för inferensoptimering.

### Viktiga funktioner

- **Hårdvaruanpassad optimering**: Väljer automatiskt de bästa optimeringsteknikerna för din målplattform
- **40+ inbyggda optimeringskomponenter**: Inkluderar modellkomprimering, kvantisering, grafoptimering med mera
- **Enkel CLI-gränssnitt**: Enkla kommandon för vanliga optimeringsuppgifter
- **Stöd för flera ramverk**: Fungerar med PyTorch, Hugging Face-modeller och ONNX
- **Stöd för populära modeller**: Olive kan automatiskt optimera populära modellarkitekturer som Llama, Phi, Qwen, Gemma med flera direkt ur lådan

### Fördelar

- **Minskad utvecklingstid**: Ingen manuell experimentering med olika optimeringstekniker behövs
- **Prestandaförbättringar**: Betydande hastighetsförbättringar (upp till 6x i vissa fall)
- **Plattformsoberoende distribution**: Optimerade modeller fungerar på olika hårdvaror och operativsystem
- **Bibehållen noggrannhet**: Optimeringar bevarar modellens kvalitet samtidigt som prestandan förbättras

## Installation

### Förutsättningar

- Python 3.8 eller högre
- pip-pakethanterare
- Virtuell miljö (rekommenderas)

### Grundläggande installation

Skapa och aktivera en virtuell miljö:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installera Olive med funktioner för automatisk optimering:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valfria beroenden

Olive erbjuder olika valfria beroenden för ytterligare funktioner:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifiera installationen

```bash
olive --help
```

Om installationen lyckas bör du se Olive CLI-hjälpmeddelandet.

## Snabbstartsguide

### Din första optimering

Låt oss optimera en liten språkmodell med Olives funktion för automatisk optimering:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Vad detta kommando gör

Optimeringsprocessen innefattar: att hämta modellen från den lokala cachen, fånga ONNX-grafen och lagra vikterna i en ONNX-datafil, optimera ONNX-grafen och kvantisera modellen till int4 med RTN-metoden.

### Förklaring av kommandoparametrar

- `--model_name_or_path`: Hugging Face-modellidentifierare eller lokal sökväg
- `--output_path`: Katalog där den optimerade modellen sparas
- `--device`: Målenhet (cpu, gpu)
- `--provider`: Utförandeleverantör (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Använd ONNX Runtime Generate AI för inferens
- `--precision`: Kvantiseringsprecision (int4, int8, fp16)
- `--log_level`: Loggnivå (0=minimal, 1=detaljerad)

## Exempel: Konvertera Qwen3 till ONNX INT4

Baserat på det tillhandahållna Hugging Face-exemplet på [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), så här optimerar du en Qwen3-modell:

### Steg 1: Ladda ner modellen (valfritt)

För att minimera nedladdningstiden, cacha endast nödvändiga filer:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Steg 2: Optimera Qwen3-modellen

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Steg 3: Testa den optimerade modellen

Skapa ett enkelt Python-skript för att testa din optimerade modell:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktur för utdata

Efter optimering kommer din utdatakatalog att innehålla:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Avancerad användning

### Konfigurationsfiler

För mer komplexa optimeringsarbetsflöden kan du använda JSON-konfigurationsfiler:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Kör med konfiguration:

```bash
olive run --config config.json
```

### GPU-optimering

För CUDA GPU-optimering:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

För DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finjustering med Olive

Olive stöder också finjustering av modeller:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Bästa praxis

### 1. Modellval
- Börja med mindre modeller för testning (t.ex. 0.5B-7B parametrar)
- Säkerställ att din målmodellarkitektur stöds av Olive

### 2. Hårdvaruöverväganden
- Matcha din optimeringsmål med din distributionshårdvara
- Använd GPU-optimering om du har CUDA-kompatibel hårdvara
- Överväg DirectML för Windows-maskiner med integrerad grafik

### 3. Precisionsval
- **INT4**: Maximal komprimering, viss noggrannhetsförlust
- **INT8**: Bra balans mellan storlek och noggrannhet
- **FP16**: Minimal noggrannhetsförlust, måttlig storleksreduktion

### 4. Testning och validering
- Testa alltid optimerade modeller med dina specifika användningsfall
- Jämför prestandamått (latens, genomströmning, noggrannhet)
- Använd representativa indata för utvärdering

### 5. Iterativ optimering
- Börja med automatisk optimering för snabba resultat
- Använd konfigurationsfiler för finjusterad kontroll
- Experimentera med olika optimeringspass

## Felsökning

### Vanliga problem

#### 1. Installationsproblem
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-problem
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Minnesproblem
- Använd mindre batchstorlekar under optimering
- Försök med kvantisering med högre precision först (int8 istället för int4)
- Säkerställ tillräckligt med diskutrymme för modellcachning

#### 4. Problem med modellinläsning
- Kontrollera modellens sökväg och åtkomsträttigheter
- Kontrollera om modellen kräver `trust_remote_code=True`
- Säkerställ att alla nödvändiga modelfiler är nedladdade

### Få hjälp

- **Dokumentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exempel**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes Repository

### Introduktion till Olive Recipes

[Microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)-arkivet kompletterar huvudverktyget Olive genom att tillhandahålla en omfattande samling av färdiga optimeringsrecept för populära AI-modeller. Detta arkiv fungerar som en praktisk referens för både optimering av offentligt tillgängliga modeller och skapande av optimeringsarbetsflöden för proprietära modeller.

### Viktiga funktioner

- **100+ färdiga recept**: Färdiga optimeringskonfigurationer för populära modeller
- **Stöd för flera arkitekturer**: Inkluderar transformer-modeller, visionsmodeller och multimodala arkitekturer
- **Hårdvaruspecifika optimeringar**: Recept anpassade för CPU, GPU och specialiserade acceleratorer
- **Populära modelfamiljer**: Inkluderar Phi, Llama, Qwen, Gemma, Mistral med flera

### Stödda modelfamiljer

Arkivet innehåller optimeringsrecept för:

#### Språkmodeller
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5-serien (0.5B till 14B)
- **Google Gemma**: Olika Gemma-modellkonfigurationer
- **Mistral AI**: Mistral-7B-serien
- **DeepSeek**: R1-Distill-seriens modeller

#### Visions- och multimodala modeller
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP-modeller**: Olika CLIP-ViT-konfigurationer
- **ResNet**: ResNet-50-optimeringar
- **Vision Transformers**: ViT-base-patch16-224

#### Specialiserade modeller
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Bas- och flerspråkiga varianter
- **Sentence Transformers**: all-MiniLM-L6-v2

### Använda Olive Recipes

#### Metod 1: Klona specifikt recept

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metod 2: Använd recept som mall

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Receptstruktur

Varje receptkatalog innehåller vanligtvis:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Exempel: Använda Phi-4-mini-receptet

Låt oss använda Phi-4-mini-receptet som exempel:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfigurationsfilen innehåller vanligtvis:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Anpassa recept

#### Ändra målplattform

För att ändra målplattform, uppdatera avsnittet `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Justera optimeringsparametrar

Ändra avsnittet `passes` för olika optimeringsnivåer:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Skapa ditt eget recept

1. **Börja med en liknande modell**: Hitta ett recept för en modell med liknande arkitektur
2. **Uppdatera modellkonfigurationen**: Ändra modellnamn/sökväg i konfigurationen
3. **Justera parametrar**: Ändra optimeringsparametrar vid behov
4. **Testa och validera**: Kör optimeringen och validera resultaten
5. **Bidra tillbaka**: Överväg att bidra med ditt recept till arkivet

### Fördelar med att använda recept

#### 1. **Beprövade konfigurationer**
- Testade optimeringsinställningar för specifika modeller
- Undviker trial-and-error vid sökning efter optimala parametrar

#### 2. **Hårdvaruspecifik justering**
- Föroptimerade för olika utförandeleverantörer
- Färdiga konfigurationer för CPU-, GPU- och NPU-mål

#### 3. **Omfattande täckning**
- Stödjer de mest populära open source-modellerna
- Regelbundna uppdateringar med nya modellsläpp

#### 4. **Gemenskapsbidrag**
- Samarbetsutveckling med AI-gemenskapen
- Delad kunskap och bästa praxis

### Bidra till Olive Recipes

Om du har optimerat en modell som inte täcks i arkivet:

1. **Forka arkivet**: Skapa en egen fork av olive-recipes
2. **Skapa receptkatalog**: Lägg till en ny katalog för din modell
3. **Inkludera konfiguration**: Lägg till olive_config.json och stödjande filer
4. **Dokumentera användning**: Tillhandahåll en tydlig README med instruktioner
5. **Skicka pull request**: Bidra tillbaka till gemenskapen

### Prestandabenchmark

Många recept inkluderar prestandabenchmark som visar:
- **Latensförbättringar**: Typiskt 2-6x snabbare än grundnivån
- **Minskad minnesanvändning**: 50-75% minskning med kvantisering
- **Noggrannhetsbevarande**: 95-99% bibehållen noggrannhet

### Integration med AI-verktyg

Recepten fungerar sömlöst med:
- **VS Code AI Toolkit**: Direkt integration för modelloptimering
- **Azure Machine Learning**: Molnbaserade optimeringsarbetsflöden
- **ONNX Runtime**: Optimerad inferensdistribution

## Ytterligare resurser

### Officiella länkar
- **GitHub-repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime-dokumentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face-exempel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Gemenskapsexempel
- **Jupyter Notebooks**: Finns i Olive GitHub-repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit för VS Code översikt — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogginlägg**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Relaterade verktyg
- **ONNX Runtime**: Högpresterande inferensmotor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Källa till många kompatibla modeller — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Molnbaserade optimeringsarbetsflöden — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Vad händer härnäst

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiserade översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på dess ursprungliga språk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.