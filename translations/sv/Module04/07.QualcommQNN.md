<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:54:09+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "sv"
}
-->
# Avsnitt 7: Qualcomm QNN (Qualcomm Neural Network) Optimeringssvit

## Innehållsförteckning
1. [Introduktion](../../../Module04)
2. [Vad är Qualcomm QNN?](../../../Module04)
3. [Installation](../../../Module04)
4. [Snabbstartsguide](../../../Module04)
5. [Exempel: Konvertera och optimera modeller med QNN](../../../Module04)
6. [Avancerad användning](../../../Module04)
7. [Bästa praxis](../../../Module04)
8. [Felsökning](../../../Module04)
9. [Ytterligare resurser](../../../Module04)

## Introduktion

Qualcomm QNN (Qualcomm Neural Network) är ett omfattande AI-inferensramverk som är utformat för att utnyttja den fulla potentialen hos Qualcomms AI-hårdvaruacceleratorer, inklusive Hexagon NPU, Adreno GPU och Kryo CPU. Oavsett om du riktar dig mot mobila enheter, edge computing-plattformar eller bilsystem, erbjuder QNN optimerade inferensmöjligheter som drar nytta av Qualcomms specialiserade AI-bearbetningsenheter för maximal prestanda och energieffektivitet.

## Vad är Qualcomm QNN?

Qualcomm QNN är ett enhetligt AI-inferensramverk som gör det möjligt för utvecklare att effektivt distribuera AI-modeller över Qualcomms heterogena datorkonfiguration. Det tillhandahåller ett enhetligt programmeringsgränssnitt för att få tillgång till Hexagon NPU (Neural Processing Unit), Adreno GPU och Kryo CPU, och väljer automatiskt den optimala bearbetningsenheten för olika modellager och operationer.

### Nyckelfunktioner

- **Heterogen databehandling**: Enhetlig åtkomst till NPU, GPU och CPU med automatisk arbetsfördelning
- **Hårdvaruoptimering**: Specialiserade optimeringar för Qualcomm Snapdragon-plattformar
- **Stöd för kvantisering**: Avancerade tekniker för INT8, INT16 och blandad precision
- **Verktyg för modellkonvertering**: Direkt stöd för TensorFlow, PyTorch, ONNX och Caffe-modeller
- **Optimerad för Edge AI**: Speciellt utformad för mobila och edge-distributionsscenarier med fokus på energieffektivitet

### Fördelar

- **Maximal prestanda**: Utnyttja specialiserad AI-hårdvara för upp till 15x prestandaförbättringar
- **Energieffektivitet**: Optimerad för mobila och batteridrivna enheter med intelligent energihantering
- **Låg latens**: Hårdvaruaccelererad inferens med minimal overhead för realtidsapplikationer
- **Skalbar distribution**: Från smartphones till bilplattformar inom Qualcomms ekosystem
- **Produktionsklar**: Beprövat ramverk som används i miljontals distribuerade enheter

## Installation

### Förutsättningar

- Qualcomm QNN SDK (kräver registrering hos Qualcomm)
- Python 3.7 eller högre
- Kompatibel Qualcomm-hårdvara eller simulator
- Android NDK (för mobil distribution)
- Linux- eller Windows-utvecklingsmiljö

### QNN SDK-installation

1. **Registrera och ladda ner**: Besök Qualcomm Developer Network för att registrera dig och ladda ner QNN SDK
2. **Packa upp SDK**: Packa upp QNN SDK till din utvecklingskatalog
3. **Ställ in miljövariabler**: Konfigurera sökvägar för QNN-verktyg och bibliotek

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python-miljöinställning

Skapa och aktivera en virtuell miljö:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Installera nödvändiga Python-paket:

```bash
pip install numpy tensorflow torch onnx
```

### Verifiera installationen

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Om installationen lyckas bör du se hjälpinformation för varje QNN-verktyg.

## Snabbstartsguide

### Din första modellkonvertering

Låt oss konvertera en enkel PyTorch-modell för att köra på Qualcomm-hårdvara:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Konvertera ONNX till QNN-format

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generera QNN-modellbibliotek

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Vad gör denna process?

Optimeringsarbetsflödet innefattar: att konvertera den ursprungliga modellen till ONNX-format, översätta ONNX till QNN:s mellanrepresentation, tillämpa hårdvaruspecifika optimeringar och generera ett kompilerat modellbibliotek för distribution.

### Förklaring av nyckelparametrar

- `--input_network`: Källa till ONNX-modellfilen
- `--output_path`: Genererad C++-källfil
- `--input_dim`: Ingångstensorers dimensioner för optimering
- `--quantization_overrides`: Anpassad kvantiseringskonfiguration
- `-t x86_64-linux-clang`: Målarkitektur och kompilator

## Exempel: Konvertera och optimera modeller med QNN

### Steg 1: Avancerad modellkonvertering med kvantisering

Så här tillämpar du anpassad kvantisering under konvertering:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konvertera med anpassad kvantisering:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Steg 2: Multi-backend optimering

Konfigurera för heterogen exekvering över NPU, GPU och CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Steg 3: Skapa kontextbinär för distribution

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Steg 4: Inferens med QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Utgångsstruktur

Efter optimering kommer din distributionskatalog att innehålla:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Avancerad användning

### Anpassad backend-konfiguration

Konfigurera specifika backend-optimeringar:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dynamisk kvantisering

Tillämpa kvantisering vid körning för bättre noggrannhet:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Prestandaprofilering

Övervaka prestanda över olika backends:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatisk backend-val

Implementera intelligent backend-val baserat på modellens egenskaper:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Bästa praxis

### 1. Optimering av modellarkitektur
- **Lagerfusion**: Kombinera operationer som Conv+BatchNorm+ReLU för bättre NPU-användning
- **Djupseparerade konvolutioner**: Föredra dessa framför standardkonvolutioner för mobil distribution
- **Kvantiseringvänliga designer**: Använd ReLU-aktiveringar och undvik operationer som inte kvantiseras väl

### 2. Kvantiseringsstrategi
- **Kvantisering efter träning**: Börja med detta för snabb distribution
- **Kalibreringsdataset**: Använd representativa data som täcker alla ingångsvariationer
- **Blandad precision**: Använd INT8 för de flesta lager, behåll kritiska lager i högre precision

### 3. Riktlinjer för backend-val
- **NPU (HTP)**: Bäst för CNN-arbetsbelastningar, kvantiserade modeller och strömkänsliga applikationer
- **GPU**: Optimalt för beräkningsintensiva operationer, större modeller och FP16-precision
- **CPU**: Reserv för ej stödda operationer och felsökning

### 4. Prestandaoptimering
- **Batchstorlek**: Använd batchstorlek 1 för realtidsapplikationer, större batchar för genomströmning
- **Förbearbetning av indata**: Minimera datakopiering och konverteringsöverhead
- **Återanvändning av kontext**: Förkompilera kontexter för att undvika kompilering vid körning

### 5. Minneshantering
- **Tensorallokering**: Använd statisk allokering när det är möjligt för att undvika overhead vid körning
- **Minnespooler**: Implementera anpassade minnespooler för ofta allokerade tensorer
- **Bufferåteranvändning**: Återanvänd in-/utgångsbuffrar mellan inferenskörningar

### 6. Strömoptimering
- **Prestandalägen**: Använd lämpliga prestandalägen baserat på termiska begränsningar
- **Dynamisk frekvensskalning**: Tillåt systemet att skala frekvensen baserat på arbetsbelastning
- **Hantering av viloläge**: Släpp resurser korrekt när de inte används

## Felsökning

### Vanliga problem

#### 1. Problem med SDK-installation
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Fel vid modellkonvertering
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Problem med kvantisering
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Prestandaproblem
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Minnesproblem
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Backend-kompatibilitet
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Prestandafelsökning

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Få hjälp

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN-dokumentation**: Finns i SDK-paketet
- **Community-forum**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Teknisk support**: Via Qualcomms utvecklarportal

## Ytterligare resurser

### Officiella länkar
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon-plattformar**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Utvecklarportal**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Lärresurser
- **Kom igång-guide**: Finns i QNN SDK-dokumentationen
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimeringsguide**: SDK-dokumentationen innehåller omfattande optimeringsriktlinjer
- **Videotutorials**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integrationsverktyg
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Föroptimerade modeller för Qualcomm-hårdvara
- **Android Neural Networks API**: Integration med Android NNAPI
- **TensorFlow Lite Delegate**: Qualcomm-delegat för TFLite

### Prestandajämförelser
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Exempel från communityn
- **Exempelapplikationer**: Finns i QNN SDK-exempelkatalogen
- **GitHub-repositorier**: Exempel och verktyg från communityn
- **Tekniska bloggar**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Relaterade verktyg
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Avancerade kvantiserings- och kompressionstekniker
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - För jämförelse och reservdistribution
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Plattformoberoende inferensmotor

### Hårdvaruspecifikationer
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon-plattformar**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Vad händer härnäst

Fortsätt din Edge AI-resa genom att utforska [Modul 5: SLMOps och produktionsdistribution](../Module05/README.md) för att lära dig om operativa aspekter av livscykelhantering för små språkmodeller.

---

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-översättningstjänsten [Co-op Translator](https://github.com/Azure/co-op-translator). Även om vi strävar efter noggrannhet, bör det noteras att automatiserade översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på dess ursprungliga språk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell mänsklig översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.