<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:25:57+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "ne"
}
-->
# खण्ड २: स्थानीय वातावरणमा परिनियोजन - गोपनीयता-प्रथम समाधानहरू

साना भाषा मोडेलहरू (SLMs) को स्थानीय परिनियोजन गोपनीयता-संरक्षण गर्ने, लागत-प्रभावकारी AI समाधानतर्फको एक नयाँ दृष्टिकोण हो। यो व्यापक मार्गदर्शकले दुई शक्तिशाली फ्रेमवर्कहरू—Ollama र Microsoft Foundry Local—को अन्वेषण गर्दछ, जसले विकासकर्ताहरूलाई SLMs को पूर्ण क्षमता उपयोग गर्न सक्षम बनाउँछ, जबकि परिनियोजन वातावरणमा पूर्ण नियन्त्रण कायम राख्छ।

## परिचय

यस पाठमा, हामी स्थानीय वातावरणमा साना भाषा मोडेलहरूको उन्नत परिनियोजन रणनीतिहरू अन्वेषण गर्नेछौं। हामी स्थानीय AI परिनियोजनका आधारभूत अवधारणाहरू कभर गर्नेछौं, दुई प्रमुख प्लेटफर्महरू (Ollama र Microsoft Foundry Local) को अध्ययन गर्नेछौं, र उत्पादन-तय समाधानहरूको व्यावहारिक कार्यान्वयन मार्गदर्शन प्रदान गर्नेछौं।

## सिकाइ उद्देश्यहरू

यस पाठको अन्त्यसम्ममा, तपाईं सक्षम हुनुहुनेछ:

- स्थानीय SLM परिनियोजन फ्रेमवर्कहरूको वास्तुकला र फाइदाहरू बुझ्न।
- Ollama र Microsoft Foundry Local प्रयोग गरेर उत्पादन-तय परिनियोजन कार्यान्वयन गर्न।
- विशिष्ट आवश्यकताहरू र सीमाहरूको आधारमा उपयुक्त प्लेटफर्मको तुलना र चयन गर्न।
- प्रदर्शन, सुरक्षा, र स्केलेबिलिटीका लागि स्थानीय परिनियोजनहरू अनुकूलित गर्न।

## स्थानीय SLM परिनियोजन वास्तुकलाहरूको समझ

स्थानीय SLM परिनियोजन क्लाउड-निर्भर AI सेवाहरूबाट स्थानीय, गोपनीयता-संरक्षण समाधानतर्फको एक आधारभूत परिवर्तन हो। यस दृष्टिकोणले संगठनहरूलाई आफ्नो AI पूर्वाधारमा पूर्ण नियन्त्रण कायम राख्न सक्षम बनाउँछ, जबकि डाटा सार्वभौमिकता र सञ्चालन स्वतन्त्रता सुनिश्चित गर्दछ।

### परिनियोजन फ्रेमवर्क वर्गीकरणहरू

विभिन्न परिनियोजन दृष्टिकोणहरूको समझले विशिष्ट प्रयोग केसहरूको लागि सही रणनीति चयन गर्न मद्दत गर्दछ:

- **विकास-केंद्रित**: प्रयोग र प्रोटोटाइपको लागि सरल सेटअप
- **उद्यम-स्तर**: उद्यम एकीकरण क्षमताहरू सहित उत्पादन-तय समाधानहरू  
- **क्रस-प्लेटफर्म**: विभिन्न अपरेटिङ प्रणालीहरू र हार्डवेयरमा सार्वभौमिक अनुकूलता

### स्थानीय SLM परिनियोजनका प्रमुख फाइदाहरू

स्थानीय SLM परिनियोजनले उद्यम र गोपनीयता-संवेदनशील अनुप्रयोगहरूको लागि आदर्श बनाउने धेरै आधारभूत फाइदाहरू प्रदान गर्दछ:

**गोपनीयता र सुरक्षा**: स्थानीय प्रशोधनले संवेदनशील डाटा कहिल्यै संगठनको पूर्वाधारबाट बाहिर नजाने सुनिश्चित गर्दछ, GDPR, HIPAA, र अन्य नियामक आवश्यकताहरूको पालना सक्षम बनाउँछ। वर्गीकृत वातावरणहरूको लागि एयर-ग्याप परिनियोजन सम्भव छ, जबकि पूर्ण अडिट ट्रेलहरूले सुरक्षा निगरानी कायम राख्छ।

**लागत प्रभावकारिता**: प्रति-टोकन मूल्य निर्धारण मोडेलहरूको उन्मूलनले सञ्चालन लागतलाई उल्लेखनीय रूपमा कम गर्दछ। कम ब्यान्डविथ आवश्यकताहरू र घटाइएको क्लाउड निर्भरता उद्यम बजेटिङको लागि पूर्वानुमान योग्य लागत संरचना प्रदान गर्दछ।

**प्रदर्शन र विश्वसनीयता**: नेटवर्क विलम्बता बिना छिटो अनुमान समयले वास्तविक-समय अनुप्रयोगहरू सक्षम गर्दछ। अफलाइन कार्यक्षमताले इन्टरनेट जडानको परवाह नगरी निरन्तर सञ्चालन सुनिश्चित गर्दछ, जबकि स्थानीय स्रोत अनुकूलनले स्थिर प्रदर्शन प्रदान गर्दछ।

## Ollama: सार्वभौमिक स्थानीय परिनियोजन प्लेटफर्म

### मुख्य वास्तुकला र दर्शन

Ollama एक सार्वभौमिक, विकासकर्ता-अनुकूल प्लेटफर्मको रूपमा इन्जिनियर गरिएको छ जसले विभिन्न हार्डवेयर कन्फिगरेसनहरू र अपरेटिङ प्रणालीहरूमा स्थानीय LLM परिनियोजनलाई लोकतान्त्रिक बनाउँछ।

**प्राविधिक आधार**: बलियो llama.cpp फ्रेमवर्कमा निर्माण गरिएको, Ollama ले GGUF मोडेल ढाँचाको कुशल प्रयोग गर्दछ। क्रस-प्लेटफर्म अनुकूलताले Windows, macOS, र Linux वातावरणहरूमा स्थिर व्यवहार सुनिश्चित गर्दछ, जबकि बुद्धिमानी स्रोत व्यवस्थापनले CPU, GPU, र मेमोरी उपयोगलाई अनुकूलित गर्दछ।

**डिजाइन दर्शन**: Ollama ले कार्यक्षमता त्याग नगरी सरलता प्राथमिकता दिन्छ, तत्काल उत्पादकताको लागि शून्य-कन्फिगरेसन परिनियोजन प्रदान गर्दछ। प्लेटफर्मले फराकिलो मोडेल अनुकूलता कायम राख्छ जबकि विभिन्न मोडेल वास्तुकलाहरूमा स्थिर APIs प्रदान गर्दछ।

### उन्नत सुविधाहरू र क्षमता

**मोडेल व्यवस्थापन उत्कृष्टता**: Ollama ले स्वचालित पुलिङ, क्यासिङ, र संस्करण सहित व्यापक मोडेल जीवनचक्र व्यवस्थापन प्रदान गर्दछ। प्लेटफर्मले Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral, र विशेष एम्बेडिङ मोडेलहरू सहित विस्तृत मोडेल पारिस्थितिकी तन्त्रलाई समर्थन गर्दछ।

**Modelfiles मार्फत अनुकूलन**: उन्नत प्रयोगकर्ताहरूले विशिष्ट प्यारामिटरहरू, प्रणाली संकेतहरू, र व्यवहार संशोधनहरूसँग अनुकूलित मोडेल कन्फिगरेसनहरू सिर्जना गर्न सक्छन्। यसले डोमेन-विशिष्ट अनुकूलनहरू र विशेष अनुप्रयोग आवश्यकताहरू सक्षम गर्दछ।

**प्रदर्शन अनुकूलन**: Ollama ले उपलब्ध हार्डवेयर एक्सेलेरेशनलाई स्वचालित रूपमा पत्ता लगाउँछ र प्रयोग गर्दछ, जसमा NVIDIA CUDA, Apple Metal, र OpenCL समावेश छ। बुद्धिमानी मेमोरी व्यवस्थापनले विभिन्न हार्डवेयर कन्फिगरेसनहरूमा इष्टतम स्रोत उपयोग सुनिश्चित गर्दछ।

### उत्पादन कार्यान्वयन रणनीतिहरू

**स्थापना र सेटअप**: Ollama ले देशी इन्स्टलरहरू, प्याकेज म्यानेजरहरू (WinGet, Homebrew, APT), र कन्टेनराइज्ड परिनियोजनहरूको लागि Docker कन्टेनरहरू मार्फत प्लेटफर्महरूमा सरल स्थापना प्रदान गर्दछ।

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**आवश्यक आदेशहरू र अपरेशनहरू**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**उन्नत कन्फिगरेसन**: Modelfiles उद्यम आवश्यकताहरूको लागि परिष्कृत अनुकूलन सक्षम गर्दछ:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### विकासकर्ता एकीकरण उदाहरणहरू

**Python API एकीकरण**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**JavaScript/TypeScript एकीकरण (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**RESTful API प्रयोग cURL संग**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### प्रदर्शन ट्युनिङ र अनुकूलन

**मेमोरी र थ्रेड कन्फिगरेसन**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**विभिन्न हार्डवेयरको लागि क्वान्टाइजेशन चयन**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: उद्यम एज AI प्लेटफर्म

### उद्यम-स्तर वास्तुकला

Microsoft Foundry Local उत्पादन एज AI परिनियोजनहरूको लागि विशेष रूपमा डिजाइन गरिएको एक व्यापक उद्यम समाधान हो, जसले Microsoft पारिस्थितिकी तन्त्रमा गहिरो एकीकरण प्रदान गर्दछ।

**ONNX-आधारित आधार**: उद्योग-मानक ONNX Runtime मा निर्माण गरिएको, Foundry Local ले विविध हार्डवेयर वास्तुकलाहरूमा अनुकूलित प्रदर्शन प्रदान गर्दछ। प्लेटफर्मले Windows ML एकीकरणलाई देशी Windows अनुकूलनको लागि उपयोग गर्दछ, जबकि क्रस-प्लेटफर्म अनुकूलता कायम राख्छ।

**हार्डवेयर एक्सेलेरेशन उत्कृष्टता**: Foundry Local ले CPU, GPU, र NPUहरूमा बुद्धिमानी हार्डवेयर पत्ता लगाउने र अनुकूलन प्रदान गर्दछ। हार्डवेयर विक्रेताहरू (AMD, Intel, NVIDIA, Qualcomm) सँगको गहिरो सहयोगले उद्यम हार्डवेयर कन्फिगरेसनहरूमा इष्टतम प्रदर्शन सुनिश्चित गर्दछ।

### उन्नत विकासकर्ता अनुभव

**बहु-इन्टरफेस पहुँच**: Foundry Local ले मोडेल व्यवस्थापन र परिनियोजनको लागि शक्तिशाली CLI, देशी एकीकरणको लागि बहु-भाषा SDKs (Python, NodeJS), र सहज माइग्रेशनको लागि OpenAI अनुकूलता सहित RESTful APIs प्रदान गर्दछ।

**Visual Studio एकीकरण**: प्लेटफर्मले AI Toolkit for VS Code सँग सहज एकीकरण प्रदान गर्दछ, जसले विकास वातावरण भित्र मोडेल रूपान्तरण, क्वान्टाइजेशन, र अनुकूलन उपकरणहरू प्रदान गर्दछ। यस एकीकरणले विकास कार्यप्रवाहलाई गति दिन्छ र परिनियोजन जटिलता कम गर्दछ।

**मोडेल अनुकूलन पाइपलाइन**: Microsoft Olive एकीकरणले गतिशील क्वान्टाइजेशन, ग्राफ अनुकूलन, र हार्डवेयर-विशिष्ट ट्युनिङ सहित परिष्कृत मोडेल अनुकूलन कार्यप्रवाह सक्षम गर्दछ। Azure ML मार्फत क्लाउड-आधारित रूपान्तरण क्षमताहरूले ठूला मोडेलहरूको लागि स्केलेबल अनुकूलन प्रदान गर्दछ।

### उत्पादन कार्यान्वयन रणनीतिहरू

**स्थापना र कन्फिगरेसन**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**मोडेल व्यवस्थापन अपरेशनहरू**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**उन्नत परिनियोजन कन्फिगरेसन**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### उद्यम पारिस्थितिकी तन्त्र एकीकरण

**सुरक्षा र अनुपालन**: Foundry Local ले भूमिका-आधारित पहुँच नियन्त्रण, अडिट लगिङ, अनुपालन रिपोर्टिङ, र एन्क्रिप्टेड मोडेल भण्डारण सहित उद्यम-स्तर सुरक्षा सुविधाहरू प्रदान गर्दछ। Microsoft सुरक्षा पूर्वाधारसँगको एकीकरणले उद्यम सुरक्षा नीतिहरूको पालना सुनिश्चित गर्दछ।

**निर्मित AI सेवाहरू**: प्लेटफर्मले स्थानीय भाषा प्रशोधनको लागि Phi Silica, छवि सुधार र विश्लेषणको लागि AI Imaging, र सामान्य उद्यम AI कार्यहरूको लागि विशेष APIs सहित तयार-प्रयोग AI क्षमताहरू प्रदान गर्दछ।

## तुलनात्मक विश्लेषण: Ollama vs Foundry Local

### प्राविधिक वास्तुकला तुलना

| **पक्ष** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **मोडेल ढाँचा** | GGUF (llama.cpp मार्फत) | ONNX (ONNX Runtime मार्फत) |
| **प्लेटफर्म फोकस** | सार्वभौमिक क्रस-प्लेटफर्म | Windows/उद्यम अनुकूलन |
| **हार्डवेयर एकीकरण** | सामान्य GPU/CPU समर्थन | गहिरो Windows ML, NPU समर्थन |
| **अनुकूलन** | llama.cpp क्वान्टाइजेशन | Microsoft Olive + ONNX Runtime |
| **उद्यम सुविधाहरू** | समुदाय-चालित | उद्यम-स्तर SLAs सहित |

### प्रदर्शन विशेषताहरू

**Ollama प्रदर्शन बलहरू**:
- llama.cpp अनुकूलन मार्फत उत्कृष्ट CPU प्रदर्शन
- विभिन्न प्लेटफर्म र हार्डवेयरमा स्थिर व्यवहार
- बुद्धिमानी मोडेल लोडिङको साथ कुशल मेमोरी उपयोग
- विकास र परीक्षण परिदृश्यहरूको लागि छिटो कोल्ड-स्टार्ट समय

**Foundry Local प्रदर्शन फाइदाहरू**:
- आधुनिक Windows हार्डवेयरमा उत्कृष्ट NPU उपयोग
- विक्रेता साझेदारी मार्फत GPU एक्सेलेरेशन अनुकूलित
- उद्यम-स्तर प्रदर्शन निगरानी र अनुकूलन
- उत्पादन वातावरणहरूको लागि स्केलेबल परिनियोजन क्षमता

### विकास अनुभव विश्लेषण

**Ollama विकासकर्ता अनुभव**:
- न्यूनतम सेटअप आवश्यकताहरूको साथ तत्काल उत्पादकता
- सबै अपरेशनहरूको लागि सहज कमाण्ड-लाइन इन्टरफेस
- व्यापक समुदाय समर्थन र दस्तावेजीकरण
- Modelfiles मार्फत लचिलो अनुकूलन

**Foundry Local विकासकर्ता अनुभव**:
- Visual Studio पारिस्थितिकी तन्त्रसँग व्यापक IDE एकीकरण
- टीम सहयोग सुविधाहरू सहित उद्यम विकास कार्यप्रवाह
- Microsoft समर्थन च्यानलहरूसँग व्यावसायिक समर्थन
- उन्नत डिबगिङ र अनुकूलन उपकरणहरू

### प्रयोग केस अनुकूलन

**Ollama चयन गर्नुहोस् जब**:
- क्रस-प्लेटफर्म अनुप्रयोगहरू विकास गर्दै स्थिर व्यवहार आवश्यक छ
- खुला-स्रोत पारदर्शिता र समुदाय योगदानलाई प्राथमिकता दिँदै
- सीमित स्रोतहरू वा बजेट बाधाहरूको साथ काम गर्दै
- प्रयोगात्मक वा अनुसन्धान-केंद्रित अनुप्रयोगहरू निर्माण गर्दै
- विभिन्न वास्तुकलाहरूमा फराकिलो मोडेल अनुकूलता आवश्यक छ

**Foundry Local चयन गर्नुहोस् जब**:
- कडा प्रदर्शन आवश्यकताहरूको साथ उद्यम अनुप्रयोगहरू परिनियोजन गर्दै
- Windows-विशिष्ट हार्डवेयर अनुकूलनहरू (NPU, Windows ML) उपयोग गर्दै
- उद्यम समर्थन, SLAs, र अनुपालन सुविधाहरू आवश्यक छ
- Microsoft पारिस्थितिकी तन्त्र एकीकरणको साथ उत्पादन अनुप्रयोगहरू निर्माण गर्दै
- उन्नत अनुकूलन उपकरणहरू र व्यावसायिक विकास कार्यप्रवाह आवश्यक छ

## उन्नत परिनियोजन रणनीतिहरू

### कन्टेनराइज्ड परिनियोजन ढाँचाहरू

**Ollama कन्टेनराइजेशन**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Foundry Local उद्यम परिनियोजन**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### प्रदर्शन अनुकूलन प्रविधिहरू

**Ollama अनुकूलन रणनीतिहरू**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Foundry Local अनुकूलन**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## सुरक्षा र अनुपालन विचारहरू

### उद्यम सुरक्षा कार्यान्वयन

**Ollama सुरक्षा उत्तम अभ्यासहरू**:
- फायरवाल नियमहरू र VPN पहुँचको साथ नेटवर्क अलगाव
- रिभर्स प्रोक्सी एकीकरण मार्फत प्रमाणीकरण
- मोडेल अखण्डता प्रमाणीकरण र सुरक्षित मोडेल वितरण
- API पहुँच र मोडेल अपरेशनहरूको लागि अडिट लगिङ

**Foundry Local उद्यम सुरक्षा**:
- Active Directory एकीकरणको साथ निर्मित भूमिका-आधारित पहुँच नियन्त्रण
- अनुपालन रिपोर्टिङको साथ व्यापक अडिट ट्रेलहरू
- एन्क्रिप्टेड मोडेल भण्डारण र सुरक्षित मोडेल परिनियोजन
- Microsoft सुरक्षा पूर्वाधारसँगको एकीकरण

### अनुपालन र नियामक आवश्यकताहरू

दुवै प्लेटफर्मले निम्न मार्फत नियामक अनुपालन समर्थन गर्दछ:
- स्थानीय प्रशोधन सुनिश्चित गर्ने डाटा निवास नियन्त्रणहरू
- नियामक रिपोर्टिङ आवश्यकताहरूको लागि अडिट लगिङ
- संवेदनशील डाटा ह्यान्डलिङको लागि पहुँच नियन्त्रणहरू
- डाटा सुरक्षा सुनिश्चित गर्नको लागि विश्राम र ट्रान्जिटमा एन्क्रिप्शन

## उत्पादन परिनियोजनका लागि उत्तम अभ्यासहरू

### निगरानी र अवलोकनीयता

**निगरानी गर्नुपर्ने प्रमुख मेट्रिक्सहरू**:
- मोडेल अनुमान विलम्बता र थ्रुपुट
- स्रोत उपयोग (CPU, GPU, मेमोरी)
- API प्रतिक्रिया समय र त्रुटि दर
- मोडेल सटीकता र प्रदर्शन विचलन

**निगरानी कार्यान्वयन**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### निरन्तर एकीकरण र परिनियोजन

**CI/CD पाइपलाइन एकीकरण**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## भविष्यका प्रवृत्तिहरू र विचारहरू

### उदीयमान प्रविधिहरू

स्थानीय SLM परिनियोजन परिदृश्यले केही प्रमुख प्रवृत्तिहरूसँग विकास जारी राख्छ:

**उन्नत मोडेल वास्तुकलाहरू**: सुधारिएको दक्षता र क्षमता अनुपातहरू सहित अर्को पुस्ताका SLMs देखा पर्दैछन्, जसमा गतिशील स्केलिङको लागि mixture-of-experts मोडेलहरू र एज परिनियोजनका लागि विशेष वास्तुकलाहरू समावेश छन्।

**हार्डवेयर एकीकरण**: विशेष AI हार्डवेयर सहित गहिरो एकीकरण, जसमा NPUs, कस्टम सिलिकन, र एज कम्प्युटिङ एक्सेलेरेटरहरू समावेश छन्, सुधारिएको प्रदर्शन क्षमताहरू प्रदान गर्नेछ।

**पारिस्थितिकी तन्त्रको विकास**: परिनियोजन प्लेटफर्महरूमा मानकीकरण प्रयासहरू र विभिन्न फ्रेमवर्कहरू बीच सुधारिएको अन्तरसञ्चालनले बहु-प्लेटफर्म परिनियोजनलाई सरल बनाउनेछ।

### उद्योग अपनाउने ढाँचाहरू

**उद्यम अपनाउने**: गोपनीयता आवश्यकताहरू, लागत अनुकूलन, र नियामक अनुपालन आवश्यकताहरूले प्रेरित बढ्दो उद्यम अपनाउने। सरकार र रक्षा क्षेत्रहरू विशेष रूपमा एयर-ग्याप परिनियोजनमा केन्द्रित छन्।

**वैश्विक विचारहरू**: अन्तर्राष्ट्रिय डाटा सार्वभौमिकता आवश्यकताहरूले स्थानीय परिनियोजन अपनाउनेलाई प्रेरित गर्दैछन्, विशेष गरी कडा डाटा संरक्षण नियमहरू भएका क्षेत्रहरूमा।

## चुनौतीहरू र विचारहरू

### प्राविधिक चुनौतीहरू

**पूर्वाधार आवश्यकताहरू**: स्थानीय परिनियोजनले सावधानीपूर्वक क्षमता योजना र हार्डवेयर चयन आवश्यक छ। संगठनहरूले बढ्दो कार्यभारहरूको लागि स्केलेबिलिटी सुनिश्चित गर्दै प्रदर्शन आवश्यकताहरूलाई लागत बाधाहरूको साथ सन्तुलन गर्नुपर्छ।

**🔧 मर्मत र अपडेटहरू**: नियमित मोडेल अपडेटहरू, सुरक्षा प्याचहरू, र प्रदर्शन अनुकूलनले समर्पित स्रोतहरू र विशेषज्ञता आवश्यक छ। उत्पादन वातावरणहरूको लागि स्वचालित परिनियोजन पाइपलाइनहरू आवश्यक हुन्छन्।

### सुरक्षा विचारहरू

**मोडेल सुरक्षा**: अनधिकृत पहुँच वा निकासीबाट स्वामित्व मोडेल

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको छ। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।