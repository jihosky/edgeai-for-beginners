<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T12:02:19+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ne"
}
-->
# खण्ड ३ : माइक्रोसफ्ट ओलिभ अप्टिमाइजेसन सूट

## सामग्री सूची
1. [परिचय](../../../Module04)
2. [माइक्रोसफ्ट ओलिभ के हो?](../../../Module04)
3. [स्थापना](../../../Module04)
4. [छिटो सुरु गर्ने मार्गदर्शन](../../../Module04)
5. [उदाहरण: Qwen3 लाई ONNX INT4 मा रूपान्तरण गर्ने](../../../Module04)
6. [उन्नत प्रयोग](../../../Module04)
7. [ओलिभ रेसिपीहरूको भण्डार](../../../Module04)
8. [सर्वोत्तम अभ्यासहरू](../../../Module04)
9. [समस्या समाधान](../../../Module04)
10. [थप स्रोतहरू](../../../Module04)

## परिचय

माइक्रोसफ्ट ओलिभ एक शक्तिशाली, प्रयोग गर्न सजिलो हार्डवेयर-प्रेरित मोडेल अप्टिमाइजेसन उपकरण हो जसले विभिन्न हार्डवेयर प्लेटफर्महरूमा मेसिन लर्निङ मोडेलहरू तैनात गर्नको लागि अप्टिमाइजेसन प्रक्रिया सरल बनाउँछ। तपाईं CPU, GPU, वा विशेष AI एक्सेलेरेटरहरू लक्षित गर्दै हुनुहुन्छ भने, ओलिभले मोडेलको शुद्धता कायम राख्दै उत्कृष्ट प्रदर्शन प्राप्त गर्न मद्दत गर्दछ।

## माइक्रोसफ्ट ओलिभ के हो?

ओलिभ एक प्रयोग गर्न सजिलो हार्डवेयर-प्रेरित मोडेल अप्टिमाइजेसन उपकरण हो जसले मोडेल कम्प्रेसन, अप्टिमाइजेसन, र कम्पाइलिङमा उद्योग-अग्रणी प्रविधिहरू समेट्छ। यो ONNX Runtime सँग E2E इन्फरेन्स अप्टिमाइजेसन समाधानको रूपमा काम गर्दछ।

### मुख्य विशेषताहरू

- **हार्डवेयर-प्रेरित अप्टिमाइजेसन**: तपाईंको लक्षित हार्डवेयरको लागि स्वतः उत्कृष्ट अप्टिमाइजेसन प्रविधिहरू चयन गर्दछ
- **४०+ निर्मित अप्टिमाइजेसन कम्पोनेन्टहरू**: मोडेल कम्प्रेसन, क्वान्टाइजेसन, ग्राफ अप्टिमाइजेसन, र थप समेट्छ
- **सजिलो CLI इन्टरफेस**: सामान्य अप्टिमाइजेसन कार्यहरूको लागि सरल आदेशहरू
- **मल्टि-फ्रेमवर्क समर्थन**: PyTorch, Hugging Face मोडेलहरू, र ONNX सँग काम गर्दछ
- **लोकप्रिय मोडेल समर्थन**: ओलिभले Llama, Phi, Qwen, Gemma जस्ता लोकप्रिय मोडेल आर्किटेक्चरहरूलाई स्वतः अप्टिमाइज गर्न सक्छ

### फाइदाहरू

- **विकास समय घटाइन्छ**: विभिन्न अप्टिमाइजेसन प्रविधिहरूको साथ म्यानुअल प्रयोगको आवश्यकता छैन
- **प्रदर्शन सुधार**: महत्वपूर्ण गति सुधार (केही अवस्थामा ६x सम्म)
- **क्रस-प्लेटफर्म तैनाती**: अप्टिमाइज गरिएको मोडेलहरू विभिन्न हार्डवेयर र अपरेटिङ सिस्टमहरूमा काम गर्दछ
- **शुद्धता कायम राखिन्छ**: अप्टिमाइजेसनले प्रदर्शन सुधार गर्दै मोडेलको गुणस्तर सुरक्षित राख्छ

## स्थापना

### पूर्वापेक्षाहरू

- Python 3.8 वा उच्च संस्करण
- pip प्याकेज म्यानेजर
- भर्चुअल वातावरण (सिफारिस गरिएको)

### आधारभूत स्थापना

भर्चुअल वातावरण सिर्जना र सक्रिय गर्नुहोस्:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ओलिभलाई स्वतः अप्टिमाइजेसन सुविधाहरूको साथ स्थापना गर्नुहोस्:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### वैकल्पिक निर्भरता

ओलिभले थप सुविधाहरूको लागि विभिन्न वैकल्पिक निर्भरता प्रदान गर्दछ:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### स्थापना प्रमाणित गर्नुहोस्

```bash
olive --help
```

यदि सफल भयो भने, तपाईंले ओलिभ CLI सहायता सन्देश देख्नुहुनेछ।

## छिटो सुरु गर्ने मार्गदर्शन

### तपाईंको पहिलो अप्टिमाइजेसन

ओलिभको स्वतः अप्टिमाइजेसन सुविधाको प्रयोग गरेर सानो भाषा मोडेल अप्टिमाइज गरौं:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### यो आदेशले के गर्छ

अप्टिमाइजेसन प्रक्रियामा समावेश छ: स्थानीय क्यासबाट मोडेल प्राप्त गर्नु, ONNX ग्राफलाई कब्जा गर्नु र ONNX डेटा फाइलमा तौलहरू भण्डारण गर्नु, ONNX ग्राफलाई अप्टिमाइज गर्नु, र RTN विधि प्रयोग गरेर मोडेललाई int4 मा क्वान्टाइज गर्नु।

### आदेश प्यारामिटरहरू व्याख्या गरिएको

- `--model_name_or_path`: Hugging Face मोडेल पहिचानकर्ता वा स्थानीय पथ
- `--output_path`: अप्टिमाइज गरिएको मोडेल बचत गरिने निर्देशिका
- `--device`: लक्षित उपकरण (cpu, gpu)
- `--provider`: कार्यान्वयन प्रदायक (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: इन्फरेन्सको लागि ONNX Runtime Generate AI प्रयोग गर्नुहोस्
- `--precision`: क्वान्टाइजेसन शुद्धता (int4, int8, fp16)
- `--log_level`: लगिङ्गको स्तर (0=न्यूनतम, 1=विस्तृत)

## उदाहरण: Qwen3 लाई ONNX INT4 मा रूपान्तरण गर्ने

Hugging Face मा प्रदान गरिएको उदाहरण [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) को आधारमा, यहाँ Qwen3 मोडेललाई अप्टिमाइज गर्ने तरिका छ:

### चरण १: मोडेल डाउनलोड गर्नुहोस् (वैकल्पिक)

डाउनलोड समय कम गर्न, केवल आवश्यक फाइलहरू क्यास गर्नुहोस्:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### चरण २: Qwen3 मोडेल अप्टिमाइज गर्नुहोस्

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### चरण ३: अप्टिमाइज गरिएको मोडेल परीक्षण गर्नुहोस्

तपाईंको अप्टिमाइज गरिएको मोडेल परीक्षण गर्न एक साधारण Python स्क्रिप्ट सिर्जना गर्नुहोस्:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### आउटपुट संरचना

अप्टिमाइजेसन पछि, तपाईंको आउटपुट निर्देशिकामा समावेश हुनेछ:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## उन्नत प्रयोग

### कन्फिगरेसन फाइलहरू

अझ जटिल अप्टिमाइजेसन कार्यप्रवाहहरूको लागि, तपाईं JSON कन्फिगरेसन फाइलहरू प्रयोग गर्न सक्नुहुन्छ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

कन्फिगरेसनको साथ चलाउनुहोस्:

```bash
olive run --config config.json
```

### GPU अप्टिमाइजेसन

CUDA GPU अप्टिमाइजेसनको लागि:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) को लागि:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ओलिभको साथ फाइन-ट्युनिङ

ओलिभले मोडेलहरू फाइन-ट्युनिङ गर्न पनि समर्थन गर्दछ:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## सर्वोत्तम अभ्यासहरू

### १. मोडेल चयन
- परीक्षणको लागि साना मोडेलहरूबाट सुरु गर्नुहोस् (जस्तै, ०.५B-७B प्यारामिटरहरू)
- सुनिश्चित गर्नुहोस् कि तपाईंको लक्षित मोडेल आर्किटेक्चर ओलिभद्वारा समर्थित छ

### २. हार्डवेयर विचारहरू
- तपाईंको अप्टिमाइजेसन लक्ष्यलाई तपाईंको तैनाती हार्डवेयरसँग मिलाउनुहोस्
- CUDA-संगत हार्डवेयर भएमा GPU अप्टिमाइजेसन प्रयोग गर्नुहोस्
- Windows मेसिनहरूमा DirectML विचार गर्नुहोस्

### ३. शुद्धता चयन
- **INT4**: अधिकतम कम्प्रेसन, थोरै शुद्धता ह्रास
- **INT8**: आकार र शुद्धताको राम्रो सन्तुलन
- **FP16**: न्यूनतम शुद्धता ह्रास, मध्यम आकार घटाउने

### ४. परीक्षण र मान्यता
- सधैं तपाईंको विशिष्ट प्रयोग केसहरूसँग अप्टिमाइज गरिएको मोडेल परीक्षण गर्नुहोस्
- प्रदर्शन मेट्रिक्स तुलना गर्नुहोस् (लेटेन्सी, थ्रुपुट, शुद्धता)
- मूल्याङ्कनको लागि प्रतिनिधि इनपुट डेटा प्रयोग गर्नुहोस्

### ५. पुनरावृत्त अप्टिमाइजेसन
- छिटो नतिजाको लागि स्वतः अप्टिमाइजेसनबाट सुरु गर्नुहोस्
- सूक्ष्म नियन्त्रणको लागि कन्फिगरेसन फाइलहरू प्रयोग गर्नुहोस्
- विभिन्न अप्टिमाइजेसन पासहरूसँग प्रयोग गर्नुहोस्

## समस्या समाधान

### सामान्य समस्याहरू

#### १. स्थापना समस्याहरू
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### २. CUDA/GPU समस्याहरू
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ३. मेमोरी समस्याहरू
- अप्टिमाइजेसनको क्रममा साना ब्याच साइजहरू प्रयोग गर्नुहोस्
- पहिले उच्च शुद्धतासँग क्वान्टाइजेसन प्रयास गर्नुहोस् (int8 को सट्टा int4)
- मोडेल क्यासिङको लागि पर्याप्त डिस्क स्पेस सुनिश्चित गर्नुहोस्

#### ४. मोडेल लोडिङ त्रुटिहरू
- मोडेल पथ र पहुँच अनुमति प्रमाणित गर्नुहोस्
- जाँच गर्नुहोस् कि मोडेललाई `trust_remote_code=True` चाहिन्छ
- सुनिश्चित गर्नुहोस् कि सबै आवश्यक मोडेल फाइलहरू डाउनलोड गरिएको छ

### सहयोग प्राप्त गर्नुहोस्

- **डकुमेन्टेसन**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub समस्याहरू**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **उदाहरणहरू**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## ओलिभ रेसिपीहरूको भण्डार

### ओलिभ रेसिपीहरूको परिचय

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) भण्डारले लोकप्रिय AI मोडेलहरूको लागि तयार-प्रयोग अप्टिमाइजेसन रेसिपीहरूको व्यापक संग्रह प्रदान गरेर मुख्य ओलिभ उपकरणलाई पूरक बनाउँछ। यो भण्डारले सार्वजनिक रूपमा उपलब्ध मोडेलहरू अप्टिमाइज गर्न र स्वामित्व मोडेलहरूको लागि अप्टिमाइजेसन कार्यप्रवाहहरू सिर्जना गर्न व्यावहारिक सन्दर्भको रूपमा सेवा गर्दछ।

### मुख्य विशेषताहरू

- **१००+ पूर्व-निर्मित रेसिपीहरू**: लोकप्रिय मोडेलहरूको लागि तयार-प्रयोग अप्टिमाइजेसन कन्फिगरेसनहरू
- **मल्टि-आर्किटेक्चर समर्थन**: ट्रान्सफर्मर मोडेलहरू, भिजन मोडेलहरू, र मल्टिमोडल आर्किटेक्चरहरू समेट्छ
- **हार्डवेयर-विशिष्ट अप्टिमाइजेसनहरू**: CPU, GPU, र विशेष एक्सेलेरेटरहरूको लागि रेसिपीहरू
- **लोकप्रिय मोडेल परिवारहरू**: Phi, Llama, Qwen, Gemma, Mistral, र धेरै अन्य समावेश गर्दछ

### समर्थित मोडेल परिवारहरू

भण्डारले निम्नका लागि अप्टिमाइजेसन रेसिपीहरू समावेश गर्दछ:

#### भाषा मोडेलहरू
- **माइक्रोसफ्ट Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 श्रृंखला (०.५B देखि १४B)
- **Google Gemma**: विभिन्न Gemma मोडेल कन्फिगरेसनहरू
- **Mistral AI**: Mistral-7B श्रृंखला
- **DeepSeek**: R1-Distill श्रृंखला मोडेलहरू

#### भिजन र मल्टिमोडल मोडेलहरू
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP मोडेलहरू**: विभिन्न CLIP-ViT कन्फिगरेसनहरू
- **ResNet**: ResNet-50 अप्टिमाइजेसनहरू
- **भिजन ट्रान्सफर्मरहरू**: ViT-base-patch16-224

#### विशेष मोडेलहरू
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: आधारभूत र बहुभाषी भेरियन्टहरू
- **Sentence Transformers**: all-MiniLM-L6-v2

### ओलिभ रेसिपीहरू प्रयोग गर्दै

#### विधि १: विशिष्ट रेसिपी क्लोन गर्नुहोस्

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### विधि २: रेसिपीलाई टेम्प्लेटको रूपमा प्रयोग गर्नुहोस्

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### रेसिपी संरचना

प्रत्येक रेसिपी निर्देशिकामा सामान्यतया समावेश हुन्छ:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### उदाहरण: Phi-4-mini रेसिपी प्रयोग गर्दै

Phi-4-mini रेसिपीलाई उदाहरणको रूपमा प्रयोग गरौं:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

कन्फिगरेसन फाइल सामान्यतया समावेश गर्दछ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### रेसिपीहरू अनुकूलित गर्दै

#### लक्षित हार्डवेयर परिमार्जन गर्नुहोस्

लक्ष्य हार्डवेयर परिवर्तन गर्न, `systems` खण्ड अपडेट गर्नुहोस्:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### अप्टिमाइजेसन प्यारामिटरहरू समायोजन गर्नुहोस्

विभिन्न अप्टिमाइजेसन स्तरहरूको लागि `passes` खण्ड परिमार्जन गर्नुहोस्:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### तपाईंको आफ्नै रेसिपी सिर्जना गर्नुहोस्

1. **समान मोडेलबाट सुरु गर्नुहोस्**: समान आर्किटेक्चर भएको मोडेलको लागि रेसिपी खोज्नुहोस्
2. **मोडेल कन्फिगरेसन अपडेट गर्नुहोस्**: कन्फिगरेसनमा मोडेल नाम/पथ परिवर्तन गर्नुहोस्
3. **प्यारामिटरहरू समायोजन गर्नुहोस्**: आवश्यक अनुसार अप्टिमाइजेसन प्यारामिटरहरू परिमार्जन गर्नुहोस्
4. **परीक्षण र मान्यता गर्नुहोस्**: अप्टिमाइजेसन चलाउनुहोस् र नतिजा प्रमाणित गर्नुहोस्
5. **पुनः योगदान गर्नुहोस्**: तपाईंको रेसिपी भण्डारमा योगदान गर्ने विचार गर्नुहोस्

### रेसिपी प्रयोगको फाइदाहरू

#### १. **प्रमाणित कन्फिगरेसनहरू**
- विशिष्ट मोडेलहरूको लागि परीक्षण गरिएको अप्टिमाइजेसन सेटिङहरू
- इष्टतम प्यारामिटरहरू फेला पार्न परीक्षण-त्रुटि टार्छ

#### २. **हार्डवेयर-विशिष्ट ट्युनिङ**
- विभिन्न कार्यान्वयन प्रदायकहरूको लागि पूर्व-अप्टिमाइज गरिएको
- CPU, GPU, र NPU लक्ष्यहरूको लागि तयार-प्रयोग कन्फिगरेसनहरू

#### ३. **व्यापक कवरेज**
- सबैभन्दा लोकप्रिय खुला-स्रोत मोडेलहरूलाई समर्थन गर्दछ
- नयाँ मोडेल रिलीजहरूसँग नियमित अपडेटहरू

#### ४. **समुदाय योगदानहरू**
- AI समुदायसँग सहकारी विकास
- साझा ज्ञान र सर्वोत्तम अभ्यासहरू

### ओलिभ रेसिपीहरूमा योगदान गर्दै

यदि तपाईंले भण्डारमा समावेश नगरिएको मोडेल अप्टिमाइज गर्नुभएको छ भने:

1. **भण्डारलाई फोर्क गर्नुहोस्**: तपाईंको आफ्नै फोर्क सिर्जना गर्नुहोस्
2. **रेसिपी निर्देशिका सिर्जना गर्नुहोस्**: तपाईंको मोडेलको लागि नयाँ निर्देशिका थप्नुहोस्
3. **कन्फिगरेसन समावेश गर्नुहोस्**: olive_config.json र सहयोगी फाइलहरू थप्नुहोस्
4. **प्रयोगको कागजात बनाउनुहोस्**: स्पष्ट README निर्देशनहरू प्रदान गर्नुहोस्
5. **पुल अनुरोध सबमिट गर्नुहोस्**: समुदायमा पुनः योगदान गर्नुहोस्

### प्रदर्शन बेंचमार्कहरू

धेरै रेसिपीहरूले प्रदर्शन बेंचमार्कहरू समावेश गर्दछ जसले देखाउँछ:
- **लेटेन्सी सुधारहरू**: आधारभूत भन्दा सामान्यतया २-६x गति सुधार
- **मेमोरी घटाउने**: क्वान्टाइजेसनको साथ ५०-७५% मेमोरी प्रयोग घटाउने
- **शुद्धता संरक्षण**: ९५-९९% शुद्धता कायम राख्ने

### AI उपकरणसँग एकीकरण

रेसिपीहरू सहज रूपमा काम गर्दछ:
- **VS Code AI Toolkit**: मोडेल अप्टिमाइजेसनको लागि प्रत्यक्ष एकीकरण
- **Azure Machine Learning**: क्लाउड-आधारित अप्टिमाइजेसन कार्यप्रवाहहरू
- **ONNX Runtime**: अप्टिमाइज गरिएको इन्फरेन्स तैनाती

## थप स्रोतहरू

### आधिकारिक लिंकहरू
- **GitHub भण्डार**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ओलिभ रेसिपी भण्डार**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime डकुमेन्टेसन**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **H

---

**अस्वीकरण**:  
यो दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) प्रयोग गरेर अनुवाद गरिएको छ। हामी शुद्धताको लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छ। यसको मूल भाषा मा रहेको दस्तावेज़लाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीको लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुने छैनौं।