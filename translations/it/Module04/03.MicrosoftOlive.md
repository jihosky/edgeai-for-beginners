<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T12:27:53+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "it"
}
-->
# Sezione 3: Microsoft Olive Optimization Suite

## Indice
1. [Introduzione](../../../Module04)
2. [Cos'è Microsoft Olive?](../../../Module04)
3. [Installazione](../../../Module04)
4. [Guida rapida](../../../Module04)
5. [Esempio: Conversione di Qwen3 in ONNX INT4](../../../Module04)
6. [Uso avanzato](../../../Module04)
7. [Repository di Olive Recipes](../../../Module04)
8. [Best Practices](../../../Module04)
9. [Risoluzione dei problemi](../../../Module04)
10. [Risorse aggiuntive](../../../Module04)

## Introduzione

Microsoft Olive è un toolkit potente e facile da usare per l'ottimizzazione dei modelli, progettato per semplificare il processo di ottimizzazione dei modelli di machine learning per il deployment su diverse piattaforme hardware. Che tu stia lavorando con CPU, GPU o acceleratori AI specializzati, Olive ti aiuta a ottenere prestazioni ottimali mantenendo l'accuratezza del modello.

## Cos'è Microsoft Olive?

Olive è uno strumento di ottimizzazione dei modelli hardware-aware che integra tecniche leader del settore per compressione, ottimizzazione e compilazione dei modelli. Funziona con ONNX Runtime come soluzione di ottimizzazione E2E per l'inferenza.

### Caratteristiche principali

- **Ottimizzazione hardware-aware**: Seleziona automaticamente le migliori tecniche di ottimizzazione per l'hardware di destinazione
- **Oltre 40 componenti di ottimizzazione integrati**: Include compressione del modello, quantizzazione, ottimizzazione del grafo e altro
- **Interfaccia CLI semplice**: Comandi intuitivi per attività di ottimizzazione comuni
- **Supporto multi-framework**: Compatibile con PyTorch, modelli Hugging Face e ONNX
- **Supporto per modelli popolari**: Olive può ottimizzare automaticamente architetture di modelli popolari come Llama, Phi, Qwen, Gemma, ecc.

### Vantaggi

- **Riduzione del tempo di sviluppo**: Non è necessario sperimentare manualmente diverse tecniche di ottimizzazione
- **Miglioramenti delle prestazioni**: Incrementi significativi di velocità (fino a 6 volte in alcuni casi)
- **Deployment cross-platform**: Modelli ottimizzati funzionano su diversi hardware e sistemi operativi
- **Mantenimento dell'accuratezza**: Le ottimizzazioni preservano la qualità del modello migliorando le prestazioni

## Installazione

### Prerequisiti

- Python 3.8 o superiore
- Gestore di pacchetti pip
- Ambiente virtuale (consigliato)

### Installazione di base

Crea e attiva un ambiente virtuale:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installa Olive con funzionalità di ottimizzazione automatica:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dipendenze opzionali

Olive offre varie dipendenze opzionali per funzionalità aggiuntive:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verifica dell'installazione

```bash
olive --help
```

Se l'installazione è riuscita, dovresti vedere il messaggio di aiuto della CLI di Olive.

## Guida rapida

### La tua prima ottimizzazione

Ottimizziamo un piccolo modello linguistico utilizzando la funzione di ottimizzazione automatica di Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Cosa fa questo comando

Il processo di ottimizzazione include: acquisizione del modello dalla cache locale, cattura del grafo ONNX e memorizzazione dei pesi in un file di dati ONNX, ottimizzazione del grafo ONNX e quantizzazione del modello a int4 utilizzando il metodo RTN.

### Spiegazione dei parametri del comando

- `--model_name_or_path`: Identificatore del modello Hugging Face o percorso locale
- `--output_path`: Directory in cui verrà salvato il modello ottimizzato
- `--device`: Dispositivo di destinazione (cpu, gpu)
- `--provider`: Provider di esecuzione (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Utilizza ONNX Runtime Generate AI per l'inferenza
- `--precision`: Precisione della quantizzazione (int4, int8, fp16)
- `--log_level`: Livello di verbosità dei log (0=minimale, 1=verboso)

## Esempio: Conversione di Qwen3 in ONNX INT4

Basandoci sull'esempio fornito da Hugging Face [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), ecco come ottimizzare un modello Qwen3:

### Passo 1: Scarica il modello (opzionale)

Per ridurre i tempi di download, memorizza nella cache solo i file essenziali:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Passo 2: Ottimizza il modello Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Passo 3: Testa il modello ottimizzato

Crea uno script Python semplice per testare il tuo modello ottimizzato:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struttura dell'output

Dopo l'ottimizzazione, la tua directory di output conterrà:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Uso avanzato

### File di configurazione

Per flussi di lavoro di ottimizzazione più complessi, puoi utilizzare file di configurazione JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Esegui con configurazione:

```bash
olive run --config config.json
```

### Ottimizzazione GPU

Per l'ottimizzazione GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Per DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning con Olive

Olive supporta anche il fine-tuning dei modelli:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best Practices

### 1. Selezione del modello
- Inizia con modelli più piccoli per i test (es. 0.5B-7B parametri)
- Assicurati che l'architettura del modello di destinazione sia supportata da Olive

### 2. Considerazioni hardware
- Adatta il tuo obiettivo di ottimizzazione all'hardware di deployment
- Usa l'ottimizzazione GPU se disponi di hardware compatibile con CUDA
- Considera DirectML per macchine Windows con grafica integrata

### 3. Selezione della precisione
- **INT4**: Massima compressione, lieve perdita di accuratezza
- **INT8**: Buon equilibrio tra dimensioni e accuratezza
- **FP16**: Perdita minima di accuratezza, riduzione moderata delle dimensioni

### 4. Test e validazione
- Testa sempre i modelli ottimizzati con i tuoi casi d'uso specifici
- Confronta metriche di prestazione (latenza, throughput, accuratezza)
- Usa dati di input rappresentativi per la valutazione

### 5. Ottimizzazione iterativa
- Inizia con l'ottimizzazione automatica per risultati rapidi
- Usa file di configurazione per un controllo più dettagliato
- Sperimenta con diversi passaggi di ottimizzazione

## Risoluzione dei problemi

### Problemi comuni

#### 1. Problemi di installazione
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemi CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemi di memoria
- Usa dimensioni di batch più piccole durante l'ottimizzazione
- Prova la quantizzazione con precisione più alta (int8 invece di int4)
- Assicurati di avere spazio su disco sufficiente per la cache del modello

#### 4. Errori di caricamento del modello
- Verifica il percorso del modello e i permessi di accesso
- Controlla se il modello richiede `trust_remote_code=True`
- Assicurati che tutti i file richiesti del modello siano scaricati

### Ottenere aiuto

- **Documentazione**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemi su GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Esempi**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repository di Olive Recipes

### Introduzione a Olive Recipes

Il repository [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) completa il toolkit principale Olive fornendo una raccolta completa di ricette di ottimizzazione pronte all'uso per modelli AI popolari. Questo repository funge da riferimento pratico sia per ottimizzare modelli pubblicamente disponibili sia per creare flussi di lavoro di ottimizzazione per modelli proprietari.

### Caratteristiche principali

- **Oltre 100 ricette predefinite**: Configurazioni di ottimizzazione pronte per modelli popolari
- **Supporto multi-architettura**: Include modelli di trasformatori, modelli di visione e architetture multimodali
- **Ottimizzazioni specifiche per hardware**: Ricette adattate per CPU, GPU e acceleratori specializzati
- **Famiglie di modelli popolari**: Include Phi, Llama, Qwen, Gemma, Mistral e molti altri

### Famiglie di modelli supportate

Il repository include ricette di ottimizzazione per:

#### Modelli linguistici
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, serie Qwen2.5 (0.5B a 14B)
- **Google Gemma**: Configurazioni di modelli Gemma
- **Mistral AI**: Serie Mistral-7B
- **DeepSeek**: Modelli della serie R1-Distill

#### Modelli di visione e multimodali
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Modelli CLIP**: Configurazioni CLIP-ViT
- **ResNet**: Ottimizzazioni ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Modelli specializzati
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Varianti base e multilingue
- **Sentence Transformers**: all-MiniLM-L6-v2

### Utilizzo delle ricette Olive

#### Metodo 1: Clona una ricetta specifica

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metodo 2: Usa la ricetta come modello

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Struttura della ricetta

Ogni directory di ricetta contiene tipicamente:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Esempio: Utilizzo della ricetta Phi-4-mini

Usiamo la ricetta Phi-4-mini come esempio:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Il file di configurazione include tipicamente:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Personalizzazione delle ricette

#### Modifica dell'hardware di destinazione

Per cambiare l'hardware di destinazione, aggiorna la sezione `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Regolazione dei parametri di ottimizzazione

Modifica la sezione `passes` per livelli di ottimizzazione diversi:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Creazione di una propria ricetta

1. **Inizia con un modello simile**: Trova una ricetta per un modello con architettura simile
2. **Aggiorna la configurazione del modello**: Cambia il nome/percorso del modello nella configurazione
3. **Regola i parametri**: Modifica i parametri di ottimizzazione secondo necessità
4. **Testa e valida**: Esegui l'ottimizzazione e valida i risultati
5. **Contribuisci al repository**: Considera di contribuire la tua ricetta alla community

### Vantaggi dell'utilizzo delle ricette

#### 1. **Configurazioni comprovate**
- Impostazioni di ottimizzazione testate per modelli specifici
- Evita il tentativo ed errore nel trovare i parametri ottimali

#### 2. **Ottimizzazione specifica per hardware**
- Pre-ottimizzate per diversi provider di esecuzione
- Configurazioni pronte per CPU, GPU e NPU

#### 3. **Copertura completa**
- Supporta i modelli open-source più popolari
- Aggiornamenti regolari con nuove versioni di modelli

#### 4. **Contributi della community**
- Sviluppo collaborativo con la community AI
- Condivisione di conoscenze e best practices

### Contributo alle ricette Olive

Se hai ottimizzato un modello non coperto nel repository:

1. **Fai il fork del repository**: Crea un tuo fork di olive-recipes
2. **Crea una directory per la ricetta**: Aggiungi una nuova directory per il tuo modello
3. **Includi la configurazione**: Aggiungi olive_config.json e file di supporto
4. **Documenta l'utilizzo**: Fornisci un README chiaro con istruzioni
5. **Invia una pull request**: Contribuisci alla community

### Benchmark delle prestazioni

Molte ricette includono benchmark delle prestazioni che mostrano:
- **Miglioramenti della latenza**: Tipico incremento di velocità 2-6x rispetto al baseline
- **Riduzione della memoria**: Riduzione dell'uso della memoria del 50-75% con la quantizzazione
- **Mantenimento dell'accuratezza**: Preservazione dell'accuratezza tra il 95-99%

### Integrazione con AI Toolkit

Le ricette funzionano perfettamente con:
- **VS Code AI Toolkit**: Integrazione diretta per l'ottimizzazione dei modelli
- **Azure Machine Learning**: Flussi di lavoro di ottimizzazione basati su cloud
- **ONNX Runtime**: Deployment ottimizzato per l'inferenza

## Risorse aggiuntive

### Link ufficiali
- **Repository GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repository Olive Recipes**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Documentazione ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Esempio Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Esempi della community
- **Jupyter Notebooks**: Disponibili nel repository GitHub di Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Estensione VS Code**: Panoramica AI Toolkit per VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Post sul blog**: Blog Microsoft Open Source — https://opensource.microsoft.com/blog/

### Strumenti correlati
- **ONNX Runtime**: Motore di inferenza ad alte prestazioni — https://onnxruntime.ai/
- **Hugging Face Transformers**: Fonte di molti modelli compatibili — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Flussi di lavoro di ottimizzazione basati su cloud — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Cosa fare dopo

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione AI [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche potrebbero contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.