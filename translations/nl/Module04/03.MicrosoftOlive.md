<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:24:58+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "nl"
}
-->
# Sectie 3: Microsoft Olive Optimalisatie Suite

## Inhoudsopgave
1. [Introductie](../../../Module04)
2. [Wat is Microsoft Olive?](../../../Module04)
3. [Installatie](../../../Module04)
4. [Snelstartgids](../../../Module04)
5. [Voorbeeld: Qwen3 converteren naar ONNX INT4](../../../Module04)
6. [Geavanceerd gebruik](../../../Module04)
7. [Olive Recepten Repository](../../../Module04)
8. [Best practices](../../../Module04)
9. [Problemen oplossen](../../../Module04)
10. [Aanvullende bronnen](../../../Module04)

## Introductie

Microsoft Olive is een krachtige, gebruiksvriendelijke hardware-bewuste toolkit voor modeloptimalisatie die het proces vereenvoudigt om machine learning-modellen te optimaliseren voor implementatie op verschillende hardwareplatforms. Of je nu CPU's, GPU's of gespecialiseerde AI-versnellers gebruikt, Olive helpt je om optimale prestaties te bereiken terwijl de nauwkeurigheid van het model behouden blijft.

## Wat is Microsoft Olive?

Olive is een gebruiksvriendelijke hardware-bewuste tool voor modeloptimalisatie die toonaangevende technieken combineert op het gebied van modelcompressie, optimalisatie en compilatie. Het werkt samen met ONNX Runtime als een end-to-end oplossing voor inferentieoptimalisatie.

### Belangrijkste kenmerken

- **Hardware-bewuste optimalisatie**: Selecteert automatisch de beste optimalisatietechnieken voor jouw doelhardware
- **40+ ingebouwde optimalisatiecomponenten**: Omvat modelcompressie, kwantisatie, grafiekoptimalisatie en meer
- **Eenvoudige CLI-interface**: Simpele commando's voor veelvoorkomende optimalisatietaken
- **Ondersteuning voor meerdere frameworks**: Werkt met PyTorch, Hugging Face-modellen en ONNX
- **Ondersteuning voor populaire modellen**: Olive kan populaire modelarchitecturen zoals Llama, Phi, Qwen, Gemma, enz. automatisch optimaliseren

### Voordelen

- **Minder ontwikkeltijd**: Geen noodzaak om handmatig te experimenteren met verschillende optimalisatietechnieken
- **Prestatieverbeteringen**: Significante snelheidsverbeteringen (tot 6x in sommige gevallen)
- **Cross-platform implementatie**: Geoptimaliseerde modellen werken op verschillende hardware en besturingssystemen
- **Behoud van nauwkeurigheid**: Optimalisaties behouden de kwaliteit van het model terwijl de prestaties verbeteren

## Installatie

### Vereisten

- Python 3.8 of hoger
- pip package manager
- Virtuele omgeving (aanbevolen)

### Basisinstallatie

Maak en activeer een virtuele omgeving:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installeer Olive met automatische optimalisatiefuncties:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Optionele afhankelijkheden

Olive biedt verschillende optionele afhankelijkheden voor extra functies:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Installatie verifiëren

```bash
olive --help
```

Als de installatie succesvol is, zou je het Olive CLI-helpbericht moeten zien.

## Snelstartgids

### Je eerste optimalisatie

Laten we een klein taalmodel optimaliseren met Olive's automatische optimalisatiefunctie:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Wat doet dit commando?

Het optimalisatieproces omvat: het verkrijgen van het model uit de lokale cache, het vastleggen van de ONNX-grafiek en het opslaan van de gewichten in een ONNX-databestand, het optimaliseren van de ONNX-grafiek en het kwantiseren van het model naar int4 met behulp van de RTN-methode.

### Uitleg van de commando-parameters

- `--model_name_or_path`: Hugging Face-modelidentificator of lokaal pad
- `--output_path`: Map waar het geoptimaliseerde model wordt opgeslagen
- `--device`: Doelapparaat (cpu, gpu)
- `--provider`: Uitvoeringsprovider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gebruik ONNX Runtime Generate AI voor inferentie
- `--precision`: Kwantisatienauwkeurigheid (int4, int8, fp16)
- `--log_level`: Logniveau (0=minimaal, 1=uitgebreid)

## Voorbeeld: Qwen3 converteren naar ONNX INT4

Gebaseerd op het gegeven Hugging Face-voorbeeld bij [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), hier is hoe je een Qwen3-model kunt optimaliseren:

### Stap 1: Model downloaden (optioneel)

Om downloadtijd te minimaliseren, cache alleen essentiële bestanden:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Stap 2: Qwen3-model optimaliseren

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Stap 3: Het geoptimaliseerde model testen

Maak een eenvoudige Python-script om je geoptimaliseerde model te testen:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Outputstructuur

Na optimalisatie bevat je uitvoermap:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Geavanceerd gebruik

### Configuratiebestanden

Voor complexere optimalisatieworkflows kun je JSON-configuratiebestanden gebruiken:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Uitvoeren met configuratie:

```bash
olive run --config config.json
```

### GPU-optimalisatie

Voor CUDA GPU-optimalisatie:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Voor DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fijn afstemmen met Olive

Olive ondersteunt ook het fijn afstemmen van modellen:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best practices

### 1. Modelselectie
- Begin met kleinere modellen voor testen (bijv. 0,5B-7B parameters)
- Zorg ervoor dat je doelmodelarchitectuur wordt ondersteund door Olive

### 2. Hardwareoverwegingen
- Stem je optimalisatiedoel af op je implementatiehardware
- Gebruik GPU-optimalisatie als je CUDA-compatibele hardware hebt
- Overweeg DirectML voor Windows-machines met geïntegreerde grafische kaarten

### 3. Nauwkeurigheidsselectie
- **INT4**: Maximale compressie, lichte nauwkeurigheidsverlies
- **INT8**: Goede balans tussen grootte en nauwkeurigheid
- **FP16**: Minimale nauwkeurigheidsverlies, matige groottevermindering

### 4. Testen en valideren
- Test geoptimaliseerde modellen altijd met je specifieke gebruiksscenario's
- Vergelijk prestatiestatistieken (latentie, doorvoer, nauwkeurigheid)
- Gebruik representatieve invoergegevens voor evaluatie

### 5. Iteratieve optimalisatie
- Begin met automatische optimalisatie voor snelle resultaten
- Gebruik configuratiebestanden voor gedetailleerde controle
- Experimenteer met verschillende optimalisatiepasses

## Problemen oplossen

### Veelvoorkomende problemen

#### 1. Installatieproblemen
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-problemen
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Geheugenproblemen
- Gebruik kleinere batchgroottes tijdens optimalisatie
- Probeer eerst kwantisatie met hogere nauwkeurigheid (int8 in plaats van int4)
- Zorg voor voldoende schijfruimte voor modelcaching

#### 4. Model laadfouten
- Controleer het modelpad en de toegangsrechten
- Controleer of het model `trust_remote_code=True` vereist
- Zorg ervoor dat alle benodigde modelbestanden zijn gedownload

### Hulp krijgen

- **Documentatie**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Voorbeelden**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recepten Repository

### Introductie tot Olive Recepten

De [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) repository vult de hoofdtoolkit van Olive aan door een uitgebreide verzameling kant-en-klare optimalisatierecepten voor populaire AI-modellen te bieden. Deze repository dient als praktische referentie voor zowel het optimaliseren van openbaar beschikbare modellen als het creëren van optimalisatieworkflows voor eigen modellen.

### Belangrijkste kenmerken

- **100+ Vooraf gebouwde recepten**: Kant-en-klare optimalisatieconfiguraties voor populaire modellen
- **Ondersteuning voor meerdere architecturen**: Omvat transformer-modellen, vision-modellen en multimodale architecturen
- **Hardware-specifieke optimalisaties**: Recepten afgestemd op CPU, GPU en gespecialiseerde versnellers
- **Populaire modelfamilies**: Inclusief Phi, Llama, Qwen, Gemma, Mistral en nog veel meer

### Ondersteunde modelfamilies

De repository bevat optimalisatierecepten voor:

#### Taalmodellen
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B tot 14B)
- **Google Gemma**: Verschillende Gemma-modelconfiguraties
- **Mistral AI**: Mistral-7B series
- **DeepSeek**: R1-Distill series modellen

#### Vision- en multimodale modellen
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP-modellen**: Verschillende CLIP-ViT-configuraties
- **ResNet**: ResNet-50 optimalisaties
- **Vision Transformers**: ViT-base-patch16-224

#### Gespecialiseerde modellen
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Basis- en meertalige varianten
- **Sentence Transformers**: all-MiniLM-L6-v2

### Olive Recepten gebruiken

#### Methode 1: Specifiek recept klonen

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Methode 2: Recept als sjabloon gebruiken

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Receptstructuur

Elke receptmap bevat doorgaans:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Voorbeeld: Phi-4-mini recept gebruiken

Laten we het Phi-4-mini recept als voorbeeld gebruiken:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Het configuratiebestand bevat doorgaans:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Recepten aanpassen

#### Doelhardware wijzigen

Om de doelhardware te wijzigen, update je de sectie `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimalisatieparameters aanpassen

Wijzig de sectie `passes` voor verschillende optimalisatieniveaus:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Je eigen recept maken

1. **Begin met een vergelijkbaar model**: Zoek een recept voor een model met een vergelijkbare architectuur
2. **Update modelconfiguratie**: Wijzig de modelnaam/pad in de configuratie
3. **Pas parameters aan**: Wijzig optimalisatieparameters indien nodig
4. **Test en valideer**: Voer de optimalisatie uit en valideer de resultaten
5. **Draag bij**: Overweeg om je recept bij te dragen aan de repository

### Voordelen van recepten gebruiken

#### 1. **Bewezen configuraties**
- Geteste optimalisatie-instellingen voor specifieke modellen
- Vermijdt trial-and-error bij het vinden van optimale parameters

#### 2. **Hardware-specifieke afstemming**
- Vooraf geoptimaliseerd voor verschillende uitvoeringsproviders
- Kant-en-klare configuraties voor CPU-, GPU- en NPU-doelen

#### 3. **Uitgebreide dekking**
- Ondersteunt de meest populaire open-source modellen
- Regelmatige updates met nieuwe modelreleases

#### 4. **Communitybijdragen**
- Samenwerking met de AI-community
- Gedeelde kennis en best practices

### Bijdragen aan Olive Recepten

Als je een model hebt geoptimaliseerd dat niet in de repository wordt behandeld:

1. **Fork de repository**: Maak je eigen fork van olive-recipes
2. **Maak een receptmap**: Voeg een nieuwe map toe voor je model
3. **Voeg configuratie toe**: Voeg olive_config.json en ondersteunende bestanden toe
4. **Documenteer gebruik**: Zorg voor een duidelijke README met instructies
5. **Dien een pull request in**: Draag bij aan de community

### Prestatiebenchmarks

Veel recepten bevatten prestatiebenchmarks die laten zien:
- **Latentieverbeteringen**: Typische 2-6x snelheidsverbetering ten opzichte van de basislijn
- **Geheugenreductie**: 50-75% minder geheugengebruik met kwantisatie
- **Behoud van nauwkeurigheid**: 95-99% nauwkeurigheidsbehoud

### Integratie met AI-toolkit

De recepten werken naadloos samen met:
- **VS Code AI Toolkit**: Directe integratie voor modeloptimalisatie
- **Azure Machine Learning**: Cloudgebaseerde optimalisatieworkflows
- **ONNX Runtime**: Geoptimaliseerde inferentie-implementatie

## Aanvullende bronnen

### Officiële links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recepten Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Documentatie**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Voorbeeld**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Communityvoorbeelden
- **Jupyter Notebooks**: Beschikbaar in de Olive GitHub-repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extensie**: AI Toolkit voor VS Code overzicht — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogposts**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Gerelateerde tools
- **ONNX Runtime**: High-performance inferentie-engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Bron van veel compatibele modellen — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloudgebaseerde optimalisatieworkflows — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Wat nu?

- [04: OpenVINO Toolkit Optimalisatie Suite](./04.openvino.md)

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor eventuele misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.