<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:55:47+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "nl"
}
-->
# Sectie 7: Qualcomm QNN (Qualcomm Neural Network) Optimalisatie Suite

## Inhoudsopgave
1. [Introductie](../../../Module04)
2. [Wat is Qualcomm QNN?](../../../Module04)
3. [Installatie](../../../Module04)
4. [Snelstartgids](../../../Module04)
5. [Voorbeeld: Modellen converteren en optimaliseren met QNN](../../../Module04)
6. [Geavanceerd gebruik](../../../Module04)
7. [Best practices](../../../Module04)
8. [Problemen oplossen](../../../Module04)
9. [Aanvullende bronnen](../../../Module04)

## Introductie

Qualcomm QNN (Qualcomm Neural Network) is een uitgebreid AI-inferentieframework dat is ontworpen om het volledige potentieel van Qualcomm's AI-hardwareversnellers te benutten, waaronder de Hexagon NPU, Adreno GPU en Kryo CPU. Of je nu mobiele apparaten, edge computing-platforms of autosystemen target, QNN biedt geoptimaliseerde inferentiemogelijkheden die gebruik maken van Qualcomm's gespecialiseerde AI-verwerkingsunits voor maximale prestaties en energie-efficiëntie.

## Wat is Qualcomm QNN?

Qualcomm QNN is een uniform AI-inferentieframework waarmee ontwikkelaars AI-modellen efficiënt kunnen implementeren binnen Qualcomm's heterogene computatiearchitectuur. Het biedt een uniforme programmeerinterface voor toegang tot de Hexagon NPU (Neural Processing Unit), Adreno GPU en Kryo CPU, waarbij automatisch de optimale verwerkingsunit wordt geselecteerd voor verschillende modellagen en -bewerkingen.

### Belangrijkste kenmerken

- **Heterogene computing**: Uniforme toegang tot NPU, GPU en CPU met automatische taakverdeling
- **Hardware-gebaseerde optimalisatie**: Gespecialiseerde optimalisaties voor Qualcomm Snapdragon-platforms
- **Ondersteuning voor kwantisatie**: Geavanceerde INT8-, INT16- en mixed-precision kwantisatietechnieken
- **Modelconversietools**: Directe ondersteuning voor TensorFlow-, PyTorch-, ONNX- en Caffe-modellen
- **Edge AI geoptimaliseerd**: Speciaal ontworpen voor mobiele en edge-implementaties met focus op energie-efficiëntie

### Voordelen

- **Maximale prestaties**: Gebruik gespecialiseerde AI-hardware voor tot 15x prestatieverbeteringen
- **Energie-efficiëntie**: Geoptimaliseerd voor mobiele en batterijgevoede apparaten met intelligente energiebeheer
- **Lage latentie**: Hardwareversnelde inferentie met minimale overhead voor realtime toepassingen
- **Schaalbare implementatie**: Van smartphones tot autosystemen binnen Qualcomm's ecosysteem
- **Productieklaar**: Bewezen framework dat wordt gebruikt in miljoenen apparaten

## Installatie

### Vereisten

- Qualcomm QNN SDK (registratie bij Qualcomm vereist)
- Python 3.7 of hoger
- Compatibele Qualcomm-hardware of simulator
- Android NDK (voor mobiele implementatie)
- Linux- of Windows-ontwikkelomgeving

### QNN SDK instellen

1. **Registreren en downloaden**: Bezoek Qualcomm Developer Network om te registreren en de QNN SDK te downloaden
2. **SDK uitpakken**: Pak de QNN SDK uit naar je ontwikkelmap
3. **Omgevingsvariabelen instellen**: Configureer paden voor QNN-tools en -bibliotheken

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python-omgeving instellen

Maak en activeer een virtuele omgeving:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Installeer vereiste Python-pakketten:

```bash
pip install numpy tensorflow torch onnx
```

### Installatie verifiëren

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Als dit succesvol is, zou je helpinformatie moeten zien voor elke QNN-tool.

## Snelstartgids

### Je eerste modelconversie

Laten we een eenvoudig PyTorch-model converteren om op Qualcomm-hardware te draaien:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### ONNX naar QNN-formaat converteren

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### QNN-modelbibliotheek genereren

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Wat dit proces doet

De optimalisatieworkflow omvat: het converteren van het originele model naar ONNX-formaat, het vertalen van ONNX naar QNN-tussenrepresentatie, het toepassen van hardware-specifieke optimalisaties en het genereren van een gecompileerde modelbibliotheek voor implementatie.

### Uitleg van belangrijke parameters

- `--input_network`: Bronbestand van het ONNX-model
- `--output_path`: Gegenereerd C++-bronbestand
- `--input_dim`: Invoertensorafmetingen voor optimalisatie
- `--quantization_overrides`: Aangepaste kwantisatieconfiguratie
- `-t x86_64-linux-clang`: Doelarchitectuur en compiler

## Voorbeeld: Modellen converteren en optimaliseren met QNN

### Stap 1: Geavanceerde modelconversie met kwantisatie

Hier is hoe je aangepaste kwantisatie toepast tijdens conversie:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Converteren met aangepaste kwantisatie:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Stap 2: Multi-backend optimalisatie

Configureren voor heterogene uitvoering over NPU, GPU en CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Stap 3: Context-binary maken voor implementatie

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Stap 4: Inferentie met QNN-runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Outputstructuur

Na optimalisatie bevat je implementatiemap:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Geavanceerd gebruik

### Aangepaste backendconfiguratie

Specifieke backendoptimalisaties configureren:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dynamische kwantisatie

Kwantisatie toepassen tijdens runtime voor betere nauwkeurigheid:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Prestatieprofilering

Prestaties monitoren over verschillende backends:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Geautomatiseerde backendselectie

Intelligente backendselectie implementeren op basis van modelkenmerken:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Best practices

### 1. Modelarchitectuur optimalisatie
- **Laagfusie**: Combineer bewerkingen zoals Conv+BatchNorm+ReLU voor betere NPU-benutting
- **Dieptegescheiden convoluties**: Geef de voorkeur aan deze boven standaardconvoluties voor mobiele implementatie
- **Kwantisatievriendelijke ontwerpen**: Gebruik ReLU-activeringen en vermijd bewerkingen die niet goed kwantiseren

### 2. Kwantisatiestrategie
- **Post-training kwantisatie**: Begin hiermee voor snelle implementatie
- **Kalibratiedataset**: Gebruik representatieve data die alle invoervariaties dekt
- **Gemengde precisie**: Gebruik INT8 voor de meeste lagen, houd kritieke lagen in hogere precisie

### 3. Richtlijnen voor backendselectie
- **NPU (HTP)**: Beste voor CNN-workloads, gekwantiseerde modellen en energiegevoelige toepassingen
- **GPU**: Optimaal voor rekenintensieve bewerkingen, grotere modellen en FP16-precisie
- **CPU**: Terugvaloptie voor niet-ondersteunde bewerkingen en debugging

### 4. Prestatieoptimalisatie
- **Batchgrootte**: Gebruik batchgrootte 1 voor realtime toepassingen, grotere batches voor doorvoer
- **Invoerpreprocessing**: Minimaliseer datakopieën en conversie-overhead
- **Contexthergebruik**: Precompileer contexten om runtime-compilatie-overhead te vermijden

### 5. Geheugenbeheer
- **Tensorallocatie**: Gebruik statische allocatie waar mogelijk om runtime-overhead te vermijden
- **Geheugenpools**: Implementeer aangepaste geheugenpools voor vaak toegewezen tensors
- **Bufferhergebruik**: Hergebruik invoer-/uitvoerbuffers over inferentieaanroepen

### 6. Energieoptimalisatie
- **Prestatieniveaus**: Gebruik geschikte prestatieniveaus op basis van thermische beperkingen
- **Dynamische frequentieschaal**: Laat het systeem frequentie schalen op basis van workload
- **Beheer van inactieve toestand**: Laat middelen correct vrij wanneer ze niet worden gebruikt

## Problemen oplossen

### Veelvoorkomende problemen

#### 1. Problemen met SDK-installatie
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Fouten bij modelconversie
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Problemen met kwantisatie
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Prestatieproblemen
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Geheugenproblemen
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Backendcompatibiliteit
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Prestatie-debugging

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Hulp krijgen

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN-documentatie**: Beschikbaar in SDK-pakket
- **Communityforums**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Technische ondersteuning**: Via Qualcomm ontwikkelaarsportaal

## Aanvullende bronnen

### Officiële links
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon-platforms**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Ontwikkelaarsportaal**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Leermiddelen
- **Snelstartgids**: Beschikbaar in QNN SDK-documentatie
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimalisatiegids**: SDK-documentatie bevat uitgebreide optimalisatierichtlijnen
- **Videotutorials**: [Qualcomm Developer YouTube-kanaal](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integratietools
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Voorgeoptimaliseerde modellen voor Qualcomm-hardware
- **Android Neural Networks API**: Integratie met Android NNAPI
- **TensorFlow Lite Delegate**: Qualcomm-delegate voor TFLite

### Prestatiebenchmarks
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Communityvoorbeelden
- **Voorbeeldtoepassingen**: Beschikbaar in QNN SDK-voorbeeldmap
- **GitHub-repositories**: Door de community bijgedragen voorbeelden en tools
- **Technische blogs**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Gerelateerde tools
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Geavanceerde kwantisatie- en compressietechnieken
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Voor vergelijking en fallback-implementatie
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Cross-platform inferentie-engine

### Hardware-specificaties
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon-platforms**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Wat nu?

Ga verder met je Edge AI-reis door [Module 5: SLMOps en productie-implementatie](../Module05/README.md) te verkennen om meer te leren over operationele aspecten van Small Language Model lifecycle management.

---

**Disclaimer**:  
Dit document is vertaald met behulp van de AI-vertalingsservice [Co-op Translator](https://github.com/Azure/co-op-translator). Hoewel we streven naar nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.