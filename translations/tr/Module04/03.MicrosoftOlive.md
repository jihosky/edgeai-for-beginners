<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T12:40:51+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "tr"
}
-->
# Bölüm 3: Microsoft Olive Optimizasyon Paketi

## İçindekiler
1. [Giriş](../../../Module04)
2. [Microsoft Olive Nedir?](../../../Module04)
3. [Kurulum](../../../Module04)
4. [Hızlı Başlangıç Kılavuzu](../../../Module04)
5. [Örnek: Qwen3'ü ONNX INT4'e Dönüştürme](../../../Module04)
6. [Gelişmiş Kullanım](../../../Module04)
7. [Olive Tarifleri Deposu](../../../Module04)
8. [En İyi Uygulamalar](../../../Module04)
9. [Sorun Giderme](../../../Module04)
10. [Ek Kaynaklar](../../../Module04)

## Giriş

Microsoft Olive, makine öğrenimi modellerini farklı donanım platformlarında dağıtım için optimize etme sürecini basitleştiren güçlü ve kullanımı kolay bir donanım odaklı model optimizasyon aracıdır. İster CPU, GPU, ister özel AI hızlandırıcıları hedefliyor olun, Olive, model doğruluğunu korurken en iyi performansı elde etmenize yardımcı olur.

## Microsoft Olive Nedir?

Olive, model sıkıştırma, optimizasyon ve derleme gibi endüstri lideri teknikleri bir araya getiren, kullanımı kolay bir donanım odaklı model optimizasyon aracıdır. ONNX Runtime ile birlikte uçtan uca bir çıkarım optimizasyon çözümü olarak çalışır.

### Temel Özellikler

- **Donanım Odaklı Optimizasyon**: Hedef donanımınız için en iyi optimizasyon tekniklerini otomatik olarak seçer
- **40+ Dahili Optimizasyon Bileşeni**: Model sıkıştırma, kuantizasyon, grafik optimizasyonu ve daha fazlasını kapsar
- **Kolay CLI Arayüzü**: Yaygın optimizasyon görevleri için basit komutlar
- **Çoklu Çerçeve Desteği**: PyTorch, Hugging Face modelleri ve ONNX ile çalışır
- **Popüler Model Desteği**: Olive, Llama, Phi, Qwen, Gemma gibi popüler model mimarilerini kutudan çıkar çıkmaz optimize edebilir

### Faydalar

- **Azaltılmış Geliştirme Süresi**: Farklı optimizasyon tekniklerini manuel olarak denemenize gerek yok
- **Performans Kazançları**: Önemli hız iyileştirmeleri (bazı durumlarda 6 kata kadar)
- **Çapraz Platform Dağıtımı**: Optimize edilmiş modeller farklı donanım ve işletim sistemlerinde çalışır
- **Korunan Doğruluk**: Optimizasyonlar performansı artırırken model kalitesini korur

## Kurulum

### Ön Koşullar

- Python 3.8 veya üzeri
- pip paket yöneticisi
- Sanal ortam (önerilir)

### Temel Kurulum

Sanal bir ortam oluşturun ve etkinleştirin:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Otomatik optimizasyon özellikleriyle Olive'i yükleyin:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opsiyonel Bağımlılıklar

Olive, ek özellikler için çeşitli opsiyonel bağımlılıklar sunar:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Kurulumu Doğrulama

```bash
olive --help
```

Başarılı olursa, Olive CLI yardım mesajını görmelisiniz.

## Hızlı Başlangıç Kılavuzu

### İlk Optimizasyonunuz

Olive'in otomatik optimizasyon özelliğini kullanarak küçük bir dil modelini optimize edelim:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Bu Komut Ne Yapar?

Optimizasyon süreci şunları içerir: modeli yerel önbellekten almak, ONNX Grafiğini yakalamak ve ağırlıkları bir ONNX veri dosyasında depolamak, ONNX Grafiğini optimize etmek ve modeli RTN yöntemiyle int4'e kuantize etmek.

### Komut Parametreleri Açıklaması

- `--model_name_or_path`: Hugging Face model tanımlayıcısı veya yerel yol
- `--output_path`: Optimize edilmiş modelin kaydedileceği dizin
- `--device`: Hedef cihaz (cpu, gpu)
- `--provider`: Çalıştırma sağlayıcısı (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Çıkarım için ONNX Runtime Generate AI kullanımı
- `--precision`: Kuantizasyon hassasiyeti (int4, int8, fp16)
- `--log_level`: Günlük ayrıntı düzeyi (0=minimal, 1=detaylı)

## Örnek: Qwen3'ü ONNX INT4'e Dönüştürme

Hugging Face'deki [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) örneğine dayanarak bir Qwen3 modelini nasıl optimize edeceğinizi görelim:

### Adım 1: Modeli İndirme (Opsiyonel)

İndirme süresini en aza indirmek için yalnızca gerekli dosyaları önbelleğe alın:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Adım 2: Qwen3 Modelini Optimize Etme

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Adım 3: Optimize Edilmiş Modeli Test Etme

Optimize edilmiş modelinizi test etmek için basit bir Python betiği oluşturun:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Çıktı Yapısı

Optimizasyondan sonra, çıktı dizininiz şunları içerecektir:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Gelişmiş Kullanım

### Yapılandırma Dosyaları

Daha karmaşık optimizasyon iş akışları için JSON yapılandırma dosyalarını kullanabilirsiniz:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Yapılandırma ile çalıştırın:

```bash
olive run --config config.json
```

### GPU Optimizasyonu

CUDA GPU optimizasyonu için:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) için:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive ile İnce Ayar

Olive ayrıca modelleri ince ayar yapmayı destekler:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## En İyi Uygulamalar

### 1. Model Seçimi
- Test için daha küçük modellerle başlayın (ör. 0.5B-7B parametreler)
- Hedef model mimarinizin Olive tarafından desteklendiğinden emin olun

### 2. Donanım Düşünceleri
- Optimizasyon hedefinizi dağıtım donanımınıza uygun hale getirin
- CUDA uyumlu donanımınız varsa GPU optimizasyonunu kullanın
- Windows makinelerinde DirectML'yi düşünün

### 3. Hassasiyet Seçimi
- **INT4**: Maksimum sıkıştırma, hafif doğruluk kaybı
- **INT8**: Boyut ve doğruluk arasında iyi bir denge
- **FP16**: Minimum doğruluk kaybı, orta düzeyde boyut azaltma

### 4. Test ve Doğrulama
- Optimize edilmiş modelleri özel kullanım durumlarınızla test edin
- Performans metriklerini karşılaştırın (gecikme, verimlilik, doğruluk)
- Değerlendirme için temsilci giriş verileri kullanın

### 5. Yinelemeli Optimizasyon
- Hızlı sonuçlar için otomatik optimizasyonla başlayın
- Ayrıntılı kontrol için yapılandırma dosyalarını kullanın
- Farklı optimizasyon geçişlerini deneyin

## Sorun Giderme

### Yaygın Sorunlar

#### 1. Kurulum Problemleri
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU Sorunları
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Bellek Sorunları
- Optimizasyon sırasında daha küçük toplu iş boyutları kullanın
- Önce daha yüksek hassasiyetle kuantizasyonu deneyin (int8 yerine int4)
- Model önbellekleme için yeterli disk alanı sağlayın

#### 4. Model Yükleme Hataları
- Model yolunu ve erişim izinlerini doğrulayın
- Modelin `trust_remote_code=True` gerektirip gerektirmediğini kontrol edin
- Gerekli tüm model dosyalarının indirildiğinden emin olun

### Yardım Alma

- **Dokümantasyon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Sorunları**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Örnekler**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Tarifleri Deposu

### Olive Tariflerine Giriş

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) deposu, popüler AI modelleri için kapsamlı bir hazır optimizasyon tarifleri koleksiyonu sunarak ana Olive araç setini tamamlar. Bu depo, hem halka açık modelleri optimize etmek hem de özel modeller için optimizasyon iş akışları oluşturmak için pratik bir referans görevi görür.

### Temel Özellikler

- **100+ Hazır Tarif**: Popüler modeller için hazır optimizasyon yapılandırmaları
- **Çoklu Mimari Desteği**: Dönüştürücü modeller, görüntü modelleri ve çok modlu mimarileri kapsar
- **Donanıma Özel Optimizasyonlar**: CPU, GPU ve özel hızlandırıcılar için tasarlanmış tarifler
- **Popüler Model Aileleri**: Phi, Llama, Qwen, Gemma, Mistral ve daha fazlasını içerir

### Desteklenen Model Aileleri

Depo, aşağıdaki modeller için optimizasyon tarifleri içerir:

#### Dil Modelleri
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 serisi (0.5B'den 14B'ye)
- **Google Gemma**: Çeşitli Gemma model yapılandırmaları
- **Mistral AI**: Mistral-7B serisi
- **DeepSeek**: R1-Distill serisi modeller

#### Görüntü ve Çok Modlu Modeller
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Modelleri**: Çeşitli CLIP-ViT yapılandırmaları
- **ResNet**: ResNet-50 optimizasyonları
- **Görüntü Dönüştürücüler**: ViT-base-patch16-224

#### Özel Modeller
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Temel ve çok dilli varyantlar
- **Cümle Dönüştürücüler**: all-MiniLM-L6-v2

### Olive Tariflerini Kullanma

#### Yöntem 1: Belirli Tarifi Klonlama

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Yöntem 2: Tarifi Şablon Olarak Kullanma

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Tarif Yapısı

Her tarif dizini genellikle şunları içerir:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Örnek: Phi-4-mini Tarifini Kullanma

Phi-4-mini tarifini bir örnek olarak kullanalım:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Yapılandırma dosyası genellikle şunları içerir:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Tarifleri Özelleştirme

#### Hedef Donanımı Değiştirme

Hedef donanımı değiştirmek için `systems` bölümünü güncelleyin:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimizasyon Parametrelerini Ayarlama

Farklı optimizasyon seviyeleri için `passes` bölümünü değiştirin:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Kendi Tarifinizi Oluşturma

1. **Benzer Bir Modelle Başlayın**: Benzer mimariye sahip bir model için bir tarif bulun
2. **Model Yapılandırmasını Güncelleyin**: Yapılandırmada model adını/yolunu değiştirin
3. **Parametreleri Ayarlayın**: Gerekli optimizasyon parametrelerini değiştirin
4. **Test ve Doğrulama**: Optimizasyonu çalıştırın ve sonuçları doğrulayın
5. **Geri Katkıda Bulunun**: Tarifinizi depoya katkı olarak eklemeyi düşünün

### Tarif Kullanmanın Faydaları

#### 1. **Kanıtlanmış Yapılandırmalar**
- Belirli modeller için test edilmiş optimizasyon ayarları
- En iyi parametreleri bulmak için deneme yanılmayı önler

#### 2. **Donanıma Özel Ayar**
- Farklı çalıştırma sağlayıcıları için önceden optimize edilmiş
- CPU, GPU ve NPU hedefleri için hazır yapılandırmalar

#### 3. **Kapsamlı Kapsama**
- En popüler açık kaynak modellerini destekler
- Yeni model sürümleriyle düzenli güncellemeler

#### 4. **Topluluk Katkıları**
- AI topluluğu ile işbirlikçi geliştirme
- Paylaşılan bilgi ve en iyi uygulamalar

### Olive Tariflerine Katkıda Bulunma

Depoda yer almayan bir modeli optimize ettiyseniz:

1. **Depoyu Çatallayın**: olive-recipes için kendi çatallanızı oluşturun
2. **Tarif Dizini Oluşturun**: Modeliniz için yeni bir dizin ekleyin
3. **Yapılandırmayı Dahil Edin**: olive_config.json ve destekleyici dosyalar ekleyin
4. **Kullanımı Belgeleyin**: Açık talimatlarla README sağlayın
5. **Çekme İsteği Gönderin**: Topluluğa geri katkıda bulunun

### Performans Karşılaştırmaları

Birçok tarif, performans karşılaştırmaları içerir ve şunları gösterir:
- **Gecikme İyileştirmeleri**: Temel modele göre tipik 2-6 kat hızlanma
- **Bellek Azaltımı**: Kuantizasyon ile %50-75 bellek kullanımı azaltımı
- **Doğruluk Koruma**: %95-99 doğruluk korunumu

### AI Araç Seti ile Entegrasyon

Tarifler şu araçlarla sorunsuz çalışır:
- **VS Code AI Araç Seti**: Model optimizasyonu için doğrudan entegrasyon
- **Azure Machine Learning**: Bulut tabanlı optimizasyon iş akışları
- **ONNX Runtime**: Optimize edilmiş çıkarım dağıtımı

## Ek Kaynaklar

### Resmi Bağlantılar
- **GitHub Deposu**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Tarifleri Deposu**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Dokümantasyonu**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Örneği**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Topluluk Örnekleri
- **Jupyter Notebooks**: Olive GitHub deposunda mevcut — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Uzantısı**: VS Code için AI Araç Seti genel bakışı — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Yazıları**: Microsoft Açık Kaynak Blogu — https://opensource.microsoft.com/blog/

### İlgili Araçlar
- **ONNX Runtime**: Yüksek performanslı çıkarım motoru — https://onnxruntime.ai/
- **Hugging Face Transformers**: Birçok uyumlu modelin kaynağı — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Bulut tabanlı optimizasyon iş akışları — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Sıradaki Adım

- [04: OpenVINO Toolkit Optimizasyon Paketi](./04.openvino.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlık içerebileceğini lütfen unutmayın. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.