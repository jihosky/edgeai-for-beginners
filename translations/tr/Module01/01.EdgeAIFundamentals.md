<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:46:03+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "tr"
}
-->
# Bölüm 1: EdgeAI Temelleri

EdgeAI, yapay zeka dağıtımında bir paradigma değişimini temsil eder ve yapay zeka yeteneklerini yalnızca bulut tabanlı işlemeye güvenmek yerine doğrudan uç cihazlara taşır. EdgeAI'nin, kaynakları sınırlı cihazlarda yerel yapay zeka işlemesini nasıl mümkün kıldığını, performansı makul bir seviyede tutarken gizlilik, gecikme ve çevrimdışı çalışma gibi zorlukları nasıl ele aldığını anlamak önemlidir.

## Giriş

Bu derste EdgeAI ve temel kavramlarını inceleyeceğiz. Geleneksel yapay zeka hesaplama paradigmasını, uç bilişimin zorluklarını, EdgeAI'yi mümkün kılan temel teknolojileri ve çeşitli endüstrilerdeki pratik uygulamaları ele alacağız.

## Öğrenme Hedefleri

Bu dersin sonunda şunları yapabileceksiniz:

- Geleneksel bulut tabanlı yapay zeka ile EdgeAI yaklaşımları arasındaki farkı anlayın.
- Yapay zeka işlemesini uç cihazlarda mümkün kılan temel teknolojileri tanımlayın.
- EdgeAI uygulamalarının faydalarını ve sınırlamalarını tanıyın.
- EdgeAI bilgilerini gerçek dünya senaryolarında ve kullanım durumlarında uygulayın.

## Geleneksel Yapay Zeka Hesaplama Paradigmasını Anlamak

Geleneksel olarak, üretken yapay zeka uygulamaları, büyük dil modellerini (LLM'ler) etkili bir şekilde çalıştırmak için yüksek performanslı hesaplama altyapısına dayanır. Kuruluşlar genellikle bu modelleri bulut ortamlarındaki GPU kümelerinde dağıtır ve yeteneklerine API arayüzleri aracılığıyla erişir.

Bu merkezi model birçok uygulama için iyi çalışır, ancak uç bilişim senaryolarında doğal sınırlamaları vardır. Geleneksel yaklaşım, kullanıcı sorgularını uzak sunuculara göndermeyi, güçlü donanımlarla işlemeyi ve sonuçları internet üzerinden geri göndermeyi içerir. Bu yöntem, en son modellerin kullanılabilirliğini sağlarken, internet bağlantısına bağımlılık yaratır, gecikme sorunlarını ortaya çıkarır ve hassas verilerin dış sunuculara iletilmesi gerektiğinde gizlilik endişelerini artırır.

Geleneksel yapay zeka hesaplama paradigmalarıyla çalışırken anlamamız gereken bazı temel kavramlar şunlardır:

- **☁️ Bulut Tabanlı İşleme**: Yapay zeka modelleri, yüksek hesaplama kaynaklarına sahip güçlü sunucu altyapısında çalışır.
- **🔌 API Tabanlı Erişim**: Uygulamalar, yapay zeka yeteneklerine yerel işlem yerine uzak API çağrılarıyla erişir.
- **🎛️ Merkezi Model Yönetimi**: Modeller merkezi olarak korunur ve güncellenir, tutarlılık sağlar ancak ağ bağlantısı gerektirir.
- **📈 Kaynak Ölçeklenebilirliği**: Bulut altyapısı, değişen hesaplama taleplerini karşılamak için dinamik olarak ölçeklenebilir.

## Uç Bilişimin Zorlukları

Dizüstü bilgisayarlar, cep telefonları ve Raspberry Pi, NVIDIA Orin Nano gibi Nesnelerin İnterneti (IoT) cihazları gibi uç cihazlar, benzersiz hesaplama kısıtlamaları sunar. Bu cihazlar, veri merkezi altyapısına kıyasla genellikle sınırlı işlem gücüne, belleğe ve enerji kaynaklarına sahiptir.

Bu tür cihazlarda geleneksel LLM'leri çalıştırmak, bu donanım sınırlamaları nedeniyle tarihsel olarak zordu. Ancak, uç yapay zeka işlemesine duyulan ihtiyaç çeşitli senaryolarda giderek daha önemli hale gelmiştir. İnternet bağlantısının güvenilmez veya mevcut olmadığı durumları düşünün; örneğin, uzak endüstriyel sahalar, transit halindeki araçlar veya zayıf ağ kapsama alanına sahip bölgeler. Ayrıca, yüksek güvenlik standartları gerektiren uygulamalar, örneğin tıbbi cihazlar, finansal sistemler veya hükümet uygulamaları, gizliliği ve uyumluluğu korumak için hassas verileri yerel olarak işlemelidir.

### Uç Bilişim Kısıtlamaları

Uç bilişim ortamları, geleneksel bulut tabanlı yapay zeka çözümlerinin karşılaşmadığı birkaç temel kısıtlamayla karşı karşıyadır:

- **Sınırlı İşlem Gücü**: Uç cihazlar, sunucu sınıfı donanıma kıyasla daha az CPU çekirdeğine ve daha düşük saat hızlarına sahiptir.
- **Bellek Kısıtlamaları**: Uç cihazlarda mevcut RAM ve depolama kapasitesi önemli ölçüde sınırlıdır.
- **Güç Sınırlamaları**: Pil ile çalışan cihazlar, uzun süreli çalışma için performans ile enerji tüketimini dengelemelidir.
- **Termal Yönetim**: Kompakt form faktörleri, yük altında sürdürülebilir performansı etkileyen soğutma yeteneklerini sınırlar.

## EdgeAI Nedir?

### Kavram: EdgeAI Tanımı

EdgeAI, yapay zeka algoritmalarının doğrudan uç cihazlarda—verilerin oluşturulduğu ve toplandığı ağın "uç" kısmındaki fiziksel donanımda—dağıtımı ve çalıştırılmasını ifade eder. Bu cihazlar arasında akıllı telefonlar, IoT sensörleri, akıllı kameralar, otonom araçlar, giyilebilir cihazlar ve endüstriyel ekipmanlar bulunur. Geleneksel yapay zeka sistemlerinin işleme için bulut sunucularına güvenmesine karşın, EdgeAI zekayı doğrudan veri kaynağına taşır.

EdgeAI'nin temelinde, yapay zeka işlemesini merkezileştirilmiş veri merkezlerinden uzaklaştırmak ve dijital ekosistemimizi oluşturan geniş cihaz ağına dağıtmak yer alır. Bu, yapay zeka sistemlerinin tasarımı ve dağıtımı konusunda temel bir mimari değişimi temsil eder.

EdgeAI'nin temel kavramsal sütunları şunlardır:

- **Yakınlık İşlemesi**: Hesaplama, verinin kaynaklandığı yere fiziksel olarak yakın gerçekleşir.
- **Dağıtılmış Zeka**: Karar verme yetenekleri birden fazla cihaza dağıtılır.
- **Veri Egemenliği**: Bilgi yerel kontrol altında kalır, genellikle cihazdan hiç ayrılmaz.
- **Otonom Çalışma**: Cihazlar, sürekli bağlantı gerektirmeden akıllı bir şekilde çalışabilir.
- **Gömülü Yapay Zeka**: Zeka, günlük cihazların içsel bir yeteneği haline gelir.

### EdgeAI Mimari Görselleştirme

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI, yapay zeka dağıtımında bir paradigma değişimini temsil eder ve yapay zeka yeteneklerini yalnızca bulut tabanlı işlemeye güvenmek yerine doğrudan uç cihazlara taşır. Bu yaklaşım, yapay zeka modellerinin sınırlı hesaplama kaynaklarına sahip cihazlarda yerel olarak çalışmasını sağlar ve sürekli internet bağlantısı gerektirmeden gerçek zamanlı çıkarım yetenekleri sunar.

EdgeAI, yapay zeka modellerini daha verimli ve kaynakları sınırlı cihazlarda dağıtıma uygun hale getirmek için tasarlanmış çeşitli teknolojileri ve teknikleri kapsar. Amaç, yapay zeka modellerinin hesaplama ve bellek gereksinimlerini önemli ölçüde azaltırken makul bir performans seviyesini korumaktır.

Farklı cihaz türleri ve kullanım durumları arasında EdgeAI uygulamalarını mümkün kılan temel yaklaşımlara bir göz atalım.

### Temel EdgeAI İlkeleri

EdgeAI, geleneksel bulut tabanlı yapay zekadan farklı kılan birkaç temel ilkeye dayanır:

- **Yerel İşleme**: Yapay zeka çıkarımı, dış bağlantı gerektirmeden doğrudan uç cihazda gerçekleşir.
- **Kaynak Optimizasyonu**: Modeller, hedef cihazların donanım kısıtlamalarına özel olarak optimize edilir.
- **Gerçek Zamanlı Performans**: Zaman açısından hassas uygulamalar için işlem minimum gecikmeyle gerçekleşir.
- **Gizlilik Odaklı Tasarım**: Hassas veriler cihazda kalır, güvenlik ve uyumluluğu artırır.

## EdgeAI'yi Mümkün Kılan Temel Teknolojiler

### Model Kuantizasyonu

EdgeAI'deki en önemli tekniklerden biri model kuantizasyonudur. Bu süreç, model parametrelerinin hassasiyetini genellikle 32 bit kayan nokta sayılarından 8 bit tam sayılara veya daha düşük hassasiyet formatlarına düşürmeyi içerir. Bu hassasiyetin azaltılması endişe verici gibi görünse de, araştırmalar birçok yapay zeka modelinin performansını önemli ölçüde azaltmadan düşük hassasiyetle çalışabileceğini göstermiştir.

Kuantizasyon, kayan nokta değerlerinin aralığını daha küçük bir dizi ayrık değere eşleyerek çalışır. Örneğin, her parametreyi temsil etmek için 32 bit kullanmak yerine, kuantizasyon yalnızca 8 bit kullanabilir, bu da bellek gereksinimlerinde 4 kat azalma sağlar ve genellikle daha hızlı çıkarım sürelerine yol açar.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Farklı kuantizasyon teknikleri şunları içerir:

- **Eğitim Sonrası Kuantizasyon (PTQ)**: Model eğitimi tamamlandıktan sonra uygulanır, yeniden eğitim gerektirmez.
- **Kuantizasyon Farkındalıklı Eğitim (QAT)**: Daha iyi doğruluk için eğitim sırasında kuantizasyon etkilerini içerir.
- **Dinamik Kuantizasyon**: Ağırlıkları int8'e kuantize eder ancak aktivasyonları dinamik olarak hesaplar.
- **Statik Kuantizasyon**: Hem ağırlıklar hem de aktivasyonlar için tüm kuantizasyon parametrelerini önceden hesaplar.

EdgeAI dağıtımları için uygun kuantizasyon stratejisinin seçimi, belirli model mimarisi, performans gereksinimleri ve hedef cihazın donanım yeteneklerine bağlıdır.

### Model Sıkıştırma ve Optimizasyon

Kuantizasyonun ötesinde, çeşitli sıkıştırma teknikleri model boyutunu ve hesaplama gereksinimlerini azaltmaya yardımcı olur. Bunlar şunları içerir:

**Budama**: Bu teknik, sinir ağlarından gereksiz bağlantıları veya nöronları kaldırır. Modelin performansına çok az katkıda bulunan parametreleri belirleyerek ve ortadan kaldırarak, budama model boyutunu önemli ölçüde azaltabilir ve doğruluğu koruyabilir.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Bilgi Damıtma**: Bu yaklaşım, daha küçük bir "öğrenci" modelin daha büyük bir "öğretmen" modelin davranışını taklit etmek üzere eğitilmesini içerir. Öğrenci model, öğretmenin çıktısını yaklaşık olarak öğrenir ve genellikle önemli ölçüde daha az parametreyle benzer performans elde eder.

**Model Mimari Optimizasyonu**: Araştırmacılar, performansı hesaplama verimliliğiyle dengeleyen MobileNets, EfficientNets gibi hafif mimariler gibi uç dağıtım için özel olarak tasarlanmış mimariler geliştirmiştir.

### Küçük Dil Modelleri (SLM'ler)

EdgeAI'deki yükselen bir trend, Küçük Dil Modelleri'nin (SLM'ler) geliştirilmesidir. Bu modeller, kompakt ve verimli olacak şekilde tasarlanmış olup, anlamlı doğal dil yetenekleri sunar. SLM'ler, dikkatli mimari seçimler, verimli eğitim teknikleri ve belirli alanlara veya görevlere odaklanmış eğitim yoluyla bu başarıyı elde eder.

Geleneksel yaklaşımların büyük modelleri sıkıştırmayı içermesine karşın, SLM'ler genellikle daha küçük veri setleri ve uç dağıtım için özel olarak tasarlanmış optimize edilmiş mimarilerle eğitilir. Bu yaklaşım, yalnızca daha küçük değil, aynı zamanda belirli kullanım durumları için daha verimli modellerle sonuçlanabilir.

## EdgeAI için Donanım Hızlandırma

Modern uç cihazlar, giderek artan bir şekilde yapay zeka iş yüklerini hızlandırmak için tasarlanmış özel donanımlar içerir:

### Sinir İşleme Birimleri (NPUs)

NPUs, özellikle sinir ağı hesaplamaları için tasarlanmış özel işlemcilerdir. Bu çipler, geleneksel CPU'lara kıyasla yapay zeka çıkarım görevlerini çok daha verimli bir şekilde gerçekleştirebilir ve genellikle daha düşük güç tüketimi sağlar. Günümüzde birçok modern akıllı telefon, dizüstü bilgisayar ve IoT cihazı, cihaz üzerinde yapay zeka işlemesini mümkün kılmak için NPU'lar içerir.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

NPU'lara sahip cihazlar şunları içerir:

- **Apple**: Neural Engine içeren A-serisi ve M-serisi çipler
- **Qualcomm**: Hexagon DSP/NPU'lu Snapdragon işlemciler
- **Samsung**: NPU'lu Exynos işlemciler
- **Intel**: Movidius VPU'lar ve Habana Labs hızlandırıcılar
- **Microsoft**: Windows Copilot+ PC'ler ve NPU'lar

### 🎮 GPU Hızlandırma

Uç cihazlar veri merkezlerinde bulunan güçlü GPU'lara sahip olmasa da, birçoğu yapay zeka iş yüklerini hızlandırabilen entegre veya ayrık GPU'lar içerir. Modern mobil GPU'lar ve entegre grafik işlemciler, yapay zeka çıkarım görevleri için önemli performans iyileştirmeleri sağlayabilir.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU Optimizasyonu

Sadece CPU'ya sahip cihazlar bile optimize edilmiş uygulamalar sayesinde EdgeAI'den faydalanabilir. Modern CPU'lar, yapay zeka iş yükleri için özel talimatlar içerir ve yapay zeka çıkarımı için CPU performansını en üst düzeye çıkarmak üzere yazılım çerçeveleri geliştirilmiştir.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

EdgeAI ile çalışan yazılım mühendisleri için, bu donanım hızlandırma seçeneklerini nasıl kullanacaklarını anlamak, hedef cihazlarda çıkarım performansını ve enerji verimliliğini optimize etmek açısından kritik öneme sahiptir.

## EdgeAI'nin Faydaları

### Gizlilik ve Güvenlik

EdgeAI'nin en büyük avantajlarından biri, artırılmış gizlilik ve güvenliktir. Veriler cihazda yerel olarak işlendiği için hassas bilgiler kullanıcının kontrolünden çıkmaz. Bu, kişisel veriler, tıbbi bilgiler veya gizli iş verilerini işleyen uygulamalar için özellikle önemlidir.

### Azaltılmış Gecikme

EdgeAI, verileri işleme için uzak sunuculara göndermeye olan ihtiyacı ortadan kaldırarak gecikmeyi önemli ölçüde azaltır. Bu, otonom araçlar, endüstriyel otomasyon veya anında yanıt gerektiren etkileşimli uygulamalar gibi gerçek zamanlı uygulamalar için kritik öneme sahiptir.

### Çevrimdışı Çalışma Yeteneği

EdgeAI, internet bağlantısı olmadığında bile yapay zeka işlevselliğini mümkün kılar. Bu, uzak bölgelerde, seyahat sırasında veya ağ güvenilirliğinin endişe verici olduğu durumlarda uygulamalar için değerlidir.

### Maliyet Verimliliği

Bulut tabanlı yapay zeka hizmetlerine olan bağımlılığı azaltarak, EdgeAI operasyonel maliyetleri düşürmeye yardımcı olabilir, özellikle yüksek kullanım hacmine sahip uygulamalar için. Kuruluşlar, sürekli API maliyetlerinden kaçınabilir ve bant genişliği gereksinimlerini azaltabilir.

### Ölçeklenebilirlik

EdgeAI, hesaplama yükünü veri merkezlerinde merkezileştirmek yerine uç cihazlara dağıtır. Bu, altyapı maliyetlerini azaltmaya ve genel sistem ölçeklenebilirliğini artırmaya yardımcı olabilir.

## EdgeAI Uygulamaları

### Akıllı Cihazlar ve IoT

EdgeAI, sesli asistanların komutları yerel olarak işleyebilmesinden, videoları buluta göndermeden nesneleri ve insanları tanımlayabilen akıllı kameralara kadar birçok akıllı cihaz özelliğini destekler. IoT cihazları, kestirimci bakım, çevresel izleme ve otomatik karar verme için EdgeAI kullanır.

### Mobil Uygulamalar

Akıllı telefonlar ve tabletler, fotoğraf iyileştirme, gerçek zamanlı çeviri, artırılmış gerçeklik ve kişiselleştirilmiş öneriler gibi çeşitli özellikler için EdgeAI kullanır. Bu uygulamalar, yerel işlemenin düşük gecikme ve gizlilik avantajlarından faydalanır.

### Endüstriyel Uygulamalar

Üretim ve endüstriyel ortamlar, kalite kontrol, kestirimci bakım ve süreç optimizasyonu için EdgeAI kullanır. Bu uygulamalar genellikle gerçek zamanlı işlem gerektirir ve sınırlı bağlantı ortamlarında çalışabilir.

### Sağlık

Tıbbi cihazlar ve sağlık uygulamaları, hasta takibi, tanı yardımı ve tedavi önerileri için EdgeAI kullanır. Yerel işlemenin gizlilik ve güvenlik avantajları, sağlık uygulamalarında özellikle önemlidir.

## Zorluklar ve Sınırlamalar

### Performans Tavizleri

EdgeAI genellikle model boyutu, hesaplama verimliliği ve performans arasında tavizler içerir. Kuantizasyon ve budama gibi teknikler kaynak gereksinimlerini önemli ölçüde azaltabilir, ancak model doğruluğunu veya yeteneğini de etkileyebilir.

### Geliştirme Zorluğu

EdgeAI uygulamaları geliştirmek, özel bilgi ve araçlar gerektirir. Geliştiriciler, optimizasyon tekniklerini, donanım yeteneklerini ve dağıtım kısıtlamalarını anlamalıdır, bu da geliştirme sürecini karmaşıklaştırabilir.

### Donanım Sınırlamaları

Uç donanımdaki ilerlemelere rağmen, bu cihazlar hala veri merkezi altyapısına kıyasla önemli sınırlamalara sahiptir. Tüm yapay zeka uygulamaları uç cihazlarda etkili bir şekilde dağıtılamaz ve bazıları hibrit yaklaşımlar gerektirebilir.

###
- [02: EdgeAI Uygulamaları](02.RealWorldCaseStudies.md)

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çeviriler hata veya yanlışlıklar içerebilir. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan herhangi bir yanlış anlama veya yanlış yorumlama durumunda sorumluluk kabul edilmez.