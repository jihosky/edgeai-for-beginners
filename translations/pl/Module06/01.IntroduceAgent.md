<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T12:32:22+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "pl"
}
-->
# Agenci AI i Małe Modele Językowe: Kompleksowy Przewodnik

## Wprowadzenie

W tym poradniku zgłębimy temat agentów AI oraz Małych Modeli Językowych (SLM) oraz ich zaawansowane strategie wdrażania w środowiskach obliczeniowych na krawędzi. Omówimy podstawowe koncepcje agentowej AI, techniki optymalizacji SLM, praktyczne strategie wdrażania na urządzeniach o ograniczonych zasobach oraz Microsoft Agent Framework do budowy systemów agentowych gotowych do produkcji.

Krajobraz sztucznej inteligencji przechodzi w 2025 roku fundamentalną zmianę. Podczas gdy rok 2023 był rokiem chatbotów, a 2024 przyniósł boom na kopiloty, rok 2025 należy do agentów AI — inteligentnych systemów, które myślą, rozumują, planują, korzystają z narzędzi i wykonują zadania przy minimalnym wkładzie człowieka, coraz częściej napędzanych przez wydajne Małe Modele Językowe. Microsoft Agent Framework wyróżnia się jako wiodące rozwiązanie do budowy tych inteligentnych systemów z możliwościami offline na krawędzi.

## Cele nauki

Po ukończeniu tego poradnika będziesz w stanie:

- 🤖 Zrozumieć podstawowe koncepcje agentów AI i systemów agentowych
- 🔬 Zidentyfikować zalety Małych Modeli Językowych w porównaniu z Dużymi Modelami Językowymi w aplikacjach agentowych
- 🚀 Poznać zaawansowane strategie wdrażania SLM w środowiskach obliczeniowych na krawędzi
- 📱 Wdrażać praktyczne agenty zasilane SLM w aplikacjach rzeczywistych
- 🏗️ Budować agenty gotowe do produkcji za pomocą Microsoft Agent Framework
- 🌐 Wdrażać agenty offline na krawędzi z integracją lokalnych LLM i SLM
- 🔧 Zintegrować Microsoft Agent Framework z Foundry Local do wdrożeń na krawędzi

## Zrozumienie agentów AI: Podstawy i klasyfikacje

### Definicja i podstawowe koncepcje

Agent sztucznej inteligencji (AI) odnosi się do systemu lub programu zdolnego do autonomicznego wykonywania zadań w imieniu użytkownika lub innego systemu poprzez projektowanie swojego przepływu pracy i wykorzystywanie dostępnych narzędzi. W przeciwieństwie do tradycyjnej AI, która jedynie odpowiada na pytania, agent może działać niezależnie, aby osiągnąć cele.

### Ramy klasyfikacji agentów

Zrozumienie granic agentów pomaga w wyborze odpowiednich typów agentów dla różnych scenariuszy obliczeniowych:

- **🔬 Proste agenty reaktywne**: Systemy oparte na regułach, które reagują na natychmiastowe postrzeganie (termostaty, podstawowa automatyzacja)
- **📱 Agenty oparte na modelu**: Systemy utrzymujące wewnętrzny stan i pamięć (roboty odkurzające, systemy nawigacyjne)
- **⚖️ Agenty oparte na celach**: Systemy planujące i wykonujące sekwencje w celu osiągnięcia celów (planery tras, harmonogramy zadań)
- **🧠 Agenty uczące się**: Adaptacyjne systemy poprawiające wydajność w czasie (systemy rekomendacji, spersonalizowani asystenci)

### Kluczowe zalety agentów AI

Agenci AI oferują kilka podstawowych zalet, które czynią ich idealnymi dla aplikacji obliczeniowych na krawędzi:

**Autonomia operacyjna**: Agenci zapewniają niezależne wykonywanie zadań bez ciągłego nadzoru człowieka, co czyni ich idealnymi dla aplikacji w czasie rzeczywistym. Wymagają minimalnego nadzoru, jednocześnie utrzymując adaptacyjne zachowanie, umożliwiając wdrożenie na urządzeniach o ograniczonych zasobach przy zmniejszonym obciążeniu operacyjnym.

**Elastyczność wdrożenia**: Systemy te umożliwiają możliwości AI na urządzeniu bez wymogu połączenia z internetem, zwiększają prywatność i bezpieczeństwo dzięki lokalnemu przetwarzaniu, mogą być dostosowane do aplikacji specyficznych dla danej dziedziny i są odpowiednie dla różnych środowisk obliczeniowych na krawędzi.

**Efektywność kosztowa**: Systemy agentowe oferują efektywne kosztowo wdrożenie w porównaniu z rozwiązaniami opartymi na chmurze, z obniżonymi kosztami operacyjnymi i mniejszymi wymaganiami dotyczącymi przepustowości dla aplikacji na krawędzi.

## Zaawansowane strategie Małych Modeli Językowych

### Podstawy SLM (Małych Modeli Językowych)

Mały Model Językowy (SLM) to model językowy, który może zmieścić się na zwykłym urządzeniu elektronicznym konsumenckim i wykonywać wnioskowanie z opóźnieniem wystarczająco niskim, aby być praktycznym w obsłudze żądań agentowych jednego użytkownika. W praktyce SLM to zazwyczaj modele z mniej niż 10 miliardami parametrów.

**Funkcje odkrywania formatów**: SLM oferują zaawansowane wsparcie dla różnych poziomów kwantyzacji, kompatybilności międzyplatformowej, optymalizacji wydajności w czasie rzeczywistym i możliwości wdrożenia na krawędzi. Użytkownicy mogą korzystać z zwiększonej prywatności dzięki lokalnemu przetwarzaniu oraz wsparciu WebGPU dla wdrożeń w przeglądarce.

**Kolekcje poziomów kwantyzacji**: Popularne formaty SLM obejmują Q4_K_M dla zrównoważonej kompresji w aplikacjach mobilnych, serię Q5_K_S dla wdrożeń na krawędzi skoncentrowanych na jakości, Q8_0 dla niemal oryginalnej precyzji na wydajnych urządzeniach na krawędzi oraz eksperymentalne formaty, takie jak Q2_K dla scenariuszy o ultra-niskich zasobach.

### GGUF (General GGML Universal Format) dla wdrożeń SLM

GGUF służy jako główny format do wdrażania kwantyzowanych SLM na CPU i urządzeniach na krawędzi, specjalnie zoptymalizowany dla aplikacji agentowych:

**Funkcje zoptymalizowane dla agentów**: Format zapewnia kompleksowe zasoby do konwersji i wdrażania SLM z rozszerzonym wsparciem dla wywoływania narzędzi, generowania strukturalnych wyników i rozmów wieloetapowych. Kompatybilność międzyplatformowa zapewnia spójne zachowanie agentów na różnych urządzeniach na krawędzi.

**Optymalizacja wydajności**: GGUF umożliwia efektywne wykorzystanie pamięci dla przepływów pracy agentów, wspiera dynamiczne ładowanie modeli dla systemów wieloagentowych i zapewnia zoptymalizowane wnioskowanie dla interakcji agentów w czasie rzeczywistym.

### Ramy SLM zoptymalizowane dla krawędzi

#### Optymalizacja Llama.cpp dla agentów

Llama.cpp oferuje najnowocześniejsze techniki kwantyzacji specjalnie zoptymalizowane dla wdrożeń agentowych SLM:

**Kwantyzacja specyficzna dla agentów**: Ramy wspierają Q4_0 (optymalne dla wdrożeń mobilnych agentów z redukcją rozmiaru o 75%), Q5_1 (zrównoważona jakość-kompresja dla agentów wnioskowania na krawędzi) oraz Q8_0 (jakość niemal oryginalna dla systemów agentowych w produkcji). Zaawansowane formaty umożliwiają ultra-skompresowanych agentów dla ekstremalnych scenariuszy na krawędzi.

**Korzyści z wdrożenia**: Wnioskowanie zoptymalizowane dla CPU z przyspieszeniem SIMD zapewnia efektywne wykorzystanie pamięci przez agenta. Kompatybilność międzyplatformowa na architekturach x86, ARM i Apple Silicon umożliwia uniwersalne możliwości wdrożenia agentów.

#### Ramy Apple MLX dla agentów SLM

Apple MLX zapewnia natywną optymalizację specjalnie zaprojektowaną dla agentów zasilanych SLM na urządzeniach Apple Silicon:

**Optymalizacja agentów na Apple Silicon**: Ramy wykorzystują zintegrowaną architekturę pamięci z integracją Metal Performance Shaders, automatyczną mieszankę precyzji dla wnioskowania agentów oraz zoptymalizowaną przepustowość pamięci dla systemów wieloagentowych. Agenci SLM wykazują wyjątkową wydajność na chipach serii M.

**Funkcje rozwojowe**: Wsparcie dla API Python i Swift z optymalizacjami specyficznymi dla agentów, automatyczne różnicowanie dla uczenia się agentów oraz bezproblemowa integracja z narzędziami rozwojowymi Apple zapewniają kompleksowe środowiska rozwoju agentów.

#### ONNX Runtime dla agentów SLM międzyplatformowych

ONNX Runtime zapewnia uniwersalny silnik wnioskowania, który umożliwia agentom SLM działanie w sposób spójny na różnych platformach sprzętowych i systemach operacyjnych:

**Uniwersalne wdrożenie**: ONNX Runtime zapewnia spójne zachowanie agentów SLM na platformach Windows, Linux, macOS, iOS i Android. Ta kompatybilność międzyplatformowa umożliwia programistom pisanie raz i wdrażanie wszędzie, znacznie redukując koszty rozwoju i utrzymania dla aplikacji wieloplatformowych.

**Opcje przyspieszenia sprzętowego**: Ramy zapewniają zoptymalizowane dostawców wykonania dla różnych konfiguracji sprzętowych, w tym CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) i specjalistycznych akceleratorów (Intel VPU, Qualcomm NPU). Agenci SLM mogą automatycznie wykorzystywać najlepszy dostępny sprzęt bez zmian w kodzie.

**Funkcje gotowe do produkcji**: ONNX Runtime oferuje funkcje klasy korporacyjnej niezbędne do wdrożenia agentów w produkcji, w tym optymalizację grafów dla szybszego wnioskowania, zarządzanie pamięcią dla środowisk o ograniczonych zasobach oraz kompleksowe narzędzia profilowania do analizy wydajności. Ramy wspierają zarówno API Python, jak i C++ dla elastycznej integracji.

## SLM vs LLM w systemach agentowych: Zaawansowane porównanie

### Zalety SLM w aplikacjach agentowych

**Efektywność operacyjna**: SLM zapewniają 10-30× redukcję kosztów w porównaniu z LLM dla zadań agentowych, umożliwiając odpowiedzi agentowe w czasie rzeczywistym na dużą skalę. Oferują szybsze czasy wnioskowania dzięki zmniejszonej złożoności obliczeniowej, co czyni je idealnymi dla interaktywnych aplikacji agentowych.

**Możliwości wdrożenia na krawędzi**: SLM umożliwiają wykonywanie zadań agentowych na urządzeniu bez zależności od internetu, zwiększoną prywatność dzięki lokalnemu przetwarzaniu oraz dostosowanie do aplikacji specyficznych dla danej dziedziny, odpowiednich dla różnych środowisk obliczeniowych na krawędzi.

**Optymalizacja specyficzna dla agentów**: SLM doskonale radzą sobie z wywoływaniem narzędzi, generowaniem strukturalnych wyników i rutynowymi przepływami decyzyjnymi, które stanowią 70-80% typowych zadań agentowych.

### Kiedy używać SLM vs LLM w systemach agentowych

**Idealne dla SLM**:
- **Powtarzalne zadania agentowe**: Wprowadzanie danych, wypełnianie formularzy, rutynowe wywołania API
- **Integracja narzędzi**: Zapytania do baz danych, operacje na plikach, interakcje systemowe
- **Strukturalne przepływy pracy**: Podążanie za zdefiniowanymi procesami agentowymi
- **Agenci specyficzni dla dziedziny**: Obsługa klienta, harmonogramowanie, podstawowa analiza
- **Przetwarzanie lokalne**: Operacje agentowe wrażliwe na prywatność

**Lepsze dla LLM**:
- **Złożone rozumowanie**: Rozwiązywanie nowych problemów, planowanie strategiczne
- **Rozmowy otwarte**: Ogólne rozmowy, kreatywne dyskusje
- **Zadania wymagające szerokiej wiedzy**: Badania wymagające rozległej wiedzy ogólnej
- **Nowe sytuacje**: Obsługa całkowicie nowych scenariuszy agentowych

### Hybrydowa architektura agentów

Optymalne podejście łączy SLM i LLM w heterogenicznych systemach agentowych:

**Inteligentna orkiestracja agentów**:
1. **SLM jako główny**: Obsługa 70-80% rutynowych zadań agentowych lokalnie
2. **LLM w razie potrzeby**: Przekierowanie złożonych zapytań do większych modeli w chmurze
3. **Specjalistyczne SLM**: Różne małe modele dla różnych dziedzin agentowych
4. **Optymalizacja kosztów**: Minimalizacja kosztownych wywołań LLM poprzez inteligentne przekierowanie

## Strategie wdrażania agentów SLM w produkcji

### Foundry Local: Runtime AI klasy korporacyjnej na krawędzi

Foundry Local (https://github.com/microsoft/foundry-local) jest flagowym rozwiązaniem Microsoftu do wdrażania Małych Modeli Językowych w środowiskach produkcyjnych na krawędzi. Zapewnia kompletny runtime zaprojektowany specjalnie dla agentów zasilanych SLM z funkcjami klasy korporacyjnej i bezproblemowymi możliwościami integracji.

**Podstawowa architektura i funkcje**:
- **Kompatybilne API OpenAI**: Pełna kompatybilność z SDK OpenAI i integracjami Agent Framework
- **Automatyczna optymalizacja sprzętu**: Inteligentny wybór wariantów modeli w oparciu o dostępny sprzęt (CUDA GPU, Qualcomm NPU, CPU)
- **Zarządzanie modelami**: Automatyczne pobieranie, buforowanie i zarządzanie cyklem życia modeli SLM
- **Odkrywanie usług**: Wykrywanie usług bez konfiguracji dla frameworków agentowych
- **Optymalizacja zasobów**: Inteligentne zarządzanie pamięcią i efektywność energetyczna dla wdrożeń na krawędzi

#### Instalacja i konfiguracja

**Instalacja międzyplatformowa**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Szybki start dla rozwoju agentów**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integracja z Agent Framework

**Integracja SDK Foundry Local**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automatyczny wybór modelu i optymalizacja sprzętu**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Wzorce wdrożenia produkcyjnego

**Konfiguracja produkcyjna dla jednego agenta**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orkiestracja produkcyjna dla wielu agentów**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Funkcje korporacyjne i monitorowanie

**Monitorowanie zdrowia i obserwowalność**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Zarządzanie zasobami i automatyczne skalowanie**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Zaawansowana konfiguracja i optymalizacja

**Konfiguracja niestandardowych modeli**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Lista kontrolna wdrożenia produkcyjnego**:

✅ **Konfiguracja usług**:
- Skonfiguruj odpowiednie aliasy modeli dla przypadków użycia
- Ustaw limity zasobów i progi monitorowania
- Włącz kontrole zdrowia i zbieranie metryk
- Skonfiguruj automatyczny restart i przełączanie awaryjne

✅ **Konfiguracja bezpieczeństwa**:
- Włącz dostęp API tylko lokalny (bez ekspozycji zewnętrznej)
- Skonfiguruj odpowiednie zarządzanie kluczami API
- Ustaw logowanie audytowe dla interakcji agentów
- Wprowadź ograniczenia szybkości dla użytkowania produkcyjnego

✅ **Optymalizacja wydajności**:
- Przetestuj wydajność modelu pod spodziewanym obciążeniem
- Skonfiguruj odpowiednie poziomy kwantyzacji
- Ustaw strategie buforowania i rozgrzewania modeli
- Monitoruj wzorce użycia pamięci i CPU

✅ **Testowanie integracji**:
- Przetestuj integrację frameworku agentowego
- Zweryfikuj możliwości działania offline
- Przetestuj scenariusze przełączania awaryjnego i odzyskiwania
- Zweryfikuj przepływy pracy agentów od początku do końca

### Ollama: Uproszczone wdrożenie agentów SLM

### Ollama: Wdrożenie agentów SLM skoncentrowane na
- Testowanie integracji z Microsoft Agent Framework  
- Weryfikacja możliwości działania offline  
- Testowanie scenariuszy awaryjnych i obsługi błędów  
- Walidacja przepływów pracy agentów od początku do końca  

**Porównanie z Foundry Local**:

| Funkcja | Foundry Local | Ollama |
|---------|---------------|--------|
| **Docelowy przypadek użycia** | Produkcja w przedsiębiorstwie | Rozwój i społeczność |
| **Ekosystem modeli** | Kuratorowane przez Microsoft | Rozbudowana społeczność |
| **Optymalizacja sprzętu** | Automatyczna (CUDA/NPU/CPU) | Konfiguracja ręczna |
| **Funkcje dla przedsiębiorstw** | Wbudowany monitoring, bezpieczeństwo | Narzędzia społecznościowe |
| **Złożoność wdrożenia** | Prosta (instalacja przez winget) | Prosta (instalacja przez curl) |
| **Kompatybilność API** | OpenAI + rozszerzenia | Standard OpenAI |
| **Wsparcie** | Oficjalne wsparcie Microsoft | Wsparcie społecznościowe |
| **Najlepsze dla** | Agenci produkcyjni | Prototypowanie, badania |

**Kiedy wybrać Ollama**:  
- **Rozwój i prototypowanie**: Szybkie eksperymentowanie z różnymi modelami  
- **Modele społecznościowe**: Dostęp do najnowszych modeli tworzonych przez społeczność  
- **Zastosowanie edukacyjne**: Nauka i nauczanie rozwoju agentów AI  
- **Projekty badawcze**: Badania akademickie wymagające dostępu do różnorodnych modeli  
- **Modele niestandardowe**: Tworzenie i testowanie modeli dostosowanych do potrzeb  

### VLLM: Wydajne wnioskowanie dla agentów SLM  

VLLM (Very Large Language Model inference) oferuje silnik wnioskowania o wysokiej przepustowości i efektywności pamięci, zoptymalizowany specjalnie dla wdrożeń produkcyjnych SLM na dużą skalę. Podczas gdy Foundry Local koncentruje się na łatwości użytkowania, a Ollama kładzie nacisk na modele społecznościowe, VLLM wyróżnia się w scenariuszach wymagających wysokiej wydajności, maksymalnej przepustowości i efektywnego wykorzystania zasobów.  

**Podstawowa architektura i funkcje**:  
- **PagedAttention**: Rewolucyjne zarządzanie pamięcią dla efektywnego obliczania uwagi  
- **Dynamiczne grupowanie**: Inteligentne grupowanie żądań dla optymalnej przepustowości  
- **Optymalizacja GPU**: Zaawansowane jądra CUDA i wsparcie dla równoległości tensorów  
- **Kompatybilność z OpenAI**: Pełna kompatybilność API dla bezproblemowej integracji  
- **Spekulacyjne dekodowanie**: Zaawansowane techniki przyspieszania wnioskowania  
- **Wsparcie dla kwantyzacji**: Kwantyzacja INT4, INT8 i FP16 dla efektywności pamięci  

#### Instalacja i konfiguracja  

**Opcje instalacji**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Szybki start dla rozwoju agentów**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### Integracja z Agent Framework  

**VLLM z Microsoft Agent Framework**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**Konfiguracja wieloagentowa o wysokiej przepustowości**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### Wzorce wdrożenia produkcyjnego  

**Usługa produkcyjna VLLM dla przedsiębiorstw**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### Funkcje dla przedsiębiorstw i monitoring  

**Zaawansowany monitoring wydajności VLLM**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### Zaawansowana konfiguracja i optymalizacja  

**Szablony konfiguracji produkcyjnej VLLM**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Lista kontrolna wdrożenia produkcyjnego VLLM**:  

✅ **Optymalizacja sprzętu**:  
- Konfiguracja równoległości tensorów dla zestawów multi-GPU  
- Włączenie kwantyzacji (AWQ/GPTQ) dla efektywności pamięci  
- Ustawienie optymalnego wykorzystania pamięci GPU (85-95%)  
- Konfiguracja odpowiednich rozmiarów grup dla przepustowości  

✅ **Dostrajanie wydajności**:  
- Włączenie buforowania prefiksów dla powtarzających się zapytań  
- Konfiguracja wstępnego wypełniania w segmentach dla długich sekwencji  
- Ustawienie spekulacyjnego dekodowania dla szybszego wnioskowania  
- Optymalizacja max_num_seqs w zależności od sprzętu  

✅ **Funkcje produkcyjne**:  
- Konfiguracja monitorowania stanu zdrowia i zbierania metryk  
- Ustawienie automatycznego restartu i przełączania awaryjnego  
- Implementacja kolejkowania żądań i równoważenia obciążenia  
- Konfiguracja kompleksowego logowania i alertów  

✅ **Bezpieczeństwo i niezawodność**:  
- Konfiguracja reguł zapory i kontroli dostępu  
- Ustawienie limitów API i uwierzytelniania  
- Implementacja łagodnego zamykania i czyszczenia  
- Konfiguracja kopii zapasowych i odzyskiwania po awarii  

✅ **Testowanie integracji**:  
- Testowanie integracji z Microsoft Agent Framework  
- Walidacja scenariuszy o wysokiej przepustowości  
- Testowanie procedur awaryjnych i odzyskiwania  
- Benchmarking wydajności pod obciążeniem  

**Porównanie z innymi rozwiązaniami**:

| Funkcja | VLLM | Foundry Local | Ollama |
|---------|------|---------------|--------|
| **Docelowy przypadek użycia** | Produkcja o wysokiej przepustowości | Łatwość użytkowania w przedsiębiorstwie | Rozwój i społeczność |
| **Wydajność** | Maksymalna przepustowość | Zrównoważona | Dobra |
| **Efektywność pamięci** | Optymalizacja PagedAttention | Automatyczna optymalizacja | Standardowa |
| **Złożoność konfiguracji** | Wysoka (wiele parametrów) | Niska (automatyczna) | Niska (prosta) |
| **Skalowalność** | Doskonała (równoległość tensorów/pipeline) | Dobra | Ograniczona |
| **Kwantyzacja** | Zaawansowana (AWQ, GPTQ, FP8) | Automatyczna | Standardowa GGUF |
| **Funkcje dla przedsiębiorstw** | Wymagana implementacja niestandardowa | Wbudowane | Narzędzia społecznościowe |
| **Najlepsze dla** | Agenci produkcyjni na dużą skalę | Produkcja w przedsiębiorstwie | Rozwój |

**Kiedy wybrać VLLM**:  
- **Wymagania o wysokiej przepustowości**: Przetwarzanie setek żądań na sekundę  
- **Wdrożenia na dużą skalę**: Zestawy multi-GPU, wdrożenia multi-node  
- **Krytyczne dla wydajności**: Czas odpowiedzi poniżej sekundy na dużą skalę  
- **Zaawansowana optymalizacja**: Potrzeba niestandardowej kwantyzacji i grupowania  
- **Efektywność zasobów**: Maksymalne wykorzystanie kosztownego sprzętu GPU  

## Zastosowania agentów SLM w rzeczywistości  

### Agenci SLM dla obsługi klienta  
- **Możliwości SLM**: Sprawdzanie kont, resetowanie haseł, sprawdzanie statusu zamówień  
- **Korzyści kosztowe**: 10-krotne obniżenie kosztów wnioskowania w porównaniu z agentami LLM  
- **Wydajność**: Szybsze czasy odpowiedzi przy zachowaniu jakości dla rutynowych zapytań  

### Agenci SLM dla procesów biznesowych  
- **Agenci przetwarzania faktur**: Ekstrakcja danych, walidacja informacji, przekazywanie do zatwierdzenia  
- **Agenci zarządzania e-mailami**: Kategoryzacja, priorytetyzacja, automatyczne tworzenie odpowiedzi  
- **Agenci planowania**: Koordynacja spotkań, zarządzanie kalendarzami, wysyłanie przypomnień  

### Osobiste cyfrowe asystenty SLM  
- **Agenci zarządzania zadaniami**: Tworzenie, aktualizacja, organizacja list zadań  
- **Agenci zbierania informacji**: Badanie tematów, lokalne podsumowywanie wyników  
- **Agenci komunikacji**: Tworzenie e-maili, wiadomości, postów w mediach społecznościowych  

### Agenci SLM dla handlu i finansów  
- **Agenci monitorowania rynku**: Śledzenie cen, identyfikacja trendów w czasie rzeczywistym  
- **Agenci generowania raportów**: Automatyczne tworzenie codziennych/tygodniowych podsumowań  
- **Agenci oceny ryzyka**: Analiza pozycji portfela przy użyciu lokalnych danych  

### Agenci SLM dla wsparcia w opiece zdrowotnej  
- **Agenci planowania wizyt**: Koordynacja spotkań, wysyłanie automatycznych przypomnień  
- **Agenci dokumentacji**: Lokalna generacja podsumowań medycznych, raportów  
- **Agenci zarządzania receptami**: Śledzenie uzupełnień, sprawdzanie interakcji prywatnie  

## Microsoft Agent Framework: Rozwój agentów gotowych do produkcji  

### Przegląd i architektura  

Microsoft Agent Framework oferuje kompleksową, korporacyjną platformę do budowy, wdrażania i zarządzania agentami AI, którzy mogą działać zarówno w chmurze, jak i w środowiskach offline na urządzeniach brzegowych. Framework został zaprojektowany specjalnie do pracy z małymi modelami językowymi i scenariuszami obliczeń brzegowych, co czyni go idealnym dla wdrożeń wymagających prywatności i ograniczonych zasobów.  

**Podstawowe komponenty frameworka**:  
- **Środowisko uruchomieniowe agenta**: Lekka platforma wykonawcza zoptymalizowana dla urządzeń brzegowych  
- **System integracji narzędzi**: Rozszerzalna architektura wtyczek do łączenia zewnętrznych usług i API  
- **Zarządzanie stanem**: Trwała pamięć agenta i obsługa kontekstu między sesjami  
- **Warstwa bezpieczeństwa**: Wbudowane mechanizmy bezpieczeństwa dla wdrożeń korporacyjnych  
- **Silnik orkiestracji**: Koordynacja wieloagentowa i zarządzanie przepływem pracy  

### Kluczowe funkcje dla wdrożeń brzegowych  

**Architektura offline-first**: Microsoft Agent Framework został zaprojektowany zgodnie z zasadami offline-first, umożliwiając agentom efektywne działanie bez stałego połączenia z internetem. Obejmuje to lokalne wnioskowanie modeli, lokalne bazy wiedzy, wykonywanie narzędzi offline i łagodne pogarszanie funkcji, gdy usługi chmurowe są niedostępne.  

**Optymalizacja zasobów**: Framework zapewnia inteligentne zarządzanie zasobami dzięki automatycznej optymalizacji pamięci dla SLM, równoważeniu obciążenia CPU/GPU dla urządzeń brzegowych, adaptacyjnemu wyborowi modeli w zależności od dostępnych zasobów oraz wzorcom wnioskowania oszczędzającym energię dla wdrożeń mobilnych.  

**Bezpieczeństwo i prywatność**: Funkcje bezpieczeństwa na poziomie korporacyjnym obejmują lokalne przetwarzanie danych w celu zachowania prywatności, szyfrowane kanały komunikacji agentów, kontrolę dostępu opartą na rolach dla możliwości agentów oraz rejestrowanie audytowe dla wymagań zgodności.  

### Integracja z Foundry Local  

Microsoft Agent Framework bezproblemowo integruje się z Foundry Local, oferując kompletne rozwiązanie AI dla urządzeń brzegowych:  

**Automatyczne wykrywanie modeli**: Framework automatycznie wykrywa i łączy się z instancjami Foundry Local, odkrywa dostępne modele SLM i wybiera optymalne modele na podstawie wymagań agenta oraz możliwości sprzętowych.  

**Dynamiczne ładowanie modeli**: Agenci mogą dynamicznie ładować różne modele SLM dla określonych zadań, umożliwiając systemy wielomodelowe, w których różne modele obsługują różne typy żądań, oraz automatyczne przełączanie między modelami w zależności od dostępności i wydajności.  

**Optymalizacja wydajności**: Zintegrowane mechanizmy buforowania redukują czas ładowania modeli, pooling połączeń optymalizuje wywołania API do Foundry Local, a inteligentne grupowanie poprawia przepustowość dla wielu żądań agentów.  

### Tworzenie agentów z Microsoft Agent Framework  

#### Definicja i konfiguracja agenta  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### Integracja narzędzi dla scenariuszy brzegowych  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### Orkiestracja wieloagentowa  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### Zaawansowane wzorce wdrożenia brzegowego  

#### Hierarchiczna architektura agentów  

**Lokalne klastry agentów**: Wdrażanie wielu wyspecjalizowanych agentów SLM na urządzeniach brzegowych, z których każdy jest zoptymalizowany dla określonych zadań. Użycie lekkich modeli, takich jak Qwen2.5-0.5B, do prostego routingu i planowania, średnich modeli, takich jak Phi-4-Mini, do obsługi klienta i dokumentacji, oraz większych modeli do złożonego rozumowania, gdy zasoby na to pozwalają.  

**Koordynacja edge-to-cloud**: Implementacja inteligentnych wzorców eskalacji, w których lokalni agenci obsługują rutynowe zadania, agenci chmurowi zapewniają złożone rozumowanie, gdy pozwala na to łączność, a płynne przekazywanie między przetwarzaniem brzegowym a chmurowym utrzymuje ciągłość.  

#### Konfiguracje wdrożenia  

**Wdrożenie na jednym urządzeniu**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**Rozproszone wdrożenie brzegowe**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### Optymalizacja wydajności dla agentów brzegowych  

#### Strategie wyboru modeli  

**Przypisanie modeli do zadań**: Microsoft Agent Framework umożliwia inteligentny wybór modeli na podstawie złożoności zadania i wymagań:  

- **Proste zadania** (Q&A, routing): Qwen2.5-0.5B (500MB, <100ms odpowiedź)  
- **Średnie zadania** (obsługa klienta, planowanie): Phi-4-Mini (2.4GB, 200-500ms odpowiedź)  
- **Złożone zadania** (analiza techniczna, planowanie): Phi-4 (7GB, 1-3s odpowiedź, gdy zasoby na to pozwalają)  

**Dynamiczne przełączanie modeli**: Agenci mogą przełączać się między modelami na podstawie aktualnego obciążenia systemu, oceny złożoności zadania, poziomów priorytetów użytkownika i dostępnych zasobów sprzętowych.  

#### Zarządzanie pamięcią i zasobami  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  

### Wzorce integracji dla przedsiębiorstw  

#### Bezpieczeństwo i zgodność  

**Lokalne przetwarzanie danych**: Wszystkie procesy agenta odbywają się lokalnie, co zapewnia, że wrażliwe dane nigdy nie opuszczają urządzenia brzegowego. Obejmuje to ochronę informacji klientów, zgodność z HIPAA dla agentów opieki zdrowotnej, bezpieczeństwo danych finansowych dla agentów bankowych oraz zgodność z GDPR dla wdrożeń w Europie.  

**Kontrola dostępu**: Uprawnienia oparte na rolach kontrolują, które narzędzia mogą być dostępne dla agentów, uwierzytelnianie użytkowników dla interakcji z agentami oraz ścieżki audytu dla wszystkich działań i decyzji agentów.  

#### Monitoring i obserwowalność  

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```
  

### Przykłady wdrożeń w rzeczywistości  

#### System agentów brzegowych dla handlu detalicznego  

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```
  
#### Agent wsparcia w opiece zdrowotnej  

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```
  

### Najlepsze praktyki dla Microsoft Agent Framework  

#### Wytyczne dotyczące rozwoju  

1. **Zacznij od prostych rozwiązań**: Rozpocznij od scenariuszy z jednym agentem, zanim zbudujesz złożone systemy wieloagentowe  
2. **Dobór modelu**: Wybierz najmniejszy model, który spełnia wymagania dotyczące dokładności  
3. **Projektowanie narzędzi**: Twórz narzędzia skoncentrowane na jednym celu, zamiast złożonych narzędzi wielofunkcyjnych  
4. **Obsługa błędów**: Implementuj
**Wybór Frameworku do Wdrażania Agentów**: Wybierz frameworki optymalizacyjne w zależności od docelowego sprzętu i wymagań agenta. Użyj Llama.cpp do wdrażania agentów zoptymalizowanych pod kątem CPU, Apple MLX do aplikacji agentów na Apple Silicon oraz ONNX dla kompatybilności międzyplatformowej.

## Praktyczna Konwersja Agentów SLM i Zastosowania

### Scenariusze Wdrażania Agentów w Rzeczywistych Warunkach

**Aplikacje Mobilne Agentów**: Format Q4_K doskonale sprawdza się w aplikacjach agentów na smartfony dzięki minimalnemu zużyciu pamięci, podczas gdy Q8_0 zapewnia zrównoważoną wydajność w systemach agentów na tabletach. Format Q5_K oferuje najwyższą jakość dla mobilnych agentów produktywności.

**Komputery Stacjonarne i Edge Computing dla Agentów**: Q5_K zapewnia optymalną wydajność w aplikacjach agentów na komputery stacjonarne, Q8_0 oferuje wysoką jakość wnioskowania w środowiskach stacji roboczych, a Q4_K umożliwia efektywne przetwarzanie na urządzeniach edge.

**Badania i Eksperymentalne Zastosowania Agentów**: Zaawansowane formaty kwantyzacji umożliwiają eksplorację ultra-niskiej precyzji wnioskowania agentów w badaniach akademickich i aplikacjach typu proof-of-concept wymagających ekstremalnych ograniczeń zasobów.

### Benchmarki Wydajności Agentów SLM

**Szybkość Wnioskowania Agentów**: Q4_K osiąga najszybsze czasy odpowiedzi agentów na mobilnych CPU, Q5_K zapewnia zrównoważony stosunek szybkości do jakości dla ogólnych aplikacji agentów, Q8_0 oferuje najwyższą jakość dla złożonych zadań agentów, a formaty eksperymentalne maksymalizują przepustowość dla wyspecjalizowanego sprzętu agentów.

**Wymagania Pamięciowe Agentów**: Poziomy kwantyzacji dla agentów wahają się od Q2_K (poniżej 500MB dla małych modeli agentów) do Q8_0 (około 50% oryginalnego rozmiaru), przy czym konfiguracje eksperymentalne osiągają maksymalną kompresję dla środowisk agentów o ograniczonych zasobach.

## Wyzwania i Rozważania dla Agentów SLM

### Kompromisy Wydajności w Systemach Agentów

Wdrażanie agentów SLM wymaga starannego rozważenia kompromisów między rozmiarem modelu, szybkością odpowiedzi agenta i jakością wyników. Podczas gdy Q4_K oferuje wyjątkową szybkość i efektywność dla mobilnych agentów, Q8_0 zapewnia najwyższą jakość dla złożonych zadań agentów. Q5_K stanowi kompromis odpowiedni dla większości ogólnych aplikacji agentów.

### Kompatybilność Sprzętowa dla Agentów SLM

Różne urządzenia edge mają różne możliwości wdrażania agentów SLM. Q4_K działa efektywnie na podstawowych procesorach dla prostych agentów, Q5_K wymaga umiarkowanych zasobów obliczeniowych dla zrównoważonej wydajności agentów, a Q8_0 korzysta z zaawansowanego sprzętu dla zaawansowanych możliwości agentów.

### Bezpieczeństwo i Prywatność w Systemach Agentów SLM

Podczas gdy agenci SLM umożliwiają lokalne przetwarzanie dla zwiększonej prywatności, należy wdrożyć odpowiednie środki bezpieczeństwa w celu ochrony modeli agentów i danych w środowiskach edge. Jest to szczególnie ważne przy wdrażaniu formatów agentów o wysokiej precyzji w środowiskach korporacyjnych lub skompresowanych formatów agentów w aplikacjach obsługujących wrażliwe dane.

## Przyszłe Trendy w Rozwoju Agentów SLM

Krajobraz agentów SLM nadal ewoluuje dzięki postępom w technikach kompresji, metodach optymalizacji i strategiach wdrażania edge. Przyszłe rozwój obejmuje bardziej efektywne algorytmy kwantyzacji dla modeli agentów, ulepszone metody kompresji dla przepływów pracy agentów oraz lepszą integrację z akceleratorami sprzętowymi edge dla przetwarzania agentów.

**Prognozy Rynkowe dla Agentów SLM**: Według najnowszych badań, automatyzacja oparta na agentach może wyeliminować 40–60% powtarzalnych zadań poznawczych w przepływach pracy przedsiębiorstw do 2027 roku, a SLM-y będą liderem tej transformacji dzięki swojej efektywności kosztowej i elastyczności wdrażania.

**Trendy Technologiczne w Agentach SLM**:
- **Specjalizowane Agenty SLM**: Modele specyficzne dla domeny, szkolone do określonych zadań agentów i branż
- **Edge Computing dla Agentów**: Ulepszone możliwości agentów na urządzeniach z zachowaniem prywatności i zmniejszonymi opóźnieniami
- **Orkiestracja Agentów**: Lepsza koordynacja między wieloma agentami SLM z dynamicznym routowaniem i równoważeniem obciążenia
- **Demokratyzacja**: Elastyczność SLM umożliwia szerszy udział w rozwoju agentów w różnych organizacjach

## Pierwsze Kroki z Agentami SLM

### Krok 1: Konfiguracja Środowiska Microsoft Agent Framework

**Instalacja Zależności**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicjalizacja Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Krok 2: Wybierz SLM dla Aplikacji Agentów
Popularne opcje dla Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Doskonały do ogólnych zadań agentów z zrównoważoną wydajnością
- **Qwen2.5-0.5B (0.5B)**: Ultra-efektywny dla prostych agentów routingu i klasyfikacji
- **Qwen2.5-Coder-0.5B (0.5B)**: Specjalizowany do zadań związanych z kodem
- **Phi-4 (7B)**: Zaawansowane rozumowanie dla złożonych scenariuszy edge, gdy zasoby na to pozwalają

### Krok 3: Utwórz Swojego Pierwszego Agenta z Microsoft Agent Framework

**Podstawowa Konfiguracja Agenta**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Krok 4: Zdefiniuj Zakres i Wymagania Agenta
Rozpocznij od skoncentrowanych, dobrze zdefiniowanych aplikacji agentów za pomocą Microsoft Agent Framework:
- **Agenci jednego obszaru**: Obsługa klienta LUB planowanie LUB badania
- **Jasne cele agenta**: Specyficzne, mierzalne cele dla wydajności agenta
- **Ograniczona integracja narzędzi**: Maksymalnie 3-5 narzędzi dla początkowego wdrożenia agenta
- **Zdefiniowane granice agenta**: Jasne ścieżki eskalacji dla złożonych scenariuszy
- **Projektowanie edge-first**: Priorytet dla funkcjonalności offline i lokalnego przetwarzania

### Krok 5: Wdrażanie Edge z Microsoft Agent Framework

**Konfiguracja Zasobów**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Wdrożenie Środków Bezpieczeństwa dla Agentów Edge**:
- **Lokalna walidacja wejścia**: Sprawdzanie żądań bez zależności od chmury
- **Filtrowanie wyników offline**: Zapewnienie lokalnej jakości odpowiedzi
- **Kontrole bezpieczeństwa edge**: Wdrożenie zabezpieczeń bez konieczności połączenia z internetem
- **Lokalne monitorowanie**: Śledzenie wydajności i zgłaszanie problemów za pomocą telemetrii edge

### Krok 6: Pomiar i Optymalizacja Wydajności Agentów Edge
- **Wskaźniki ukończenia zadań agenta**: Monitorowanie wskaźników sukcesu w scenariuszach offline
- **Czasy odpowiedzi agenta**: Zapewnienie czasów odpowiedzi poniżej sekundy dla wdrożenia edge
- **Wykorzystanie zasobów**: Śledzenie zużycia pamięci, CPU i baterii na urządzeniach edge
- **Efektywność kosztowa**: Porównanie kosztów wdrożenia edge z alternatywami opartymi na chmurze
- **Niezawodność offline**: Pomiar wydajności agenta podczas przerw w sieci

## Kluczowe Wnioski z Implementacji Agentów SLM

1. **SLM-y są wystarczające dla agentów**: W przypadku większości zadań agentów małe modele działają równie dobrze jak duże, oferując znaczące korzyści
2. **Efektywność kosztowa w agentach**: 10-30x tańsze w eksploatacji agenty SLM, co czyni je ekonomicznie opłacalnymi dla szerokiego wdrożenia
3. **Specjalizacja działa dla agentów**: SLM-y dostosowane do konkretnych zadań często przewyższają ogólne LLM-y w określonych aplikacjach agentów
4. **Hybrydowa architektura agentów**: Używaj SLM-ów do rutynowych zadań agentów, LLM-ów do złożonego rozumowania, gdy jest to konieczne
5. **Microsoft Agent Framework umożliwia wdrożenie produkcyjne**: Zapewnia narzędzia klasy korporacyjnej do budowy, wdrażania i zarządzania agentami edge
6. **Zasady projektowania edge-first**: Agenci zdolni do pracy offline z lokalnym przetwarzaniem zapewniają prywatność i niezawodność
7. **Integracja Foundry Local**: Bezproblemowe połączenie między Microsoft Agent Framework a lokalnym wnioskowaniem modeli
8. **Przyszłość to agenci SLM**: Małe modele językowe z frameworkami produkcyjnymi są przyszłością agentowej AI, umożliwiając demokratyzację i efektywne wdrażanie agentów

## Źródła i Dalsza Lektura

### Podstawowe Prace Badawcze i Publikacje

#### Agenci AI i Systemy Agentowe
- **"Language Agents as Optimizable Graphs"** (2024) - Podstawowe badania nad architekturą agentów i strategiami optymalizacji
  - Autorzy: Wenyue Hua, Lishan Yang, et al.
  - Link: https://arxiv.org/abs/2402.16823
  - Kluczowe Wnioski: Projektowanie i optymalizacja agentów opartych na grafach

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Autorzy: Zhiheng Xi, Wenxiang Chen, et al.
  - Link: https://arxiv.org/abs/2309.07864
  - Kluczowe Wnioski: Kompleksowy przegląd możliwości i zastosowań agentów opartych na LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Autorzy: Theodore Sumers, Shunyu Yao, et al.
  - Link: https://arxiv.org/abs/2309.02427
  - Kluczowe Wnioski: Ramy poznawcze dla projektowania inteligentnych agentów

#### Małe Modele Językowe i Optymalizacja
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Autorzy: Zespół Microsoft Research
  - Link: https://arxiv.org/abs/2404.14219
  - Kluczowe Wnioski: Zasady projektowania SLM i strategie wdrażania mobilnego

- **"Qwen2.5 Technical Report"** (2024)
  - Autorzy: Zespół Alibaba Cloud
  - Link: https://arxiv.org/abs/2407.10671
  - Kluczowe Wnioski: Zaawansowane techniki szkolenia SLM i optymalizacja wydajności

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Autorzy: Peiyuan Zhang, Guangtao Zeng, et al.
  - Link: https://arxiv.org/abs/2401.02385
  - Kluczowe Wnioski: Projektowanie ultra-kompaktowych modeli i efektywność szkolenia

### Oficjalna Dokumentacja i Frameworki

#### Microsoft Agent Framework
- **Oficjalna Dokumentacja**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **Repozytorium GitHub**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Główne Repozytorium**: https://github.com/microsoft/foundry-local
- **Dokumentacja**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Główne Repozytorium**: https://github.com/vllm-project/vllm
- **Dokumentacja**: https://docs.vllm.ai/


#### Ollama
- **Oficjalna Strona**: https://ollama.ai/
- **Repozytorium GitHub**: https://github.com/ollama/ollama

### Frameworki Optymalizacyjne Modeli

#### Llama.cpp
- **Repozytorium**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentacja**: https://microsoft.github.io/Olive/
- **Repozytorium GitHub**: https://github.com/microsoft/Olive

#### OpenVINO
- **Oficjalna Strona**: https://docs.openvino.ai/

#### Apple MLX
- **Repozytorium**: https://github.com/ml-explore/mlx

### Raporty Branżowe i Analizy Rynkowe

#### Badania Rynku Agentów AI
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Link: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Kluczowe Wnioski: Trendy rynkowe i wzorce adopcji w przedsiębiorstwach

#### Benchmarki Techniczne

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Link: https://mlcommons.org/en/inference-edge/
  - Kluczowe Wnioski: Standaryzowane metryki wydajności dla wdrożeń edge

### Standardy i Specyfikacje

#### Format Modeli i Standardy
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Format modelu międzyplatformowego dla interoperacyjności
- **Specyfikacja GGUF**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Format modelu kwantyzowanego dla wnioskowania na CPU
- **Specyfikacja API OpenAI**: https://platform.openai.com/docs/api-reference
  - Standardowy format API dla integracji modeli językowych

#### Bezpieczeństwo i Zgodność
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - Systemy AI**: Ramy dla systemów AI i bezpieczeństwa
- **Standardy IEEE dla AI**: https://standards.ieee.org/industry-connections/ai/

Przejście na agentów zasilanych SLM stanowi fundamentalną zmianę w podejściu do wdrażania AI. Microsoft Agent Framework, w połączeniu z lokalnymi platformami i efektywnymi Małymi Modelami Językowymi, oferuje kompletne rozwiązanie do budowy agentów gotowych do produkcji, które działają skutecznie w środowiskach edge. Skupiając się na efektywności, specjalizacji i praktycznej użyteczności, ten stos technologiczny sprawia, że agenci AI stają się bardziej dostępni, przystępni cenowo i skuteczni w rzeczywistych zastosowaniach w każdej branży i środowisku edge computing.

W miarę postępów do 2025 roku, połączenie coraz bardziej zaawansowanych małych modeli, zaawansowanych frameworków agentów takich jak Microsoft Agent Framework oraz solidnych platform wdrożeniowych edge otworzy nowe możliwości dla systemów autonomicznych, które mogą działać efektywnie na urządzeniach edge, jednocześnie zachowując prywatność, obniżając koszty i zapewniając wyjątkowe doświadczenia użytkownika.

**Kolejne Kroki w Implementacji**:
1. **Eksploruj Wywoływanie Funkcji**: Dowiedz się, jak SLM-y obsługują integrację narzędzi i strukturalne wyniki
2. **Opanuj Protokół Kontekstu Modelu (MCP)**: Zrozum zaawansowane wzorce komunikacji agentów
3. **Buduj Agentów Produkcyjnych**: Użyj Microsoft Agent Framework do wdrożeń klasy korporacyjnej
4. **Optymalizuj dla Edge**: Zastosuj zaawansowane techniki optymalizacji dla środowisk o ograniczonych zasob

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za autorytatywne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.