<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:52:29+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "pl"
}
-->
# Sekcja 7: Qualcomm QNN (Qualcomm Neural Network) Optimization Suite

## Spis treści
1. [Wprowadzenie](../../../Module04)
2. [Czym jest Qualcomm QNN?](../../../Module04)
3. [Instalacja](../../../Module04)
4. [Przewodnik szybkiego startu](../../../Module04)
5. [Przykład: Konwersja i optymalizacja modeli za pomocą QNN](../../../Module04)
6. [Zaawansowane użycie](../../../Module04)
7. [Najlepsze praktyki](../../../Module04)
8. [Rozwiązywanie problemów](../../../Module04)
9. [Dodatkowe zasoby](../../../Module04)

## Wprowadzenie

Qualcomm QNN (Qualcomm Neural Network) to kompleksowe środowisko do wnioskowania AI, zaprojektowane w celu maksymalnego wykorzystania potencjału akceleratorów sprzętowych Qualcomm, takich jak Hexagon NPU, Adreno GPU i Kryo CPU. Niezależnie od tego, czy celem są urządzenia mobilne, platformy edge computing czy systemy motoryzacyjne, QNN oferuje zoptymalizowane możliwości wnioskowania, które wykorzystują specjalistyczne jednostki przetwarzania AI Qualcomm, zapewniając maksymalną wydajność i efektywność energetyczną.

## Czym jest Qualcomm QNN?

Qualcomm QNN to zunifikowane środowisko wnioskowania AI, które umożliwia programistom efektywne wdrażanie modeli AI w heterogenicznej architekturze obliczeniowej Qualcomm. Zapewnia jednolity interfejs programistyczny do korzystania z Hexagon NPU (Neural Processing Unit), Adreno GPU i Kryo CPU, automatycznie wybierając optymalną jednostkę przetwarzania dla różnych warstw i operacji modelu.

### Kluczowe funkcje

- **Heterogeniczne obliczenia**: Zunifikowany dostęp do NPU, GPU i CPU z automatycznym rozdzielaniem obciążenia
- **Optymalizacja sprzętowa**: Specjalistyczne optymalizacje dla platform Snapdragon Qualcomm
- **Wsparcie kwantyzacji**: Zaawansowane techniki kwantyzacji INT8, INT16 i mieszanej precyzji
- **Narzędzia do konwersji modeli**: Bezpośrednie wsparcie dla modeli TensorFlow, PyTorch, ONNX i Caffe
- **Optymalizacja dla Edge AI**: Zaprojektowane specjalnie dla scenariuszy wdrożeniowych na urządzeniach mobilnych i edge z naciskiem na efektywność energetyczną

### Korzyści

- **Maksymalna wydajność**: Wykorzystanie specjalistycznego sprzętu AI dla nawet 15-krotnego wzrostu wydajności
- **Efektywność energetyczna**: Optymalizacja dla urządzeń mobilnych i zasilanych bateriami z inteligentnym zarządzaniem energią
- **Niska latencja**: Wnioskowanie przyspieszone sprzętowo z minimalnym opóźnieniem dla aplikacji w czasie rzeczywistym
- **Skalowalne wdrożenie**: Od smartfonów po platformy motoryzacyjne w ekosystemie Qualcomm
- **Gotowość produkcyjna**: Sprawdzone środowisko używane w milionach wdrożonych urządzeń

## Instalacja

### Wymagania wstępne

- Qualcomm QNN SDK (wymaga rejestracji w Qualcomm)
- Python 3.7 lub nowszy
- Kompatybilny sprzęt Qualcomm lub symulator
- Android NDK (do wdrożeń mobilnych)
- Środowisko deweloperskie Linux lub Windows

### Konfiguracja QNN SDK

1. **Rejestracja i pobranie**: Odwiedź Qualcomm Developer Network, aby zarejestrować się i pobrać QNN SDK
2. **Rozpakowanie SDK**: Rozpakuj QNN SDK do katalogu deweloperskiego
3. **Ustawienie zmiennych środowiskowych**: Skonfiguruj ścieżki dla narzędzi i bibliotek QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Konfiguracja środowiska Python

Utwórz i aktywuj wirtualne środowisko:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Zainstaluj wymagane pakiety Python:

```bash
pip install numpy tensorflow torch onnx
```

### Weryfikacja instalacji

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Jeśli instalacja przebiegła pomyślnie, powinny pojawić się informacje pomocnicze dla każdego narzędzia QNN.

## Przewodnik szybkiego startu

### Pierwsza konwersja modelu

Przeprowadźmy konwersję prostego modelu PyTorch do uruchomienia na sprzęcie Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Konwersja ONNX do formatu QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generowanie biblioteki modelu QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Co robi ten proces

Przepływ optymalizacji obejmuje: konwersję oryginalnego modelu do formatu ONNX, tłumaczenie ONNX na pośrednią reprezentację QNN, zastosowanie optymalizacji specyficznych dla sprzętu oraz wygenerowanie skompilowanej biblioteki modelu do wdrożenia.

### Wyjaśnienie kluczowych parametrów

- `--input_network`: Plik źródłowy modelu ONNX
- `--output_path`: Wygenerowany plik źródłowy C++
- `--input_dim`: Wymiary tensora wejściowego do optymalizacji
- `--quantization_overrides`: Niestandardowa konfiguracja kwantyzacji
- `-t x86_64-linux-clang`: Docelowa architektura i kompilator

## Przykład: Konwersja i optymalizacja modeli za pomocą QNN

### Krok 1: Zaawansowana konwersja modelu z kwantyzacją

Oto jak zastosować niestandardową kwantyzację podczas konwersji:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konwersja z niestandardową kwantyzacją:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Krok 2: Optymalizacja wielozadaniowa

Konfiguracja dla heterogenicznego wykonania na NPU, GPU i CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Krok 3: Tworzenie binarnego kontekstu do wdrożenia

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Krok 4: Wnioskowanie z użyciem QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Struktura wyjściowa

Po optymalizacji katalog wdrożeniowy będzie zawierał:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Zaawansowane użycie

### Niestandardowa konfiguracja backendu

Konfiguracja specyficznych optymalizacji backendu:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dynamiczna kwantyzacja

Zastosowanie kwantyzacji w czasie rzeczywistym dla lepszej dokładności:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profilowanie wydajności

Monitorowanie wydajności na różnych backendach:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatyczny wybór backendu

Implementacja inteligentnego wyboru backendu na podstawie charakterystyki modelu:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Najlepsze praktyki

### 1. Optymalizacja architektury modelu
- **Fuzja warstw**: Łączenie operacji takich jak Conv+BatchNorm+ReLU dla lepszego wykorzystania NPU
- **Głębokie separowalne konwolucje**: Preferowanie ich zamiast standardowych konwolucji dla wdrożeń mobilnych
- **Przyjazne dla kwantyzacji projekty**: Używanie aktywacji ReLU i unikanie operacji trudnych do kwantyzacji

### 2. Strategia kwantyzacji
- **Kwantyzacja po treningu**: Rozpocznij od tego dla szybkiego wdrożenia
- **Zbiór danych kalibracyjnych**: Używaj reprezentatywnych danych obejmujących wszystkie wariacje wejściowe
- **Mieszana precyzja**: Używaj INT8 dla większości warstw, zachowując wyższą precyzję dla kluczowych warstw

### 3. Wytyczne dotyczące wyboru backendu
- **NPU (HTP)**: Najlepsze dla obciążeń CNN, modeli kwantyzowanych i aplikacji wrażliwych na zużycie energii
- **GPU**: Optymalne dla operacji wymagających dużej mocy obliczeniowej, większych modeli i precyzji FP16
- **CPU**: Rezerwowe dla operacji nieobsługiwanych i debugowania

### 4. Optymalizacja wydajności
- **Rozmiar batcha**: Używaj rozmiaru batcha 1 dla aplikacji w czasie rzeczywistym, większych batchów dla przepustowości
- **Przetwarzanie wejściowe**: Minimalizuj kopiowanie danych i narzut konwersji
- **Ponowne użycie kontekstu**: Prekompiluj konteksty, aby uniknąć narzutu kompilacji w czasie wykonania

### 5. Zarządzanie pamięcią
- **Alokacja tensorów**: Używaj alokacji statycznej, gdy to możliwe, aby uniknąć narzutu w czasie wykonania
- **Pule pamięci**: Implementuj niestandardowe pule pamięci dla często alokowanych tensorów
- **Ponowne użycie buforów**: Wykorzystuj ponownie bufory wejściowe/wyjściowe w kolejnych wywołaniach wnioskowania

### 6. Optymalizacja zużycia energii
- **Tryby wydajności**: Używaj odpowiednich trybów wydajności w zależności od ograniczeń termicznych
- **Dynamiczne skalowanie częstotliwości**: Pozwól systemowi skalować częstotliwość w zależności od obciążenia
- **Zarządzanie stanem bezczynności**: Prawidłowo zwalniaj zasoby, gdy nie są używane

## Rozwiązywanie problemów

### Typowe problemy

#### 1. Problemy z instalacją SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Błędy konwersji modelu
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Problemy z kwantyzacją
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Problemy z wydajnością
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Problemy z pamięcią
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Kompatybilność backendu
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Debugowanie wydajności

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Uzyskiwanie pomocy

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Dokumentacja QNN**: Dostępna w pakiecie SDK
- **Fora społecznościowe**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Wsparcie techniczne**: Poprzez portal deweloperski Qualcomm

## Dodatkowe zasoby

### Oficjalne linki
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Platformy Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portal deweloperski**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Zasoby edukacyjne
- **Przewodnik dla początkujących**: Dostępny w dokumentacji QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Przewodnik optymalizacji**: Dokumentacja SDK zawiera kompleksowe wytyczne dotyczące optymalizacji
- **Samouczki wideo**: [Kanał YouTube Qualcomm Developer Network](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Narzędzia integracyjne
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Wstępnie zoptymalizowane modele dla sprzętu Qualcomm
- **Android Neural Networks API**: Integracja z Android NNAPI
- **TensorFlow Lite Delegate**: Delegat Qualcomm dla TFLite

### Benchmarki wydajności
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Przykłady społecznościowe
- **Przykładowe aplikacje**: Dostępne w katalogu przykładów QNN SDK
- **Repozytoria GitHub**: Przykłady i narzędzia stworzone przez społeczność
- **Blogi techniczne**: [Blog Qualcomm Developer](https://developer.qualcomm.com/blog)

### Powiązane narzędzia
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Zaawansowane techniki kwantyzacji i kompresji
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Do porównania i wdrożeń zapasowych
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Silnik wnioskowania wieloplatformowego

### Specyfikacje sprzętowe
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Platformy Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Co dalej

Kontynuuj swoją podróż z Edge AI, eksplorując [Moduł 5: SLMOps i wdrożenie produkcyjne](../Module05/README.md), aby dowiedzieć się więcej o operacyjnych aspektach zarządzania cyklem życia małych modeli językowych.

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za autorytatywne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.