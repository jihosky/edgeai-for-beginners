<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T12:34:25+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "pl"
}
-->
# Sekcja 3: Microsoft Olive Optimization Suite

## Spis treści
1. [Wprowadzenie](../../../Module04)
2. [Czym jest Microsoft Olive?](../../../Module04)
3. [Instalacja](../../../Module04)
4. [Szybki przewodnik](../../../Module04)
5. [Przykład: Konwersja Qwen3 do ONNX INT4](../../../Module04)
6. [Zaawansowane użycie](../../../Module04)
7. [Repozytorium przepisów Olive](../../../Module04)
8. [Najlepsze praktyki](../../../Module04)
9. [Rozwiązywanie problemów](../../../Module04)
10. [Dodatkowe zasoby](../../../Module04)

## Wprowadzenie

Microsoft Olive to potężne, łatwe w użyciu narzędzie do optymalizacji modeli uwzględniające specyfikę sprzętu, które upraszcza proces optymalizacji modeli uczenia maszynowego do wdrożenia na różnych platformach sprzętowych. Niezależnie od tego, czy celem są procesory CPU, GPU czy specjalistyczne akceleratory AI, Olive pomaga osiągnąć optymalną wydajność przy zachowaniu dokładności modelu.

## Czym jest Microsoft Olive?

Olive to łatwe w użyciu narzędzie do optymalizacji modeli uwzględniające specyfikę sprzętu, które łączy w sobie wiodące techniki w zakresie kompresji, optymalizacji i kompilacji modeli. Współpracuje z ONNX Runtime jako kompleksowe rozwiązanie do optymalizacji inferencji.

### Kluczowe funkcje

- **Optymalizacja uwzględniająca sprzęt**: Automatycznie wybiera najlepsze techniki optymalizacji dla docelowego sprzętu
- **Ponad 40 wbudowanych komponentów optymalizacyjnych**: Obejmuje kompresję modeli, kwantyzację, optymalizację grafów i inne
- **Prosty interfejs CLI**: Łatwe polecenia do typowych zadań optymalizacyjnych
- **Wsparcie dla wielu frameworków**: Współpracuje z PyTorch, modelami Hugging Face i ONNX
- **Wsparcie dla popularnych modeli**: Olive automatycznie optymalizuje popularne architektury modeli, takie jak Llama, Phi, Qwen, Gemma itp., bez dodatkowej konfiguracji

### Korzyści

- **Skrócony czas rozwoju**: Brak konieczności ręcznego eksperymentowania z różnymi technikami optymalizacji
- **Zyski wydajnościowe**: Znaczące przyspieszenie (do 6x w niektórych przypadkach)
- **Wieloplatformowe wdrożenie**: Modele zoptymalizowane działają na różnych sprzętach i systemach operacyjnych
- **Zachowana dokładność**: Optymalizacje zachowują jakość modelu przy jednoczesnym zwiększeniu wydajności

## Instalacja

### Wymagania wstępne

- Python 3.8 lub nowszy
- Menedżer pakietów pip
- Wirtualne środowisko (zalecane)

### Podstawowa instalacja

Utwórz i aktywuj wirtualne środowisko:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Zainstaluj Olive z funkcjami automatycznej optymalizacji:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opcjonalne zależności

Olive oferuje różne opcjonalne zależności dla dodatkowych funkcji:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Weryfikacja instalacji

```bash
olive --help
```

Jeśli instalacja zakończy się sukcesem, powinien pojawić się komunikat pomocy CLI Olive.

## Szybki przewodnik

### Twoja pierwsza optymalizacja

Zoptymalizuj mały model językowy za pomocą funkcji automatycznej optymalizacji Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Co robi to polecenie

Proces optymalizacji obejmuje: pobranie modelu z lokalnej pamięci podręcznej, przechwycenie grafu ONNX i zapisanie wag w pliku danych ONNX, optymalizację grafu ONNX oraz kwantyzację modelu do int4 metodą RTN.

### Wyjaśnienie parametrów polecenia

- `--model_name_or_path`: Identyfikator modelu Hugging Face lub lokalna ścieżka
- `--output_path`: Katalog, w którym zostanie zapisany zoptymalizowany model
- `--device`: Docelowe urządzenie (cpu, gpu)
- `--provider`: Dostawca wykonania (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Użycie ONNX Runtime Generate AI do inferencji
- `--precision`: Precyzja kwantyzacji (int4, int8, fp16)
- `--log_level`: Szczegółowość logowania (0=minimalna, 1=szczegółowa)

## Przykład: Konwersja Qwen3 do ONNX INT4

Na podstawie przykładu Hugging Face dostępnego pod adresem [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), oto jak zoptymalizować model Qwen3:

### Krok 1: Pobierz model (opcjonalnie)

Aby zminimalizować czas pobierania, zapisz tylko niezbędne pliki w pamięci podręcznej:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Krok 2: Zoptymalizuj model Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Krok 3: Przetestuj zoptymalizowany model

Utwórz prosty skrypt w Pythonie, aby przetestować zoptymalizowany model:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura wyników

Po optymalizacji katalog wynikowy będzie zawierał:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Zaawansowane użycie

### Pliki konfiguracyjne

Dla bardziej złożonych przepływów pracy optymalizacyjnej możesz użyć plików konfiguracyjnych JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Uruchom z konfiguracją:

```bash
olive run --config config.json
```

### Optymalizacja GPU

Dla optymalizacji GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Dla DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Dostosowanie z Olive

Olive obsługuje również dostosowanie modeli:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najlepsze praktyki

### 1. Wybór modelu
- Zacznij od mniejszych modeli do testowania (np. 0.5B-7B parametrów)
- Upewnij się, że architektura docelowego modelu jest obsługiwana przez Olive

### 2. Uwzględnienie sprzętu
- Dopasuj cel optymalizacji do sprzętu wdrożeniowego
- Używaj optymalizacji GPU, jeśli posiadasz sprzęt zgodny z CUDA
- Rozważ DirectML dla komputerów z Windows z zintegrowaną grafiką

### 3. Wybór precyzji
- **INT4**: Maksymalna kompresja, niewielka utrata dokładności
- **INT8**: Dobry balans między rozmiarem a dokładnością
- **FP16**: Minimalna utrata dokładności, umiarkowane zmniejszenie rozmiaru

### 4. Testowanie i walidacja
- Zawsze testuj zoptymalizowane modele w swoich specyficznych przypadkach użycia
- Porównuj metryki wydajności (opóźnienie, przepustowość, dokładność)
- Używaj reprezentatywnych danych wejściowych do oceny

### 5. Iteracyjna optymalizacja
- Zacznij od automatycznej optymalizacji dla szybkich wyników
- Używaj plików konfiguracyjnych dla precyzyjnej kontroli
- Eksperymentuj z różnymi etapami optymalizacji

## Rozwiązywanie problemów

### Typowe problemy

#### 1. Problemy z instalacją
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemy z CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemy z pamięcią
- Użyj mniejszych rozmiarów partii podczas optymalizacji
- Najpierw spróbuj kwantyzacji z wyższą precyzją (int8 zamiast int4)
- Upewnij się, że masz wystarczającą ilość miejsca na dysku na pamięć podręczną modelu

#### 4. Błędy ładowania modelu
- Sprawdź ścieżkę modelu i uprawnienia dostępu
- Upewnij się, że model wymaga `trust_remote_code=True`
- Sprawdź, czy wszystkie wymagane pliki modelu zostały pobrane

### Uzyskiwanie pomocy

- **Dokumentacja**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemy na GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Przykłady**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repozytorium przepisów Olive

### Wprowadzenie do przepisów Olive

Repozytorium [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) uzupełnia główne narzędzie Olive, oferując kompleksową kolekcję gotowych przepisów optymalizacyjnych dla popularnych modeli AI. Repozytorium służy jako praktyczny punkt odniesienia zarówno dla optymalizacji modeli publicznie dostępnych, jak i tworzenia przepływów optymalizacyjnych dla modeli własnych.

### Kluczowe funkcje

- **Ponad 100 gotowych przepisów**: Gotowe konfiguracje optymalizacyjne dla popularnych modeli
- **Wsparcie dla wielu architektur**: Obejmuje modele transformatorowe, modele wizji komputerowej i architektury multimodalne
- **Optymalizacje uwzględniające sprzęt**: Przepisy dostosowane do CPU, GPU i specjalistycznych akceleratorów
- **Popularne rodziny modeli**: Zawiera Phi, Llama, Qwen, Gemma, Mistral i wiele innych

### Obsługiwane rodziny modeli

Repozytorium zawiera przepisy optymalizacyjne dla:

#### Modele językowe
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, seria Qwen2.5 (0.5B do 14B)
- **Google Gemma**: Różne konfiguracje modeli Gemma
- **Mistral AI**: Seria Mistral-7B
- **DeepSeek**: Modele serii R1-Distill

#### Modele wizji i multimodalne
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Modele CLIP**: Różne konfiguracje CLIP-ViT
- **ResNet**: Optymalizacje ResNet-50
- **Transformery wizji**: ViT-base-patch16-224

#### Modele specjalistyczne
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Warianty podstawowe i wielojęzyczne
- **Transformery zdań**: all-MiniLM-L6-v2

### Korzystanie z przepisów Olive

#### Metoda 1: Klonowanie konkretnego przepisu

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metoda 2: Użycie przepisu jako szablonu

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Struktura przepisu

Każdy katalog przepisu zazwyczaj zawiera:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Przykład: Użycie przepisu Phi-4-mini

Użyjmy przepisu Phi-4-mini jako przykładu:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Plik konfiguracyjny zazwyczaj zawiera:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Dostosowywanie przepisów

#### Zmiana docelowego sprzętu

Aby zmienić docelowy sprzęt, zaktualizuj sekcję `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Dostosowanie parametrów optymalizacji

Zmodyfikuj sekcję `passes` dla różnych poziomów optymalizacji:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Tworzenie własnego przepisu

1. **Rozpocznij od podobnego modelu**: Znajdź przepis dla modelu o podobnej architekturze
2. **Zaktualizuj konfigurację modelu**: Zmień nazwę/ścieżkę modelu w konfiguracji
3. **Dostosuj parametry**: Zmień parametry optymalizacji według potrzeb
4. **Testuj i waliduj**: Uruchom optymalizację i sprawdź wyniki
5. **Wnieś swój wkład**: Rozważ dodanie swojego przepisu do repozytorium

### Korzyści z używania przepisów

#### 1. **Sprawdzone konfiguracje**
- Przetestowane ustawienia optymalizacji dla konkretnych modeli
- Unikanie prób i błędów w poszukiwaniu optymalnych parametrów

#### 2. **Dostosowanie do sprzętu**
- Wstępnie zoptymalizowane dla różnych dostawców wykonania
- Gotowe konfiguracje dla CPU, GPU i NPU

#### 3. **Szerokie pokrycie**
- Obsługuje najpopularniejsze modele open-source
- Regularne aktualizacje z nowymi wydaniami modeli

#### 4. **Wkład społeczności**
- Współpraca z społecznością AI
- Dzielenie się wiedzą i najlepszymi praktykami

### Wkład w przepisy Olive

Jeśli zoptymalizowałeś model, który nie jest objęty repozytorium:

1. **Sforkuj repozytorium**: Utwórz własny fork olive-recipes
2. **Utwórz katalog przepisu**: Dodaj nowy katalog dla swojego modelu
3. **Dodaj konfigurację**: Dodaj olive_config.json i pliki wspierające
4. **Udokumentuj użycie**: Zapewnij jasny README z instrukcjami
5. **Złóż pull request**: Wnieś swój wkład do społeczności

### Benchmarki wydajności

Wiele przepisów zawiera benchmarki wydajności pokazujące:
- **Poprawa opóźnienia**: Typowe przyspieszenie od 2 do 6 razy w porównaniu z bazą
- **Redukcja pamięci**: Redukcja zużycia pamięci o 50-75% dzięki kwantyzacji
- **Zachowanie dokładności**: Zachowanie dokładności na poziomie 95-99%

### Integracja z narzędziami AI

Przepisy działają bezproblemowo z:
- **VS Code AI Toolkit**: Bezpośrednia integracja dla optymalizacji modeli
- **Azure Machine Learning**: Przepływy optymalizacyjne w chmurze
- **ONNX Runtime**: Zoptymalizowane wdrożenie inferencji

## Dodatkowe zasoby

### Oficjalne linki
- **Repozytorium GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repozytorium przepisów Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Dokumentacja ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Przykład Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Przykłady społeczności
- **Notatniki Jupyter**: Dostępne w repozytorium Olive na GitHub — https://github.com/microsoft/Olive/tree/main/examples
- **Rozszerzenie VS Code**: Przegląd AI Toolkit dla VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Posty na blogu**: Blog Microsoft Open Source — https://opensource.microsoft.com/blog/

### Powiązane narzędzia
- **ONNX Runtime**: Silnik inferencji o wysokiej wydajności — https://onnxruntime.ai/
- **Hugging Face Transformers**: Źródło wielu kompatybilnych modeli — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Przepływy optymalizacyjne w chmurze — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Co dalej

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, prosimy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za wiarygodne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.