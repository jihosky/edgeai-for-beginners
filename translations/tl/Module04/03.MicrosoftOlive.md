<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:51:48+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "tl"
}
-->
# Seksyon 3: Microsoft Olive Optimization Suite

## Talaan ng Nilalaman
1. [Panimula](../../../Module04)
2. [Ano ang Microsoft Olive?](../../../Module04)
3. [Pag-install](../../../Module04)
4. [Mabilisang Gabay sa Pagsisimula](../../../Module04)
5. [Halimbawa: Pag-convert ng Qwen3 sa ONNX INT4](../../../Module04)
6. [Advanced na Paggamit](../../../Module04)
7. [Repository ng Olive Recipes](../../../Module04)
8. [Mga Pinakamahusay na Praktika](../../../Module04)
9. [Pag-aayos ng Problema](../../../Module04)
10. [Karagdagang Mga Mapagkukunan](../../../Module04)

## Panimula

Ang Microsoft Olive ay isang makapangyarihan at madaling gamitin na hardware-aware na toolkit para sa pag-optimize ng modelo na nagpapadali sa proseso ng pag-optimize ng mga modelo ng machine learning para sa deployment sa iba't ibang hardware platform. Kung ang target mo ay CPUs, GPUs, o mga espesyal na AI accelerators, tinutulungan ka ng Olive na makamit ang pinakamainam na performance habang pinapanatili ang katumpakan ng modelo.

## Ano ang Microsoft Olive?

Ang Olive ay isang madaling gamitin na hardware-aware na tool para sa pag-optimize ng modelo na pinagsasama ang mga nangungunang teknolohiya sa industriya sa model compression, optimization, at compilation. Gumagana ito kasama ang ONNX Runtime bilang isang E2E inference optimization solution.

### Pangunahing Katangian

- **Hardware-Aware Optimization**: Awtomatikong pinipili ang pinakamahusay na mga teknik sa pag-optimize para sa iyong target na hardware
- **40+ Built-in Optimization Components**: Saklaw ang model compression, quantization, graph optimization, at iba pa
- **Madaling CLI Interface**: Simpleng mga utos para sa karaniwang mga gawain sa pag-optimize
- **Suporta sa Multi-Framework**: Gumagana sa PyTorch, Hugging Face models, at ONNX
- **Suporta sa Mga Sikat na Modelo**: Awtomatikong na-optimize ng Olive ang mga sikat na arkitektura ng modelo tulad ng Llama, Phi, Qwen, Gemma, at iba pa

### Mga Benepisyo

- **Mas Maikling Oras ng Pag-develop**: Hindi na kailangang manu-manong mag-eksperimento sa iba't ibang teknik sa pag-optimize
- **Pagtaas ng Performance**: Malaking pagtaas ng bilis (hanggang 6x sa ilang kaso)
- **Deployment sa Iba't Ibang Platform**: Gumagana ang mga na-optimize na modelo sa iba't ibang hardware at operating systems
- **Pinanatiling Katumpakan**: Pinapanatili ang kalidad ng modelo habang pinapabuti ang performance

## Pag-install

### Mga Kinakailangan

- Python 3.8 o mas mataas
- pip package manager
- Virtual environment (inirerekomenda)

### Pangunahing Pag-install

Gumawa at i-activate ang virtual environment:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

I-install ang Olive na may auto-optimization features:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opsyonal na Dependencies

Nag-aalok ang Olive ng iba't ibang opsyonal na dependencies para sa karagdagang mga tampok:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### I-verify ang Pag-install

```bash
olive --help
```

Kung matagumpay, makikita mo ang Olive CLI help message.

## Mabilisang Gabay sa Pagsisimula

### Ang Iyong Unang Pag-optimize

I-optimize ang isang maliit na language model gamit ang auto-optimization feature ng Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ano ang Ginagawa ng Utos na Ito

Ang proseso ng pag-optimize ay kinabibilangan ng: pagkuha ng modelo mula sa lokal na cache, pag-capture ng ONNX Graph at pag-store ng weights sa ONNX data file, pag-optimize ng ONNX Graph, at pag-quantize ng modelo sa int4 gamit ang RTN method.

### Paliwanag ng Mga Parameter ng Utos

- `--model_name_or_path`: Hugging Face model identifier o lokal na path
- `--output_path`: Directory kung saan ise-save ang na-optimize na modelo
- `--device`: Target na device (cpu, gpu)
- `--provider`: Execution provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gumamit ng ONNX Runtime Generate AI para sa inference
- `--precision`: Quantization precision (int4, int8, fp16)
- `--log_level`: Logging verbosity (0=minimal, 1=verbose)

## Halimbawa: Pag-convert ng Qwen3 sa ONNX INT4

Batay sa ibinigay na Hugging Face example sa [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), narito kung paano i-optimize ang isang Qwen3 model:

### Hakbang 1: I-download ang Modelo (Opsyonal)

Upang mabawasan ang oras ng pag-download, i-cache lamang ang mahahalagang files:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Hakbang 2: I-optimize ang Qwen3 Model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hakbang 3: Subukan ang Na-optimize na Modelo

Gumawa ng simpleng Python script upang subukan ang iyong na-optimize na modelo:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Output Structure

Pagkatapos ng pag-optimize, ang iyong output directory ay maglalaman ng:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Advanced na Paggamit

### Configuration Files

Para sa mas kumplikadong workflows ng pag-optimize, maaari kang gumamit ng JSON configuration files:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Patakbuhin gamit ang configuration:

```bash
olive run --config config.json
```

### GPU Optimization

Para sa CUDA GPU optimization:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Para sa DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning gamit ang Olive

Sinusuportahan din ng Olive ang fine-tuning ng mga modelo:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Mga Pinakamahusay na Praktika

### 1. Pagpili ng Modelo
- Magsimula sa mas maliliit na modelo para sa testing (hal., 0.5B-7B parameters)
- Siguraduhing suportado ng Olive ang target na arkitektura ng modelo

### 2. Mga Pagsasaalang-alang sa Hardware
- I-match ang iyong target na pag-optimize sa iyong deployment hardware
- Gumamit ng GPU optimization kung mayroon kang CUDA-compatible hardware
- Isaalang-alang ang DirectML para sa Windows machines na may integrated graphics

### 3. Pagpili ng Precision
- **INT4**: Maximum compression, bahagyang pagkawala ng katumpakan
- **INT8**: Magandang balanse ng laki at katumpakan
- **FP16**: Minimal na pagkawala ng katumpakan, katamtamang pagbabawas ng laki

### 4. Pagsubok at Pag-validate
- Laging subukan ang mga na-optimize na modelo gamit ang iyong partikular na use cases
- Ihambing ang mga performance metrics (latency, throughput, katumpakan)
- Gumamit ng representative input data para sa evaluation

### 5. Iterative Optimization
- Magsimula sa auto-optimization para sa mabilisang resulta
- Gumamit ng configuration files para sa mas detalyadong kontrol
- Mag-eksperimento sa iba't ibang optimization passes

## Pag-aayos ng Problema

### Karaniwang Mga Isyu

#### 1. Mga Problema sa Pag-install
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Mga Isyu sa CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Mga Problema sa Memorya
- Gumamit ng mas maliit na batch sizes sa panahon ng pag-optimize
- Subukan ang quantization na may mas mataas na precision muna (int8 sa halip na int4)
- Siguraduhing may sapat na disk space para sa model caching

#### 4. Mga Error sa Pag-load ng Modelo
- I-verify ang model path at access permissions
- Tingnan kung ang modelo ay nangangailangan ng `trust_remote_code=True`
- Siguraduhing na-download ang lahat ng kinakailangang model files

### Pagkuha ng Tulong

- **Dokumentasyon**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Mga Halimbawa**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repository ng Olive Recipes

### Panimula sa Olive Recipes

Ang [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) repository ay nagdadagdag sa pangunahing Olive toolkit sa pamamagitan ng pagbibigay ng komprehensibong koleksyon ng mga handa nang gamitin na optimization recipes para sa mga sikat na AI models. Ang repository na ito ay nagsisilbing praktikal na reference para sa parehong pag-optimize ng mga pampublikong modelo at paglikha ng mga workflow ng pag-optimize para sa mga proprietary models.

### Pangunahing Katangian

- **100+ Pre-built Recipes**: Mga handa nang gamitin na optimization configurations para sa mga sikat na modelo
- **Suporta sa Multi-Architecture**: Saklaw ang mga transformer models, vision models, at multimodal architectures
- **Hardware-Specific Optimizations**: Mga recipe na iniakma para sa CPU, GPU, at mga espesyal na accelerators
- **Mga Sikat na Model Families**: Kasama ang Phi, Llama, Qwen, Gemma, Mistral, at marami pang iba

### Sinusuportahang Model Families

Ang repository ay naglalaman ng optimization recipes para sa:

#### Mga Language Models
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B hanggang 14B)
- **Google Gemma**: Iba't ibang Gemma model configurations
- **Mistral AI**: Mistral-7B series
- **DeepSeek**: R1-Distill series models

#### Vision at Multimodal Models
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: Iba't ibang CLIP-ViT configurations
- **ResNet**: ResNet-50 optimizations
- **Vision Transformers**: ViT-base-patch16-224

#### Mga Espesyal na Modelo
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Base at multilingual variants
- **Sentence Transformers**: all-MiniLM-L6-v2

### Paggamit ng Olive Recipes

#### Paraan 1: I-clone ang Tiyak na Recipe

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Paraan 2: Gamitin ang Recipe bilang Template

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Estruktura ng Recipe

Ang bawat recipe directory ay karaniwang naglalaman ng:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Halimbawa: Paggamit ng Phi-4-mini Recipe

Gamitin natin ang Phi-4-mini recipe bilang halimbawa:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Ang configuration file ay karaniwang naglalaman ng:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Pag-customize ng Recipes

#### Pagbabago ng Target na Hardware

Upang baguhin ang target na hardware, i-update ang seksyon ng `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Pag-aayos ng Optimization Parameters

Baguhin ang seksyon ng `passes` para sa iba't ibang optimization levels:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Paglikha ng Sariling Recipe

1. **Magsimula sa Katulad na Modelo**: Maghanap ng recipe para sa modelo na may katulad na arkitektura
2. **I-update ang Configuration ng Modelo**: Palitan ang pangalan/path ng modelo sa configuration
3. **Ayusin ang Mga Parameter**: Baguhin ang optimization parameters kung kinakailangan
4. **Subukan at I-validate**: Patakbuhin ang optimization at i-validate ang resulta
5. **Mag-contribute Pabalik**: Isaalang-alang ang pag-contribute ng iyong recipe sa repository

### Mga Benepisyo ng Paggamit ng Recipes

#### 1. **Mga Napatunayang Configuration**
- Nasubok na mga setting ng optimization para sa partikular na mga modelo
- Iniiwasan ang trial-and-error sa paghahanap ng optimal na mga parameter

#### 2. **Tuning na Naka-angkop sa Hardware**
- Pre-optimized para sa iba't ibang execution providers
- Mga handa nang gamitin na configuration para sa CPU, GPU, at NPU targets

#### 3. **Komprehensibong Saklaw**
- Sinusuportahan ang pinakasikat na open-source models
- Regular na ina-update sa mga bagong release ng modelo

#### 4. **Mga Kontribusyon ng Komunidad**
- Collaborative na pag-develop kasama ang AI community
- Shared na kaalaman at pinakamahusay na mga praktika

### Pag-contribute sa Olive Recipes

Kung na-optimize mo ang isang modelo na hindi saklaw ng repository:

1. **I-fork ang Repository**: Gumawa ng sariling fork ng olive-recipes
2. **Gumawa ng Recipe Directory**: Magdagdag ng bagong directory para sa iyong modelo
3. **Isama ang Configuration**: Magdagdag ng olive_config.json at mga supporting files
4. **Dokumentasyon ng Paggamit**: Magbigay ng malinaw na README na may mga instruksyon
5. **Mag-submit ng Pull Request**: Mag-contribute pabalik sa komunidad

### Mga Benchmark ng Performance

Maraming recipe ang naglalaman ng mga performance benchmark na nagpapakita ng:
- **Pagbuti ng Latency**: Karaniwang 2-6x na bilis kumpara sa baseline
- **Pagbawas ng Memorya**: 50-75% na pagbawas sa paggamit ng memorya gamit ang quantization
- **Pagpapanatili ng Katumpakan**: 95-99% na pagpapanatili ng katumpakan

### Integrasyon sa AI Toolkit

Ang mga recipe ay seamless na gumagana sa:
- **VS Code AI Toolkit**: Direktang integrasyon para sa pag-optimize ng modelo
- **Azure Machine Learning**: Mga cloud-based na workflow ng pag-optimize
- **ONNX Runtime**: Na-optimize na inference deployment

## Karagdagang Mga Mapagkukunan

### Opisyal na Mga Link
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Mga Halimbawa ng Komunidad
- **Jupyter Notebooks**: Available sa Olive GitHub repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit para sa VS Code overview — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Mga Blog Post**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Mga Kaugnay na Tools
- **ONNX Runtime**: High-performance inference engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Pinagmulan ng maraming compatible na modelo — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Mga cloud-based na workflow ng pag-optimize — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Ano ang susunod

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang AI translation service na [Co-op Translator](https://github.com/Azure/co-op-translator). Bagama't sinisikap naming maging tumpak, mangyaring tandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o hindi pagkakatugma. Ang orihinal na dokumento sa kanyang katutubong wika ang dapat ituring na mapagkakatiwalaang pinagmulan. Para sa mahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.