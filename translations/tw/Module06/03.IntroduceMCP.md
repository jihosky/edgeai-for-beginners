<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T11:22:32+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "tw"
}
-->
# 第三章 - 模型上下文協議 (MCP) 整合

## MCP (模型上下文協議) 簡介

模型上下文協議 (MCP) 是一個開源標準，用於將人工智慧應用程式連接到外部系統。透過 MCP，像 Claude 或 ChatGPT 這樣的 AI 應用程式可以連接到資料來源（例如本地檔案、資料庫）、工具（例如搜尋引擎、計算器）以及工作流程（例如專門的提示），使其能夠存取關鍵資訊並執行任務。

可以將 MCP 想像成 AI 應用程式的 **USB-C 接口**。就像 USB-C 提供了一種標準化的方式來連接電子設備，MCP 提供了一種標準化的方式來連接 AI 應用程式與外部系統。

### MCP 能實現什麼？

MCP 為 AI 應用程式解鎖了強大的功能：

- **個性化 AI 助理**：代理可以存取您的 Google 日曆和 Notion，成為更個性化的 AI 助理
- **進階程式碼生成**：Claude Code 可以根據 Figma 設計生成整個網頁應用程式
- **企業資料整合**：企業聊天機器人可以連接到組織內的多個資料庫，讓使用者能透過聊天分析資料
- **創意工作流程**：AI 模型可以在 Blender 上創建 3D 設計並使用 3D 列印機列印
- **即時資訊存取**：連接到外部資料來源以獲取最新資訊
- **複雜的多步操作**：結合多個工具和系統執行複雜的工作流程

### MCP 為什麼重要？

MCP 為整個生態系統帶來了多方面的好處：

**對開發者而言**：MCP 減少了開發和整合 AI 應用程式或代理的時間和複雜性。

**對 AI 應用程式而言**：MCP 提供了存取資料來源、工具和應用程式的生態系統，提升功能並改善最終使用者體驗。

**對最終使用者而言**：MCP 使 AI 應用程式或代理更具能力，能在必要時存取您的資料並代表您採取行動。

## MCP 中的小型語言模型 (SLMs)

小型語言模型代表了一種高效的 AI 部署方式，具有以下幾個優勢：

### SLMs 的優勢
- **資源效率**：較低的計算需求
- **更快的響應時間**：減少即時應用的延遲  
- **成本效益**：基礎設施需求最小化
- **隱私性**：可在本地運行，無需傳輸資料
- **客製化**：更容易針對特定領域進行微調

### 為什麼 SLMs 與 MCP 配合良好

SLMs 與 MCP 的結合創造了一種強大的組合，模型的推理能力透過外部工具得到增強，彌補了其較小參數數量的不足，並提升功能。

## Python MCP SDK 概述

Python MCP SDK 提供了構建支持 MCP 的應用程式的基礎。SDK 包括：

- **客戶端庫**：用於連接 MCP 伺服器
- **伺服器框架**：用於創建自定義 MCP 伺服器
- **協議處理器**：用於管理通信
- **工具整合**：用於執行外部功能

## 實際應用：Phi-4 MCP 客戶端

讓我們探討使用 Microsoft 的 Phi-4 小型模型整合 MCP 功能的實際應用。

### MCP 架構概述

MCP 採用 **客戶端-伺服器架構**，其中 MCP 主機（例如 Claude Code 或 Claude Desktop 等 AI 應用程式）建立與一個或多個 MCP 伺服器的連接。MCP 主機透過為每個 MCP 伺服器創建一個 MCP 客戶端來完成此操作。

#### 主要參與者

- **MCP 主機**：協調並管理一個或多個 MCP 客戶端的 AI 應用程式
- **MCP 客戶端**：維持與 MCP 伺服器的連接，並從 MCP 伺服器獲取上下文供 MCP 主機使用
- **MCP 伺服器**：提供上下文給 MCP 客戶端的程式

#### 雙層架構

MCP 包括兩個不同的層：

**資料層**：定義基於 JSON-RPC 的客戶端-伺服器通信協議，包括：
- 生命週期管理（連接初始化、功能協商）
- 核心原語（工具、資源、提示）
- 客戶端功能（取樣、引導、日誌記錄）
- 實用功能（通知、進度追蹤）

**傳輸層**：定義通信機制和通道：
- **STDIO 傳輸**：使用標準輸入/輸出流進行本地進程（最佳性能，無網絡開銷）
- **可流式 HTTP 傳輸**：使用 HTTP POST 和可選的伺服器推送事件進行遠程伺服器通信（支持標準 HTTP 驗證）

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### MCP 核心原語

MCP 定義了原語，指定了可以與 AI 應用程式共享的上下文資訊類型以及可以執行的操作範圍。

#### 伺服器原語

MCP 定義了伺服器可以暴露的三個核心原語：

**工具**：AI 應用程式可以調用的可執行功能
- 範例：檔案操作、API 調用、資料庫查詢
- 方法：`tools/list`、`tools/call`
- 支持動態發現和執行

**資源**：提供上下文資訊的資料來源
- 範例：檔案內容、資料庫記錄、API 回應
- 方法：`resources/list`、`resources/read`
- 允許存取結構化資料

**提示**：幫助結構化與語言模型互動的可重用模板
- 範例：系統提示、少量示例
- 方法：`prompts/list`、`prompts/get`
- 標準化 AI 互動模式

#### 客戶端原語

MCP 也定義了客戶端可以暴露的原語，以實現更豐富的互動：

**取樣**：允許伺服器向客戶端的 AI 應用程式請求語言模型完成
- 方法：`sampling/complete`
- 支持模型無關的伺服器開發
- 提供存取主機語言模型的能力

**引導**：允許伺服器向使用者請求額外資訊
- 方法：`elicitation/request`
- 支持使用者互動和確認
- 支持動態資訊收集

**日誌記錄**：允許伺服器向客戶端發送日誌消息
- 用於調試和監控
- 提供伺服器操作的可見性

### MCP 協議生命週期

#### 初始化和功能協商

MCP 是一種有狀態的協議，需要生命週期管理。初始化過程具有以下幾個重要目的：

1. **協議版本協商**：確保客戶端和伺服器使用兼容的協議版本（例如 "2025-06-18"）
2. **功能發現**：每一方宣告支持的功能和原語
3. **身份交換**：提供身份和版本資訊

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### 工具發現和執行

初始化後，客戶端可以發現並執行工具：

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### 即時通知

MCP 支持即時通知以進行動態更新：

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## 入門指南：逐步指導

### 第一步：環境設置

安裝所需的依賴項：
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### 第二步：基本配置

設置您的環境變數：
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### 第三步：運行您的第一個 MCP 客戶端

**基本 Ollama 設置：**
```bash
python ghmodel_mcp_demo.py
```

**使用 vLLM 後端：**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**伺服器推送事件連接：**
```bash
python ghmodel_mcp_demo.py --run sse
```

**自定義 MCP 伺服器：**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### 第四步：程式化使用

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 高級功能

### 多後端支持

該實現支持 Ollama 和 vLLM 後端，您可以根據需求進行選擇：

- **Ollama**：更適合本地開發和測試
- **vLLM**：針對生產和高吞吐量場景進行優化

### 靈活的連接協議

支持兩種連接模式：

**STDIO 模式**：直接進程通信
- 更低延遲
- 適合本地工具
- 簡單設置

**SSE 模式**：基於 HTTP 的流式傳輸
- 支持網絡
- 更適合分布式系統
- 即時更新

### 工具整合能力

系統可以與各種工具整合：
- 網頁自動化（Playwright）
- 檔案操作
- API 交互
- 系統命令
- 自定義功能

## 錯誤處理和最佳實踐

### 全面錯誤管理

該實現包括針對以下情況的強大錯誤處理：

**連接錯誤：**
- MCP 伺服器故障
- 網絡超時
- 連接問題

**工具執行錯誤：**
- 缺少工具
- 參數驗證
- 執行失敗

**回應處理錯誤：**
- JSON 解析問題
- 格式不一致
- LLM 回應異常

### 最佳實踐

1. **資源管理**：使用異步上下文管理器
2. **錯誤處理**：實施全面的 try-catch 塊
3. **日誌記錄**：啟用適當的日誌級別
4. **安全性**：驗證輸入並清理輸出
5. **性能**：使用連接池和緩存

## 實際應用

### 網頁自動化
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### 資料處理
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API 整合
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## 性能優化

### 記憶體管理
- 高效的消息歷史記錄處理
- 適當的資源清理
- 連接池

### 網絡優化
- 異步 HTTP 操作
- 可配置的超時
- 優雅的錯誤恢復

### 並行處理
- 非阻塞 I/O
- 工具的並行執行
- 高效的異步模式

## 安全考量

### 資料保護
- 安全的 API 金鑰管理
- 輸入驗證
- 輸出清理

### 網絡安全
- 支持 HTTPS
- 本地端點默認設置
- 安全令牌處理

### 執行安全性
- 工具篩選
- 沙盒環境
- 審計日誌記錄

## MCP 生態系統與開發

### MCP 項目範圍

模型上下文協議生態系統包括以下幾個主要組成部分：

- **[MCP 規範](https://modelcontextprotocol.io/specification/latest)**：官方規範，概述客戶端和伺服器的實施要求
- **[MCP SDKs](https://modelcontextprotocol.io/docs/sdk)**：實現 MCP 的不同程式語言 SDK
- **MCP 開發工具**：包括 [MCP Inspector](https://github.com/modelcontextprotocol/inspector) 的 MCP 伺服器和客戶端開發工具
- **[MCP 參考伺服器實現](https://github.com/modelcontextprotocol/servers)**：MCP 伺服器的參考實現

### 開始 MCP 開發

要開始使用 MCP：

**構建伺服器**：[創建 MCP 伺服器](https://modelcontextprotocol.io/docs/develop/build-server)，以暴露您的資料和工具

**構建客戶端**：[開發應用程式](https://modelcontextprotocol.io/docs/develop/build-client)，以連接 MCP 伺服器

**學習概念**：[了解 MCP 的核心概念](https://modelcontextprotocol.io/docs/learn/architecture) 和架構

## 結論

整合 MCP 的小型語言模型代表了 AI 應用程式開發的一個新範式。透過結合小型模型的效率與外部工具的力量，開發者可以創建既資源高效又功能強大的智能系統。

模型上下文協議提供了一種標準化的方式來連接 AI 應用程式與外部系統，就像 USB-C 提供電子設備的通用連接標準一樣。這種標準化使得：

- **無縫整合**：將 AI 模型連接到多樣化的資料來源和工具
- **生態系統增長**：一次構建，可在多個 AI 應用程式中使用
- **功能增強**：透過外部功能擴展 SLMs
- **即時更新**：支持動態、響應式 AI 應用程式

關鍵要點：
- MCP 是一個連接 AI 應用程式與外部系統的開放標準
- 協議支持工具、資源和提示作為核心原語
- 即時通知支持動態、響應式應用程式
- 正確的生命週期管理和錯誤處理對於生產使用至關重要
- 生態系統提供了全面的 SDK 和開發工具

## 參考資料與進一步閱讀

### 官方 MCP 文檔

- **[模型上下文協議官方網站](https://modelcontextprotocol.io/)** - 完整文檔和規範
- **[MCP 入門指南](https://modelcontextprotocol.io/docs/getting-started/intro)** - 介紹和核心概念
- **[MCP 架構概述](https://modelcontextprotocol.io/docs/learn/architecture)** - 詳細技術架構
- **[MCP 規範](https://modelcontextprotocol.io/specification/latest)** - 官方協議規範
- **[MCP SDKs 文檔](https://modelcontextprotocol.io/docs/sdk)** - 特定程式語言的 SDK 指南

### 開發資源

- **[MCP 初學者指南](https://aka.ms/mcp-for-beginners)** - MCP 的全面初學者指南
- **[MCP GitHub 組織](https://github.com/modelcontextprotocol)** - 官方存儲庫和示例
- **[MCP 伺服器存儲庫](https://github.com/modelcontextprotocol/servers)** - 參考伺服器實現
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - 開發和調試工具
- **[構建 MCP 伺服器指南](https://modelcontextprotocol.io/docs/develop/build-server)** - 伺服器開發教程
- **[構建 MCP 客戶端指南](https://modelcontextprotocol.io/docs/develop/build-client)** - 客戶端開發教程

### 小型語言模型與邊緣 AI

- **[Microsoft Phi 模型](https://aka.ms/phicookbook)** - Phi 模型系列
- **[Foundry Local 文檔](https://github.com/microsoft/Foundry-Local)** - Microsoft 的邊緣 AI 執行環境
- **[Ollama 文件](https://ollama.ai/docs)** - 本地 LLM 部署平台  
- **[vLLM 文件](https://docs.vllm.ai/)** - 高效能 LLM 服務  

### 技術標準與協議  

- **[JSON-RPC 2.0 規範](https://www.jsonrpc.org/)** - MCP 使用的底層 RPC 協議  
- **[JSON Schema](https://json-schema.org/)** - MCP 工具的架構定義標準  
- **[OpenAPI 規範](https://swagger.io/specification/)** - API 文件標準  
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - 即時更新的網頁標準  

### AI代理開發  

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - 可用於生產的代理開發框架  
- **[LangChain 文件](https://docs.langchain.com/)** - 代理與工具整合框架  
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - 微軟的 AI 編排 SDK  

### 行業報告與研究  

- **[Anthropic 的模型上下文協議公告](https://www.anthropic.com/news/model-context-protocol)** - MCP 的原始介紹  
- **[小型語言模型調查](https://arxiv.org/abs/2410.20011)** - 關於 SLM 研究的學術調查  
- **[邊緣 AI 市場分析](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - 行業趨勢與預測  
- **[AI代理開發最佳實踐](https://arxiv.org/abs/2309.02427)** - 關於代理架構的研究  

本節提供了建立您自己的基於 SLM 的 MCP 應用的基礎，開啟了自動化、數據處理和智能系統整合的可能性。  

## ➡️ 下一步  

- [模組 7. 邊緣 AI 範例](../Module07/README.md)  

---

**免責聲明**：  
本文件已使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵資訊，建議使用專業人工翻譯。我們對因使用此翻譯而產生的任何誤解或誤釋不承擔責任。