<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "efb0e70d6e87d0795f4d381c3bc99074",
  "translation_date": "2025-10-21T06:55:44+00:00",
  "source_file": "Module07/aitoolkit.md",
  "language_code": "tw"
}
-->
# AI 工具包適用於 Visual Studio Code - 邊緣 AI 開發指南

## 介紹

歡迎使用 AI 工具包的完整指南，幫助您在 Visual Studio Code 中進行邊緣 AI 開發。隨著人工智慧從集中式雲端運算轉向分散式邊緣設備，開發者需要強大的整合工具來應對邊緣部署的獨特挑戰，例如資源限制和離線操作需求。

AI 工具包為 Visual Studio Code 提供了一個完整的開發環境，專門設計用於構建、測試和優化能在邊緣設備上高效運行的 AI 應用程式。無論您是為物聯網感測器、移動設備、嵌入式系統還是邊緣伺服器開發，這款工具包都能在熟悉的 VS Code 環境中簡化整個開發工作流程。

本指南將帶您了解如何在邊緣 AI 專案中利用 AI 工具包的基本概念、工具和最佳實踐，從初始模型選擇到生產部署。

## 概述

AI 工具包是 Visual Studio Code 的一個強大擴展，能簡化代理開發和 AI 應用程式創建。該工具包提供了全面的功能，支持探索、評估和部署來自多家提供商（包括 Anthropic、OpenAI、GitHub、Google）的 AI 模型，同時支持使用 ONNX 和 Ollama 進行本地模型執行。

AI 工具包的獨特之處在於其涵蓋整個 AI 開發生命周期的全面方法。與傳統 AI 開發工具僅專注於單一方面不同，AI 工具包提供了一個整合環境，涵蓋模型發現、實驗、代理開發、評估和部署——全部在熟悉的 VS Code 環境中完成。

該平台專門設計用於快速原型設計和生產部署，具備提示生成、快速入門、無縫 MCP（模型上下文協議）工具整合以及廣泛的評估功能等特性。對於邊緣 AI 開發而言，這意味著您可以高效地開發、測試和優化 AI 應用程式以適應邊緣部署場景，同時保持完整的開發工作流程。

## 學習目標

完成本指南後，您將能夠：

### 核心能力
- **安裝和配置** AI 工具包以適應邊緣 AI 開發工作流程
- **導航和使用** AI 工具包介面，包括模型目錄、遊樂場和代理構建器
- **選擇和評估**適合邊緣部署的 AI 模型，基於性能和資源限制
- **轉換和優化**模型，使用 ONNX 格式和量化技術以適應邊緣設備

### 邊緣 AI 開發技能
- **設計和實現**邊緣 AI 應用程式，使用整合開發環境
- **進行模型測試**，在類似邊緣的條件下使用本地推理和資源監控
- **創建和定制**針對邊緣部署場景優化的 AI 代理
- **評估模型性能**，使用與邊緣運算相關的指標（延遲、記憶體使用率、準確性）

### 優化和部署
- **應用量化和剪枝技術**，在保持可接受性能的同時減少模型大小
- **優化模型**以適應特定邊緣硬體平台，包括 CPU、GPU 和 NPU 加速
- **實施最佳實踐**，用於邊緣 AI 開發，包括資源管理和備援策略
- **準備模型和應用程式**以進行邊緣設備上的生產部署

### 高級邊緣 AI 概念
- **整合邊緣 AI 框架**，包括 ONNX Runtime、Windows ML 和 TensorFlow Lite
- **實現多模型架構**和聯邦學習場景以適應邊緣環境
- **排除常見邊緣 AI 問題**，包括記憶體限制、推理速度和硬體兼容性
- **設計監控和日誌策略**，用於生產中的邊緣 AI 應用程式

### 實際應用
- **構建端到端邊緣 AI 解決方案**，從模型選擇到部署
- **展示熟練度**，在邊緣特定的開發工作流程和優化技術方面
- **應用所學概念**於真實世界的邊緣 AI 用例，包括物聯網、移動和嵌入式應用程式
- **評估和比較**不同的邊緣 AI 部署策略及其權衡

## 邊緣 AI 開發的主要功能

### 1. 模型目錄和發現
- **多提供商支持**：瀏覽並訪問來自 Anthropic、OpenAI、GitHub、Google 等提供商的 AI 模型
- **本地模型整合**：簡化 ONNX 和 Ollama 模型的發現以適應邊緣部署
- **GitHub 模型**：直接整合 GitHub 的模型托管以簡化訪問
- **模型比較**：並排比較模型以找到適合邊緣設備限制的最佳平衡

### 2. 互動式遊樂場
- **互動測試環境**：在受控環境中快速試驗模型功能
- **多模態支持**：使用圖像、文本和其他邊緣場景中的典型輸入進行測試
- **即時試驗**：即時反饋模型的響應和性能
- **參數優化**：根據邊緣部署需求微調模型參數

### 3. 提示（代理）構建器
- **自然語言生成**：使用自然語言描述生成起始提示
- **迭代改進**：根據模型響應和性能改進提示
- **任務分解**：通過提示鏈接和結構化輸出分解複雜任務
- **變數支持**：在提示中使用變數以實現動態代理行為
- **生產代碼生成**：生成適合快速應用程式開發的生產就緒代碼

### 4. 批量運行和評估
- **多模型測試**：同時執行多個提示以測試所選模型
- **高效大規模測試**：高效測試各種輸入和配置
- **自定義測試案例**：使用測試案例運行代理以驗證功能
- **性能比較**：比較不同模型和配置的結果

### 5. 使用數據集進行模型評估
- **標準指標**：使用內建評估器測試 AI 模型（F1 分數、相關性、相似性、一致性）
- **自定義評估器**：為特定用例創建自己的評估指標
- **數據集整合**：使用全面的數據集測試模型
- **性能測量**：量化模型性能以進行邊緣部署決策

### 6. 微調功能
- **模型定制**：根據特定用例和領域定制模型
- **專業化適配**：使模型適應專業領域和需求
- **邊緣優化**：專門針對邊緣部署限制進行模型微調
- **領域特定訓練**：創建適合特定邊緣用例的模型

### 7. MCP 工具整合
- **外部工具連接**：通過模型上下文協議伺服器連接代理到外部工具
- **實際操作**：使代理能夠查詢數據庫、訪問 API 或執行自定義邏輯
- **現有 MCP 伺服器**：使用命令（stdio）或 HTTP（伺服器發送事件）協議的工具
- **自定義 MCP 開發**：在代理構建器中構建和搭建新的 MCP 伺服器並進行測試

### 8. 代理開發和測試
- **函數調用支持**：使代理能夠動態調用外部函數
- **即時整合測試**：通過即時運行和工具使用進行整合測試
- **代理版本控制**：代理的版本控制，並可比較評估結果
- **調試和追蹤**：本地追蹤和調試功能以支持代理開發

## 邊緣 AI 開發工作流程

### 第一階段：模型發現和選擇
1. **探索模型目錄**：使用模型目錄尋找適合邊緣部署的模型
2. **比較性能**：根據大小、準確性和推理速度評估模型
3. **本地測試**：使用 Ollama 或 ONNX 模型進行本地測試以便邊緣部署
4. **評估資源需求**：確定目標邊緣設備的記憶體和計算需求

### 第二階段：模型優化
1. **轉換為 ONNX**：將選定模型轉換為 ONNX 格式以適應邊緣兼容性
2. **應用量化**：通過 INT8 或 INT4 量化減少模型大小
3. **硬體優化**：針對目標邊緣硬體（ARM、x86、專用加速器）進行優化
4. **性能驗證**：驗證優化後的模型是否保持可接受的準確性

### 第三階段：應用程式開發
1. **代理設計**：使用代理構建器創建邊緣優化的 AI 代理
2. **提示工程**：開發能有效與較小的邊緣模型配合的提示
3. **整合測試**：在模擬邊緣條件下測試代理
4. **代碼生成**：生成適合邊緣部署的生產代碼

### 第四階段：評估和測試
1. **批量評估**：測試多種配置以找到最佳邊緣設置
2. **性能分析**：分析推理速度、記憶體使用率和準確性
3. **邊緣模擬**：在類似目標邊緣部署環境的條件下進行測試
4. **壓力測試**：在各種負載條件下評估性能

### 第五階段：部署準備
1. **最終優化**：根據測試結果進行最終優化
2. **部署打包**：打包模型和代碼以進行邊緣部署
3. **文檔編寫**：記錄部署需求和配置
4. **監控設置**：為邊緣部署準備監控和日誌記錄

## 邊緣 AI 開發的目標受眾

### 邊緣 AI 開發者
- 構建 AI 驅動的邊緣設備和物聯網解決方案的應用程式開發者
- 將 AI 功能整合到資源受限設備中的嵌入式系統開發者
- 為智能手機和平板電腦創建設備端 AI 應用程式的移動開發者

### 邊緣 AI 工程師
- 優化模型以適應邊緣部署並管理推理管道的 AI 工程師
- 部署和管理分散式邊緣基礎設施中 AI 模型的 DevOps 工程師
- 優化 AI 工作負載以適應邊緣硬體限制的性能工程師

### 研究人員和教育工作者
- 開發高效模型和演算法以適應邊緣運算的 AI 研究人員
- 教授邊緣 AI 概念並演示優化技術的教育工作者
- 學習邊緣 AI 部署挑戰和解決方案的學生

## 邊緣 AI 用例

### 智能物聯網設備
- **即時圖像識別**：在物聯網攝像頭和感測器上部署計算機視覺模型
- **語音處理**：在智能音箱上實現語音識別和自然語言處理
- **預測性維護**：在工業邊緣設備上運行異常檢測模型
- **環境監測**：部署用於環境應用的感測器數據分析模型

### 移動和嵌入式應用程式
- **設備端翻譯**：實現可離線工作的語言翻譯模型
- **擴增實境**：部署即時物體識別和追蹤以支持 AR 應用程式
- **健康監測**：在可穿戴設備和醫療設備上運行健康分析模型
- **自主系統**：為無人機、機器人和車輛實現決策模型

### 邊緣運算基礎設施
- **邊緣數據中心**：在邊緣數據中心部署 AI 模型以支持低延遲應用
- **CDN 整合**：將 AI 處理能力整合到內容分發網絡中
- **5G 邊緣**：利用 5G 邊緣運算支持 AI 驅動的應用程式
- **霧運算**：在霧運算環境中實現 AI 處理

## 安裝和設置

### 擴展安裝
直接從 Visual Studio Code Marketplace 安裝 AI 工具包擴展：

**擴展 ID**：`ms-windows-ai-studio.windows-ai-studio`

**安裝方法**：
1. **VS Code Marketplace**：在擴展視圖中搜索 "AI Toolkit"
2. **命令行**：`code --install-extension ms-windows-ai-studio.windows-ai-studio`
3. **直接安裝**：從 [VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio) 下載

### 邊緣 AI 開發的先決條件
- **Visual Studio Code**：建議使用最新版本
- **Python 環境**：Python 3.8+，安裝所需的 AI 庫
- **ONNX Runtime**（可選）：用於 ONNX 模型推理
- **Ollama**（可選）：用於本地模型服務
- **硬體加速工具**：CUDA、OpenVINO 或平台特定加速器

### 初始配置
1. **擴展啟用**：打開 VS Code，確認 AI 工具包顯示在活動欄中
2. **模型提供商設置**：配置訪問 GitHub、OpenAI、Anthropic 或其他模型提供商
3. **本地環境**：設置 Python 環境並安裝所需的套件
4. **硬體加速**：如果可用，配置 GPU/NPU 加速
5. **MCP 整合**：如有需要，設置模型上下文協議伺服器

### 初次設置檢查清單
- [ ] AI 工具包擴展已安裝並啟用
- [ ] 模型目錄可訪問且模型可發現
- [ ] 遊樂場可用於模型測試
- [ ] 代理構建器可用於提示開發
- [ ] 本地開發環境已配置
- [ ] 硬體加速（如可用）已正確配置

## 開始使用 AI 工具包

### 快速入門指南

我們建議從 GitHub 托管的模型開始，以獲得最簡化的體驗：

1. **安裝**：按照[安裝指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)設置 AI 工具包
2. **模型發現**：在擴展樹視圖中選擇 **CATALOG > Models**，探索可用模型
3. **GitHub 模型**：從 GitHub 托管的模型開始以獲得最佳整合
4. **遊樂場測試**：從任意模型卡中選擇 **Try in Playground**，開始試驗模型功能

### 邊緣 AI 開發逐步指南

#### 第一步：模型探索和選擇
1. 在 VS Code 活動欄中打開 AI 工具包視圖
2. 瀏覽模型目錄以尋找適合邊緣部署的模型
3. 根據邊緣需求篩選提供商（GitHub、ONNX、Ollama）
4. 使用 **Try in Playground** 即時測試模型功能

#### 第二步：代理開發
1. 使用 **
2. 使用自然語言描述生成初始提示
3. 根據模型回應進行提示迭代和改進
4. 整合 MCP 工具以增強代理能力

#### 第三步：測試與評估
1. 使用 **Bulk Run** 測試多個提示在選定模型上的表現
2. 使用測試案例運行代理以驗證功能
3. 使用內建或自定義指標評估準確性和性能
4. 比較不同模型和配置

#### 第四步：微調與優化
1. 為特定邊緣使用案例定制模型
2. 應用領域專屬微調
3. 優化以滿足邊緣部署限制
4. 版本化並比較不同代理配置

#### 第五步：部署準備
1. 使用 Agent Builder 生成生產就緒的代碼
2. 設置 MCP 伺服器連接以供生產使用
3. 為邊緣設備準備部署包
4. 配置監控和評估指標

## AI 工具包範例

試試我們的範例
[AI 工具包範例](https://github.com/Azure-Samples/AI_Toolkit_Samples)旨在幫助開發者和研究人員有效地探索和實現 AI 解決方案。

我們的範例包括：

範例代碼：預建範例展示 AI 功能，例如訓練、部署或將模型整合到應用中。
文檔：指南和教程幫助用戶了解 AI 工具包功能及其使用方法。
先決條件

- Visual Studio Code
- Visual Studio Code 的 AI 工具包
- GitHub 精細化個人訪問令牌 (PAT)
- Foundry Local

## 邊緣 AI 開發最佳實踐

### 模型選擇
- **大小限制**：選擇符合目標設備內存限制的模型
- **推理速度**：優先選擇推理速度快的模型以滿足實時應用需求
- **準確性權衡**：在模型準確性和資源限制之間取得平衡
- **格式兼容性**：優先選擇 ONNX 或硬件優化格式以進行邊緣部署

### 優化技術
- **量化**：使用 INT8 或 INT4 量化減少模型大小並提高速度
- **剪枝**：移除不必要的模型參數以降低計算需求
- **知識蒸餾**：創建保持大型模型性能的小型模型
- **硬件加速**：在可用時利用 NPU、GPU 或專用加速器

### 開發工作流程
- **迭代測試**：在開發過程中頻繁在類似邊緣的條件下測試
- **性能監控**：持續監控資源使用和推理速度
- **版本控制**：跟蹤模型版本和優化設置
- **文檔記錄**：記錄所有優化決策和性能權衡

### 部署考量
- **資源監控**：在生產環境中監控內存、CPU 和功耗使用
- **備援策略**：為模型故障實施備援機制
- **更新機制**：規劃模型更新和版本管理
- **安全性**：為邊緣 AI 應用實施適當的安全措施

## 與邊緣 AI 框架整合

### ONNX Runtime
- **跨平台部署**：在不同的邊緣平台上部署 ONNX 模型
- **硬件優化**：利用 ONNX Runtime 的硬件專屬優化
- **移動支持**：使用 ONNX Runtime Mobile 支持智能手機和平板應用
- **物聯網整合**：使用 ONNX Runtime 的輕量化分發在物聯網設備上部署

### Windows ML
- **Windows 設備**：優化 Windows 基礎的邊緣設備和 PC
- **NPU 加速**：利用 Windows 設備上的神經處理單元
- **DirectML**：在 Windows 平台上使用 DirectML 進行 GPU 加速
- **UWP 整合**：與通用 Windows 平台應用整合

### TensorFlow Lite
- **移動優化**：在移動和嵌入式設備上部署 TensorFlow Lite 模型
- **硬件代理**：使用專用硬件代理進行加速
- **微控制器**：使用 TensorFlow Lite Micro 在微控制器上部署
- **跨平台支持**：在 Android、iOS 和嵌入式 Linux 系統上部署

### Azure IoT Edge
- **雲-邊緣混合**：結合雲端訓練與邊緣推理
- **模塊部署**：將 AI 模型作為 IoT Edge 模塊部署
- **設備管理**：遠程管理邊緣設備和模型更新
- **遙測**：收集邊緣部署的性能數據和模型指標

## 高級邊緣 AI 場景

### 多模型部署
- **模型集成**：部署多個模型以提高準確性或冗餘性
- **A/B 測試**：在邊緣設備上同時測試不同模型
- **動態選擇**：根據當前設備條件選擇模型
- **資源共享**：優化多個部署模型的資源使用

### 聯邦學習
- **分佈式訓練**：在多個邊緣設備上訓練模型
- **隱私保護**：保持訓練數據本地化，同時共享模型改進
- **協作學習**：使設備能從集體經驗中學習
- **邊緣-雲協調**：協調邊緣設備與雲基礎設施之間的學習

### 實時處理
- **流處理**：在邊緣設備上處理連續數據流
- **低延遲推理**：優化以實現最小推理延遲
- **批量處理**：在邊緣設備上高效處理數據批次
- **自適應處理**：根據當前設備能力調整處理方式

## 邊緣 AI 開發故障排除

### 常見問題
- **內存限制**：模型過大，超出目標設備內存
- **推理速度**：模型推理速度不足以滿足實時需求
- **準確性下降**：優化導致模型準確性不可接受地降低
- **硬件兼容性**：模型與目標硬件不兼容

### 調試策略
- **性能分析**：使用 AI 工具包的追蹤功能識別瓶頸
- **資源監控**：在開發過程中監控內存和 CPU 使用
- **增量測試**：逐步測試優化以隔離問題
- **硬件模擬**：使用開發工具模擬目標硬件

### 優化解決方案
- **進一步量化**：應用更激進的量化技術
- **模型架構**：考慮不同的模型架構以適應邊緣需求
- **預處理優化**：優化數據預處理以滿足邊緣限制
- **推理優化**：使用硬件專屬推理優化

## 資源與下一步

### 官方文檔
- [AI 工具包開發者文檔](https://aka.ms/AIToolkit/doc)
- [安裝與設置指南](https://code.visualstudio.com/docs/intelligentapps/overview#_install-and-setup)
- [VS Code 智能應用文檔](https://code.visualstudio.com/docs/intelligentapps)
- [模型上下文協議 (MCP) 文檔](https://modelcontextprotocol.io/)

### 社群與支持
- [AI 工具包 GitHub 儲存庫](https://github.com/microsoft/vscode-ai-toolkit)
- [GitHub 問題與功能請求](https://aka.ms/AIToolkit/feedback)
- [Azure AI Foundry Discord 社群](https://aka.ms/azureaifoundry/discord)
- [VS Code 擴展市場](https://marketplace.visualstudio.com/items?itemName=ms-windows-ai-studio.windows-ai-studio)

### 技術資源
- [ONNX Runtime 文檔](https://onnxruntime.ai/)
- [Ollama 文檔](https://ollama.ai/)
- [Windows ML 文檔](https://docs.microsoft.com/en-us/windows/ai/)
- [Azure AI Foundry 文檔](https://learn.microsoft.com/en-us/azure/ai-foundry/)

### 學習路徑
- [邊緣 AI 基礎課程](../Module01/README.md)
- [小型語言模型指南](../Module02/README.md)
- [邊緣部署策略](../Module03/README.md)
- [Windows 邊緣 AI 開發](./windowdeveloper.md)

### 其他資源
- **儲存庫統計**：1.8k+ 星標，150+ 分叉，18+ 貢獻者
- **許可證**：MIT 許可證
- **安全性**：適用於 Microsoft 安全政策
- **遙測**：遵守 VS Code 遙測設置

## 結論

Visual Studio Code 的 AI 工具包是一個現代 AI 開發的綜合平台，提供了特別適合邊緣 AI 應用的流線型代理開發能力。其廣泛的模型目錄支持 Anthropic、OpenAI、GitHub 和 Google 等提供商，並通過 ONNX 和 Ollama 提供本地執行，為多樣化的邊緣部署場景提供了所需的靈活性。

該工具包的優勢在於其整合式方法——從 Playground 中的模型探索和實驗，到使用 Prompt Builder 進行高級代理開發，全面的評估能力，以及無縫的 MCP 工具整合。對於邊緣 AI 開發者而言，這意味著在邊緣部署前快速原型設計和測試 AI 代理，並能快速迭代和優化以適應資源受限的環境。

邊緣 AI 開發的主要優勢包括：
- **快速實驗**：在邊緣部署前快速測試模型和代理
- **多提供商靈活性**：從多個來源訪問模型以尋找最佳邊緣解決方案
- **本地開發**：使用 ONNX 和 Ollama 進行離線和隱私保護的開發
- **生產就緒**：生成生產就緒代碼並通過 MCP 整合外部工具
- **全面評估**：使用內建和自定義指標驗證邊緣 AI 性能

隨著 AI 不斷向邊緣部署場景發展，Visual Studio Code 的 AI 工具包提供了所需的開發環境和工作流程，幫助構建、測試和優化適合資源受限環境的智能應用。無論您是在開發物聯網解決方案、移動 AI 應用，還是嵌入式智能系統，該工具包的全面功能集和整合工作流程支持整個邊緣 AI 開發生命周期。

隨著持續的開發和活躍的社群（1.8k+ GitHub 星標），AI 工具包始終處於 AI 開發工具的前沿，不斷演進以滿足現代 AI 開發者在邊緣部署場景中的需求。

[下一步 Foundry Local](./foundrylocal.md)

---

**免責聲明**：  
本文件使用 AI 翻譯服務 [Co-op Translator](https://github.com/Azure/co-op-translator) 進行翻譯。儘管我們致力於提供準確的翻譯，請注意自動翻譯可能包含錯誤或不準確之處。原始文件的母語版本應被視為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而引起的任何誤解或誤釋不承擔責任。