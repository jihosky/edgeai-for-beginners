<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T11:47:17+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "bn"
}
-->
# অধ্যায় ৩ : Microsoft Olive অপ্টিমাইজেশন স্যুট

## সূচিপত্র
1. [ভূমিকা](../../../Module04)
2. [Microsoft Olive কী?](../../../Module04)
3. [ইনস্টলেশন](../../../Module04)
4. [দ্রুত শুরু করার নির্দেশিকা](../../../Module04)
5. [উদাহরণ: Qwen3 কে ONNX INT4 এ রূপান্তর করা](../../../Module04)
6. [উন্নত ব্যবহার](../../../Module04)
7. [Olive রেসিপি সংগ্রহশালা](../../../Module04)
8. [সেরা অনুশীলন](../../../Module04)
9. [সমস্যা সমাধান](../../../Module04)
10. [অতিরিক্ত সম্পদ](../../../Module04)

## ভূমিকা

Microsoft Olive একটি শক্তিশালী, সহজে ব্যবহারযোগ্য হার্ডওয়্যার-সচেতন মডেল অপ্টিমাইজেশন টুলকিট যা বিভিন্ন হার্ডওয়্যার প্ল্যাটফর্মে মেশিন লার্নিং মডেল মোতায়েনের জন্য অপ্টিমাইজেশন প্রক্রিয়াকে সহজ করে। আপনি CPU, GPU বা বিশেষায়িত AI অ্যাক্সিলারেটর লক্ষ্য করলেও, Olive আপনাকে মডেলের নির্ভুলতা বজায় রেখে সর্বোত্তম কর্মক্ষমতা অর্জনে সহায়তা করে।

## Microsoft Olive কী?

Olive একটি সহজে ব্যবহারযোগ্য হার্ডওয়্যার-সচেতন মডেল অপ্টিমাইজেশন টুল যা মডেল কম্প্রেশন, অপ্টিমাইজেশন এবং কম্পাইলেশনের ক্ষেত্রে শিল্প-নেতৃস্থানীয় কৌশলগুলি একত্রিত করে। এটি ONNX Runtime এর সাথে একটি E2E ইনফারেন্স অপ্টিমাইজেশন সমাধান হিসেবে কাজ করে।

### প্রধান বৈশিষ্ট্য

- **হার্ডওয়্যার-সচেতন অপ্টিমাইজেশন**: আপনার লক্ষ্য হার্ডওয়্যারের জন্য সেরা অপ্টিমাইজেশন কৌশলগুলি স্বয়ংক্রিয়ভাবে নির্বাচন করে
- **৪০+ বিল্ট-ইন অপ্টিমাইজেশন উপাদান**: মডেল কম্প্রেশন, কোয়ান্টাইজেশন, গ্রাফ অপ্টিমাইজেশন এবং আরও অনেক কিছু অন্তর্ভুক্ত
- **সহজ CLI ইন্টারফেস**: সাধারণ অপ্টিমাইজেশন কাজের জন্য সহজ কমান্ড
- **মাল্টি-ফ্রেমওয়ার্ক সাপোর্ট**: PyTorch, Hugging Face মডেল এবং ONNX এর সাথে কাজ করে
- **জনপ্রিয় মডেল সাপোর্ট**: Olive স্বয়ংক্রিয়ভাবে Llama, Phi, Qwen, Gemma ইত্যাদি জনপ্রিয় মডেল আর্কিটেকচার অপ্টিমাইজ করতে পারে

### সুবিধা

- **উন্নয়ন সময় হ্রাস**: বিভিন্ন অপ্টিমাইজেশন কৌশল নিয়ে ম্যানুয়ালি পরীক্ষা করার প্রয়োজন নেই
- **কর্মক্ষমতা বৃদ্ধি**: উল্লেখযোগ্য গতি উন্নতি (কিছু ক্ষেত্রে ৬x পর্যন্ত)
- **ক্রস-প্ল্যাটফর্ম মোতায়েন**: অপ্টিমাইজড মডেলগুলি বিভিন্ন হার্ডওয়্যার এবং অপারেটিং সিস্টেমে কাজ করে
- **নির্ভুলতা বজায় রাখা**: অপ্টিমাইজেশনগুলি কর্মক্ষমতা উন্নত করার সময় মডেলের গুণমান সংরক্ষণ করে

## ইনস্টলেশন

### প্রয়োজনীয়তা

- Python 3.8 বা তার বেশি
- pip প্যাকেজ ম্যানেজার
- ভার্চুয়াল পরিবেশ (প্রস্তাবিত)

### মৌলিক ইনস্টলেশন

ভার্চুয়াল পরিবেশ তৈরি এবং সক্রিয় করুন:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

অটো-অপ্টিমাইজেশন বৈশিষ্ট্য সহ Olive ইনস্টল করুন:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### ঐচ্ছিক নির্ভরতা

Olive অতিরিক্ত বৈশিষ্ট্যের জন্য বিভিন্ন ঐচ্ছিক নির্ভরতা প্রদান করে:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### ইনস্টলেশন যাচাই করুন

```bash
olive --help
```

যদি সফল হয়, তবে আপনি Olive CLI সাহায্য বার্তা দেখতে পাবেন।

## দ্রুত শুরু করার নির্দেশিকা

### আপনার প্রথম অপ্টিমাইজেশন

Olive এর অটো-অপ্টিমাইজেশন বৈশিষ্ট্য ব্যবহার করে একটি ছোট ভাষার মডেল অপ্টিমাইজ করুন:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### এই কমান্ডটি কী করে

অপ্টিমাইজেশন প্রক্রিয়ায় অন্তর্ভুক্ত রয়েছে: স্থানীয় ক্যাশ থেকে মডেল সংগ্রহ করা, ONNX গ্রাফ ক্যাপচার করা এবং ONNX ডেটা ফাইলে ওজন সংরক্ষণ করা, ONNX গ্রাফ অপ্টিমাইজ করা এবং RTN পদ্ধতি ব্যবহার করে মডেলটি int4 এ কোয়ান্টাইজ করা।

### কমান্ড প্যারামিটার ব্যাখ্যা

- `--model_name_or_path`: Hugging Face মডেল আইডেন্টিফায়ার বা স্থানীয় পথ
- `--output_path`: অপ্টিমাইজড মডেল সংরক্ষণের ডিরেক্টরি
- `--device`: লক্ষ্য ডিভাইস (cpu, gpu)
- `--provider`: এক্সিকিউশন প্রদানকারী (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI ব্যবহার করে ইনফারেন্স
- `--precision`: কোয়ান্টাইজেশন নির্ভুলতা (int4, int8, fp16)
- `--log_level`: লগিং ভার্বোসিটি (0=minimal, 1=verbose)

## উদাহরণ: Qwen3 কে ONNX INT4 এ রূপান্তর করা

Hugging Face উদাহরণ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) এর উপর ভিত্তি করে, এখানে একটি Qwen3 মডেল অপ্টিমাইজ করার পদ্ধতি:

### ধাপ ১: মডেল ডাউনলোড করুন (ঐচ্ছিক)

ডাউনলোড সময় কমানোর জন্য শুধুমাত্র প্রয়োজনীয় ফাইলগুলি ক্যাশ করুন:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ধাপ ২: Qwen3 মডেল অপ্টিমাইজ করুন

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ধাপ ৩: অপ্টিমাইজড মডেল পরীক্ষা করুন

আপনার অপ্টিমাইজড মডেল পরীক্ষা করার জন্য একটি সাধারণ Python স্ক্রিপ্ট তৈরি করুন:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### আউটপুট কাঠামো

অপ্টিমাইজেশনের পরে, আপনার আউটপুট ডিরেক্টরিতে থাকবে:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## উন্নত ব্যবহার

### কনফিগারেশন ফাইল

আরও জটিল অপ্টিমাইজেশন ওয়ার্কফ্লো জন্য, আপনি JSON কনফিগারেশন ফাইল ব্যবহার করতে পারেন:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

কনফিগারেশন সহ চালান:

```bash
olive run --config config.json
```

### GPU অপ্টিমাইজেশন

CUDA GPU অপ্টিমাইজেশনের জন্য:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) এর জন্য:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive দিয়ে ফাইন-টিউনিং

Olive মডেল ফাইন-টিউনিংও সমর্থন করে:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## সেরা অনুশীলন

### ১. মডেল নির্বাচন
- পরীক্ষার জন্য ছোট মডেল দিয়ে শুরু করুন (যেমন, ০.৫B-৭B প্যারামিটার)
- নিশ্চিত করুন যে আপনার লক্ষ্য মডেল আর্কিটেকচার Olive দ্বারা সমর্থিত

### ২. হার্ডওয়্যার বিবেচনা
- আপনার অপ্টিমাইজেশন লক্ষ্য আপনার মোতায়েন হার্ডওয়্যারের সাথে মেলে
- CUDA-সামঞ্জস্যপূর্ণ হার্ডওয়্যার থাকলে GPU অপ্টিমাইজেশন ব্যবহার করুন
- Windows মেশিনে ইন্টিগ্রেটেড গ্রাফিক্সের জন্য DirectML বিবেচনা করুন

### ৩. নির্ভুলতা নির্বাচন
- **INT4**: সর্বাধিক কম্প্রেশন, সামান্য নির্ভুলতা ক্ষতি
- **INT8**: আকার এবং নির্ভুলতার ভাল ভারসাম্য
- **FP16**: ন্যূনতম নির্ভুলতা ক্ষতি, মাঝারি আকার হ্রাস

### ৪. পরীক্ষা এবং যাচাই
- সর্বদা আপনার নির্দিষ্ট ব্যবহারের ক্ষেত্রে অপ্টিমাইজড মডেল পরীক্ষা করুন
- কর্মক্ষমতা মেট্রিক তুলনা করুন (লেটেন্সি, থ্রুপুট, নির্ভুলতা)
- মূল্যায়নের জন্য প্রতিনিধিত্বমূলক ইনপুট ডেটা ব্যবহার করুন

### ৫. পুনরাবৃত্তিমূলক অপ্টিমাইজেশন
- দ্রুত ফলাফলের জন্য অটো-অপ্টিমাইজেশন দিয়ে শুরু করুন
- সূক্ষ্ম নিয়ন্ত্রণের জন্য কনফিগারেশন ফাইল ব্যবহার করুন
- বিভিন্ন অপ্টিমাইজেশন পাস নিয়ে পরীক্ষা করুন

## সমস্যার সমাধান

### সাধারণ সমস্যা

#### ১. ইনস্টলেশন সমস্যা
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### ২. CUDA/GPU সমস্যা
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### ৩. মেমরি সমস্যা
- অপ্টিমাইজেশনের সময় ছোট ব্যাচ সাইজ ব্যবহার করুন
- প্রথমে উচ্চ নির্ভুলতা সহ কোয়ান্টাইজেশন চেষ্টা করুন (int8 এর পরিবর্তে int4)
- মডেল ক্যাশিংয়ের জন্য পর্যাপ্ত ডিস্ক স্পেস নিশ্চিত করুন

#### ৪. মডেল লোডিং ত্রুটি
- মডেল পথ এবং অ্যাক্সেস অনুমতি যাচাই করুন
- যাচাই করুন যে মডেলটি `trust_remote_code=True` প্রয়োজন কিনা
- নিশ্চিত করুন যে সমস্ত প্রয়োজনীয় মডেল ফাইল ডাউনলোড করা হয়েছে

### সাহায্য পাওয়া

- **ডকুমেন্টেশন**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **উদাহরণ**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive রেসিপি সংগ্রহশালা

### Olive রেসিপি পরিচিতি

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) সংগ্রহশালা প্রধান Olive টুলকিটের পরিপূরক হিসেবে কাজ করে এবং জনপ্রিয় AI মডেলের জন্য ব্যবহারযোগ্য অপ্টিমাইজেশন রেসিপির একটি বিস্তৃত সংগ্রহ প্রদান করে। এই সংগ্রহশালা পাবলিকলি উপলব্ধ মডেল অপ্টিমাইজ করার পাশাপাশি মালিকানাধীন মডেলের জন্য অপ্টিমাইজেশন ওয়ার্কফ্লো তৈরির একটি ব্যবহারিক রেফারেন্স হিসেবে কাজ করে।

### প্রধান বৈশিষ্ট্য

- **১০০+ প্রি-বিল্ট রেসিপি**: জনপ্রিয় মডেলের জন্য ব্যবহারযোগ্য অপ্টিমাইজেশন কনফিগারেশন
- **মাল্টি-আর্কিটেকচার সাপোর্ট**: ট্রান্সফর্মার মডেল, ভিশন মডেল এবং মাল্টিমোডাল আর্কিটেকচার অন্তর্ভুক্ত
- **হার্ডওয়্যার-নির্দিষ্ট অপ্টিমাইজেশন**: CPU, GPU এবং বিশেষায়িত অ্যাক্সিলারেটরের জন্য রেসিপি
- **জনপ্রিয় মডেল পরিবার**: Phi, Llama, Qwen, Gemma, Mistral এবং আরও অনেক কিছু অন্তর্ভুক্ত

### সমর্থিত মডেল পরিবার

সঙ্গে অপ্টিমাইজেশন রেসিপি অন্তর্ভুক্ত:

#### ভাষার মডেল
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 সিরিজ (০.৫B থেকে ১৪B)
- **Google Gemma**: বিভিন্ন Gemma মডেল কনফিগারেশন
- **Mistral AI**: Mistral-7B সিরিজ
- **DeepSeek**: R1-Distill সিরিজ মডেল

#### ভিশন এবং মাল্টিমোডাল মডেল
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP মডেল**: বিভিন্ন CLIP-ViT কনফিগারেশন
- **ResNet**: ResNet-50 অপ্টিমাইজেশন
- **ভিশন ট্রান্সফর্মার**: ViT-base-patch16-224

#### বিশেষায়িত মডেল
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: বেস এবং মাল্টিলিঙ্গুয়াল ভ্যারিয়েন্ট
- **Sentence Transformers**: all-MiniLM-L6-v2

### Olive রেসিপি ব্যবহার

#### পদ্ধতি ১: নির্দিষ্ট রেসিপি ক্লোন করুন

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### পদ্ধতি ২: টেমপ্লেট হিসেবে রেসিপি ব্যবহার করুন

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### রেসিপি কাঠামো

প্রতিটি রেসিপি ডিরেক্টরিতে সাধারণত থাকে:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### উদাহরণ: Phi-4-mini রেসিপি ব্যবহার

চলুন Phi-4-mini রেসিপি একটি উদাহরণ হিসেবে ব্যবহার করি:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

কনফিগারেশন ফাইল সাধারণত অন্তর্ভুক্ত করে:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### রেসিপি কাস্টমাইজ করা

#### লক্ষ্য হার্ডওয়্যার পরিবর্তন

লক্ষ্য হার্ডওয়্যার পরিবর্তন করতে, `systems` সেকশন আপডেট করুন:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### অপ্টিমাইজেশন প্যারামিটার সামঞ্জস্য করা

বিভিন্ন অপ্টিমাইজেশন স্তরের জন্য `passes` সেকশন পরিবর্তন করুন:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### আপনার নিজস্ব রেসিপি তৈরি করা

1. **একই ধরনের মডেল দিয়ে শুরু করুন**: অনুরূপ আর্কিটেকচারের একটি মডেলের জন্য একটি রেসিপি খুঁজুন
2. **মডেল কনফিগারেশন আপডেট করুন**: কনফিগারেশনে মডেলের নাম/পথ পরিবর্তন করুন
3. **প্যারামিটার সামঞ্জস্য করুন**: প্রয়োজন অনুযায়ী অপ্টিমাইজেশন প্যারামিটার পরিবর্তন করুন
4. **পরীক্ষা এবং যাচাই করুন**: অপ্টিমাইজেশন চালান এবং ফলাফল যাচাই করুন
5. **ফিরে অবদান রাখুন**: আপনার রেসিপি সংগ্রহশালায় অবদান রাখার কথা বিবেচনা করুন

### রেসিপি ব্যবহারের সুবিধা

#### ১. **প্রমাণিত কনফিগারেশন**
- নির্দিষ্ট মডেলের জন্য পরীক্ষিত অপ্টিমাইজেশন সেটিংস
- সর্বোত্তম প্যারামিটার খুঁজে বের করার জন্য ট্রায়াল-এন্ড-এরর এড়ানো

#### ২. **হার্ডওয়্যার-নির্দিষ্ট টিউনিং**
- বিভিন্ন এক্সিকিউশন প্রদানকারীর জন্য প্রি-অপ্টিমাইজড
- CPU, GPU এবং NPU লক্ষ্যগুলির জন্য প্রস্তুত-ব্যবহারযোগ্য কনফিগারেশন

#### ৩. **বিস্তৃত কভারেজ**
- সবচেয়ে জনপ্রিয় ওপেন-সোর্স মডেলগুলিকে সমর্থন করে
- নতুন মডেল রিলিজের সাথে নিয়মিত আপডেট

#### ৪. **কমিউনিটি অবদান**
- AI কমিউনিটির সাথে সহযোগিতামূলক উন্নয়ন
- ভাগ করা জ্ঞান এবং সেরা অনুশীলন

### Olive রেসিপিতে অবদান রাখা

যদি আপনি একটি মডেল অপ্টিমাইজ করেন যা সংগ্রহশালায় অন্তর্ভুক্ত নয়:

1. **সংগ্রহশালা ফর্ক করুন**: olive-recipes এর একটি নিজস্ব ফর্ক তৈরি করুন
2. **রেসিপি ডিরেক্টরি তৈরি করুন**: আপনার মডেলের জন্য একটি নতুন ডিরেক্টরি যোগ করুন
3. **কনফিগারেশন অন্তর্ভুক্ত করুন**: olive_config.json এবং সহায়ক ফাইল যোগ করুন
4. **ব্যবহারের ডকুমেন্ট করুন**: স্পষ্ট README সহ নির্দেশিকা প্রদান করুন
5. **পুল রিকোয়েস্ট জমা দিন**: কমিউনিটিতে অবদান রাখুন

### কর্মক্ষমতা বেঞ্চমার্ক

অনেক রেসিপি কর্মক্ষমতা বেঞ্চমার্ক অন্তর্ভুক্ত করে যা দেখায়:
- **লেটেন্সি উন্নতি**: বেসলাইন থেকে সাধারণত ২-৬x গতি বৃদ্ধি
- **মেমরি হ্রাস**: কোয়ান্টাইজেশনের মাধ্যমে ৫০-৭৫% মেমরি ব্যবহার হ্রাস
- **নির্ভুলতা সংরক্ষণ**: ৯৫-৯৯% নির্ভুলতা সংরক্ষণ

### AI টুলকিটের সাথে ইন্টিগ্রেশন

রেসিপিগুলি নির্বিঘ্নে কাজ করে:
- **VS Code AI Toolkit**: মডেল অপ্টিমাইজেশনের জন্য সরাসরি ইন্টিগ্রেশন
- **Azure Machine Learning**: ক্লাউড-ভিত্তিক অপ্টিমাইজেশন ওয়ার্কফ্লো
- **ONNX Runtime**: অপ্টিমাইজড ইনফারেন্স মোতায়েন

## অতিরিক্ত সম্পদ

### অফিসিয়াল লিঙ্ক
- **GitHub সংগ্রহশালা**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive

---

**অস্বীকৃতি**:  
এই নথিটি AI অনুবাদ পরিষেবা [Co-op Translator](https://github.com/Azure/co-op-translator) ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসাধ্য সঠিকতার জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য, পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যা হলে আমরা দায়ী থাকব না।