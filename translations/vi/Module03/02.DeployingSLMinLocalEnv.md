<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:46:45+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "vi"
}
-->
# Phần 2: Triển khai Môi trường Cục bộ - Giải pháp Ưu tiên Bảo mật

Triển khai cục bộ các Mô hình Ngôn ngữ Nhỏ (SLMs) đại diện cho một sự thay đổi lớn hướng tới các giải pháp AI bảo mật, tiết kiệm chi phí. Hướng dẫn toàn diện này khám phá hai khung nền tảng mạnh mẽ—Ollama và Microsoft Foundry Local—giúp các nhà phát triển khai thác toàn bộ tiềm năng của SLMs trong khi vẫn duy trì quyền kiểm soát hoàn toàn đối với môi trường triển khai của họ.

## Giới thiệu

Trong bài học này, chúng ta sẽ khám phá các chiến lược triển khai nâng cao cho Mô hình Ngôn ngữ Nhỏ trong môi trường cục bộ. Chúng ta sẽ tìm hiểu các khái niệm cơ bản về triển khai AI cục bộ, xem xét hai nền tảng hàng đầu (Ollama và Microsoft Foundry Local), và cung cấp hướng dẫn thực tiễn để triển khai các giải pháp sẵn sàng cho sản xuất.

## Mục tiêu học tập

Kết thúc bài học này, bạn sẽ có thể:

- Hiểu kiến trúc và lợi ích của các khung triển khai SLM cục bộ.
- Triển khai các giải pháp sẵn sàng cho sản xuất bằng Ollama và Microsoft Foundry Local.
- So sánh và chọn nền tảng phù hợp dựa trên yêu cầu và hạn chế cụ thể.
- Tối ưu hóa triển khai cục bộ về hiệu suất, bảo mật và khả năng mở rộng.

## Hiểu Kiến trúc Triển khai SLM Cục bộ

Triển khai SLM cục bộ đại diện cho một sự thay đổi cơ bản từ các dịch vụ AI phụ thuộc vào đám mây sang các giải pháp bảo mật tại chỗ. Cách tiếp cận này cho phép các tổ chức duy trì quyền kiểm soát hoàn toàn đối với cơ sở hạ tầng AI của họ trong khi đảm bảo chủ quyền dữ liệu và sự độc lập trong vận hành.

### Phân loại Khung Triển khai

Hiểu các cách tiếp cận triển khai khác nhau giúp chọn chiến lược phù hợp cho các trường hợp sử dụng cụ thể:

- **Tập trung vào Phát triển**: Thiết lập đơn giản để thử nghiệm và tạo mẫu.
- **Cấp Doanh nghiệp**: Các giải pháp sẵn sàng cho sản xuất với khả năng tích hợp doanh nghiệp.
- **Đa Nền tảng**: Tương thích trên nhiều hệ điều hành và phần cứng.

### Lợi ích Chính của Triển khai SLM Cục bộ

Triển khai SLM cục bộ mang lại nhiều lợi ích cơ bản khiến nó trở thành lựa chọn lý tưởng cho các ứng dụng doanh nghiệp và nhạy cảm về bảo mật:

**Bảo mật và Riêng tư**: Xử lý cục bộ đảm bảo dữ liệu nhạy cảm không bao giờ rời khỏi cơ sở hạ tầng của tổ chức, cho phép tuân thủ GDPR, HIPAA và các yêu cầu pháp lý khác. Các triển khai cách ly mạng có thể thực hiện được trong các môi trường bảo mật, trong khi các bản ghi kiểm toán đầy đủ duy trì sự giám sát an ninh.

**Hiệu quả Chi phí**: Loại bỏ các mô hình định giá theo token giúp giảm đáng kể chi phí vận hành. Yêu cầu băng thông thấp hơn và giảm sự phụ thuộc vào đám mây cung cấp cấu trúc chi phí dự đoán được cho ngân sách doanh nghiệp.

**Hiệu suất và Độ tin cậy**: Thời gian suy luận nhanh hơn mà không có độ trễ mạng cho phép các ứng dụng thời gian thực. Chức năng ngoại tuyến đảm bảo hoạt động liên tục bất kể kết nối internet, trong khi tối ưu hóa tài nguyên cục bộ cung cấp hiệu suất nhất quán.

## Ollama: Nền tảng Triển khai Cục bộ Toàn cầu

### Kiến trúc và Triết lý Cốt lõi

Ollama được thiết kế như một nền tảng thân thiện với nhà phát triển, phổ biến, giúp dân chủ hóa triển khai LLM cục bộ trên các cấu hình phần cứng và hệ điều hành đa dạng.

**Nền tảng Kỹ thuật**: Được xây dựng trên khung llama.cpp mạnh mẽ, Ollama sử dụng định dạng mô hình GGUF hiệu quả để đạt hiệu suất tối ưu. Tương thích đa nền tảng đảm bảo hành vi nhất quán trên các môi trường Windows, macOS và Linux, trong khi quản lý tài nguyên thông minh tối ưu hóa việc sử dụng CPU, GPU và bộ nhớ.

**Triết lý Thiết kế**: Ollama ưu tiên sự đơn giản mà không làm giảm chức năng, cung cấp triển khai không cần cấu hình để đạt năng suất ngay lập tức. Nền tảng duy trì khả năng tương thích rộng với các mô hình trong khi cung cấp API nhất quán trên các kiến trúc mô hình khác nhau.

### Tính năng và Khả năng Nâng cao

**Quản lý Mô hình Xuất sắc**: Ollama cung cấp quản lý vòng đời mô hình toàn diện với khả năng tự động tải xuống, lưu trữ và phiên bản hóa. Nền tảng hỗ trợ hệ sinh thái mô hình phong phú bao gồm Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral, và các mô hình nhúng chuyên biệt.

**Tùy chỉnh Qua Modelfiles**: Người dùng nâng cao có thể tạo cấu hình mô hình tùy chỉnh với các tham số cụ thể, lời nhắc hệ thống và sửa đổi hành vi. Điều này cho phép tối ưu hóa theo lĩnh vực và đáp ứng các yêu cầu ứng dụng chuyên biệt.

**Tối ưu hóa Hiệu suất**: Ollama tự động phát hiện và sử dụng tăng tốc phần cứng có sẵn bao gồm NVIDIA CUDA, Apple Metal, và OpenCL. Quản lý bộ nhớ thông minh đảm bảo sử dụng tài nguyên tối ưu trên các cấu hình phần cứng khác nhau.

### Chiến lược Triển khai Sản xuất

**Cài đặt và Thiết lập**: Ollama cung cấp cài đặt đơn giản trên các nền tảng thông qua trình cài đặt gốc, trình quản lý gói (WinGet, Homebrew, APT), và các container Docker cho các triển khai container hóa.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Các Lệnh và Hoạt động Cơ bản**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Cấu hình Nâng cao**: Modelfiles cho phép tùy chỉnh tinh vi cho các yêu cầu doanh nghiệp:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Ví dụ Tích hợp cho Nhà phát triển

**Tích hợp API Python**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Tích hợp JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Sử dụng API RESTful với cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Tinh chỉnh & Tối ưu hóa Hiệu suất

**Cấu hình Bộ nhớ & Luồng**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Lựa chọn Lượng tử hóa cho Phần cứng Khác nhau**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Nền tảng AI Cạnh Doanh nghiệp

### Kiến trúc Cấp Doanh nghiệp

Microsoft Foundry Local đại diện cho một giải pháp doanh nghiệp toàn diện được thiết kế đặc biệt cho các triển khai AI cạnh sản xuất với tích hợp sâu vào hệ sinh thái Microsoft.

**Nền tảng Dựa trên ONNX**: Được xây dựng trên ONNX Runtime tiêu chuẩn ngành, Foundry Local cung cấp hiệu suất tối ưu trên các kiến trúc phần cứng đa dạng. Nền tảng tận dụng tích hợp Windows ML để tối ưu hóa Windows gốc trong khi vẫn duy trì khả năng tương thích đa nền tảng.

**Tăng tốc Phần cứng Xuất sắc**: Foundry Local có khả năng phát hiện và tối ưu hóa phần cứng thông minh trên CPU, GPU và NPU. Sự hợp tác sâu sắc với các nhà cung cấp phần cứng (AMD, Intel, NVIDIA, Qualcomm) đảm bảo hiệu suất tối ưu trên các cấu hình phần cứng doanh nghiệp.

### Trải nghiệm Nhà phát triển Nâng cao

**Truy cập Đa Giao diện**: Foundry Local cung cấp các giao diện phát triển toàn diện bao gồm CLI mạnh mẽ để quản lý và triển khai mô hình, SDK đa ngôn ngữ (Python, NodeJS) để tích hợp gốc, và API RESTful với khả năng tương thích OpenAI để di chuyển liền mạch.

**Tích hợp Visual Studio**: Nền tảng tích hợp liền mạch với AI Toolkit cho VS Code, cung cấp các công cụ chuyển đổi mô hình, lượng tử hóa và tối ưu hóa trong môi trường phát triển. Sự tích hợp này tăng tốc quy trình phát triển và giảm độ phức tạp triển khai.

**Pipeline Tối ưu hóa Mô hình**: Tích hợp Microsoft Olive cho phép các quy trình tối ưu hóa mô hình tinh vi bao gồm lượng tử hóa động, tối ưu hóa đồ thị, và điều chỉnh phần cứng cụ thể. Khả năng chuyển đổi dựa trên đám mây thông qua Azure ML cung cấp tối ưu hóa quy mô lớn cho các mô hình lớn.

### Chiến lược Triển khai Sản xuất

**Cài đặt và Cấu hình**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Hoạt động Quản lý Mô hình**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Cấu hình Triển khai Nâng cao**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Tích hợp Hệ sinh thái Doanh nghiệp

**Bảo mật và Tuân thủ**: Foundry Local cung cấp các tính năng bảo mật cấp doanh nghiệp bao gồm kiểm soát truy cập dựa trên vai trò, ghi nhật ký kiểm toán, báo cáo tuân thủ, và lưu trữ mô hình được mã hóa. Tích hợp với cơ sở hạ tầng bảo mật Microsoft đảm bảo tuân thủ các chính sách bảo mật doanh nghiệp.

**Dịch vụ AI Tích hợp**: Nền tảng cung cấp các khả năng AI sẵn sàng sử dụng bao gồm Phi Silica cho xử lý ngôn ngữ cục bộ, AI Imaging cho tăng cường và phân tích hình ảnh, và các API chuyên biệt cho các nhiệm vụ AI doanh nghiệp phổ biến.

## Phân tích So sánh: Ollama vs Foundry Local

### So sánh Kiến trúc Kỹ thuật

| **Khía cạnh** | **Ollama** | **Foundry Local** |
|---------------|------------|-------------------|
| **Định dạng Mô hình** | GGUF (qua llama.cpp) | ONNX (qua ONNX Runtime) |
| **Tập trung Nền tảng** | Tương thích đa nền tảng | Tối ưu hóa Windows/Doanh nghiệp |
| **Tích hợp Phần cứng** | Hỗ trợ GPU/CPU chung | Tích hợp sâu Windows ML, hỗ trợ NPU |
| **Tối ưu hóa** | Lượng tử hóa llama.cpp | Microsoft Olive + ONNX Runtime |
| **Tính năng Doanh nghiệp** | Dựa vào cộng đồng | Cấp doanh nghiệp với SLAs |

### Đặc điểm Hiệu suất

**Thế mạnh Hiệu suất của Ollama**:
- Hiệu suất CPU xuất sắc qua tối ưu hóa llama.cpp.
- Hành vi nhất quán trên các nền tảng và phần cứng khác nhau.
- Sử dụng bộ nhớ hiệu quả với tải mô hình thông minh.
- Thời gian khởi động nhanh cho các kịch bản phát triển và thử nghiệm.

**Ưu điểm Hiệu suất của Foundry Local**:
- Sử dụng NPU vượt trội trên phần cứng Windows hiện đại.
- Tăng tốc GPU tối ưu qua hợp tác với các nhà cung cấp.
- Giám sát và tối ưu hóa hiệu suất cấp doanh nghiệp.
- Khả năng triển khai quy mô lớn cho môi trường sản xuất.

### Phân tích Trải nghiệm Nhà phát triển

**Trải nghiệm Nhà phát triển của Ollama**:
- Yêu cầu thiết lập tối thiểu với năng suất ngay lập tức.
- Giao diện dòng lệnh trực quan cho mọi hoạt động.
- Hỗ trợ cộng đồng rộng rãi và tài liệu phong phú.
- Tùy chỉnh linh hoạt qua Modelfiles.

**Trải nghiệm Nhà phát triển của Foundry Local**:
- Tích hợp IDE toàn diện với hệ sinh thái Visual Studio.
- Quy trình phát triển doanh nghiệp với các tính năng hợp tác nhóm.
- Kênh hỗ trợ chuyên nghiệp với sự hỗ trợ từ Microsoft.
- Công cụ gỡ lỗi và tối ưu hóa nâng cao.

### Tối ưu hóa Trường hợp Sử dụng

**Chọn Ollama Khi**:
- Phát triển ứng dụng đa nền tảng yêu cầu hành vi nhất quán.
- Ưu tiên tính minh bạch mã nguồn mở và đóng góp cộng đồng.
- Làm việc với nguồn lực hoặc ngân sách hạn chế.
- Xây dựng các ứng dụng thử nghiệm hoặc tập trung vào nghiên cứu.
- Yêu cầu khả năng tương thích rộng với các kiến trúc mô hình khác nhau.

**Chọn Foundry Local Khi**:
- Triển khai ứng dụng doanh nghiệp với yêu cầu hiệu suất nghiêm ngặt.
- Tận dụng tối ưu hóa phần cứng cụ thể của Windows (NPU, Windows ML).
- Yêu cầu hỗ trợ doanh nghiệp, SLAs, và các tính năng tuân thủ.
- Xây dựng ứng dụng sản xuất với tích hợp hệ sinh thái Microsoft.
- Cần các công cụ tối ưu hóa nâng cao và quy trình phát triển chuyên nghiệp.

## Chiến lược Triển khai Nâng cao

### Mẫu Triển khai Container hóa

**Container hóa Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Triển khai Doanh nghiệp Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Kỹ thuật Tối ưu hóa Hiệu suất

**Chiến lược Tối ưu hóa Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Tối ưu hóa Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Cân nhắc về Bảo mật và Tuân thủ

### Triển khai Bảo mật Doanh nghiệp

**Thực hành Bảo mật Tốt nhất của Ollama**:
- Cách ly mạng với quy tắc tường lửa và truy cập VPN.
- Xác thực qua tích hợp proxy ngược.
- Xác minh tính toàn vẹn mô hình và phân phối mô hình an toàn.
- Ghi nhật ký kiểm toán cho truy cập API và hoạt động mô hình.

**Bảo mật Doanh nghiệp Foundry Local**:
- Kiểm soát truy cập dựa trên vai trò tích hợp Active Directory.
- Bản ghi kiểm toán toàn diện với báo cáo tuân thủ.
- Lưu trữ mô hình được mã hóa và triển khai mô hình an toàn.
- Tích hợp với cơ sở hạ tầng bảo mật Microsoft.

### Yêu cầu Tuân thủ và Quy định

Cả hai nền tảng đều hỗ trợ tuân thủ pháp lý thông qua:
- Kiểm soát nơi lưu trữ dữ liệu đảm bảo xử lý cục bộ.
- Ghi nhật ký kiểm toán cho các yêu cầu báo cáo pháp lý.
- Kiểm soát truy cập để xử lý dữ liệu nhạy cảm.
- Mã hóa khi lưu trữ và truyền tải để bảo vệ dữ liệu.

## Thực hành Tốt nhất cho Triển khai Sản xuất

### Giám sát và Khả năng Quan sát

**Các Chỉ số Chính cần Giám sát**:
- Độ trễ và thông lượng suy luận mô hình.
- Sử dụng tài nguyên (CPU, GPU, bộ nhớ).
- Thời gian phản hồi API và tỷ lệ lỗi.
- Độ chính xác mô hình và sự lệch hiệu suất.

**Triển khai Giám sát**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Tích hợp Triển khai và Phát triển Liên tục

**Tích hợp Pipeline CI/CD**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Xu hướng và Cân nhắc Tương lai

### Công nghệ Mới Nổi

Cảnh quan triển khai SLM cục bộ tiếp tục phát triển với một số xu hướng chính:

**Kiến trúc Mô hình Nâng cao**: Các SLM thế hệ tiếp theo với tỷ lệ hiệu quả và khả năng cải thiện đang xuất hiện, bao gồm các mô hình chuyên gia hỗn hợp cho khả năng mở rộng động và các kiến trúc chuyên biệt cho triển khai cạnh.

**Tích hợp Phần cứng**: Tích hợp sâu hơn với phần cứng AI chuyên biệt bao gồm NPU, silicon tùy chỉnh, và bộ tăng tốc tính toán cạnh sẽ cung cấp khả năng hiệu suất nâng cao.

**Sự Tiến hóa Hệ sinh thái**: Các nỗ lực tiêu chuẩn hóa trên các nền tảng triển khai và khả năng tương tác được cải thiện giữa các khung sẽ đơn giản hóa triển khai đa nền tảng.

### Mô hình Áp dụng Ngành

**Áp dụng Doanh nghiệp**: Tăng cường áp dụng doanh nghiệp được thúc đẩy bởi yêu cầu bảo mật, tối ưu hóa chi phí, và nhu cầu tuân thủ pháp lý. Các lĩnh vực chính phủ và quốc phòng đặc biệt tập trung vào triển khai cách ly mạng.

**Cân nhắc Toàn cầu**: Các yêu cầu chủ quyền dữ liệu quốc tế đang thúc đẩy việc áp dụng triển khai cục bộ, đặc biệt ở các khu vực có quy định bảo vệ dữ liệu nghiêm ngặt.

## Thách thức và Cân nhắc

### Thách thức Kỹ thuật

**Yêu cầu Cơ sở hạ tầng**: Triển khai cục bộ đòi hỏi lập kế hoạch năng lực cẩn thận và lựa chọn phần cứng. Các tổ chức phải cân bằng giữa yêu cầu hiệu suất và hạn chế chi phí trong khi đảm bảo khả năng mở rộng cho khối lượng công việc ngày càng tăng.

**🔧 Bảo trì và Cập nhật**: Cập nhật mô hình thường xuyên, các bản vá bảo mật, và tối ưu hóa hiệu suất đòi hỏi nguồn lực và chuyên môn chuyên dụng. Các pipeline triển khai tự động trở nên cần thiết cho các môi trường sản xuất.

### Cân nhắc Bảo mật

**Bảo mật Mô hình**: Bảo vệ các mô hình độc quyền khỏi truy cập hoặc trích xuất trái phép đòi hỏi các biện pháp bảo mật toàn diện bao gồm mã hóa, kiểm soát truy cập, và ghi nhật ký kiểm toán.

**Bảo vệ Dữ liệu**: Đảm bảo xử lý dữ liệu an toàn trong toàn bộ pipeline suy luận trong khi duy trì các tiêu chuẩn hiệu suất và khả năng sử dụng.

## Danh sách Kiểm tra Triển khai Thực tiễn

### ✅ Đánh giá Trước Triển khai

- [ ] Phân tích yêu cầu phần cứng và lập kế hoạch năng lực.
- [ ] X

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với thông tin quan trọng, chúng tôi khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.