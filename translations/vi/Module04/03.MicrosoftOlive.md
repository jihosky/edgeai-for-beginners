<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:38:01+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "vi"
}
-->
# Mục 3: Bộ công cụ tối ưu hóa Microsoft Olive

## Mục lục
1. [Giới thiệu](../../../Module04)
2. [Microsoft Olive là gì?](../../../Module04)
3. [Cài đặt](../../../Module04)
4. [Hướng dẫn nhanh](../../../Module04)
5. [Ví dụ: Chuyển đổi Qwen3 sang ONNX INT4](../../../Module04)
6. [Sử dụng nâng cao](../../../Module04)
7. [Kho công thức Olive](../../../Module04)
8. [Thực hành tốt nhất](../../../Module04)
9. [Khắc phục sự cố](../../../Module04)
10. [Tài nguyên bổ sung](../../../Module04)

## Giới thiệu

Microsoft Olive là một bộ công cụ tối ưu hóa mô hình mạnh mẽ, dễ sử dụng, nhận biết phần cứng, giúp đơn giản hóa quy trình tối ưu hóa các mô hình học máy để triển khai trên các nền tảng phần cứng khác nhau. Dù bạn đang nhắm đến CPU, GPU hay các bộ tăng tốc AI chuyên dụng, Olive giúp bạn đạt được hiệu suất tối ưu mà vẫn duy trì độ chính xác của mô hình.

## Microsoft Olive là gì?

Olive là một công cụ tối ưu hóa mô hình nhận biết phần cứng, dễ sử dụng, kết hợp các kỹ thuật hàng đầu trong ngành về nén mô hình, tối ưu hóa và biên dịch. Nó hoạt động với ONNX Runtime như một giải pháp tối ưu hóa suy luận từ đầu đến cuối.

### Các tính năng chính

- **Tối ưu hóa nhận biết phần cứng**: Tự động chọn các kỹ thuật tối ưu hóa tốt nhất cho phần cứng mục tiêu của bạn
- **Hơn 40 thành phần tối ưu hóa tích hợp**: Bao gồm nén mô hình, lượng hóa, tối ưu hóa đồ thị và nhiều hơn nữa
- **Giao diện CLI dễ sử dụng**: Các lệnh đơn giản cho các tác vụ tối ưu hóa phổ biến
- **Hỗ trợ đa khung**: Hoạt động với PyTorch, các mô hình Hugging Face và ONNX
- **Hỗ trợ mô hình phổ biến**: Olive có thể tự động tối ưu hóa các kiến trúc mô hình phổ biến như Llama, Phi, Qwen, Gemma, v.v. ngay từ đầu

### Lợi ích

- **Giảm thời gian phát triển**: Không cần thử nghiệm thủ công với các kỹ thuật tối ưu hóa khác nhau
- **Cải thiện hiệu suất**: Tăng tốc đáng kể (lên đến 6 lần trong một số trường hợp)
- **Triển khai đa nền tảng**: Các mô hình được tối ưu hóa hoạt động trên các phần cứng và hệ điều hành khác nhau
- **Duy trì độ chính xác**: Các tối ưu hóa bảo toàn chất lượng mô hình trong khi cải thiện hiệu suất

## Cài đặt

### Yêu cầu trước

- Python 3.8 trở lên
- Trình quản lý gói pip
- Môi trường ảo (khuyến nghị)

### Cài đặt cơ bản

Tạo và kích hoạt môi trường ảo:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Cài đặt Olive với các tính năng tự động tối ưu hóa:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Các phụ thuộc tùy chọn

Olive cung cấp nhiều phụ thuộc tùy chọn cho các tính năng bổ sung:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Xác minh cài đặt

```bash
olive --help
```

Nếu thành công, bạn sẽ thấy thông báo trợ giúp CLI của Olive.

## Hướng dẫn nhanh

### Tối ưu hóa đầu tiên của bạn

Hãy tối ưu hóa một mô hình ngôn ngữ nhỏ bằng tính năng tự động tối ưu hóa của Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Lệnh này làm gì

Quy trình tối ưu hóa bao gồm: lấy mô hình từ bộ nhớ cache cục bộ, chụp đồ thị ONNX và lưu trọng số trong tệp dữ liệu ONNX, tối ưu hóa đồ thị ONNX và lượng hóa mô hình sang int4 bằng phương pháp RTN.

### Giải thích các tham số lệnh

- `--model_name_or_path`: Định danh mô hình Hugging Face hoặc đường dẫn cục bộ
- `--output_path`: Thư mục nơi mô hình được tối ưu hóa sẽ được lưu
- `--device`: Thiết bị mục tiêu (cpu, gpu)
- `--provider`: Nhà cung cấp thực thi (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Sử dụng ONNX Runtime Generate AI để suy luận
- `--precision`: Độ chính xác lượng hóa (int4, int8, fp16)
- `--log_level`: Mức độ chi tiết nhật ký (0=tối thiểu, 1=chi tiết)

## Ví dụ: Chuyển đổi Qwen3 sang ONNX INT4

Dựa trên ví dụ Hugging Face được cung cấp tại [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), đây là cách tối ưu hóa mô hình Qwen3:

### Bước 1: Tải xuống mô hình (Tùy chọn)

Để giảm thời gian tải xuống, chỉ lưu vào bộ nhớ cache các tệp cần thiết:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Bước 2: Tối ưu hóa mô hình Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Bước 3: Kiểm tra mô hình đã tối ưu hóa

Tạo một tập lệnh Python đơn giản để kiểm tra mô hình đã tối ưu hóa của bạn:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Cấu trúc đầu ra

Sau khi tối ưu hóa, thư mục đầu ra của bạn sẽ chứa:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Sử dụng nâng cao

### Tệp cấu hình

Đối với các quy trình tối ưu hóa phức tạp hơn, bạn có thể sử dụng tệp cấu hình JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Chạy với cấu hình:

```bash
olive run --config config.json
```

### Tối ưu hóa GPU

Đối với tối ưu hóa GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Đối với DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Tinh chỉnh với Olive

Olive cũng hỗ trợ tinh chỉnh mô hình:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Thực hành tốt nhất

### 1. Lựa chọn mô hình
- Bắt đầu với các mô hình nhỏ để thử nghiệm (ví dụ: 0.5B-7B tham số)
- Đảm bảo kiến trúc mô hình mục tiêu của bạn được Olive hỗ trợ

### 2. Cân nhắc phần cứng
- Khớp mục tiêu tối ưu hóa của bạn với phần cứng triển khai
- Sử dụng tối ưu hóa GPU nếu bạn có phần cứng tương thích CUDA
- Cân nhắc DirectML cho máy Windows với đồ họa tích hợp

### 3. Lựa chọn độ chính xác
- **INT4**: Nén tối đa, mất độ chính xác nhẹ
- **INT8**: Cân bằng tốt giữa kích thước và độ chính xác
- **FP16**: Mất độ chính xác tối thiểu, giảm kích thước vừa phải

### 4. Kiểm tra và xác nhận
- Luôn kiểm tra các mô hình đã tối ưu hóa với các trường hợp sử dụng cụ thể của bạn
- So sánh các chỉ số hiệu suất (độ trễ, thông lượng, độ chính xác)
- Sử dụng dữ liệu đầu vào đại diện để đánh giá

### 5. Tối ưu hóa lặp lại
- Bắt đầu với tự động tối ưu hóa để có kết quả nhanh
- Sử dụng tệp cấu hình để kiểm soát chi tiết
- Thử nghiệm với các lần tối ưu hóa khác nhau

## Khắc phục sự cố

### Các vấn đề phổ biến

#### 1. Vấn đề cài đặt
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Vấn đề CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Vấn đề bộ nhớ
- Sử dụng kích thước lô nhỏ hơn trong quá trình tối ưu hóa
- Thử lượng hóa với độ chính xác cao hơn trước (int8 thay vì int4)
- Đảm bảo đủ dung lượng đĩa để lưu vào bộ nhớ cache mô hình

#### 4. Lỗi tải mô hình
- Xác minh đường dẫn mô hình và quyền truy cập
- Kiểm tra xem mô hình có yêu cầu `trust_remote_code=True` không
- Đảm bảo tất cả các tệp mô hình cần thiết đã được tải xuống

### Nhận trợ giúp

- **Tài liệu**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Vấn đề GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Ví dụ**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Kho công thức Olive

### Giới thiệu về công thức Olive

Kho [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) bổ sung cho bộ công cụ Olive chính bằng cách cung cấp một bộ sưu tập toàn diện các công thức tối ưu hóa sẵn sàng sử dụng cho các mô hình AI phổ biến. Kho này đóng vai trò như một tài liệu tham khảo thực tế cho cả việc tối ưu hóa các mô hình công khai và tạo quy trình tối ưu hóa cho các mô hình độc quyền.

### Các tính năng chính

- **Hơn 100 công thức dựng sẵn**: Cấu hình tối ưu hóa sẵn sàng sử dụng cho các mô hình phổ biến
- **Hỗ trợ đa kiến trúc**: Bao gồm các mô hình transformer, mô hình thị giác và kiến trúc đa phương thức
- **Tối ưu hóa theo phần cứng**: Các công thức được thiết kế riêng cho CPU, GPU và các bộ tăng tốc chuyên dụng
- **Các họ mô hình phổ biến**: Bao gồm Phi, Llama, Qwen, Gemma, Mistral và nhiều hơn nữa

### Các họ mô hình được hỗ trợ

Kho bao gồm các công thức tối ưu hóa cho:

#### Mô hình ngôn ngữ
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B đến 14B)
- **Google Gemma**: Các cấu hình mô hình Gemma khác nhau
- **Mistral AI**: Dòng Mistral-7B
- **DeepSeek**: Các mô hình dòng R1-Distill

#### Mô hình thị giác và đa phương thức
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Mô hình CLIP**: Các cấu hình CLIP-ViT khác nhau
- **ResNet**: Tối ưu hóa ResNet-50
- **Transformer thị giác**: ViT-base-patch16-224

#### Mô hình chuyên biệt
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Các biến thể cơ bản và đa ngôn ngữ
- **Transformer câu**: all-MiniLM-L6-v2

### Sử dụng công thức Olive

#### Phương pháp 1: Sao chép công thức cụ thể

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Phương pháp 2: Sử dụng công thức làm mẫu

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Cấu trúc công thức

Mỗi thư mục công thức thường chứa:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Ví dụ: Sử dụng công thức Phi-4-mini

Hãy sử dụng công thức Phi-4-mini làm ví dụ:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Tệp cấu hình thường bao gồm:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Tùy chỉnh công thức

#### Thay đổi phần cứng mục tiêu

Để thay đổi phần cứng mục tiêu, cập nhật phần `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Điều chỉnh tham số tối ưu hóa

Sửa đổi phần `passes` để có các mức tối ưu hóa khác nhau:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Tạo công thức của riêng bạn

1. **Bắt đầu với mô hình tương tự**: Tìm một công thức cho mô hình có kiến trúc tương tự
2. **Cập nhật cấu hình mô hình**: Thay đổi tên/đường dẫn mô hình trong cấu hình
3. **Điều chỉnh tham số**: Sửa đổi các tham số tối ưu hóa theo nhu cầu
4. **Kiểm tra và xác nhận**: Chạy tối ưu hóa và xác nhận kết quả
5. **Đóng góp lại**: Cân nhắc đóng góp công thức của bạn vào kho

### Lợi ích của việc sử dụng công thức

#### 1. **Cấu hình đã được kiểm chứng**
- Cài đặt tối ưu hóa đã được thử nghiệm cho các mô hình cụ thể
- Tránh thử nghiệm và sai sót trong việc tìm các tham số tối ưu

#### 2. **Tối ưu hóa theo phần cứng**
- Được tối ưu hóa trước cho các nhà cung cấp thực thi khác nhau
- Cấu hình sẵn sàng sử dụng cho các mục tiêu CPU, GPU và NPU

#### 3. **Phạm vi toàn diện**
- Hỗ trợ các mô hình mã nguồn mở phổ biến nhất
- Cập nhật thường xuyên với các phiên bản mô hình mới

#### 4. **Đóng góp cộng đồng**
- Phát triển hợp tác với cộng đồng AI
- Chia sẻ kiến thức và thực hành tốt nhất

### Đóng góp cho công thức Olive

Nếu bạn đã tối ưu hóa một mô hình chưa được bao gồm trong kho:

1. **Fork kho**: Tạo fork của riêng bạn từ olive-recipes
2. **Tạo thư mục công thức**: Thêm một thư mục mới cho mô hình của bạn
3. **Bao gồm cấu hình**: Thêm olive_config.json và các tệp hỗ trợ
4. **Tài liệu sử dụng**: Cung cấp README rõ ràng với hướng dẫn
5. **Gửi Pull Request**: Đóng góp lại cho cộng đồng

### Các tiêu chuẩn hiệu suất

Nhiều công thức bao gồm các tiêu chuẩn hiệu suất cho thấy:
- **Cải thiện độ trễ**: Tăng tốc điển hình từ 2-6 lần so với cơ sở
- **Giảm bộ nhớ**: Giảm sử dụng bộ nhớ từ 50-75% với lượng hóa
- **Bảo toàn độ chính xác**: Bảo toàn độ chính xác từ 95-99%

### Tích hợp với bộ công cụ AI

Các công thức hoạt động liền mạch với:
- **Bộ công cụ AI VS Code**: Tích hợp trực tiếp để tối ưu hóa mô hình
- **Azure Machine Learning**: Quy trình tối ưu hóa dựa trên đám mây
- **ONNX Runtime**: Triển khai suy luận được tối ưu hóa

## Tài nguyên bổ sung

### Liên kết chính thức
- **Kho GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Kho công thức Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Tài liệu ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Ví dụ Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Ví dụ cộng đồng
- **Jupyter Notebooks**: Có sẵn trong kho GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Tiện ích mở rộng VS Code**: Tổng quan về bộ công cụ AI cho VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Bài viết blog**: Blog mã nguồn mở của Microsoft — https://opensource.microsoft.com/blog/

### Công cụ liên quan
- **ONNX Runtime**: Công cụ suy luận hiệu suất cao — https://onnxruntime.ai/
- **Hugging Face Transformers**: Nguồn của nhiều mô hình tương thích — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Quy trình tối ưu hóa dựa trên đám mây — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Tiếp theo

- [04: Bộ công cụ tối ưu hóa OpenVINO Toolkit](./04.openvino.md)

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với thông tin quan trọng, khuyến nghị sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.