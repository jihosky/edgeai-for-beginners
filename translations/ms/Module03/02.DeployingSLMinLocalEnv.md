<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:48:54+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "ms"
}
-->
# Seksyen 2: Penerapan Persekitaran Tempatan - Penyelesaian Berorientasi Privasi

Penerapan model bahasa kecil (SLM) secara tempatan mewakili perubahan paradigma ke arah penyelesaian AI yang menjaga privasi dan kos efektif. Panduan komprehensif ini meneroka dua rangka kerja yang berkuasa—Ollama dan Microsoft Foundry Local—yang membolehkan pembangun memanfaatkan potensi penuh SLM sambil mengekalkan kawalan sepenuhnya terhadap persekitaran penerapan mereka.

## Pengenalan

Dalam pelajaran ini, kita akan meneroka strategi penerapan lanjutan untuk model bahasa kecil dalam persekitaran tempatan. Kita akan membincangkan konsep asas penerapan AI secara tempatan, mengkaji dua platform utama (Ollama dan Microsoft Foundry Local), dan memberikan panduan pelaksanaan praktikal untuk penyelesaian yang sedia untuk pengeluaran.

## Objektif Pembelajaran

Pada akhir pelajaran ini, anda akan dapat:

- Memahami seni bina dan manfaat rangka kerja penerapan SLM secara tempatan.
- Melaksanakan penerapan yang sedia untuk pengeluaran menggunakan Ollama dan Microsoft Foundry Local.
- Membandingkan dan memilih platform yang sesuai berdasarkan keperluan dan kekangan tertentu.
- Mengoptimumkan penerapan tempatan untuk prestasi, keselamatan, dan skalabiliti.

## Memahami Seni Bina Penerapan SLM Tempatan

Penerapan SLM secara tempatan mewakili perubahan asas daripada perkhidmatan AI bergantung kepada awan kepada penyelesaian yang menjaga privasi di premis. Pendekatan ini membolehkan organisasi mengekalkan kawalan sepenuhnya terhadap infrastruktur AI mereka sambil memastikan kedaulatan data dan kebebasan operasi.

### Klasifikasi Rangka Kerja Penerapan

Memahami pendekatan penerapan yang berbeza membantu dalam memilih strategi yang tepat untuk kes penggunaan tertentu:

- **Berfokuskan Pembangunan**: Persediaan yang dipermudahkan untuk eksperimen dan prototaip
- **Berskala Perusahaan**: Penyelesaian yang sedia untuk pengeluaran dengan keupayaan integrasi perusahaan  
- **Merentas Platform**: Keserasian universal merentas sistem operasi dan perkakasan yang berbeza

### Kelebihan Utama Penerapan SLM Tempatan

Penerapan SLM secara tempatan menawarkan beberapa kelebihan asas yang menjadikannya ideal untuk aplikasi perusahaan dan sensitif privasi:

**Privasi dan Keselamatan**: Pemprosesan tempatan memastikan data sensitif tidak meninggalkan infrastruktur organisasi, membolehkan pematuhan kepada GDPR, HIPAA, dan keperluan peraturan lain. Penerapan yang terasing udara adalah mungkin untuk persekitaran yang diklasifikasikan, sementara jejak audit lengkap mengekalkan pengawasan keselamatan.

**Kos Efektif**: Penghapusan model harga per-token mengurangkan kos operasi dengan ketara. Keperluan jalur lebar yang lebih rendah dan pengurangan pergantungan kepada awan menyediakan struktur kos yang boleh diramal untuk perancangan bajet perusahaan.

**Prestasi dan Kebolehpercayaan**: Masa inferens yang lebih pantas tanpa latensi rangkaian membolehkan aplikasi masa nyata. Fungsi luar talian memastikan operasi berterusan tanpa mengira sambungan internet, sementara pengoptimuman sumber tempatan menyediakan prestasi yang konsisten.

## Ollama: Platform Penerapan Tempatan Universal

### Seni Bina Teras dan Falsafah

Ollama direka sebagai platform yang universal dan mesra pembangun yang mendemokrasikan penerapan LLM tempatan merentas konfigurasi perkakasan dan sistem operasi yang pelbagai.

**Asas Teknikal**: Dibina di atas rangka kerja llama.cpp yang kukuh, Ollama menggunakan format model GGUF yang cekap untuk prestasi optimum. Keserasian merentas platform memastikan tingkah laku yang konsisten merentas persekitaran Windows, macOS, dan Linux, sementara pengurusan sumber pintar mengoptimumkan penggunaan CPU, GPU, dan memori.

**Falsafah Reka Bentuk**: Ollama mengutamakan kesederhanaan tanpa mengorbankan fungsi, menawarkan penerapan tanpa konfigurasi untuk produktiviti segera. Platform ini mengekalkan keserasian model yang luas sambil menyediakan API yang konsisten merentas seni bina model yang berbeza.

### Ciri dan Keupayaan Lanjutan

**Pengurusan Model yang Cemerlang**: Ollama menyediakan pengurusan kitaran hayat model yang komprehensif dengan penarikan automatik, caching, dan versi. Platform ini menyokong ekosistem model yang luas termasuk Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral, dan model embedding khusus.

**Penyesuaian Melalui Modelfiles**: Pengguna lanjutan boleh mencipta konfigurasi model tersuai dengan parameter tertentu, arahan sistem, dan pengubahsuaian tingkah laku. Ini membolehkan pengoptimuman khusus domain dan keperluan aplikasi khusus.

**Pengoptimuman Prestasi**: Ollama secara automatik mengesan dan menggunakan pecutan perkakasan yang tersedia termasuk NVIDIA CUDA, Apple Metal, dan OpenCL. Pengurusan memori pintar memastikan penggunaan sumber yang optimum merentas konfigurasi perkakasan yang berbeza.

### Strategi Pelaksanaan Pengeluaran

**Pemasangan dan Persediaan**: Ollama menyediakan pemasangan yang dipermudahkan merentas platform melalui pemasang asli, pengurus pakej (WinGet, Homebrew, APT), dan kontena Docker untuk penerapan yang dikontena.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Perintah dan Operasi Penting**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Konfigurasi Lanjutan**: Modelfiles membolehkan penyesuaian yang canggih untuk keperluan perusahaan:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Contoh Integrasi Pembangun

**Integrasi API Python**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Integrasi JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Penggunaan API RESTful dengan cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Penalaan Prestasi & Pengoptimuman

**Konfigurasi Memori & Benang**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Pemilihan Kuantisasi untuk Perkakasan Berbeza**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Platform AI Tepi Perusahaan

### Seni Bina Berskala Perusahaan

Microsoft Foundry Local mewakili penyelesaian perusahaan yang komprehensif yang direka khusus untuk penerapan AI tepi pengeluaran dengan integrasi mendalam ke dalam ekosistem Microsoft.

**Asas Berasaskan ONNX**: Dibina di atas ONNX Runtime yang menjadi standard industri, Foundry Local menyediakan prestasi yang dioptimumkan merentas seni bina perkakasan yang pelbagai. Platform ini memanfaatkan integrasi Windows ML untuk pengoptimuman Windows asli sambil mengekalkan keserasian merentas platform.

**Kecemerlangan Pecutan Perkakasan**: Foundry Local menampilkan pengesanan perkakasan pintar dan pengoptimuman merentas CPU, GPU, dan NPU. Kerjasama mendalam dengan vendor perkakasan (AMD, Intel, NVIDIA, Qualcomm) memastikan prestasi optimum pada konfigurasi perkakasan perusahaan.

### Pengalaman Pembangun Lanjutan

**Akses Pelbagai Antara Muka**: Foundry Local menyediakan antara muka pembangunan yang komprehensif termasuk CLI yang berkuasa untuk pengurusan dan penerapan model, SDK pelbagai bahasa (Python, NodeJS) untuk integrasi asli, dan API RESTful dengan keserasian OpenAI untuk migrasi yang lancar.

**Integrasi Visual Studio**: Platform ini berintegrasi dengan AI Toolkit untuk VS Code, menyediakan alat penukaran model, kuantisasi, dan pengoptimuman dalam persekitaran pembangunan. Integrasi ini mempercepatkan aliran kerja pembangunan dan mengurangkan kerumitan penerapan.

**Saluran Pengoptimuman Model**: Integrasi Microsoft Olive membolehkan aliran kerja pengoptimuman model yang canggih termasuk kuantisasi dinamik, pengoptimuman graf, dan penalaan khusus perkakasan. Keupayaan penukaran berasaskan awan melalui Azure ML menyediakan pengoptimuman yang boleh diskala untuk model besar.

### Strategi Pelaksanaan Pengeluaran

**Pemasangan dan Konfigurasi**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Operasi Pengurusan Model**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Konfigurasi Penerapan Lanjutan**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Integrasi Ekosistem Perusahaan

**Keselamatan dan Pematuhan**: Foundry Local menyediakan ciri keselamatan berskala perusahaan termasuk kawalan akses berasaskan peranan, log audit, laporan pematuhan, dan penyimpanan model yang disulitkan. Integrasi dengan infrastruktur keselamatan Microsoft memastikan pematuhan kepada dasar keselamatan perusahaan.

**Perkhidmatan AI Terbina Dalam**: Platform ini menawarkan keupayaan AI yang sedia digunakan termasuk Phi Silica untuk pemprosesan bahasa tempatan, AI Imaging untuk peningkatan dan analisis imej, dan API khusus untuk tugas AI perusahaan biasa.

## Analisis Perbandingan: Ollama vs Foundry Local

### Perbandingan Seni Bina Teknikal

| **Aspek** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Format Model** | GGUF (melalui llama.cpp) | ONNX (melalui ONNX Runtime) |
| **Fokus Platform** | Keserasian merentas platform | Pengoptimuman Windows/Perusahaan |
| **Integrasi Perkakasan** | Sokongan GPU/CPU generik | Sokongan mendalam Windows ML, NPU |
| **Pengoptimuman** | Kuantisasi llama.cpp | Microsoft Olive + ONNX Runtime |
| **Ciri Perusahaan** | Didorong komuniti | Berskala perusahaan dengan SLA |

### Ciri Prestasi

**Kekuatan Prestasi Ollama**:
- Prestasi CPU yang luar biasa melalui pengoptimuman llama.cpp
- Tingkah laku yang konsisten merentas platform dan perkakasan yang berbeza
- Penggunaan memori yang cekap dengan pemuatan model pintar
- Masa permulaan yang pantas untuk senario pembangunan dan ujian

**Kelebihan Prestasi Foundry Local**:
- Penggunaan NPU yang unggul pada perkakasan Windows moden
- Pecutan GPU yang dioptimumkan melalui kerjasama vendor
- Pemantauan dan pengoptimuman prestasi berskala perusahaan
- Keupayaan penerapan yang boleh diskala untuk persekitaran pengeluaran

### Analisis Pengalaman Pembangun

**Pengalaman Pembangun Ollama**:
- Keperluan persediaan minimum dengan produktiviti segera
- Antara muka baris perintah yang intuitif untuk semua operasi
- Sokongan komuniti yang luas dan dokumentasi
- Penyesuaian fleksibel melalui Modelfiles

**Pengalaman Pembangun Foundry Local**:
- Integrasi IDE yang komprehensif dengan ekosistem Visual Studio
- Aliran kerja pembangunan perusahaan dengan ciri kolaborasi pasukan
- Saluran sokongan profesional dengan sokongan Microsoft
- Alat debugging dan pengoptimuman yang canggih

### Pengoptimuman Kes Penggunaan

**Pilih Ollama Apabila**:
- Membangunkan aplikasi merentas platform yang memerlukan tingkah laku konsisten
- Mengutamakan ketelusan sumber terbuka dan sumbangan komuniti
- Bekerja dengan sumber atau kekangan bajet yang terhad
- Membina aplikasi eksperimen atau berfokuskan penyelidikan
- Memerlukan keserasian model yang luas merentas seni bina yang berbeza

**Pilih Foundry Local Apabila**:
- Menerapkan aplikasi perusahaan dengan keperluan prestasi yang ketat
- Memanfaatkan pengoptimuman perkakasan khusus Windows (NPU, Windows ML)
- Memerlukan sokongan perusahaan, SLA, dan ciri pematuhan
- Membina aplikasi pengeluaran dengan integrasi ekosistem Microsoft
- Memerlukan alat pengoptimuman lanjutan dan aliran kerja pembangunan profesional

## Strategi Penerapan Lanjutan

### Corak Penerapan Berkontena

**Kontena Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Penerapan Perusahaan Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Teknik Pengoptimuman Prestasi

**Strategi Pengoptimuman Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Pengoptimuman Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Pertimbangan Keselamatan dan Pematuhan

### Pelaksanaan Keselamatan Perusahaan

**Amalan Terbaik Keselamatan Ollama**:
- Pengasingan rangkaian dengan peraturan firewall dan akses VPN
- Pengesahan melalui integrasi proksi terbalik
- Pengesahan integriti model dan pengedaran model yang selamat
- Log audit untuk akses API dan operasi model

**Keselamatan Perusahaan Foundry Local**:
- Kawalan akses berasaskan peranan terbina dalam dengan integrasi Active Directory
- Jejak audit yang komprehensif dengan laporan pematuhan
- Penyimpanan model yang disulitkan dan penerapan model yang selamat
- Integrasi dengan infrastruktur keselamatan Microsoft

### Keperluan Pematuhan dan Peraturan

Kedua-dua platform menyokong pematuhan peraturan melalui:
- Kawalan kediaman data yang memastikan pemprosesan tempatan
- Log audit untuk keperluan laporan peraturan
- Kawalan akses untuk pengendalian data sensitif
- Penyulitan semasa rehat dan dalam transit untuk perlindungan data

## Amalan Terbaik untuk Penerapan Pengeluaran

### Pemantauan dan Pemerhatian

**Metrik Utama untuk Dipantau**:
- Latensi inferens model dan throughput
- Penggunaan sumber (CPU, GPU, memori)
- Masa tindak balas API dan kadar ralat
- Ketepatan model dan penyimpangan prestasi

**Pelaksanaan Pemantauan**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Integrasi Berterusan dan Penerapan

**Integrasi Saluran CI/CD**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Trend dan Pertimbangan Masa Depan

### Teknologi Baru

Landskap penerapan SLM tempatan terus berkembang dengan beberapa trend utama:

**Seni Bina Model Lanjutan**: SLM generasi akan datang dengan nisbah kecekapan dan keupayaan yang lebih baik sedang muncul, termasuk model campuran pakar untuk penskalaan dinamik dan seni bina khusus untuk penerapan tepi.

**Integrasi Perkakasan**: Integrasi yang lebih mendalam dengan perkakasan AI khusus termasuk NPU, silikon khusus, dan pemecut pengkomputeran tepi akan menyediakan keupayaan prestasi yang dipertingkatkan.

**Evolusi Ekosistem**: Usaha standardisasi merentas platform penerapan dan peningkatan interoperabiliti antara rangka kerja yang berbeza akan mempermudahkan penerapan pelbagai platform.

### Corak Penerapan Industri

**Penerapan Perusahaan**: Peningkatan penerapan perusahaan didorong oleh keperluan privasi, pengoptimuman kos, dan keperluan pematuhan peraturan. Sektor kerajaan dan pertahanan terutamanya memberi tumpuan kepada penerapan yang terasing udara.

**Pertimbangan Global**: Keperluan kedaulatan data antarabangsa mendorong penerapan tempatan, terutamanya di rantau dengan peraturan perlindungan data yang ketat.

## Cabaran dan Pertimbangan

### Cabaran Teknikal

**Keperluan Infrastruktur**: Penerapan tempatan memerlukan perancangan kapasiti dan pemilihan perkakasan yang teliti. Organisasi mesti mengimbangi keperluan prestasi dengan kekangan kos sambil memastikan skalabiliti untuk beban kerja yang semakin meningkat.

**🔧 Penyelenggaraan dan Kemas Kini**: Kemas kini model secara berkala, tampalan keselamatan, dan pengoptimuman prestasi memerlukan sumber dan kepakaran khusus. Saluran penerapan automatik menjadi penting untuk persekitaran pengeluaran.

### Pertimbangan Keselamatan

**Keselamatan Model**: Melindungi model proprietari daripada akses atau pengekstrakan tanpa kebenaran memerlukan langkah keselamatan yang komprehensif termasuk penyulitan, kawalan akses, dan log audit.

**Perlindungan Data**: Memastikan pengendalian data yang selamat sepanjang saluran inferens sambil mengekalkan standard prestasi dan kebolehgunaan.

## Senarai Semak Pelaksanaan Praktikal

### ✅ Penilaian Pra-Penerapan

- [ ] Analisis keperluan perkakasan dan perancangan kapasiti
- [ ] Definisi seni bina rangkaian dan keperluan keselamatan
- [ ] Pemilihan model dan penanda aras prestasi
- [ ] Pengesahan keperluan pematuhan dan peraturan

### ✅ Pelaksanaan Penerapan

- [ ] Pemilihan platform berdasarkan analisis keperluan
- [ ] Pemasangan dan konfigurasi platform yang dipilih
- [ ] Pelaksanaan pengoptimuman dan kuantisasi model
- [ ] Penyelesaian integrasi API dan ujian

### ✅ Kesediaan Pengeluaran

- [ ] Konfigurasi sistem pemantauan dan amaran
- [ ] Penubuhan prosedur sandaran dan pemulihan bencana
- [ ] Penyelesaian penalaan dan pengoptimuman prestasi
- [ ] Pembangunan dokumentasi dan bahan latihan

## Kesimpulan

Pilihan antara Ollama dan Microsoft Foundry Local bergantung kepada keperluan organisasi tertentu, kekangan teknikal, dan objektif strategik. Kedua-dua platform menawarkan kelebihan yang menarik untuk penerapan SLM tempatan, dengan Ollama cemerlang dalam keserasian merentas platform dan kemudahan penggunaan, sementara Foundry Local menyediakan pengoptimuman berskala perusahaan dan integrasi ekosistem Microsoft.

Masa depan penerapan AI terletak pada pendekatan hibrid yang menggabungkan manfaat pemprosesan tempatan dengan keupayaan skala awan. Organisasi yang menguasai penerapan SLM tempatan akan berada dalam kedudukan yang baik untuk memanfaatkan teknologi AI sambil mengekalkan kawalan terhadap data dan infrastruktur mereka.

Kejayaan dalam penerapan SLM tempatan memerlukan pertimbangan yang teliti terhadap keperluan teknikal, implikasi keselamatan, dan prosedur operasi. Dengan mengikuti amalan terbaik dan memanfaatkan kek

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat penting, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.