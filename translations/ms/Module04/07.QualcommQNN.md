<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:57:18+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "ms"
}
-->
# Seksyen 7: Qualcomm QNN (Qualcomm Neural Network) Optimization Suite

## Kandungan
1. [Pengenalan](../../../Module04)
2. [Apa itu Qualcomm QNN?](../../../Module04)
3. [Pemasangan](../../../Module04)
4. [Panduan Permulaan Pantas](../../../Module04)
5. [Contoh: Menukar dan Mengoptimumkan Model dengan QNN](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Amalan Terbaik](../../../Module04)
8. [Penyelesaian Masalah](../../../Module04)
9. [Sumber Tambahan](../../../Module04)

## Pengenalan

Qualcomm QNN (Qualcomm Neural Network) adalah rangka kerja inferens AI yang komprehensif, direka untuk memanfaatkan sepenuhnya potensi pemecut perkakasan AI Qualcomm, termasuk Hexagon NPU, Adreno GPU, dan Kryo CPU. Sama ada anda menyasarkan peranti mudah alih, platform pengkomputeran tepi, atau sistem automotif, QNN menyediakan keupayaan inferens yang dioptimumkan yang memanfaatkan unit pemprosesan AI khusus Qualcomm untuk prestasi maksimum dan kecekapan tenaga.

## Apa itu Qualcomm QNN?

Qualcomm QNN adalah rangka kerja inferens AI yang bersatu yang membolehkan pembangun melaksanakan model AI dengan cekap merentasi seni bina pengkomputeran heterogen Qualcomm. Ia menyediakan antara muka pengaturcaraan bersatu untuk mengakses Hexagon NPU (Neural Processing Unit), Adreno GPU, dan Kryo CPU, secara automatik memilih unit pemprosesan yang optimum untuk lapisan model dan operasi yang berbeza.

### Ciri Utama

- **Pengkomputeran Heterogen**: Akses bersatu kepada NPU, GPU, dan CPU dengan pengagihan beban kerja automatik
- **Pengoptimuman Berasaskan Perkakasan**: Pengoptimuman khusus untuk platform Snapdragon Qualcomm
- **Sokongan Kuantisasi**: Teknik kuantisasi INT8, INT16, dan ketepatan campuran yang maju
- **Alat Penukaran Model**: Sokongan langsung untuk model TensorFlow, PyTorch, ONNX, dan Caffe
- **Dioptimumkan untuk AI Tepi**: Direka khusus untuk senario penggunaan mudah alih dan tepi dengan fokus pada kecekapan kuasa

### Kelebihan

- **Prestasi Maksimum**: Memanfaatkan perkakasan AI khusus untuk peningkatan prestasi sehingga 15x
- **Kecekapan Kuasa**: Dioptimumkan untuk peranti mudah alih dan berkuasa bateri dengan pengurusan kuasa pintar
- **Kelewatan Rendah**: Inferens dipercepatkan perkakasan dengan overhead minimum untuk aplikasi masa nyata
- **Penggunaan Skala**: Dari telefon pintar ke platform automotif merentasi ekosistem Qualcomm
- **Sedia untuk Pengeluaran**: Rangka kerja yang telah diuji digunakan dalam berjuta-juta peranti yang telah dilancarkan

## Pemasangan

### Prasyarat

- Qualcomm QNN SDK (memerlukan pendaftaran dengan Qualcomm)
- Python 3.7 atau lebih tinggi
- Perkakasan Qualcomm yang serasi atau simulator
- Android NDK (untuk penggunaan mudah alih)
- Persekitaran pembangunan Linux atau Windows

### Persediaan QNN SDK

1. **Daftar dan Muat Turun**: Lawati Qualcomm Developer Network untuk mendaftar dan memuat turun QNN SDK
2. **Ekstrak SDK**: Buka fail QNN SDK ke direktori pembangunan anda
3. **Tetapkan Pembolehubah Persekitaran**: Konfigurasikan laluan untuk alat dan perpustakaan QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Persediaan Persekitaran Python

Buat dan aktifkan persekitaran maya:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Pasang pakej Python yang diperlukan:

```bash
pip install numpy tensorflow torch onnx
```

### Sahkan Pemasangan

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Jika berjaya, anda akan melihat maklumat bantuan untuk setiap alat QNN.

## Panduan Permulaan Pantas

### Penukaran Model Pertama Anda

Mari kita tukar model PyTorch ringkas untuk dijalankan pada perkakasan Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Tukar ONNX ke Format QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Hasilkan Perpustakaan Model QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Apa yang Dilakukan oleh Proses Ini

Aliran kerja pengoptimuman melibatkan: menukar model asal ke format ONNX, menterjemahkan ONNX ke perwakilan perantaraan QNN, menerapkan pengoptimuman khusus perkakasan, dan menghasilkan perpustakaan model yang telah dikompilasi untuk penggunaan.

### Parameter Utama Dijelaskan

- `--input_network`: Fail model ONNX sumber
- `--output_path`: Fail sumber C++ yang dihasilkan
- `--input_dim`: Dimensi tensor input untuk pengoptimuman
- `--quantization_overrides`: Konfigurasi kuantisasi tersuai
- `-t x86_64-linux-clang`: Seni bina sasaran dan pengkompil

## Contoh: Menukar dan Mengoptimumkan Model dengan QNN

### Langkah 1: Penukaran Model Lanjutan dengan Kuantisasi

Berikut cara menerapkan kuantisasi tersuai semasa penukaran:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Tukar dengan kuantisasi tersuai:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Langkah 2: Pengoptimuman Multi-Backend

Konfigurasikan untuk pelaksanaan heterogen merentasi NPU, GPU, dan CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Langkah 3: Buat Binari Konteks untuk Penggunaan

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Langkah 4: Inferens dengan QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Struktur Output

Selepas pengoptimuman, direktori penggunaan anda akan mengandungi:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Penggunaan Lanjutan

### Konfigurasi Backend Tersuai

Konfigurasikan pengoptimuman backend tertentu:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Kuantisasi Dinamik

Terapkan kuantisasi semasa runtime untuk ketepatan yang lebih baik:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profil Prestasi

Pantau prestasi merentasi backend yang berbeza:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Pemilihan Backend Automatik

Laksanakan pemilihan backend pintar berdasarkan ciri model:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Amalan Terbaik

### 1. Pengoptimuman Seni Bina Model
- **Penggabungan Lapisan**: Gabungkan operasi seperti Conv+BatchNorm+ReLU untuk penggunaan NPU yang lebih baik
- **Konvolusi Terpisah Mengikut Kedalaman**: Utamakan ini berbanding konvolusi standard untuk penggunaan mudah alih
- **Reka Bentuk Mesra Kuantisasi**: Gunakan aktivasi ReLU dan elakkan operasi yang sukar untuk dikuantisasi

### 2. Strategi Kuantisasi
- **Kuantisasi Selepas Latihan**: Mulakan dengan ini untuk penggunaan pantas
- **Dataset Kalibrasi**: Gunakan data representatif yang merangkumi semua variasi input
- **Ketepatan Campuran**: Gunakan INT8 untuk kebanyakan lapisan, simpan lapisan kritikal dalam ketepatan lebih tinggi

### 3. Garis Panduan Pemilihan Backend
- **NPU (HTP)**: Terbaik untuk beban kerja CNN, model yang dikuantisasi, dan aplikasi sensitif kuasa
- **GPU**: Optimum untuk operasi intensif pengiraan, model yang lebih besar, dan ketepatan FP16
- **CPU**: Sandaran untuk operasi yang tidak disokong dan penyahpepijatan

### 4. Pengoptimuman Prestasi
- **Saiz Batch**: Gunakan saiz batch 1 untuk aplikasi masa nyata, batch lebih besar untuk throughput
- **Prapemprosesan Input**: Kurangkan overhead penyalinan dan penukaran data
- **Penggunaan Semula Konteks**: Pra-kompilasi konteks untuk mengelakkan overhead pengkompil semasa runtime

### 5. Pengurusan Memori
- **Peruntukan Tensor**: Gunakan peruntukan statik jika boleh untuk mengelakkan overhead runtime
- **Kolam Memori**: Laksanakan kolam memori tersuai untuk tensor yang sering diperuntukkan
- **Penggunaan Semula Penimbal**: Gunakan semula penimbal input/output merentasi panggilan inferens

### 6. Pengoptimuman Kuasa
- **Mod Prestasi**: Gunakan mod prestasi yang sesuai berdasarkan kekangan haba
- **Penskalaan Frekuensi Dinamik**: Benarkan sistem menskalakan frekuensi berdasarkan beban kerja
- **Pengurusan Keadaan Idle**: Lepaskan sumber dengan betul apabila tidak digunakan

## Penyelesaian Masalah

### Isu Biasa

#### 1. Masalah Pemasangan SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Kesalahan Penukaran Model
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Isu Kuantisasi
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Masalah Prestasi
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Masalah Memori
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Keserasian Backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Penyahpepijatan Prestasi

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Mendapatkan Bantuan

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Dokumentasi QNN**: Tersedia dalam pakej SDK
- **Forum Komuniti**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Sokongan Teknikal**: Melalui portal pembangun Qualcomm

## Sumber Tambahan

### Pautan Rasmi
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Platform Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portal Pembangun**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Sumber Pembelajaran
- **Panduan Permulaan**: Tersedia dalam dokumentasi QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Panduan Pengoptimuman**: Dokumentasi SDK termasuk garis panduan pengoptimuman yang komprehensif
- **Tutorial Video**: [Saluran YouTube Qualcomm Developer Network](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Alat Integrasi
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Model yang telah dioptimumkan untuk perkakasan Qualcomm
- **Android Neural Networks API**: Integrasi dengan Android NNAPI
- **TensorFlow Lite Delegate**: Delegasi Qualcomm untuk TFLite

### Penanda Aras Prestasi
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Penyelidikan AI Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Contoh Komuniti
- **Aplikasi Contoh**: Tersedia dalam direktori contoh QNN SDK
- **Repositori GitHub**: Contoh dan alat yang disumbangkan oleh komuniti
- **Blog Teknikal**: [Blog Pembangun Qualcomm](https://developer.qualcomm.com/blog)

### Alat Berkaitan
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Teknik kuantisasi dan pemampatan lanjutan
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Untuk perbandingan dan penggunaan sandaran
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Enjin inferens merentas platform

### Spesifikasi Perkakasan
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Platform Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Apa yang seterusnya

Teruskan perjalanan AI Tepi anda dengan meneroka [Module 5: SLMOps dan Penggunaan Pengeluaran](../Module05/README.md) untuk mempelajari aspek operasional pengurusan kitaran hayat Model Bahasa Kecil.

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat penting, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.