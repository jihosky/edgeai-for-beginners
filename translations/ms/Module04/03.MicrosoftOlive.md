<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:46:59+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ms"
}
-->
# Seksyen 3: Microsoft Olive Optimization Suite

## Kandungan
1. [Pengenalan](../../../Module04)
2. [Apa itu Microsoft Olive?](../../../Module04)
3. [Pemasangan](../../../Module04)
4. [Panduan Permulaan Cepat](../../../Module04)
5. [Contoh: Menukar Qwen3 kepada ONNX INT4](../../../Module04)
6. [Penggunaan Lanjutan](../../../Module04)
7. [Repositori Resipi Olive](../../../Module04)
8. [Amalan Terbaik](../../../Module04)
9. [Penyelesaian Masalah](../../../Module04)
10. [Sumber Tambahan](../../../Module04)

## Pengenalan

Microsoft Olive adalah alat pengoptimuman model yang berkuasa dan mudah digunakan, yang peka kepada perkakasan, untuk mempermudah proses pengoptimuman model pembelajaran mesin bagi pelaksanaan di pelbagai platform perkakasan. Sama ada anda menyasarkan CPU, GPU, atau pemecut AI khusus, Olive membantu anda mencapai prestasi optimum sambil mengekalkan ketepatan model.

## Apa itu Microsoft Olive?

Olive adalah alat pengoptimuman model yang peka kepada perkakasan dan mudah digunakan, yang menggabungkan teknik-teknik terkemuka dalam industri untuk pemampatan, pengoptimuman, dan pengkompilasi model. Ia berfungsi dengan ONNX Runtime sebagai penyelesaian pengoptimuman inferens E2E.

### Ciri Utama

- **Pengoptimuman Peka Perkakasan**: Secara automatik memilih teknik pengoptimuman terbaik untuk perkakasan sasaran anda
- **40+ Komponen Pengoptimuman Terbina**: Meliputi pemampatan model, kuantisasi, pengoptimuman graf, dan banyak lagi
- **Antara Muka CLI Mudah**: Perintah mudah untuk tugas pengoptimuman biasa
- **Sokongan Pelbagai Rangka Kerja**: Berfungsi dengan PyTorch, model Hugging Face, dan ONNX
- **Sokongan Model Popular**: Olive boleh mengoptimumkan seni bina model popular seperti Llama, Phi, Qwen, Gemma, dan lain-lain secara automatik

### Kelebihan

- **Pengurangan Masa Pembangunan**: Tidak perlu bereksperimen secara manual dengan teknik pengoptimuman yang berbeza
- **Peningkatan Prestasi**: Peningkatan kelajuan yang ketara (sehingga 6x dalam beberapa kes)
- **Pelaksanaan Merentas Platform**: Model yang dioptimumkan berfungsi di pelbagai perkakasan dan sistem operasi
- **Ketepatan Terpelihara**: Pengoptimuman mengekalkan kualiti model sambil meningkatkan prestasi

## Pemasangan

### Prasyarat

- Python 3.8 atau lebih tinggi
- Pengurus pakej pip
- Persekitaran maya (disyorkan)

### Pemasangan Asas

Buat dan aktifkan persekitaran maya:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Pasang Olive dengan ciri pengoptimuman automatik:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Kebergantungan Pilihan

Olive menawarkan pelbagai kebergantungan pilihan untuk ciri tambahan:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Sahkan Pemasangan

```bash
olive --help
```

Jika berjaya, anda akan melihat mesej bantuan CLI Olive.

## Panduan Permulaan Cepat

### Pengoptimuman Pertama Anda

Mari kita optimakan model bahasa kecil menggunakan ciri pengoptimuman automatik Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Apa yang Dilakukan oleh Perintah Ini

Proses pengoptimuman melibatkan: mendapatkan model dari cache tempatan, menangkap Graf ONNX dan menyimpan berat dalam fail data ONNX, mengoptimumkan Graf ONNX, dan mengkuantisasi model kepada int4 menggunakan kaedah RTN.

### Penjelasan Parameter Perintah

- `--model_name_or_path`: Pengenal model Hugging Face atau laluan tempatan
- `--output_path`: Direktori di mana model yang dioptimumkan akan disimpan
- `--device`: Peranti sasaran (cpu, gpu)
- `--provider`: Penyedia pelaksanaan (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Gunakan ONNX Runtime Generate AI untuk inferens
- `--precision`: Ketepatan kuantisasi (int4, int8, fp16)
- `--log_level`: Verbositi log (0=minimal, 1=terperinci)

## Contoh: Menukar Qwen3 kepada ONNX INT4

Berdasarkan contoh Hugging Face yang disediakan di [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), berikut adalah cara untuk mengoptimumkan model Qwen3:

### Langkah 1: Muat Turun Model (Pilihan)

Untuk meminimumkan masa muat turun, cache hanya fail penting:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Langkah 2: Optimumkan Model Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Langkah 3: Uji Model yang Dioptimumkan

Buat skrip Python ringkas untuk menguji model yang dioptimumkan:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktur Output

Selepas pengoptimuman, direktori output anda akan mengandungi:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Penggunaan Lanjutan

### Fail Konfigurasi

Untuk aliran kerja pengoptimuman yang lebih kompleks, anda boleh menggunakan fail konfigurasi JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Jalankan dengan konfigurasi:

```bash
olive run --config config.json
```

### Pengoptimuman GPU

Untuk pengoptimuman GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Untuk DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Penalaan Model dengan Olive

Olive juga menyokong penalaan model:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Amalan Terbaik

### 1. Pemilihan Model
- Mulakan dengan model yang lebih kecil untuk ujian (contohnya, parameter 0.5B-7B)
- Pastikan seni bina model sasaran anda disokong oleh Olive

### 2. Pertimbangan Perkakasan
- Padankan sasaran pengoptimuman anda dengan perkakasan pelaksanaan
- Gunakan pengoptimuman GPU jika anda mempunyai perkakasan yang serasi dengan CUDA
- Pertimbangkan DirectML untuk mesin Windows dengan grafik bersepadu

### 3. Pemilihan Ketepatan
- **INT4**: Pemampatan maksimum, sedikit kehilangan ketepatan
- **INT8**: Keseimbangan baik antara saiz dan ketepatan
- **FP16**: Kehilangan ketepatan minimum, pengurangan saiz sederhana

### 4. Ujian dan Pengesahan
- Sentiasa uji model yang dioptimumkan dengan kes penggunaan spesifik anda
- Bandingkan metrik prestasi (latensi, throughput, ketepatan)
- Gunakan data input yang mewakili untuk penilaian

### 5. Pengoptimuman Berulang
- Mulakan dengan pengoptimuman automatik untuk hasil cepat
- Gunakan fail konfigurasi untuk kawalan yang lebih terperinci
- Bereksperimen dengan laluan pengoptimuman yang berbeza

## Penyelesaian Masalah

### Masalah Biasa

#### 1. Masalah Pemasangan
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Masalah CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Masalah Memori
- Gunakan saiz batch yang lebih kecil semasa pengoptimuman
- Cuba kuantisasi dengan ketepatan lebih tinggi dahulu (int8 berbanding int4)
- Pastikan ruang cakera mencukupi untuk cache model

#### 4. Kesalahan Memuat Model
- Sahkan laluan model dan kebenaran akses
- Periksa jika model memerlukan `trust_remote_code=True`
- Pastikan semua fail model yang diperlukan telah dimuat turun

### Mendapatkan Bantuan

- **Dokumentasi**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Isu GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Contoh**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repositori Resipi Olive

### Pengenalan kepada Resipi Olive

Repositori [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) melengkapi alat utama Olive dengan menyediakan koleksi resipi pengoptimuman siap guna untuk model AI popular. Repositori ini berfungsi sebagai rujukan praktikal untuk mengoptimumkan model yang tersedia secara umum dan mencipta aliran kerja pengoptimuman untuk model proprietari.

### Ciri Utama

- **100+ Resipi Sedia Ada**: Konfigurasi pengoptimuman siap guna untuk model popular
- **Sokongan Pelbagai Seni Bina**: Meliputi model transformer, model visi, dan seni bina multimodal
- **Pengoptimuman Khusus Perkakasan**: Resipi yang disesuaikan untuk CPU, GPU, dan pemecut khusus
- **Keluarga Model Popular**: Termasuk Phi, Llama, Qwen, Gemma, Mistral, dan banyak lagi

### Keluarga Model yang Disokong

Repositori ini termasuk resipi pengoptimuman untuk:

#### Model Bahasa
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, siri Qwen2.5 (0.5B hingga 14B)
- **Google Gemma**: Pelbagai konfigurasi model Gemma
- **Mistral AI**: Siri Mistral-7B
- **DeepSeek**: Model siri R1-Distill

#### Model Visi dan Multimodal
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Model CLIP**: Pelbagai konfigurasi CLIP-ViT
- **ResNet**: Pengoptimuman ResNet-50
- **Transformer Visi**: ViT-base-patch16-224

#### Model Khusus
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Varian asas dan pelbagai bahasa
- **Transformer Ayat**: all-MiniLM-L6-v2

### Menggunakan Resipi Olive

#### Kaedah 1: Klon Resipi Tertentu

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Kaedah 2: Gunakan Resipi sebagai Templat

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Struktur Resipi

Setiap direktori resipi biasanya mengandungi:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Contoh: Menggunakan Resipi Phi-4-mini

Mari gunakan resipi Phi-4-mini sebagai contoh:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Fail konfigurasi biasanya termasuk:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Menyesuaikan Resipi

#### Mengubah Perkakasan Sasaran

Untuk menukar perkakasan sasaran, kemas kini bahagian `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Menyesuaikan Parameter Pengoptimuman

Ubah bahagian `passes` untuk tahap pengoptimuman yang berbeza:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Mencipta Resipi Anda Sendiri

1. **Mulakan dengan Model Serupa**: Cari resipi untuk model dengan seni bina serupa
2. **Kemas Kini Konfigurasi Model**: Tukar nama/laluan model dalam konfigurasi
3. **Sesuaikan Parameter**: Ubah parameter pengoptimuman mengikut keperluan
4. **Uji dan Sahkan**: Jalankan pengoptimuman dan sahkan hasil
5. **Sumbang Balik**: Pertimbangkan untuk menyumbang resipi anda kepada repositori

### Kelebihan Menggunakan Resipi

#### 1. **Konfigurasi Terbukti**
- Tetapan pengoptimuman yang telah diuji untuk model tertentu
- Mengelakkan percubaan dan kesilapan dalam mencari parameter optimum

#### 2. **Penalaan Khusus Perkakasan**
- Dioptimumkan terlebih dahulu untuk penyedia pelaksanaan yang berbeza
- Konfigurasi siap guna untuk sasaran CPU, GPU, dan NPU

#### 3. **Liputan Komprehensif**
- Menyokong model sumber terbuka yang paling popular
- Kemas kini berkala dengan keluaran model baharu

#### 4. **Sumbangan Komuniti**
- Pembangunan kolaboratif dengan komuniti AI
- Perkongsian pengetahuan dan amalan terbaik

### Menyumbang kepada Resipi Olive

Jika anda telah mengoptimumkan model yang tidak diliputi dalam repositori:

1. **Fork Repositori**: Cipta fork anda sendiri untuk resipi olive
2. **Cipta Direktori Resipi**: Tambah direktori baharu untuk model anda
3. **Masukkan Konfigurasi**: Tambah olive_config.json dan fail sokongan
4. **Dokumentasikan Penggunaan**: Sediakan README yang jelas dengan arahan
5. **Hantar Pull Request**: Sumbang balik kepada komuniti

### Penanda Aras Prestasi

Banyak resipi termasuk penanda aras prestasi yang menunjukkan:
- **Peningkatan Latensi**: Kelajuan 2-6x lebih pantas berbanding asas
- **Pengurangan Memori**: Pengurangan penggunaan memori 50-75% dengan kuantisasi
- **Pengekalan Ketepatan**: Pengekalan ketepatan 95-99%

### Integrasi dengan Alat AI

Resipi berfungsi dengan lancar dengan:
- **VS Code AI Toolkit**: Integrasi langsung untuk pengoptimuman model
- **Azure Machine Learning**: Aliran kerja pengoptimuman berasaskan awan
- **ONNX Runtime**: Pelaksanaan inferens yang dioptimumkan

## Sumber Tambahan

### Pautan Rasmi
- **Repositori GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repositori Resipi Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Dokumentasi ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Contoh Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Contoh Komuniti
- **Jupyter Notebooks**: Tersedia dalam repositori GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Sambungan VS Code**: Gambaran keseluruhan AI Toolkit untuk VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Catatan Blog**: Blog Sumber Terbuka Microsoft — https://opensource.microsoft.com/blog/

### Alat Berkaitan
- **ONNX Runtime**: Enjin inferens berprestasi tinggi — https://onnxruntime.ai/
- **Hugging Face Transformers**: Sumber banyak model yang serasi — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Aliran kerja pengoptimuman berasaskan awan — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Apa yang seterusnya

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.