<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T11:33:46+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "ko"
}
-->
# 섹션 03 - 모델 컨텍스트 프로토콜(MCP) 통합

## MCP(모델 컨텍스트 프로토콜) 소개

모델 컨텍스트 프로토콜(MCP)은 AI 애플리케이션을 외부 시스템에 연결하기 위한 오픈소스 표준입니다. MCP를 사용하면 Claude나 ChatGPT 같은 AI 애플리케이션이 데이터 소스(예: 로컬 파일, 데이터베이스), 도구(예: 검색 엔진, 계산기), 워크플로우(예: 특화된 프롬프트)에 연결하여 중요한 정보를 액세스하고 작업을 수행할 수 있습니다.

MCP는 AI 애플리케이션을 위한 **USB-C 포트**와 같습니다. USB-C가 전자 기기를 연결하는 표준화된 방법을 제공하듯이, MCP는 AI 애플리케이션을 외부 시스템에 연결하는 표준화된 방법을 제공합니다.

### MCP가 가능하게 하는 것들

MCP는 AI 애플리케이션에 강력한 기능을 제공합니다:

- **개인화된 AI 비서**: 에이전트가 Google 캘린더와 Notion에 액세스하여 더 개인화된 AI 비서 역할을 수행
- **고급 코드 생성**: Claude Code가 Figma 디자인을 사용하여 전체 웹 앱을 생성
- **기업 데이터 통합**: 기업용 챗봇이 조직 내 여러 데이터베이스에 연결하여 사용자가 채팅을 통해 데이터를 분석할 수 있도록 지원
- **창의적인 워크플로우**: AI 모델이 Blender에서 3D 디자인을 생성하고 3D 프린터로 출력
- **실시간 정보 액세스**: 외부 데이터 소스에 연결하여 최신 정보 제공
- **복잡한 다단계 작업**: 여러 도구와 시스템을 결합하여 정교한 워크플로우 수행

### MCP가 중요한 이유

MCP는 생태계 전반에 걸쳐 혜택을 제공합니다:

**개발자에게**: MCP는 AI 애플리케이션이나 에이전트를 구축하거나 통합할 때 개발 시간과 복잡성을 줄여줍니다.

**AI 애플리케이션에게**: MCP는 데이터 소스, 도구 및 앱의 생태계에 액세스하여 기능을 향상시키고 최종 사용자 경험을 개선합니다.

**최종 사용자에게**: MCP는 필요할 때 데이터를 액세스하고 사용자를 대신해 작업을 수행할 수 있는 더 유능한 AI 애플리케이션이나 에이전트를 제공합니다.

## MCP에서의 소형 언어 모델(SLM)

소형 언어 모델은 AI 배포에 효율적인 접근 방식을 제공하며 여러 장점을 가지고 있습니다:

### SLM의 장점
- **자원 효율성**: 낮은 컴퓨팅 요구 사항
- **빠른 응답 시간**: 실시간 애플리케이션에서 지연 시간 감소  
- **비용 효율성**: 최소한의 인프라 필요
- **프라이버시**: 데이터 전송 없이 로컬에서 실행 가능
- **맞춤화**: 특정 도메인에 맞게 쉽게 조정 가능

### SLM과 MCP의 조합이 효과적인 이유

SLM과 MCP를 결합하면 모델의 추론 능력이 외부 도구로 보강되어 작은 파라미터 수를 보완하면서 기능을 강화하는 강력한 조합이 됩니다.

## Python MCP SDK 개요

Python MCP SDK는 MCP를 지원하는 애플리케이션을 구축하기 위한 기반을 제공합니다. SDK는 다음을 포함합니다:

- **클라이언트 라이브러리**: MCP 서버에 연결하기 위한 라이브러리
- **서버 프레임워크**: 맞춤형 MCP 서버를 생성하기 위한 프레임워크
- **프로토콜 핸들러**: 통신 관리
- **도구 통합**: 외부 기능 실행

## 실용적 구현: Phi-4 MCP 클라이언트

Microsoft의 Phi-4 미니 모델을 MCP 기능과 통합한 실제 구현을 살펴보겠습니다.

### MCP 아키텍처 개요

MCP는 **클라이언트-서버 아키텍처**를 따르며, MCP 호스트(Claude Code 또는 Claude Desktop 같은 AI 애플리케이션)가 하나 이상의 MCP 서버에 연결을 설정합니다. MCP 호스트는 각 MCP 서버에 대해 하나의 MCP 클라이언트를 생성하여 이를 수행합니다.

#### 주요 참여자

- **MCP 호스트**: 하나 이상의 MCP 클라이언트를 조정하고 관리하는 AI 애플리케이션
- **MCP 클라이언트**: MCP 서버와의 연결을 유지하며 MCP 호스트가 사용할 컨텍스트를 MCP 서버에서 가져오는 구성 요소
- **MCP 서버**: MCP 클라이언트에 컨텍스트를 제공하는 프로그램

#### 이중 레이어 아키텍처

MCP는 두 개의 뚜렷한 레이어로 구성됩니다:

**데이터 레이어**: 클라이언트-서버 통신을 위한 JSON-RPC 기반 프로토콜 정의, 포함:
- 라이프사이클 관리(연결 초기화, 기능 협상)
- 핵심 프리미티브(도구, 리소스, 프롬프트)
- 클라이언트 기능(샘플링, 정보 요청, 로깅)
- 유틸리티 기능(알림, 진행 추적)

**전송 레이어**: 통신 메커니즘 및 채널 정의:
- **STDIO 전송**: 로컬 프로세스를 위한 표준 입력/출력 스트림 사용(최적의 성능, 네트워크 오버헤드 없음)
- **스트리밍 HTTP 전송**: HTTP POST와 선택적 서버-발송 이벤트를 사용하여 원격 서버와 통신(표준 HTTP 인증 지원)

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### MCP 핵심 프리미티브

MCP는 AI 애플리케이션과 공유할 수 있는 컨텍스트 정보 유형과 수행 가능한 작업 범위를 지정하는 프리미티브를 정의합니다.

#### 서버 프리미티브

MCP는 서버가 노출할 수 있는 세 가지 핵심 프리미티브를 정의합니다:

**도구**: AI 애플리케이션이 작업을 수행하기 위해 호출할 수 있는 실행 가능한 함수
- 예: 파일 작업, API 호출, 데이터베이스 쿼리
- 메서드: `tools/list`, `tools/call`
- 동적 발견 및 실행 지원

**리소스**: AI 애플리케이션에 컨텍스트 정보를 제공하는 데이터 소스
- 예: 파일 내용, 데이터베이스 레코드, API 응답
- 메서드: `resources/list`, `resources/read`
- 구조화된 데이터 액세스 가능

**프롬프트**: 언어 모델과의 상호작용을 구조화하는 데 도움을 주는 재사용 가능한 템플릿
- 예: 시스템 프롬프트, 몇 가지 예시
- 메서드: `prompts/list`, `prompts/get`
- AI 상호작용 패턴 표준화

#### 클라이언트 프리미티브

MCP는 또한 더 풍부한 상호작용을 가능하게 하기 위해 클라이언트가 노출할 수 있는 프리미티브를 정의합니다:

**샘플링**: 서버가 클라이언트의 AI 애플리케이션에서 언어 모델 완성을 요청할 수 있도록 허용
- 메서드: `sampling/complete`
- 모델 독립적인 서버 개발 가능
- 호스트의 언어 모델에 액세스 제공

**정보 요청**: 서버가 사용자로부터 추가 정보를 요청할 수 있도록 허용
- 메서드: `elicitation/request`
- 사용자 상호작용 및 확인 가능
- 동적 정보 수집 지원

**로깅**: 서버가 클라이언트에 로그 메시지를 보낼 수 있도록 허용
- 디버깅 및 모니터링 목적
- 서버 작업에 대한 가시성 제공

### MCP 프로토콜 라이프사이클

#### 초기화 및 기능 협상

MCP는 상태를 유지하는 프로토콜로 라이프사이클 관리가 필요합니다. 초기화 프로세스는 여러 중요한 목적을 수행합니다:

1. **프로토콜 버전 협상**: 클라이언트와 서버가 호환 가능한 프로토콜 버전을 사용하도록 보장(예: "2025-06-18")
2. **기능 발견**: 각 당사자가 지원하는 기능과 프리미티브 선언
3. **신원 교환**: 식별 및 버전 정보 제공

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### 도구 발견 및 실행

초기화 후 클라이언트는 도구를 발견하고 실행할 수 있습니다:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### 실시간 알림

MCP는 동적 업데이트를 위한 실시간 알림을 지원합니다:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## 시작하기: 단계별 가이드

### 1단계: 환경 설정

필요한 종속성 설치:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### 2단계: 기본 구성

환경 변수를 설정:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### 3단계: 첫 MCP 클라이언트 실행

**기본 Ollama 설정:**
```bash
python ghmodel_mcp_demo.py
```

**vLLM 백엔드 사용:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**서버-발송 이벤트 연결:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**맞춤형 MCP 서버:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### 4단계: 프로그래밍 방식 사용

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 고급 기능

### 다중 백엔드 지원

구현은 Ollama와 vLLM 백엔드를 모두 지원하며 요구 사항에 따라 선택할 수 있습니다:

- **Ollama**: 로컬 개발 및 테스트에 적합
- **vLLM**: 프로덕션 및 고처리량 시나리오에 최적화

### 유연한 연결 프로토콜

두 가지 연결 모드가 지원됩니다:

**STDIO 모드**: 직접 프로세스 통신
- 낮은 지연 시간
- 로컬 도구에 적합
- 간단한 설정

**SSE 모드**: HTTP 기반 스트리밍
- 네트워크 가능
- 분산 시스템에 적합
- 실시간 업데이트

### 도구 통합 기능

시스템은 다양한 도구와 통합할 수 있습니다:
- 웹 자동화(Playwright)
- 파일 작업
- API 상호작용
- 시스템 명령
- 맞춤형 함수

## 오류 처리 및 모범 사례

### 포괄적인 오류 관리

구현은 다음에 대한 강력한 오류 처리를 포함합니다:

**연결 오류:**
- MCP 서버 실패
- 네트워크 시간 초과
- 연결 문제

**도구 실행 오류:**
- 누락된 도구
- 매개변수 유효성 검사
- 실행 실패

**응답 처리 오류:**
- JSON 파싱 문제
- 형식 불일치
- LLM 응답 이상

### 모범 사례

1. **자원 관리**: 비동기 컨텍스트 관리자를 사용
2. **오류 처리**: 포괄적인 try-catch 블록 구현
3. **로깅**: 적절한 로깅 수준 활성화
4. **보안**: 입력 유효성 검사 및 출력 정리
5. **성능**: 연결 풀링 및 캐싱 사용

## 실제 응용 사례

### 웹 자동화
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### 데이터 처리
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API 통합
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## 성능 최적화

### 메모리 관리
- 효율적인 메시지 기록 처리
- 적절한 자원 정리
- 연결 풀링

### 네트워크 최적화
- 비동기 HTTP 작업
- 구성 가능한 시간 초과
- 우아한 오류 복구

### 동시 처리
- 논블로킹 I/O
- 병렬 도구 실행
- 효율적인 비동기 패턴

## 보안 고려사항

### 데이터 보호
- 안전한 API 키 관리
- 입력 유효성 검사
- 출력 정리

### 네트워크 보안
- HTTPS 지원
- 로컬 엔드포인트 기본값
- 안전한 토큰 처리

### 실행 안전성
- 도구 필터링
- 샌드박스 환경
- 감사 로깅

## MCP 생태계 및 개발

### MCP 프로젝트 범위

모델 컨텍스트 프로토콜 생태계는 다음의 주요 구성 요소를 포함합니다:

- **[MCP 사양](https://modelcontextprotocol.io/specification/latest)**: 클라이언트와 서버 구현 요구 사항을 설명하는 공식 사양
- **[MCP SDK](https://modelcontextprotocol.io/docs/sdk)**: MCP를 구현하는 다양한 프로그래밍 언어용 SDK
- **MCP 개발 도구**: MCP 서버 및 클라이언트를 개발하기 위한 도구, [MCP Inspector](https://github.com/modelcontextprotocol/inspector) 포함
- **[MCP 참조 서버 구현](https://github.com/modelcontextprotocol/servers)**: MCP 서버의 참조 구현

### MCP 개발 시작하기

MCP를 사용하여 구축을 시작하려면:

**서버 구축**: [MCP 서버 생성](https://modelcontextprotocol.io/docs/develop/build-server)하여 데이터와 도구를 노출

**클라이언트 구축**: [애플리케이션 개발](https://modelcontextprotocol.io/docs/develop/build-client)하여 MCP 서버에 연결

**개념 학습**: [핵심 개념 이해](https://modelcontextprotocol.io/docs/learn/architecture) 및 MCP 아키텍처 학습

## 결론

MCP와 통합된 SLM은 AI 애플리케이션 개발에서 패러다임 전환을 나타냅니다. 소형 모델의 효율성과 외부 도구의 강력함을 결합함으로써 개발자는 자원 효율적이면서도 매우 유능한 지능형 시스템을 만들 수 있습니다.

모델 컨텍스트 프로토콜은 USB-C가 전자 기기를 위한 보편적인 연결 표준을 제공하는 것처럼 AI 애플리케이션을 외부 시스템에 연결하는 표준화된 방법을 제공합니다. 이러한 표준화는 다음을 가능하게 합니다:

- **원활한 통합**: AI 모델을 다양한 데이터 소스와 도구에 연결
- **생태계 성장**: 한 번 구축하면 여러 AI 애플리케이션에서 사용 가능
- **기능 향상**: SLM을 외부 기능으로 보강
- **실시간 업데이트**: 동적이고 반응형 AI 애플리케이션 지원

핵심 요약:
- MCP는 AI 애플리케이션과 외부 시스템을 연결하는 오픈 표준입니다
- 프로토콜은 도구, 리소스, 프롬프트를 핵심 프리미티브로 지원합니다
- 실시간 알림은 동적이고 반응형 애플리케이션을 가능하게 합니다
- 적절한 라이프사이클 관리와 오류 처리는 프로덕션 사용에 필수적입니다
- 생태계는 포괄적인 SDK와 개발 도구를 제공합니다

## 참고 자료 및 추가 읽기

### 공식 MCP 문서

- **[모델 컨텍스트 프로토콜 공식 사이트](https://modelcontextprotocol.io/)** - 완전한 문서와 사양
- **[MCP 시작 가이드](https://modelcontextprotocol.io/docs/getting-started/intro)** - 소개 및 핵심 개념
- **[MCP 아키텍처 개요](https://modelcontextprotocol.io/docs/learn/architecture)** - 상세 기술 아키텍처
- **[MCP 사양](https://modelcontextprotocol.io/specification/latest)** - 공식 프로토콜 사양
- **[MCP SDK 문서](https://modelcontextprotocol.io/docs/sdk)** - 언어별 SDK 가이드

### 개발 리소스

- **[MCP 초보자용](https://aka.ms/mcp-for-beginners)** - 모델 컨텍스트 프로토콜에 대한 포괄적인 초보자 가이드
- **[MCP GitHub 조직](https://github.com/modelcontextprotocol)** - 공식 리포지토리와 예제
- **[MCP 서버 리포지토리](https://github.com/modelcontextprotocol/servers)** - 참조 서버 구현
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - 개발 및 디버깅 도구
- **[MCP 서버 구축 가이드](https://modelcontextprotocol.io/docs/develop/build-server)** - 서버 개발 튜토리얼
- **[MCP 클라이언트 구축 가이드](https://modelcontextprotocol.io/docs/develop/build-client)** - 클라이언트 개발 튜토리얼

### 소형 언어 모델과 엣지 AI

- **[Microsoft Phi 모델](https://aka.ms/phicookbook)** - Phi 모델 패밀리
- **[Foundry Local 문서](https://github.com/microsoft/Foundry-Local)** - Microsoft의 엣지 AI 런타임
- **[Ollama 문서](https://ollama.ai/docs)** - 로컬 LLM 배포 플랫폼  
- **[vLLM 문서](https://docs.vllm.ai/)** - 고성능 LLM 서비스  

### 기술 표준 및 프로토콜  

- **[JSON-RPC 2.0 사양](https://www.jsonrpc.org/)** - MCP에서 사용되는 기본 RPC 프로토콜  
- **[JSON Schema](https://json-schema.org/)** - MCP 도구를 위한 스키마 정의 표준  
- **[OpenAPI 사양](https://swagger.io/specification/)** - API 문서화 표준  
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - 실시간 업데이트를 위한 웹 표준  

### AI 에이전트 개발  

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - 프로덕션 준비가 완료된 에이전트 개발  
- **[LangChain 문서](https://docs.langchain.com/)** - 에이전트 및 도구 통합 프레임워크  
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - Microsoft의 AI 오케스트레이션 SDK  

### 산업 보고서 및 연구  

- **[Anthropic의 모델 컨텍스트 프로토콜 발표](https://www.anthropic.com/news/model-context-protocol)** - MCP의 초기 소개  
- **[소형 언어 모델 설문조사](https://arxiv.org/abs/2410.20011)** - SLM 연구에 대한 학술 설문조사  
- **[엣지 AI 시장 분석](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - 산업 동향 및 예측  
- **[AI 에이전트 개발 모범 사례](https://arxiv.org/abs/2309.02427)** - 에이전트 아키텍처에 대한 연구  

이 섹션은 SLM 기반 MCP 애플리케이션을 구축하기 위한 기초를 제공하며, 자동화, 데이터 처리 및 지능형 시스템 통합의 가능성을 열어줍니다.  

## ➡️ 다음 단계  

- [모듈 7. 엣지 AI 샘플](../Module07/README.md)  

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임지지 않습니다.