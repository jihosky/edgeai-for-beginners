<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-09-15T17:07:26+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "ko"
}
-->
# 섹션 4 : OpenVINO Toolkit 최적화 스위트

## 목차
1. [소개](../../../Module04)
2. [OpenVINO란 무엇인가?](../../../Module04)
3. [설치](../../../Module04)
4. [빠른 시작 가이드](../../../Module04)
5. [예제: OpenVINO를 사용한 모델 변환 및 최적화](../../../Module04)
6. [고급 사용법](../../../Module04)
7. [모범 사례](../../../Module04)
8. [문제 해결](../../../Module04)
9. [추가 자료](../../../Module04)

## 소개

OpenVINO(Open Visual Inference and Neural Network Optimization)는 클라우드, 자체 환경 기반, 엣지 환경에서 고성능 AI 솔루션을 배포하기 위한 Intel의 오픈소스 툴킷입니다. CPU, GPU, VPU 또는 AI 가속기를 대상으로 하든, OpenVINO는 모델 정확도를 유지하면서 크로스 플랫폼 배포를 가능하게 하고 포괄적인 최적화 기능을 제공합니다.

## OpenVINO란 무엇인가?

OpenVINO는 다양한 하드웨어 플랫폼에서 AI 모델을 효율적으로 최적화, 변환 및 배포할 수 있도록 지원하는 오픈소스 툴킷입니다. OpenVINO는 세 가지 주요 구성 요소로 이루어져 있습니다: 추론을 위한 OpenVINO Runtime, 모델 최적화를 위한 Neural Network Compression Framework(NNCF), 그리고 확장 가능한 배포를 위한 OpenVINO Model Server.

### 주요 기능

- **크로스 플랫폼 배포**: Linux, Windows, macOS를 지원하며 Python, C++, C API 제공
- **하드웨어 가속**: CPU, GPU, VPU 및 AI 가속기를 위한 자동 장치 탐지 및 최적화
- **모델 압축 프레임워크**: NNCF를 통한 고급 양자화, 가지치기 및 최적화 기술
- **프레임워크 호환성**: TensorFlow, ONNX, PaddlePaddle, PyTorch 모델 직접 지원
- **생성형 AI 지원**: 대규모 언어 모델 및 생성형 AI 애플리케이션 배포를 위한 OpenVINO GenAI

### 장점

- **성능 최적화**: 정확도 손실을 최소화하면서 속도 크게 향상
- **배포 풋프린트 감소**: 외부 종속성을 최소화하여 설치 및 배포 간소화
- **향상된 시작 시간**: 최적화된 모델 로딩 및 캐싱으로 애플리케이션 초기화 속도 개선
- **확장 가능한 배포**: 엣지 장치에서 클라우드 인프라까지 일관된 API 제공
- **프로덕션 준비 완료**: 포괄적인 문서와 커뮤니티 지원을 갖춘 엔터프라이즈급 신뢰성

## 설치

### 사전 요구 사항

- Python 3.8 이상
- pip 패키지 관리자
- 가상 환경(권장)
- 호환 하드웨어(Intel CPU 권장, 다양한 아키텍처 지원)

### 기본 설치

가상 환경 생성 및 활성화:

```bash
# Create virtual environment
python -m venv openvino-env

# Activate virtual environment
# On Windows:
openvino-env\Scripts\activate
# On macOS/Linux:
source openvino-env/bin/activate
```

OpenVINO Runtime 설치:

```bash
pip install openvino
```

모델 최적화를 위한 NNCF 설치:

```bash
pip install nncf
```

### OpenVINO GenAI 설치

생성형 AI 애플리케이션을 위한 설치:

```bash
pip install openvino-genai
```

### 선택적 종속성

특정 사용 사례를 위한 추가 패키지:

```bash
# For Jupyter notebooks and development tools
pip install openvino[dev]

# For TensorFlow model support
pip install openvino[tensorflow]

# For PyTorch model support
pip install openvino[pytorch]

# For ONNX model support
pip install openvino[onnx]
```

### 설치 확인

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

성공적으로 설치되면 OpenVINO 버전 정보를 확인할 수 있습니다.

## 빠른 시작 가이드

### 첫 번째 모델 최적화

OpenVINO를 사용하여 Hugging Face 모델을 변환하고 최적화해 봅시다:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# Load and convert model to OpenVINO IR format
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Save the converted model
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# Load and compile for inference
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # or "GPU", "AUTO"
)

# Create inference pipeline
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### 이 과정에서 수행되는 작업

최적화 워크플로는 다음을 포함합니다: Hugging Face에서 원본 모델 로드, OpenVINO Intermediate Representation(IR) 형식으로 변환, 기본 최적화 적용, 대상 하드웨어에 맞게 컴파일.

### 주요 매개변수 설명

- `export=True`: 모델을 OpenVINO IR 형식으로 변환
- `compile=False`: 유연성을 위해 런타임까지 컴파일 지연
- `device`: 대상 하드웨어("CPU", "GPU", "AUTO"는 자동 선택)
- `save_pretrained()`: 최적화된 모델을 저장하여 재사용 가능

## 예제: OpenVINO를 사용한 모델 변환 및 최적화

### 단계 1: NNCF 양자화를 사용한 모델 변환

후처리 양자화를 적용하는 방법:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# Initialize NNCF for quantization
model_id = "microsoft/DialoGPT-small"

# Load model in OpenVINO format
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# Create calibration dataset for quantization
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# Apply post-training quantization
core = Core()
model = core.read_model(ov_model.model_path)

# Configure quantization
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # batch_size, sequence_length
        type="long"
    )
)

# Create quantized model
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# Save quantized model
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### 단계 2: 가중치 압축을 통한 고급 최적화

Transformer 기반 모델에 가중치 압축 적용:

```python
import nncf
from openvino import Core

# Load model
core = Core()
model = core.read_model("models/dialogpt-openvino")

# Apply weight compression for LLMs
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # or INT4_ASYM, INT8
    ratio=0.8,  # Compression ratio
    group_size=128  # Group size for quantization
)

# Save compressed model
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### 단계 3: 최적화된 모델로 추론

```python
from openvino import Core
import numpy as np

# Initialize OpenVINO Core
core = Core()

# Load optimized model
model = core.read_model("models/dialogpt-compressed.xml")

# Compile model for target device
compiled_model = core.compile_model(model, "CPU")

# Get input/output information
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# Prepare input data
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# Run inference
result = compiled_model([tokens])[output_layer]

# Decode output
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### 출력 구조

최적화 후, 모델 디렉토리에는 다음이 포함됩니다:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## 고급 사용법

### NNCF YAML을 사용한 구성

복잡한 최적화 워크플로를 위해 NNCF 구성 파일 사용:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

구성 적용:

```python
import nncf
from openvino import Core

# Load model and config
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# Apply compression
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU 최적화

GPU 가속을 위한 설정:

```python
from optimum.intel import OVModelForCausalLM

# Load model with GPU device
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# Configure for high throughput
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### 배치 처리 최적화

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# Configure for batch processing
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# Process multiple inputs
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### 모델 서버 배포

OpenVINO Model Server를 사용하여 최적화된 모델 배포:

```bash
# Install OpenVINO Model Server
pip install ovms

# Start model server
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

모델 서버 클라이언트 코드:

```python
import requests
import json

# Prepare request
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # Token IDs
    }
}

# Send request to model server
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## 모범 사례

### 1. 모델 선택 및 준비
- 지원되는 프레임워크(PyTorch, TensorFlow, ONNX)의 모델 사용
- 모델 입력이 고정 또는 동적 형태인지 확인
- 보정용 대표 데이터셋으로 테스트

### 2. 최적화 전략 선택
- **후처리 양자화**: 빠른 최적화를 위해 시작
- **가중치 압축**: 대규모 언어 모델 및 Transformer에 적합
- **양자화 인식 학습**: 정확도가 중요한 경우 사용

### 3. 하드웨어별 최적화
- **CPU**: 균형 잡힌 성능을 위해 INT8 양자화 사용
- **GPU**: FP16 정밀도 및 배치 처리 활용
- **VPU**: 모델 단순화 및 레이어 융합에 집중

### 4. 성능 조정
- **처리량 모드**: 대량 배치 처리에 적합
- **지연 시간 모드**: 실시간 상호작용 애플리케이션에 적합
- **AUTO 장치**: OpenVINO가 최적의 하드웨어 선택

### 5. 메모리 관리
- 메모리 오버헤드를 방지하기 위해 동적 형태를 신중히 사용
- 모델 캐싱 구현으로 이후 로드 속도 향상
- 최적화 중 메모리 사용량 모니터링

### 6. 정확도 검증
- 최적화된 모델을 원본 성능과 비교하여 항상 검증
- 대표 테스트 데이터셋을 사용하여 평가
- 점진적 최적화 고려(보수적인 설정으로 시작)

## 문제 해결

### 일반적인 문제

#### 1. 설치 문제
```bash
# Clear pip cache and reinstall
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. 모델 변환 오류
```python
# Check model compatibility
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. 성능 문제
```python
# Enable performance hints
config = {
    "PERFORMANCE_HINT": "LATENCY",  # or "THROUGHPUT"
    "INFERENCE_PRECISION_HINT": "f32"  # or "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. 메모리 문제
- 최적화 중 모델 배치 크기 줄이기
- 대규모 데이터셋에 스트리밍 사용
- 모델 캐싱 활성화: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. 정확도 저하
- 더 높은 정밀도 사용(INT8 대신 INT4)
- 보정 데이터셋 크기 증가
- 혼합 정밀도 최적화 적용

### 성능 모니터링

```python
# Monitor inference performance
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### 도움 받기

- **문서**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub 이슈**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **커뮤니티 포럼**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## 추가 자료

### 공식 링크
- **OpenVINO 홈페이지**: [openvino.ai](https://openvino.ai/)
- **GitHub 저장소**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF 저장소**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **모델 동물원**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### 학습 자료
- **OpenVINO 노트북**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **빠른 시작 가이드**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **최적화 가이드**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### 통합 도구
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO Model Server**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### 성능 벤치마크
- **공식 벤치마크**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF 모델 동물원**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### 커뮤니티 예제
- **Jupyter 노트북**: [OpenVINO 노트북 저장소](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINO 노트북 저장소에서 제공되는 포괄적인 튜토리얼
- **샘플 애플리케이션**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - 다양한 도메인(컴퓨터 비전, NLP, 오디오)을 위한 실제 예제
- **블로그 게시물**: [Intel AI 블로그](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI 및 커뮤니티 블로그 게시물로 상세한 사용 사례 제공

### 관련 도구
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Intel 하드웨어를 위한 추가 최적화 기술
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - 모바일 및 엣지 배포 비교용
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - 크로스 플랫폼 추론 엔진 대안

## ➡️ 다음 단계

- [05: Apple MLX Framework 심층 분석](./05.AppleMLX.md)

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서를 해당 언어로 작성된 상태에서 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역을 사용함으로써 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.