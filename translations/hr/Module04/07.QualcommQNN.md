<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T16:01:08+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "hr"
}
-->
# Poglavlje 7: Qualcomm QNN (Qualcomm Neural Network) Optimizacijski paket

## Sadržaj
1. [Uvod](../../../Module04)
2. [Što je Qualcomm QNN?](../../../Module04)
3. [Instalacija](../../../Module04)
4. [Vodič za brzi početak](../../../Module04)
5. [Primjer: Pretvaranje i optimizacija modela s QNN-om](../../../Module04)
6. [Napredno korištenje](../../../Module04)
7. [Najbolje prakse](../../../Module04)
8. [Rješavanje problema](../../../Module04)
9. [Dodatni resursi](../../../Module04)

## Uvod

Qualcomm QNN (Qualcomm Neural Network) je sveobuhvatan okvir za AI inferenciju dizajniran za iskorištavanje punog potencijala Qualcommovih AI hardverskih akceleratora, uključujući Hexagon NPU, Adreno GPU i Kryo CPU. Bez obzira ciljate li mobilne uređaje, platforme za rubno računalstvo ili automobilske sustave, QNN pruža optimizirane mogućnosti inferencije koje koriste Qualcommove specijalizirane AI procesne jedinice za maksimalne performanse i energetsku učinkovitost.

## Što je Qualcomm QNN?

Qualcomm QNN je objedinjeni okvir za AI inferenciju koji omogućuje programerima učinkovito implementiranje AI modela na Qualcommovoj heterogenoj računalnoj arhitekturi. Pruža objedinjeno programsko sučelje za pristup Hexagon NPU (Neural Processing Unit), Adreno GPU i Kryo CPU, automatski birajući optimalnu procesnu jedinicu za različite slojeve modela i operacije.

### Ključne značajke

- **Heterogeno računalstvo**: Objedinjen pristup NPU-u, GPU-u i CPU-u s automatskom raspodjelom opterećenja
- **Optimizacija prilagođena hardveru**: Specijalizirane optimizacije za Qualcomm Snapdragon platforme
- **Podrška za kvantizaciju**: Napredne tehnike kvantizacije INT8, INT16 i mješovite preciznosti
- **Alati za pretvaranje modela**: Izravna podrška za TensorFlow, PyTorch, ONNX i Caffe modele
- **Optimizirano za rubnu AI**: Dizajnirano posebno za mobilne i rubne scenarije implementacije s fokusom na energetsku učinkovitost

### Prednosti

- **Maksimalne performanse**: Iskoristite specijalizirani AI hardver za do 15x poboljšanja performansi
- **Energetska učinkovitost**: Optimizirano za mobilne uređaje i uređaje na baterijski pogon s inteligentnim upravljanjem energijom
- **Niska latencija**: Inferencija ubrzana hardverom s minimalnim kašnjenjem za aplikacije u stvarnom vremenu
- **Skalabilna implementacija**: Od pametnih telefona do automobilskih platformi unutar Qualcommovog ekosustava
- **Spremno za proizvodnju**: Okvir testiran u milijunima implementiranih uređaja

## Instalacija

### Preduvjeti

- Qualcomm QNN SDK (zahtijeva registraciju na Qualcommu)
- Python 3.7 ili noviji
- Kompatibilan Qualcomm hardver ili simulator
- Android NDK (za mobilnu implementaciju)
- Linux ili Windows razvojno okruženje

### Postavljanje QNN SDK-a

1. **Registracija i preuzimanje**: Posjetite Qualcomm Developer Network za registraciju i preuzimanje QNN SDK-a
2. **Raspakirajte SDK**: Raspakirajte QNN SDK u svoj razvojni direktorij
3. **Postavite varijable okruženja**: Konfigurirajte putove za QNN alate i biblioteke

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Postavljanje Python okruženja

Kreirajte i aktivirajte virtualno okruženje:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Instalirajte potrebne Python pakete:

```bash
pip install numpy tensorflow torch onnx
```

### Provjera instalacije

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Ako je uspješno, trebali biste vidjeti informacije o pomoći za svaki QNN alat.

## Vodič za brzi početak

### Vaša prva konverzija modela

Pretvorimo jednostavan PyTorch model za rad na Qualcomm hardveru:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Pretvorba ONNX-a u QNN format

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generiranje QNN biblioteke modela

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Što ovaj proces radi

Proces optimizacije uključuje: pretvaranje izvornog modela u ONNX format, prevođenje ONNX-a u QNN međureprezentaciju, primjenu optimizacija specifičnih za hardver i generiranje kompajlirane biblioteke modela za implementaciju.

### Objašnjenje ključnih parametara

- `--input_network`: Izvorna ONNX datoteka modela
- `--output_path`: Generirana C++ izvorna datoteka
- `--input_dim`: Dimenzije ulaznog tenzora za optimizaciju
- `--quantization_overrides`: Prilagođena konfiguracija kvantizacije
- `-t x86_64-linux-clang`: Ciljana arhitektura i kompajler

## Primjer: Pretvaranje i optimizacija modela s QNN-om

### Korak 1: Napredna konverzija modela s kvantizacijom

Evo kako primijeniti prilagođenu kvantizaciju tijekom konverzije:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Pretvorba s prilagođenom kvantizacijom:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Korak 2: Optimizacija za više backendova

Konfigurirajte za heterogeno izvršavanje na NPU-u, GPU-u i CPU-u:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Korak 3: Kreiranje binarne datoteke konteksta za implementaciju

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Korak 4: Inferencija s QNN Runtime-om

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Struktura izlaza

Nakon optimizacije, vaš direktorij za implementaciju sadržavat će:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Napredno korištenje

### Prilagođena konfiguracija backendova

Konfigurirajte specifične optimizacije backendova:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dinamička kvantizacija

Primijenite kvantizaciju tijekom izvođenja za bolju točnost:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profiliranje performansi

Pratite performanse na različitim backendovima:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatski odabir backenda

Implementirajte inteligentni odabir backenda na temelju karakteristika modela:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Najbolje prakse

### 1. Optimizacija arhitekture modela
- **Spajanje slojeva**: Kombinirajte operacije poput Conv+BatchNorm+ReLU za bolju iskorištenost NPU-a
- **Dubinski odvojive konvolucije**: Preferirajte ih umjesto standardnih konvolucija za mobilnu implementaciju
- **Dizajni prilagođeni kvantizaciji**: Koristite ReLU aktivacije i izbjegavajte operacije koje se teško kvantiziraju

### 2. Strategija kvantizacije
- **Kvantizacija nakon treninga**: Započnite s ovim za brzu implementaciju
- **Kalibracijski skup podataka**: Koristite reprezentativne podatke koji pokrivaju sve varijacije ulaza
- **Mješovita preciznost**: Koristite INT8 za većinu slojeva, kritične slojeve zadržite u višoj preciznosti

### 3. Smjernice za odabir backenda
- **NPU (HTP)**: Najbolji za CNN radna opterećenja, kvantizirane modele i aplikacije osjetljive na energiju
- **GPU**: Optimalan za operacije koje zahtijevaju puno računalne snage, veće modele i FP16 preciznost
- **CPU**: Rezervni izbor za nepodržane operacije i otklanjanje pogrešaka

### 4. Optimizacija performansi
- **Veličina serije**: Koristite veličinu serije 1 za aplikacije u stvarnom vremenu, veće serije za propusnost
- **Predobrada ulaza**: Minimizirajte kopiranje podataka i troškove konverzije
- **Ponovna upotreba konteksta**: Prekompajlirajte kontekste kako biste izbjegli troškove kompajliranja tijekom izvođenja

### 5. Upravljanje memorijom
- **Alokacija tenzora**: Koristite statičku alokaciju kad god je moguće kako biste izbjegli troškove tijekom izvođenja
- **Memorijski bazeni**: Implementirajte prilagođene memorijske bazene za često alocirane tenzore
- **Ponovna upotreba međuspremnika**: Ponovno koristite ulazne/izlazne međuspremnike između poziva inferencije

### 6. Optimizacija energije
- **Načini performansi**: Koristite odgovarajuće načine performansi na temelju toplinskih ograničenja
- **Dinamičko skaliranje frekvencije**: Dopustite sustavu da skalira frekvenciju na temelju opterećenja
- **Upravljanje stanjem mirovanja**: Ispravno oslobodite resurse kada nisu u upotrebi

## Rješavanje problema

### Uobičajeni problemi

#### 1. Problemi s instalacijom SDK-a
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Pogreške pri konverziji modela
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Problemi s kvantizacijom
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Problemi s performansama
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Problemi s memorijom
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Kompatibilnost backenda
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Otklanjanje problema s performansama

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Dobivanje pomoći

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN dokumentacija**: Dostupna u SDK paketu
- **Forumi zajednice**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Tehnička podrška**: Putem Qualcomm portala za programere

## Dodatni resursi

### Službeni linkovi
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon platforme**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portal za programere**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Resursi za učenje
- **Vodič za početak**: Dostupan u QNN SDK dokumentaciji
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Vodič za optimizaciju**: SDK dokumentacija uključuje sveobuhvatne smjernice za optimizaciju
- **Video tutorijali**: [Qualcomm Developer YouTube kanal](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Alati za integraciju
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Predoptimizirani modeli za Qualcomm hardver
- **Android Neural Networks API**: Integracija s Android NNAPI-jem
- **TensorFlow Lite Delegate**: Qualcomm delegat za TFLite

### Benchmarkovi performansi
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Primjeri iz zajednice
- **Primjenske aplikacije**: Dostupne u QNN SDK direktoriju primjera
- **GitHub repozitoriji**: Primjeri i alati koje je doprinijela zajednica
- **Tehnički blogovi**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Povezani alati
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Napredne tehnike kvantizacije i kompresije
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Za usporedbu i rezervnu implementaciju
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Višestruka platforma za inferenciju

### Specifikacije hardvera
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon platforme**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Što dalje

Nastavite svoje putovanje u rubnoj AI tehnologiji istražujući [Modul 5: SLMOps i implementacija u proizvodnji](../Module05/README.md) kako biste naučili o operativnim aspektima upravljanja životnim ciklusom malih jezičnih modela.

---

**Izjava o odricanju odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane čovjeka. Ne preuzimamo odgovornost za nesporazume ili pogrešna tumačenja koja mogu proizaći iz korištenja ovog prijevoda.