<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T14:54:36+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "hr"
}
-->
# Odjeljak 3: Microsoft Olive Optimization Suite

## Sadržaj
1. [Uvod](../../../Module04)
2. [Što je Microsoft Olive?](../../../Module04)
3. [Instalacija](../../../Module04)
4. [Vodič za brzi početak](../../../Module04)
5. [Primjer: Pretvaranje Qwen3 u ONNX INT4](../../../Module04)
6. [Napredno korištenje](../../../Module04)
7. [Repozitorij Olive Recipes](../../../Module04)
8. [Najbolje prakse](../../../Module04)
9. [Rješavanje problema](../../../Module04)
10. [Dodatni resursi](../../../Module04)

## Uvod

Microsoft Olive je moćan, jednostavan alat za optimizaciju modela koji je svjestan hardvera i pojednostavljuje proces optimizacije modela strojnog učenja za implementaciju na različitim hardverskim platformama. Bez obzira ciljate li na CPU, GPU ili specijalizirane AI akceleratore, Olive vam pomaže postići optimalne performanse uz očuvanje točnosti modela.

## Što je Microsoft Olive?

Olive je jednostavan alat za optimizaciju modela svjestan hardvera koji objedinjuje vodeće tehnike u industriji za kompresiju, optimizaciju i kompilaciju modela. Radi s ONNX Runtime kao E2E rješenjem za optimizaciju inferencije.

### Ključne značajke

- **Optimizacija svjesna hardvera**: Automatski odabire najbolje tehnike optimizacije za vaš ciljani hardver
- **40+ ugrađenih komponenti za optimizaciju**: Obuhvaća kompresiju modela, kvantizaciju, optimizaciju grafa i više
- **Jednostavno CLI sučelje**: Jednostavne naredbe za uobičajene zadatke optimizacije
- **Podrška za više okvira**: Radi s PyTorch, Hugging Face modelima i ONNX
- **Podrška za popularne modele**: Olive može automatski optimizirati popularne arhitekture modela poput Llama, Phi, Qwen, Gemma itd. bez dodatnih prilagodbi

### Prednosti

- **Smanjeno vrijeme razvoja**: Nema potrebe za ručnim eksperimentiranjem s različitim tehnikama optimizacije
- **Poboljšane performanse**: Značajna ubrzanja (do 6x u nekim slučajevima)
- **Implementacija na više platformi**: Optimizirani modeli rade na različitim hardverima i operativnim sustavima
- **Očuvana točnost**: Optimizacije čuvaju kvalitetu modela uz poboljšanje performansi

## Instalacija

### Preduvjeti

- Python 3.8 ili noviji
- pip upravitelj paketa
- Virtualno okruženje (preporučeno)

### Osnovna instalacija

Kreirajte i aktivirajte virtualno okruženje:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instalirajte Olive s funkcijama automatske optimizacije:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opcionalne ovisnosti

Olive nudi razne opcionalne ovisnosti za dodatne značajke:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Provjera instalacije

```bash
olive --help
```

Ako je uspješno, trebali biste vidjeti poruku pomoći Olive CLI.

## Vodič za brzi početak

### Vaša prva optimizacija

Optimizirajmo mali jezični model koristeći Oliveovu funkciju automatske optimizacije:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Što ova naredba radi

Proces optimizacije uključuje: preuzimanje modela iz lokalne predmemorije, hvatanje ONNX grafa i spremanje težina u ONNX datoteku, optimizaciju ONNX grafa i kvantizaciju modela na int4 koristeći RTN metodu.

### Objašnjenje parametara naredbe

- `--model_name_or_path`: Hugging Face identifikator modela ili lokalna putanja
- `--output_path`: Direktorij u kojem će optimizirani model biti spremljen
- `--device`: Ciljani uređaj (cpu, gpu)
- `--provider`: Izvršni provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Koristi ONNX Runtime Generate AI za inferenciju
- `--precision`: Kvantizacijska preciznost (int4, int8, fp16)
- `--log_level`: Razina detaljnosti zapisivanja (0=minimalno, 1=detaljno)

## Primjer: Pretvaranje Qwen3 u ONNX INT4

Na temelju pruženog Hugging Face primjera na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), evo kako optimizirati Qwen3 model:

### Korak 1: Preuzimanje modela (opcionalno)

Kako biste smanjili vrijeme preuzimanja, predmemorirajte samo ključne datoteke:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Korak 2: Optimizacija Qwen3 modela

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Korak 3: Testiranje optimiziranog modela

Kreirajte jednostavan Python skript za testiranje vašeg optimiziranog modela:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura izlaza

Nakon optimizacije, vaš izlazni direktorij sadržavat će:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Napredno korištenje

### Konfiguracijske datoteke

Za složenije radne tokove optimizacije možete koristiti JSON konfiguracijske datoteke:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Pokrenite s konfiguracijom:

```bash
olive run --config config.json
```

### Optimizacija za GPU

Za CUDA GPU optimizaciju:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Za DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fino podešavanje s Olive

Olive također podržava fino podešavanje modela:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najbolje prakse

### 1. Odabir modela
- Započnite s manjim modelima za testiranje (npr. 0.5B-7B parametara)
- Provjerite je li arhitektura vašeg ciljanog modela podržana od strane Olive

### 2. Razmatranja o hardveru
- Prilagodite ciljeve optimizacije svom hardveru za implementaciju
- Koristite GPU optimizaciju ako imate CUDA-kompatibilan hardver
- Razmotrite DirectML za Windows računala s integriranom grafikom

### 3. Odabir preciznosti
- **INT4**: Maksimalna kompresija, blagi gubitak točnosti
- **INT8**: Dobar balans veličine i točnosti
- **FP16**: Minimalan gubitak točnosti, umjereno smanjenje veličine

### 4. Testiranje i validacija
- Uvijek testirajte optimizirane modele s vašim specifičnim slučajevima korištenja
- Usporedite metrike performansi (kašnjenje, propusnost, točnost)
- Koristite reprezentativne ulazne podatke za evaluaciju

### 5. Iterativna optimizacija
- Započnite s automatskom optimizacijom za brze rezultate
- Koristite konfiguracijske datoteke za preciznu kontrolu
- Eksperimentirajte s različitim prolazima optimizacije

## Rješavanje problema

### Uobičajeni problemi

#### 1. Problemi s instalacijom
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemi s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemi s memorijom
- Koristite manje veličine batcha tijekom optimizacije
- Pokušajte kvantizaciju s višom preciznošću prvo (int8 umjesto int4)
- Osigurajte dovoljno prostora na disku za predmemoriju modela

#### 4. Pogreške pri učitavanju modela
- Provjerite putanju modela i dozvole za pristup
- Provjerite zahtijeva li model `trust_remote_code=True`
- Osigurajte da su sve potrebne datoteke modela preuzete

### Dobivanje pomoći

- **Dokumentacija**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Primjeri**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repozitorij Olive Recipes

### Uvod u Olive Recipes

Repozitorij [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) nadopunjuje glavni Olive alat pružajući sveobuhvatan skup gotovih recepata za optimizaciju popularnih AI modela. Ovaj repozitorij služi kao praktična referenca za optimizaciju javno dostupnih modela i kreiranje radnih tokova optimizacije za vlasničke modele.

### Ključne značajke

- **100+ unaprijed pripremljenih recepata**: Gotove konfiguracije optimizacije za popularne modele
- **Podrška za više arhitektura**: Obuhvaća modele transformatora, vizualne modele i multimodalne arhitekture
- **Optimizacije specifične za hardver**: Recepti prilagođeni za CPU, GPU i specijalizirane akceleratore
- **Popularne obitelji modela**: Uključuje Phi, Llama, Qwen, Gemma, Mistral i mnoge druge

### Podržane obitelji modela

Repozitorij uključuje recepte za optimizaciju:

#### Jezični modeli
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 serija (0.5B do 14B)
- **Google Gemma**: Razne konfiguracije Gemma modela
- **Mistral AI**: Mistral-7B serija
- **DeepSeek**: R1-Distill serija modela

#### Vizualni i multimodalni modeli
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP modeli**: Razne CLIP-ViT konfiguracije
- **ResNet**: Optimizacije za ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Specijalizirani modeli
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Osnovne i višejezične varijante
- **Sentence Transformers**: all-MiniLM-L6-v2

### Korištenje Olive recepata

#### Metoda 1: Kloniranje specifičnog recepta

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metoda 2: Korištenje recepta kao predloška

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Struktura recepta

Svaki direktorij recepta obično sadrži:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Primjer: Korištenje Phi-4-mini recepta

Koristimo Phi-4-mini recept kao primjer:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfiguracijska datoteka obično uključuje:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Prilagodba recepata

#### Modifikacija ciljanog hardvera

Za promjenu ciljanog hardvera, ažurirajte odjeljak `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Podešavanje parametara optimizacije

Modificirajte odjeljak `passes` za različite razine optimizacije:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Kreiranje vlastitog recepta

1. **Započnite sličnim modelom**: Pronađite recept za model sa sličnom arhitekturom
2. **Ažurirajte konfiguraciju modela**: Promijenite ime/putanju modela u konfiguraciji
3. **Prilagodite parametre**: Modificirajte parametre optimizacije prema potrebi
4. **Testirajte i validirajte**: Pokrenite optimizaciju i validirajte rezultate
5. **Doprinesite zajednici**: Razmislite o doprinosu vašeg recepta u repozitorij

### Prednosti korištenja recepata

#### 1. **Dokazane konfiguracije**
- Testirane postavke optimizacije za specifične modele
- Izbjegava pokušaje i pogreške u pronalaženju optimalnih parametara

#### 2. **Prilagodba hardveru**
- Predoptimizirano za različite izvršne providere
- Gotove konfiguracije za CPU, GPU i NPU ciljeve

#### 3. **Sveobuhvatna pokrivenost**
- Podržava najpopularnije open-source modele
- Redovita ažuriranja s novim izdanjima modela

#### 4. **Doprinos zajednice**
- Suradnički razvoj s AI zajednicom
- Dijeljenje znanja i najboljih praksi

### Doprinos Olive receptima

Ako ste optimizirali model koji nije pokriven u repozitoriju:

1. **Forkajte repozitorij**: Kreirajte vlastiti fork olive-recipes
2. **Kreirajte direktorij recepta**: Dodajte novi direktorij za vaš model
3. **Dodajte konfiguraciju**: Dodajte olive_config.json i prateće datoteke
4. **Dokumentirajte korištenje**: Osigurajte jasan README s uputama
5. **Pošaljite Pull Request**: Doprinesite zajednici

### Benchmark performansi

Mnogi recepti uključuju benchmark performansi koji prikazuju:
- **Poboljšanja kašnjenja**: Tipično ubrzanje od 2-6x u odnosu na osnovnu verziju
- **Smanjenje memorije**: 50-75% smanjenje korištenja memorije kvantizacijom
- **Očuvanje točnosti**: 95-99% očuvanje točnosti

### Integracija s AI alatima

Recepti rade besprijekorno s:
- **VS Code AI Toolkit**: Direktna integracija za optimizaciju modela
- **Azure Machine Learning**: Radni tokovi optimizacije u oblaku
- **ONNX Runtime**: Optimizirana implementacija inferencije

## Dodatni resursi

### Službeni linkovi
- **GitHub repozitorij**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repozitorij Olive Recipes**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime dokumentacija**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face primjer**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Primjeri iz zajednice
- **Jupyter Notebooks**: Dostupno u Olive GitHub repozitoriju — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code ekstenzija**: Pregled AI Toolkit za VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog postovi**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Povezani alati
- **ONNX Runtime**: Visokoučinkoviti inferencijski motor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Izvor mnogih kompatibilnih modela — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Radni tokovi optimizacije u oblaku — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Što slijedi

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane čovjeka. Ne preuzimamo odgovornost za nesporazume ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.