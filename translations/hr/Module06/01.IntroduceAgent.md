<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T14:52:27+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "hr"
}
-->
# AI agenti i mali jezični modeli: Sveobuhvatan vodič

## Uvod

U ovom vodiču istražit ćemo AI agente i male jezične modele (SLM) te njihove napredne strategije implementacije za okruženja rubnog računalstva. Pokrit ćemo osnovne koncepte agentne umjetne inteligencije, tehnike optimizacije SLM-a, praktične strategije implementacije za uređaje s ograničenim resursima te Microsoft Agent Framework za izgradnju sustava agenata spremnih za proizvodnju.

Krajolik umjetne inteligencije doživljava paradigmski pomak u 2025. Dok je 2023. bila godina chatbotova, a 2024. godina kopilota, 2025. pripada AI agentima — inteligentnim sustavima koji razmišljaju, planiraju, koriste alate i izvršavaju zadatke uz minimalan ljudski unos, sve više pokretani učinkovitim malim jezičnim modelima. Microsoft Agent Framework se ističe kao vodeće rješenje za izgradnju ovih inteligentnih sustava s offline mogućnostima za rubno računalstvo.

## Ciljevi učenja

Na kraju ovog vodiča moći ćete:

- 🤖 Razumjeti osnovne koncepte AI agenata i agentnih sustava
- 🔬 Identificirati prednosti malih jezičnih modela u odnosu na velike jezične modele u agentnim aplikacijama
- 🚀 Naučiti napredne strategije implementacije SLM-a za okruženja rubnog računalstva
- 📱 Implementirati praktične agente pokretane SLM-om za stvarne aplikacije
- 🏗️ Izgraditi agente spremne za proizvodnju koristeći Microsoft Agent Framework
- 🌐 Implementirati offline agente za rubno računalstvo s lokalnom integracijom LLM-a i SLM-a
- 🔧 Integrirati Microsoft Agent Framework s Foundry Local za implementaciju na rubu

## Razumijevanje AI agenata: Osnove i klasifikacije

### Definicija i osnovni koncepti

Umjetni inteligentni (AI) agent odnosi se na sustav ili program koji je sposoban autonomno obavljati zadatke u ime korisnika ili drugog sustava dizajnirajući svoj tijek rada i koristeći dostupne alate. Za razliku od tradicionalne AI koja samo odgovara na vaša pitanja, agent može djelovati neovisno kako bi postigao ciljeve.

### Okvir klasifikacije agenata

Razumijevanje granica agenata pomaže u odabiru odgovarajućih vrsta agenata za različite scenarije računalstva:

- **🔬 Jednostavni refleksni agenti**: Sustavi temeljeni na pravilima koji reagiraju na neposredne percepcije (termostati, osnovna automatizacija)
- **📱 Modelno utemeljeni agenti**: Sustavi koji održavaju unutarnje stanje i memoriju (robotski usisavači, navigacijski sustavi)
- **⚖️ Ciljno usmjereni agenti**: Sustavi koji planiraju i izvršavaju sekvence kako bi postigli ciljeve (planeri ruta, planeri zadataka)
- **🧠 Učeći agenti**: Adaptivni sustavi koji poboljšavaju performanse tijekom vremena (sustavi preporuka, personalizirani asistenti)

### Ključne prednosti AI agenata

AI agenti nude nekoliko osnovnih prednosti koje ih čine idealnim za aplikacije rubnog računalstva:

**Operativna autonomija**: Agenti omogućuju neovisno izvršavanje zadataka bez stalnog nadzora, što ih čini idealnim za aplikacije u stvarnom vremenu. Zahtijevaju minimalan nadzor uz održavanje adaptivnog ponašanja, omogućujući implementaciju na uređajima s ograničenim resursima uz smanjene operativne troškove.

**Fleksibilnost implementacije**: Ovi sustavi omogućuju AI sposobnosti na uređaju bez zahtjeva za internetskom povezanošću, poboljšavaju privatnost i sigurnost kroz lokalnu obradu, mogu se prilagoditi za aplikacije specifične za domenu i prikladni su za različita okruženja rubnog računalstva.

**Isplativost**: Sustavi agenata nude isplativu implementaciju u usporedbi s rješenjima temeljenim na oblaku, uz smanjene operativne troškove i niže zahtjeve za propusnošću za aplikacije na rubu.

## Napredne strategije za male jezične modele

### Osnove SLM-a (Small Language Model)

Mali jezični model (SLM) je jezični model koji može stati na uobičajeni potrošački elektronički uređaj i obavljati inferenciju s latencijom dovoljno niskom da bude praktičan za agentne zahtjeve jednog korisnika. U praktičnom smislu, SLM-ovi su obično modeli s manje od 10 milijardi parametara.

**Značajke otkrivanja formata**: SLM-ovi nude naprednu podršku za različite razine kvantizacije, kompatibilnost između platformi, optimizaciju performansi u stvarnom vremenu i mogućnosti implementacije na rubu. Korisnici mogu pristupiti poboljšanoj privatnosti kroz lokalnu obradu i podršku za WebGPU za implementaciju u preglednicima.

**Zbirke razina kvantizacije**: Popularni SLM formati uključuju Q4_K_M za uravnoteženu kompresiju u mobilnim aplikacijama, Q5_K_S seriju za implementaciju usmjerenu na kvalitetu na rubu, Q8_0 za gotovo originalnu preciznost na moćnim rubnim uređajima i eksperimentalne formate poput Q2_K za scenarije s ultra-niskim resursima.

### GGUF (General GGML Universal Format) za implementaciju SLM-a

GGUF služi kao primarni format za implementaciju kvantiziranih SLM-ova na CPU i rubnim uređajima, posebno optimiziran za agentne aplikacije:

**Značajke optimizirane za agente**: Format pruža sveobuhvatne resurse za konverziju i implementaciju SLM-a s poboljšanom podrškom za pozivanje alata, generiranje strukturiranih izlaza i višekratne razgovore. Kompatibilnost između platformi osigurava dosljedno ponašanje agenata na različitim rubnim uređajima.

**Optimizacija performansi**: GGUF omogućuje učinkovito korištenje memorije za tijekove rada agenata, podržava dinamičko učitavanje modela za sustave s više agenata i pruža optimiziranu inferenciju za interakcije agenata u stvarnom vremenu.

### Okviri optimizirani za SLM na rubu

#### Optimizacija Llama.cpp za agente

Llama.cpp pruža najnovije tehnike kvantizacije posebno optimizirane za implementaciju agentnih SLM-ova:

**Kvantizacija specifična za agente**: Okvir podržava Q4_0 (optimalno za mobilnu implementaciju agenata s 75% smanjenjem veličine), Q5_1 (uravnotežena kvaliteta-kompresija za inferenciju agenata na rubu) i Q8_0 (kvaliteta bliska originalu za proizvodne sustave agenata). Napredni formati omogućuju ultra-komprimirane agente za ekstremne scenarije na rubu.

**Prednosti implementacije**: Inferencija optimizirana za CPU s SIMD ubrzanjem omogućuje učinkovito izvršavanje agenata u memoriji. Kompatibilnost između platformi na x86, ARM i Apple Silicon arhitekturama omogućuje univerzalne mogućnosti implementacije agenata.

#### Apple MLX Framework za SLM agente

Apple MLX pruža nativnu optimizaciju posebno dizajniranu za agente pokretane SLM-om na uređajima s Apple Silicon čipovima:

**Optimizacija agenata za Apple Silicon**: Okvir koristi arhitekturu ujedinjene memorije s integracijom Metal Performance Shaders, automatsku mješovitu preciznost za inferenciju agenata i optimiziranu propusnost memorije za sustave s više agenata. SLM agenti pokazuju iznimne performanse na M-seriji čipova.

**Značajke razvoja**: Podrška za Python i Swift API s optimizacijama specifičnim za agente, automatska diferencijacija za učenje agenata i besprijekorna integracija s Apple alatima za razvoj pružaju sveobuhvatna okruženja za razvoj agenata.

#### ONNX Runtime za SLM agente na više platformi

ONNX Runtime pruža univerzalni motor za inferenciju koji omogućuje SLM agentima da dosljedno rade na različitim hardverskim platformama i operativnim sustavima:

**Univerzalna implementacija**: ONNX Runtime osigurava dosljedno ponašanje SLM agenata na Windows, Linux, macOS, iOS i Android platformama. Ova kompatibilnost između platformi omogućuje programerima da pišu jednom i implementiraju svugdje, značajno smanjujući troškove razvoja i održavanja za aplikacije na više platformi.

**Opcije hardverskog ubrzanja**: Okvir pruža optimizirane izvršne pružatelje usluga za različite hardverske konfiguracije uključujući CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) i specijalizirane akceleratore (Intel VPU, Qualcomm NPU). SLM agenti mogu automatski iskoristiti najbolje dostupne hardverske resurse bez promjena u kodu.

**Značajke spremne za proizvodnju**: ONNX Runtime nudi značajke na razini poduzeća koje su ključne za implementaciju agenata u proizvodnji, uključujući optimizaciju grafova za bržu inferenciju, upravljanje memorijom za okruženja s ograničenim resursima i sveobuhvatne alate za profiliranje za analizu performansi. Okvir podržava Python i C++ API-je za fleksibilnu integraciju.

## SLM vs LLM u agentnim sustavima: Napredna usporedba

### Prednosti SLM-a u agentnim aplikacijama

**Operativna učinkovitost**: SLM-ovi pružaju smanjenje troškova od 10-30× u usporedbi s LLM-ovima za zadatke agenata, omogućujući odgovore agenata u stvarnom vremenu na velikoj skali. Nude brže vrijeme inferencije zbog smanjene računalne složenosti, što ih čini idealnim za interaktivne aplikacije agenata.

**Mogućnosti implementacije na rubu**: SLM-ovi omogućuju izvršavanje agenata na uređaju bez ovisnosti o internetu, poboljšanu privatnost kroz lokalnu obradu agenata i prilagodbu za aplikacije specifične za domenu prikladne za različita okruženja rubnog računalstva.

**Optimizacija specifična za agente**: SLM-ovi se ističu u pozivanju alata, generiranju strukturiranih izlaza i rutinskim tijekovima odlučivanja koji čine 70-80% tipičnih zadataka agenata.

### Kada koristiti SLM-ove u odnosu na LLM-ove u sustavima agenata

**Idealno za SLM-ove**:
- **Ponavljajući zadaci agenata**: Unos podataka, popunjavanje obrazaca, rutinski API pozivi
- **Integracija alata**: Upiti u bazu podataka, operacije s datotekama, interakcije sa sustavom
- **Strukturirani tijekovi rada**: Slijeđenje unaprijed definiranih procesa agenata
- **Agenti specifični za domenu**: Korisnička podrška, zakazivanje, osnovna analiza
- **Lokalna obrada**: Operacije agenata osjetljive na privatnost

**Bolje za LLM-ove**:
- **Složeno razmišljanje**: Rješavanje novih problema, strateško planiranje
- **Razgovori otvorenog tipa**: Opći razgovori, kreativne rasprave
- **Zadaci širokog znanja**: Istraživanje koje zahtijeva opsežno opće znanje
- **Nove situacije**: Rukovanje potpuno novim scenarijima agenata

### Hibridna arhitektura agenata

Optimalan pristup kombinira SLM-ove i LLM-ove u heterogenim agentnim sustavima:

**Pametna orkestracija agenata**:
1. **SLM kao primarni**: Rješavanje 70-80% rutinskih zadataka agenata lokalno
2. **LLM po potrebi**: Usmjeravanje složenih upita na veće modele u oblaku
3. **Specijalizirani SLM-ovi**: Različiti mali modeli za različite domene agenata
4. **Optimizacija troškova**: Minimiziranje skupih poziva LLM-a kroz inteligentno usmjeravanje

## Strategije implementacije agenata pokretanih SLM-om

### Foundry Local: Okruženje za rubno AI računalstvo na razini poduzeća

Foundry Local (https://github.com/microsoft/foundry-local) služi kao vodeće rješenje Microsofta za implementaciju malih jezičnih modela u proizvodnim rubnim okruženjima. Pruža kompletno okruženje za pokretanje posebno dizajnirano za agente pokretane SLM-om s značajkama na razini poduzeća i besprijekornim mogućnostima integracije.

**Osnovna arhitektura i značajke**:
- **Kompatibilan s OpenAI API-jem**: Potpuna kompatibilnost s OpenAI SDK-om i integracijama Agent Frameworka
- **Automatska optimizacija hardvera**: Inteligentan odabir varijanti modela na temelju dostupnog hardvera (CUDA GPU, Qualcomm NPU, CPU)
- **Upravljanje modelima**: Automatsko preuzimanje, predmemoriranje i upravljanje životnim ciklusom SLM modela
- **Otkrivanje usluga**: Otkrivanje usluga bez konfiguracije za okvire agenata
- **Optimizacija resursa**: Inteligentno upravljanje memorijom i energetska učinkovitost za implementaciju na rubu

#### Instalacija i postavljanje

**Instalacija na više platformi**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Brzi početak za razvoj agenata**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integracija s Agent Frameworkom

**Integracija Foundry Local SDK-a**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automatski odabir modela i optimizacija hardvera**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Obrasci implementacije u proizvodnji

**Postavljanje jednog agenta u proizvodnji**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orkestracija više agenata u proizvodnji**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Značajke za poduzeća i praćenje

**Praćenje zdravlja i vidljivost**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Upravljanje resursima i automatsko skaliranje**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Napredna konfiguracija i optimizacija

**Prilagođena konfiguracija modela**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Kontrolni popis za implementaciju u proizvodnji**:

✅ **Konfiguracija usluge**:
- Konfigurirajte odgovarajuće alias modele za slučajeve upotrebe
- Postavite ograničenja resursa i pragove za praćenje
- Omogućite provjere zdravlja i prikupljanje metrika
- Konfigurirajte automatsko ponovno pokretanje i oporavak

✅ **Postavljanje sigurnosti**:
- Omogućite API pristup samo lokalno (bez vanjske izloženosti)
- Konfigurirajte odgovarajuće upravljanje API ključevima
- Postavite zapisivanje revizije za interakcije agenata
- Implementirajte ograničenje brzine za upotrebu u proizvodnji

✅ **Optimizacija performansi**:
- Testirajte performanse modela pod očekivanim opterećenjem
- Konfigurirajte odgovarajuće razine kvantizacije
- Postavite strategije predmemoriranja i zagrijavanja modela
- Pratite obrasce korištenja memorije i CPU-a

✅ **Testiranje integracije**:
- Testirajte integraciju s okvirom agenata
- Provjerite mogućnosti offline rada
- Testirajte scenarije oporavka nakon kvara
- Potvrdite tijekove rada agenata od početka do kraja

### Ollama: Pojednostavljena implementacija SLM agenata

### Ollama: Implementacija SLM agenata usmjerena na zajednicu

Ollama pruža pristup implementaciji SLM agenata vođen zajednicom s naglaskom na jednostavnost, opsežan ekosustav modela i radne procese prilagođene programerima. Dok se Foundry Local fokusira na značajke na razini poduzeća, Ollama se ističe u brzom prototipiranju, pristupu modelima zajednice i pojednostavljenim scenarijima implementacije.

**Osnovna arhitektura i značajke**:
- **Kompatibilan s OpenAI API-jem**: Potpuna kompatibilnost s REST API-jem za besprijekornu integraciju s okvirom agenata
- **Opsežna biblioteka modela**: Pristup stotinama modela koje je doprinijela zajednica i službenih modela
- **Jednostavno upravljanje modelima**: Instalacija i prebacivanje modela jednim naredbom
- **Podrška za više platformi**: Nativna podrška za Windows, macOS i Linux
- **Optimizacija resursa**
- Testirajte integraciju Microsoft Agent Frameworka  
- Provjerite mogućnosti rada u offline načinu  
- Testirajte scenarije prebacivanja i rukovanje greškama  
- Validirajte radne tokove agenata od početka do kraja  

**Usporedba s Foundry Local**:

| Značajka | Foundry Local | Ollama |
|----------|---------------|--------|
| **Ciljana upotreba** | Produkcija u poduzećima | Razvoj i zajednica |
| **Ekosustav modela** | Kurirano od strane Microsofta | Opsežna zajednica |
| **Optimizacija hardvera** | Automatska (CUDA/NPU/CPU) | Ručna konfiguracija |
| **Značajke za poduzeća** | Ugrađeno praćenje, sigurnost | Alati zajednice |
| **Složenost implementacije** | Jednostavna (winget instalacija) | Jednostavna (curl instalacija) |
| **Kompatibilnost API-ja** | OpenAI + proširenja | Standard OpenAI |
| **Podrška** | Službeni Microsoft | Vođena zajednicom |
| **Najbolje za** | Produkcijski agenti | Prototipiranje, istraživanje |

**Kada odabrati Ollama**:  
- **Razvoj i prototipiranje**: Brzo eksperimentiranje s različitim modelima  
- **Modeli zajednice**: Pristup najnovijim modelima koje doprinosi zajednica  
- **Obrazovna upotreba**: Učenje i podučavanje razvoja AI agenata  
- **Istraživački projekti**: Akademska istraživanja koja zahtijevaju raznolik pristup modelima  
- **Prilagođeni modeli**: Izrada i testiranje prilagođenih modela s finim podešavanjem  

### VLLM: Inference visokih performansi za SLM agente  

VLLM (Inference za vrlo velike jezične modele) pruža motor za inference visokog kapaciteta i učinkovite memorije, posebno optimiziran za produkcijske SLM implementacije u velikim razmjerima. Dok se Foundry Local fokusira na jednostavnost korištenja, a Ollama naglašava modele zajednice, VLLM briljira u scenarijima visokih performansi koji zahtijevaju maksimalni kapacitet i učinkovito korištenje resursa.  

**Osnovna arhitektura i značajke**:  
- **PagedAttention**: Revolucionarno upravljanje memorijom za učinkovito računanje pažnje  
- **Dinamičko grupiranje**: Inteligentno grupiranje zahtjeva za optimalni kapacitet  
- **Optimizacija GPU-a**: Napredni CUDA kernel i podrška za paralelizam tensorima  
- **Kompatibilnost s OpenAI**: Potpuna kompatibilnost API-ja za besprijekornu integraciju  
- **Spekulativno dekodiranje**: Napredne tehnike ubrzanja inferencea  
- **Podrška za kvantizaciju**: INT4, INT8 i FP16 kvantizacija za učinkovitost memorije  

#### Instalacija i postavljanje  

**Opcije instalacije**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Brzi početak za razvoj agenata**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### Integracija s Agent Frameworkom  

**VLLM s Microsoft Agent Frameworkom**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**Postavljanje više agenata visokog kapaciteta**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### Obrasci za produkcijsku implementaciju  

**Produkcijska usluga VLLM za poduzeća**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### Značajke za poduzeća i praćenje  

**Napredno praćenje performansi VLLM-a**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### Napredna konfiguracija i optimizacija  

**Predlošci konfiguracije za produkcijski VLLM**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Kontrolni popis za produkcijsku implementaciju VLLM-a**:  

✅ **Optimizacija hardvera**:  
- Konfigurirajte paralelizam tensorima za postavke s više GPU-a  
- Omogućite kvantizaciju (AWQ/GPTQ) za učinkovitost memorije  
- Postavite optimalno korištenje GPU memorije (85-95%)  
- Konfigurirajte odgovarajuće veličine grupa za kapacitet  

✅ **Podešavanje performansi**:  
- Omogućite predmemoriranje prefiksa za ponovljene upite  
- Konfigurirajte segmentirano popunjavanje za duge sekvence  
- Postavite spekulativno dekodiranje za brži inference  
- Optimizirajte max_num_seqs prema hardveru  

✅ **Značajke za produkciju**:  
- Postavite praćenje zdravlja i prikupljanje metrika  
- Konfigurirajte automatsko ponovno pokretanje i prebacivanje  
- Implementirajte redove zahtjeva i balansiranje opterećenja  
- Postavite sveobuhvatno bilježenje i upozorenja  

✅ **Sigurnost i pouzdanost**:  
- Konfigurirajte pravila vatrozida i kontrole pristupa  
- Postavite ograničenje brzine API-ja i autentifikaciju  
- Implementirajte postupno gašenje i čišćenje  
- Konfigurirajte sigurnosne kopije i oporavak od katastrofe  

✅ **Testiranje integracije**:  
- Testirajte integraciju Microsoft Agent Frameworka  
- Validirajte scenarije visokog kapaciteta  
- Testirajte postupke prebacivanja i oporavka  
- Benchmarkirajte performanse pod opterećenjem  

**Usporedba s drugim rješenjima**:

| Značajka | VLLM | Foundry Local | Ollama |
|----------|------|---------------|--------|
| **Ciljana upotreba** | Produkcija visokog kapaciteta | Jednostavnost za poduzeća | Razvoj i zajednica |
| **Performanse** | Maksimalni kapacitet | Uravnoteženo | Dobro |
| **Učinkovitost memorije** | Optimizacija PagedAttention | Automatska optimizacija | Standardno |
| **Složenost postavljanja** | Visoka (mnogi parametri) | Niska (automatski) | Niska (jednostavno) |
| **Skalabilnost** | Izvrsna (tensor/pipeline paralelizam) | Dobra | Ograničena |
| **Kvantizacija** | Napredna (AWQ, GPTQ, FP8) | Automatska | Standardna GGUF |
| **Značajke za poduzeća** | Potrebna prilagođena implementacija | Ugrađeno | Alati zajednice |
| **Najbolje za** | Agenti za produkciju velikih razmjera | Produkcija u poduzećima | Razvoj |

**Kada odabrati VLLM**:  
- **Zahtjevi visokog kapaciteta**: Obrada stotina zahtjeva u sekundi  
- **Implementacije velikih razmjera**: Postavke s više GPU-a i čvorova  
- **Kritične performanse**: Odgovori u manje od sekunde u velikim razmjerima  
- **Napredna optimizacija**: Potreba za prilagođenom kvantizacijom i grupiranjem  
- **Učinkovitost resursa**: Maksimalno korištenje skupog GPU hardvera  

## Primjene SLM agenata u stvarnom svijetu  

### SLM agenti za korisničku podršku  
- **SLM sposobnosti**: Provjera računa, resetiranje lozinki, provjera statusa narudžbi  
- **Troškovne prednosti**: 10x smanjenje troškova inferencea u usporedbi s LLM agentima  
- **Performanse**: Brži odgovori uz dosljednu kvalitetu za rutinske upite  

### SLM agenti za poslovne procese  
- **Agenti za obradu faktura**: Ekstrakcija podataka, validacija informacija, prosljeđivanje na odobrenje  
- **Agenti za upravljanje e-poštom**: Kategorizacija, prioritizacija, automatsko sastavljanje odgovora  
- **Agenti za zakazivanje**: Koordinacija sastanaka, upravljanje kalendarima, slanje podsjetnika  

### Osobni digitalni asistenti SLM  
- **Agenti za upravljanje zadacima**: Stvaranje, ažuriranje, organizacija popisa zadataka  
- **Agenti za prikupljanje informacija**: Istraživanje tema, lokalno sažimanje nalaza  
- **Agenti za komunikaciju**: Sastavljanje e-pošte, poruka, privatnih objava na društvenim mrežama  

### SLM agenti za trgovanje i financije  
- **Agenti za praćenje tržišta**: Praćenje cijena, identifikacija trendova u stvarnom vremenu  
- **Agenti za generiranje izvještaja**: Automatsko stvaranje dnevnih/tjednih sažetaka  
- **Agenti za procjenu rizika**: Procjena pozicija portfelja koristeći lokalne podatke  

### SLM agenti za podršku u zdravstvu  
- **Agenti za zakazivanje pacijenata**: Koordinacija termina, slanje automatskih podsjetnika  
- **Agenti za dokumentaciju**: Generiranje medicinskih sažetaka, lokalnih izvještaja  
- **Agenti za upravljanje receptima**: Praćenje obnavljanja, provjera interakcija privatno  

## Microsoft Agent Framework: Razvoj agenata spremnih za produkciju  

### Pregled i arhitektura  

Microsoft Agent Framework pruža sveobuhvatnu, profesionalnu platformu za izgradnju, implementaciju i upravljanje AI agentima koji mogu raditi i u oblaku i u offline okruženjima. Okvir je posebno dizajniran za besprijekoran rad s malim jezičnim modelima i scenarijima edge računalstva, što ga čini idealnim za implementacije osjetljive na privatnost i ograničene resurse.  

**Osnovne komponente okvira**:  
- **Agent Runtime**: Lagano okruženje za izvršavanje optimizirano za edge uređaje  
- **Sustav za integraciju alata**: Proširiva arhitektura dodataka za povezivanje vanjskih usluga i API-ja  
- **Upravljanje stanjem**: Trajna memorija agenata i rukovanje kontekstom kroz sesije  
- **Sigurnosni sloj**: Ugrađene sigurnosne kontrole za implementaciju u poduzećima  
- **Orkestracijski motor**: Koordinacija više agenata i upravljanje radnim tokovima  

### Ključne značajke za edge implementaciju  

**Arhitektura usmjerena na offline rad**: Microsoft Agent Framework dizajniran je prema principima offline rada, omogućujući agentima učinkovito funkcioniranje bez stalne internetske povezanosti. To uključuje lokalni inference modela, predmemorirane baze znanja, offline izvršavanje alata i postupno smanjenje funkcionalnosti kada oblačne usluge nisu dostupne.  

**Optimizacija resursa**: Okvir pruža inteligentno upravljanje resursima s automatskom optimizacijom memorije za SLM-ove, balansiranjem opterećenja CPU/GPU za edge uređaje, adaptivnim odabirom modela na temelju dostupnih resursa i energetski učinkovitim obrascima inferencea za mobilnu implementaciju.  

**Sigurnost i privatnost**: Sigurnosne značajke na razini poduzeća uključuju lokalnu obradu podataka radi očuvanja privatnosti, šifrirane komunikacijske kanale agenata, kontrole pristupa temeljene na ulogama za sposobnosti agenata i zapisivanje radnji za potrebe usklađenosti.  

### Integracija s Foundry Local  

Microsoft Agent Framework besprijekorno se integrira s Foundry Local kako bi pružio cjelovito edge AI rješenje:  

**Automatsko otkrivanje modela**: Okvir automatski otkriva i povezuje se s instancama Foundry Local, otkriva dostupne SLM modele i odabire optimalne modele na temelju zahtjeva agenata i mogućnosti hardvera.  

**Dinamičko učitavanje modela**: Agenti mogu dinamički učitavati različite SLM-ove za specifične zadatke, omogućujući sustave agenata s više modela gdje različiti modeli obrađuju različite vrste zahtjeva, te automatsko prebacivanje između modela na temelju dostupnosti i performansi.  

**Optimizacija performansi**: Integrirani mehanizmi predmemoriranja smanjuju vrijeme učitavanja modela, grupiranje veza optimizira API pozive prema Foundry Localu, a inteligentno grupiranje poboljšava kapacitet za više zahtjeva agenata.  

### Izrada agenata s Microsoft Agent Frameworkom  

#### Definicija i konfiguracija agenata  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### Integracija alata za edge scenarije  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### Orkestracija više agenata  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### Napredni obrasci implementacije na edge uređajima  

#### Hijerarhijska arhitektura agenata  

**Lokalni klasteri agenata**: Implementirajte više specijaliziranih SLM agenata na edge uređajima, svaki optimiziran za specifične zadatke. Koristite lagane modele poput Qwen2.5-0.5B za jednostavno usmjeravanje i zakazivanje, srednje modele poput Phi-4-Mini za korisničku podršku i dokumentaciju, te veće modele za složeno zaključivanje kada resursi to dopuštaju.  

**Koordinacija edge-to-cloud**: Implementirajte inteligentne obrasce eskalacije gdje lokalni agenti obrađuju rutinske zadatke, oblačni agenti pružaju složeno zaključivanje kada je povezanost omogućena, a besprijekorni prijenos između edge i oblačne obrade održava kontinuitet.  

#### Konfiguracije implementacije  

**Implementacija na jednom uređaju**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**Distribuirana implementacija na edge uređajima**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### Optimizacija performansi za edge agente  

#### Strategije odabira modela  

**Dodjela modela prema zadatku**: Microsoft Agent Framework omogućuje inteligentan odabir modela na temelju složenosti zadatka i zahtjeva:  

- **Jednostavni zadaci** (Q&A, usmjeravanje): Qwen2.5-0.5B (500MB, <100ms odgovor)  
- **Umjereni zadaci** (korisnička podrška, zakazivanje): Phi-4-Mini (2.4GB, 200-500ms odgovor)  
- **Složeni zadaci** (tehnička analiza, planiranje): Phi-4 (7GB, 1-3s odgovor kada resursi dopuštaju)  

**Dinamičko prebacivanje modela**: Agenti mogu prebacivati između modela na temelju trenutnog opterećenja sustava, procjene složenosti zadatka, razine prioriteta korisnika i dostupnih hardverskih resursa.  

#### Upravljanje memorijom i resursima  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  

### Obrasci integracije za poduzeća  

#### Sigurnost i usklađenost  

**Lokalna obrada podataka**: Sva obrada agenata odvija se lokalno, osiguravajući da osjetljivi podaci nikada ne napuštaju edge uređaj. To uključuje zaštitu informacija o korisnicima, usklađenost s HIPAA za zdravstvene agente, sigurnost financijskih podataka za bankovne agente i usklađenost s GDPR-om za europske implementacije.  

**Kontrola pristupa**: Dozvole temeljene na ulogama kontroliraju koje alate agenti mogu koristiti, autentifikacija korisnika za interakcije s agentima i zapisivanje svih radnji i odluka agenata.  

#### Praćenje i preglednost  

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```
  

### Primjeri implementacije u stvarnom svijetu  

#### Sustav agenata za maloprodaju na edge uređajima  

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```
  
#### Agent za podršku u zdravstvu  

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```
  

### Najbolje prakse za Microsoft Agent Framework  

#### Smjernice za razvoj  

1. **Počnite jednostavno**: Započnite s scenarijima jednog agenta prije nego što izgradite složene sustave s više agenata  
2. **Prilagodba modela**: Odaberite najmanji model koji zadovoljava vaše zahtjeve za točnost  
3. **Dizajn alata**: Kreirajte fokusirane, jednostrane alate umjesto složenih alata s više funkcija  
4. **Rukovanje greškama**: Implementirajte postupno smanjenje funkcionalnosti za offline scenarije i kvarove modela  
5. **Testiranje**: Temeljito testirajte agente u offline uvjetima i okruženjima s ograničenim resursima  

#### Najbolje prakse za implementaciju  

1. **Postupno uvođenje**: Prvo implementirajte za male grupe korisnika, pažljivo pratite metrike performansi  
2. **Praćenje resursa**: Postavite upozorenja za pragove memorije, CPU-a i vremena odgovora  
3. **Strategije povratka**: Uvijek imajte rezervne planove za kvarove modela ili iscrpljenje resursa  
4. **Sigurnost na prvom mjestu**: Implementirajte sigurnosne kontrole od početka, a ne naknadno  
5. **Dokumentacija**: Održ
**Odabir okvira za implementaciju agenata**: Odaberite okvire za optimizaciju na temelju ciljanog hardvera i zahtjeva agenata. Koristite Llama.cpp za implementaciju agenata optimiziranih za CPU, Apple MLX za aplikacije agenata na Apple Siliconu i ONNX za kompatibilnost agenata na više platformi.

## Praktična konverzija SLM agenata i primjeri upotrebe

### Scenariji implementacije agenata u stvarnom svijetu

**Mobilne aplikacije agenata**: Q4_K formati izvrsno funkcioniraju u aplikacijama za pametne telefone s minimalnim zahtjevima za memorijom, dok Q8_0 pruža uravnotežene performanse za sustave agenata na tabletima. Q5_K formati nude vrhunsku kvalitetu za mobilne produktivne agente.

**Računalstvo agenata na stolnim računalima i rubnim uređajima**: Q5_K pruža optimalne performanse za aplikacije agenata na stolnim računalima, Q8_0 osigurava visoku kvalitetu zaključivanja za radne stanice, a Q4_K omogućuje učinkovitu obradu na rubnim uređajima.

**Istraživački i eksperimentalni agenti**: Napredni formati kvantizacije omogućuju istraživanje ultra-niske preciznosti zaključivanja agenata za akademska istraživanja i aplikacije dokazivanja koncepta koje zahtijevaju ekstremna ograničenja resursa.

### Benchmarking performansi SLM agenata

**Brzina zaključivanja agenata**: Q4_K postiže najbrže vrijeme odgovora agenata na mobilnim CPU-ima, Q5_K pruža uravnotežen omjer brzine i kvalitete za opće aplikacije agenata, Q8_0 nudi vrhunsku kvalitetu za složene zadatke agenata, a eksperimentalni formati omogućuju maksimalan protok za specijalizirani hardver agenata.

**Zahtjevi za memorijom agenata**: Razine kvantizacije za agente kreću se od Q2_K (manje od 500MB za male modele agenata) do Q8_0 (otprilike 50% izvorne veličine), dok eksperimentalne konfiguracije postižu maksimalnu kompresiju za okruženja s ograničenim resursima.

## Izazovi i razmatranja za SLM agente

### Kompromisi performansi u sustavima agenata

Implementacija SLM agenata zahtijeva pažljivo razmatranje kompromisa između veličine modela, brzine odgovora agenata i kvalitete izlaza. Dok Q4_K nudi iznimnu brzinu i učinkovitost za mobilne agente, Q8_0 pruža vrhunsku kvalitetu za složene zadatke agenata. Q5_K predstavlja sredinu koja je prikladna za većinu općih aplikacija agenata.

### Kompatibilnost hardvera za SLM agente

Različiti rubni uređaji imaju različite mogućnosti za implementaciju SLM agenata. Q4_K učinkovito radi na osnovnim procesorima za jednostavne agente, Q5_K zahtijeva umjerene računalne resurse za uravnotežene performanse agenata, a Q8_0 koristi prednosti vrhunskog hardvera za napredne mogućnosti agenata.

### Sigurnost i privatnost u sustavima SLM agenata

Iako SLM agenti omogućuju lokalnu obradu za poboljšanu privatnost, potrebno je implementirati odgovarajuće sigurnosne mjere za zaštitu modela agenata i podataka u rubnim okruženjima. To je posebno važno pri implementaciji formata agenata visoke preciznosti u poslovnim okruženjima ili komprimiranih formata agenata u aplikacijama koje obrađuju osjetljive podatke.

## Budući trendovi u razvoju SLM agenata

Landskap SLM agenata nastavlja se razvijati s napretkom u tehnikama kompresije, metodama optimizacije i strategijama implementacije na rubnim uređajima. Budući razvoj uključuje učinkovitije algoritme kvantizacije za modele agenata, poboljšane metode kompresije za radne procese agenata i bolju integraciju s hardverskim akceleratorima za obradu agenata na rubnim uređajima.

**Predviđanja tržišta za SLM agente**: Prema nedavnim istraživanjima, automatizacija vođena agentima mogla bi eliminirati 40–60% ponavljajućih kognitivnih zadataka u poslovnim radnim procesima do 2027. godine, pri čemu SLM-ovi predvode ovu transformaciju zbog svoje ekonomičnosti i fleksibilnosti implementacije.

**Tehnološki trendovi u SLM agentima**:
- **Specijalizirani SLM agenti**: Modeli specifični za domenu, obučeni za određene zadatke agenata i industrije
- **Računalstvo agenata na rubnim uređajima**: Poboljšane mogućnosti agenata na uređajima s poboljšanom privatnošću i smanjenom latencijom
- **Orkestracija agenata**: Bolja koordinacija između više SLM agenata s dinamičkim usmjeravanjem i balansiranjem opterećenja
- **Demokratizacija**: Fleksibilnost SLM-ova omogućuje širu participaciju u razvoju agenata među organizacijama

## Početak rada sa SLM agentima

### Korak 1: Postavljanje okruženja Microsoft Agent Framework

**Instalirajte ovisnosti**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicijalizirajte Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Korak 2: Odaberite svoj SLM za aplikacije agenata
Popularne opcije za Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Izvrsno za opće zadatke agenata s uravnoteženim performansama
- **Qwen2.5-0.5B (0.5B)**: Ultra-učinkovit za jednostavne agente za usmjeravanje i klasifikaciju
- **Qwen2.5-Coder-0.5B (0.5B)**: Specijaliziran za zadatke agenata vezane uz kodiranje
- **Phi-4 (7B)**: Napredno zaključivanje za složene scenarije na rubnim uređajima kada resursi dopuštaju

### Korak 3: Kreirajte svog prvog agenta s Microsoft Agent Framework

**Osnovno postavljanje agenata**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Korak 4: Definirajte opseg i zahtjeve agenata
Započnite s fokusiranim, dobro definiranim aplikacijama agenata koristeći Microsoft Agent Framework:
- **Agenti za jednu domenu**: Korisnička podrška ILI zakazivanje ILI istraživanje
- **Jasni ciljevi agenata**: Specifični, mjerljivi ciljevi za performanse agenata
- **Ograničena integracija alata**: Maksimalno 3-5 alata za početnu implementaciju agenata
- **Definirane granice agenata**: Jasni putevi eskalacije za složene scenarije
- **Dizajn usmjeren na rubne uređaje**: Prioritet offline funkcionalnosti i lokalne obrade

### Korak 5: Implementirajte implementaciju na rubnim uređajima s Microsoft Agent Framework

**Konfiguracija resursa**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Implementirajte sigurnosne mjere za rubne agente**:
- **Lokalna validacija unosa**: Provjerite zahtjeve bez ovisnosti o oblaku
- **Offline filtriranje izlaza**: Osigurajte da odgovori zadovoljavaju lokalne standarde kvalitete
- **Sigurnosne kontrole na rubnim uređajima**: Implementirajte sigurnost bez potrebe za internetskom povezivošću
- **Lokalno praćenje**: Pratite performanse i označite probleme koristeći telemetriju na rubnim uređajima

### Korak 6: Mjerite i optimizirajte performanse agenata na rubnim uređajima
- **Stope završetka zadataka agenata**: Pratite stope uspjeha u offline scenarijima
- **Vrijeme odgovora agenata**: Osigurajte vrijeme odgovora ispod jedne sekunde za implementaciju na rubnim uređajima
- **Iskorištenost resursa**: Pratite memoriju, CPU i potrošnju baterije na rubnim uređajima
- **Ekonomičnost**: Usporedite troškove implementacije na rubnim uređajima s alternativama temeljenim na oblaku
- **Pouzdanost offline rada**: Mjerite performanse agenata tijekom prekida mreže

## Ključni zaključci za implementaciju SLM agenata

1. **SLM-ovi su dovoljni za agente**: Za većinu zadataka agenata, mali modeli funkcioniraju jednako dobro kao veliki, uz značajne prednosti
2. **Ekonomičnost agenata**: 10-30x jeftinije za pokretanje SLM agenata, što ih čini ekonomično održivima za široku implementaciju
3. **Specijalizacija funkcionira za agente**: Fino podešeni SLM-ovi često nadmašuju opće LLM-ove u specifičnim aplikacijama agenata
4. **Hibridna arhitektura agenata**: Koristite SLM-ove za rutinske zadatke agenata, LLM-ove za složeno zaključivanje kada je potrebno
5. **Microsoft Agent Framework omogućuje produkcijsku implementaciju**: Pruža alate poslovne klase za izgradnju, implementaciju i upravljanje agentima na rubnim uređajima
6. **Principi dizajna usmjereni na rubne uređaje**: Agenti sposobni za offline rad s lokalnom obradom osiguravaju privatnost i pouzdanost
7. **Integracija Foundry Local**: Besprijekorna povezanost između Microsoft Agent Frameworka i lokalnog zaključivanja modela
8. **Budućnost su SLM agenti**: Mali jezični modeli s produkcijskim okvirima predstavljaju budućnost agentičke AI, omogućujući demokratiziranu i učinkovitu implementaciju agenata

## Reference i dodatna literatura

### Osnovni istraživački radovi i publikacije

#### AI agenti i agentički sustavi
- **"Language Agents as Optimizable Graphs"** (2024) - Temeljno istraživanje o arhitekturi agenata i strategijama optimizacije
  - Autori: Wenyue Hua, Lishan Yang, et al.
  - Link: https://arxiv.org/abs/2402.16823
  - Ključni uvidi: Dizajn agenata temeljen na grafovima i strategije optimizacije

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Autori: Zhiheng Xi, Wenxiang Chen, et al.
  - Link: https://arxiv.org/abs/2309.07864
  - Ključni uvidi: Sveobuhvatni pregled sposobnosti i aplikacija agenata temeljenih na LLM-ovima

- **"Cognitive Architectures for Language Agents"** (2024)
  - Autori: Theodore Sumers, Shunyu Yao, et al.
  - Link: https://arxiv.org/abs/2309.02427
  - Ključni uvidi: Kognitivni okviri za dizajn inteligentnih agenata

#### Mali jezični modeli i optimizacija
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Autori: Microsoft Research Team
  - Link: https://arxiv.org/abs/2404.14219
  - Ključni uvidi: Principi dizajna SLM-ova i strategije implementacije na mobilnim uređajima

- **"Qwen2.5 Technical Report"** (2024)
  - Autori: Alibaba Cloud Team
  - Link: https://arxiv.org/abs/2407.10671
  - Ključni uvidi: Napredne tehnike obuke SLM-ova i optimizacija performansi

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Autori: Peiyuan Zhang, Guangtao Zeng, et al.
  - Link: https://arxiv.org/abs/2401.02385
  - Ključni uvidi: Dizajn ultra-kompaktnih modela i učinkovitost obuke

### Službena dokumentacija i okviri

#### Microsoft Agent Framework
- **Službena dokumentacija**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub repozitorij**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Primarni repozitorij**: https://github.com/microsoft/foundry-local
- **Dokumentacija**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Glavni repozitorij**: https://github.com/vllm-project/vllm
- **Dokumentacija**: https://docs.vllm.ai/


#### Ollama
- **Službena web stranica**: https://ollama.ai/
- **GitHub repozitorij**: https://github.com/ollama/ollama

### Okviri za optimizaciju modela

#### Llama.cpp
- **Repozitorij**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentacija**: https://microsoft.github.io/Olive/
- **GitHub repozitorij**: https://github.com/microsoft/Olive

#### OpenVINO
- **Službena stranica**: https://docs.openvino.ai/

#### Apple MLX
- **Repozitorij**: https://github.com/ml-explore/mlx

### Industrijski izvještaji i analiza tržišta

#### Istraživanje tržišta AI agenata
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Link: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Ključni uvidi: Trendovi tržišta i obrasci usvajanja u poslovnom okruženju

#### Tehnički benchmarkovi

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Link: https://mlcommons.org/en/inference-edge/
  - Ključni uvidi: Standardizirani metrički podaci za implementaciju na rubnim uređajima

### Standardi i specifikacije

#### Formati modela i standardi
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Format modela za interoperabilnost na više platformi
- **GGUF specifikacija**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantizirani format modela za zaključivanje na CPU-u
- **OpenAI API specifikacija**: https://platform.openai.com/docs/api-reference
  - Standardni API format za integraciju jezičnih modela

#### Sigurnost i usklađenost
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI sustavi**: Okvir za AI sustave i sigurnost
- **IEEE standardi za AI**: https://standards.ieee.org/industry-connections/ai/

Pomak prema agentima vođenim SLM-ovima predstavlja temeljnu promjenu u pristupu implementaciji AI-a. Microsoft Agent Framework, u kombinaciji s lokalnim platformama i učinkovitim malim jezičnim modelima, pruža cjelovito rješenje za izgradnju agenata spremnih za produkciju koji učinkovito djeluju u rubnim okruženjima. Fokusiranjem na učinkovitost, specijalizaciju i praktičnu upotrebu, ovaj tehnološki paket čini AI agente dostupnijima, povoljnijima i učinkovitijima za stvarne aplikacije u svim industrijama i okruženjima rubnog računalstva.

Kako napredujemo kroz 2025. godinu, kombinacija sve sposobnijih malih modela, sofisticiranih okvira agenata poput Microsoft Agent Frameworka i robusnih platformi za implementaciju na rubnim uređajima otključat će nove mogućnosti za autonomne sustave koji mogu učinkovito djelovati na rubnim uređajima uz održavanje privatnosti, smanjenje troškova i pružanje izvanrednih korisničkih iskustava.

**Sljedeći koraci za implementaciju**:
1. **Istražite pozivanje funkcija**: Naučite kako SLM-ovi upravljaju integracijom alata i strukturiranim izlazima
2. **Savladajte Model Context Protocol (MCP)**: Razumite napredne obrasce komunikacije agenata
3. **Izgradite produkcijske agente**: Koristite Microsoft Agent Framework za implementacije poslovne klase
4. **Optimizirajte za rubne uređaje**: Primijenite napredne tehnike optimizacije za okruženja s ograničenim resursima


## ➡️ Što slijedi

- [02: Pozivanje funkcija u malim jezičnim modelima (SLM-ovima)](./02.FunctionCalling.md)

---

**Izjava o odricanju odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za nesporazume ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.