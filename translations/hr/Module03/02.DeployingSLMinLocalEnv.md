<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T10:03:37+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "hr"
}
-->
# Poglavlje 2: Implementacija u lokalnom okruženju - Rješenja koja štite privatnost

Lokalna implementacija malih jezičnih modela (SLM) predstavlja promjenu paradigme prema AI rješenjima koja štite privatnost i smanjuju troškove. Ovaj sveobuhvatni vodič istražuje dva moćna okvira—Ollama i Microsoft Foundry Local—koji omogućuju programerima da iskoriste puni potencijal SLM-ova uz zadržavanje potpune kontrole nad svojim okruženjem za implementaciju.

## Uvod

U ovoj lekciji istražit ćemo napredne strategije implementacije malih jezičnih modela u lokalnim okruženjima. Pokrit ćemo osnovne koncepte lokalne AI implementacije, analizirati dvije vodeće platforme (Ollama i Microsoft Foundry Local) te pružiti praktične smjernice za implementaciju rješenja spremnih za produkciju.

## Ciljevi učenja

Na kraju ove lekcije moći ćete:

- Razumjeti arhitekturu i prednosti okvira za lokalnu implementaciju SLM-ova.
- Implementirati rješenja spremna za produkciju koristeći Ollama i Microsoft Foundry Local.
- Usporediti i odabrati odgovarajuću platformu na temelju specifičnih zahtjeva i ograničenja.
- Optimizirati lokalne implementacije za performanse, sigurnost i skalabilnost.

## Razumijevanje arhitektura lokalne implementacije SLM-ova

Lokalna implementacija SLM-ova predstavlja temeljni pomak od AI usluga ovisnih o oblaku prema rješenjima koja štite privatnost i omogućuju rad na vlastitoj infrastrukturi. Ovaj pristup omogućuje organizacijama da zadrže potpunu kontrolu nad svojom AI infrastrukturom uz osiguranje suvereniteta podataka i operativne neovisnosti.

### Klasifikacija okvira za implementaciju

Razumijevanje različitih pristupa implementaciji pomaže u odabiru prave strategije za specifične slučajeve upotrebe:

- **Usmjereno na razvoj**: Pojednostavljena postavka za eksperimentiranje i prototipiranje
- **Razina poduzeća**: Rješenja spremna za produkciju s mogućnostima integracije u poduzeće  
- **Međusustavno**: Univerzalna kompatibilnost s različitim operativnim sustavima i hardverom

### Ključne prednosti lokalne implementacije SLM-ova

Lokalna implementacija SLM-ova nudi nekoliko temeljnih prednosti koje je čine idealnom za aplikacije osjetljive na privatnost i potrebe poduzeća:

**Privatnost i sigurnost**: Lokalna obrada osigurava da osjetljivi podaci nikada ne napuštaju infrastrukturu organizacije, omogućujući usklađenost s GDPR-om, HIPAA-om i drugim regulatornim zahtjevima. Implementacije u izoliranim mrežama moguće su za klasificirana okruženja, dok potpuni tragovi revizije omogućuju nadzor sigurnosti.

**Isplativost**: Eliminacija modela naplate po tokenu značajno smanjuje operativne troškove. Niži zahtjevi za propusnost i smanjena ovisnost o oblaku omogućuju predvidive strukture troškova za proračune poduzeća.

**Performanse i pouzdanost**: Brže vrijeme izvođenja bez mrežne latencije omogućuje aplikacije u stvarnom vremenu. Funkcionalnost izvan mreže osigurava kontinuirani rad bez obzira na internetsku povezanost, dok optimizacija lokalnih resursa pruža dosljedne performanse.

## Ollama: Univerzalna platforma za lokalnu implementaciju

### Osnovna arhitektura i filozofija

Ollama je dizajnirana kao univerzalna, programerima prilagođena platforma koja demokratizira lokalnu implementaciju LLM-ova na raznim hardverskim konfiguracijama i operativnim sustavima.

**Tehnička osnova**: Izgrađena na robusnom okviru llama.cpp, Ollama koristi učinkovit GGUF format modela za optimalne performanse. Kompatibilnost između platformi osigurava dosljedno ponašanje na Windows, macOS i Linux okruženjima, dok inteligentno upravljanje resursima optimizira korištenje CPU-a, GPU-a i memorije.

**Filozofija dizajna**: Ollama daje prednost jednostavnosti bez žrtvovanja funkcionalnosti, nudeći implementaciju bez konfiguracije za trenutnu produktivnost. Platforma održava široku kompatibilnost modela uz pružanje dosljednih API-ja za različite arhitekture modela.

### Napredne značajke i mogućnosti

**Izvrsno upravljanje modelima**: Ollama pruža sveobuhvatno upravljanje životnim ciklusom modela s automatskim preuzimanjem, predmemoriranjem i verzioniranjem. Platforma podržava opsežan ekosustav modela uključujući Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral i specijalizirane modele za ugrađivanje.

**Prilagodba putem Modelfiles**: Napredni korisnici mogu kreirati prilagođene konfiguracije modela sa specifičnim parametrima, sistemskim upitima i modifikacijama ponašanja. To omogućuje optimizacije specifične za domenu i specijalizirane zahtjeve aplikacija.

**Optimizacija performansi**: Ollama automatski otkriva i koristi dostupno ubrzanje hardvera uključujući NVIDIA CUDA, Apple Metal i OpenCL. Inteligentno upravljanje memorijom osigurava optimalno korištenje resursa na različitim hardverskim konfiguracijama.

### Strategije implementacije u produkciji

**Instalacija i postavka**: Ollama omogućuje pojednostavljenu instalaciju na različitim platformama putem nativnih instalacijskih programa, upravitelja paketa (WinGet, Homebrew, APT) i Docker kontejnera za kontejnerizirane implementacije.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Osnovne naredbe i operacije**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Napredna konfiguracija**: Modelfiles omogućuju sofisticiranu prilagodbu za zahtjeve poduzeća:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Primjeri integracije za programere

**Integracija Python API-ja**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Integracija JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Korištenje RESTful API-ja s cURL-om**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Podešavanje performansi i optimizacija

**Konfiguracija memorije i niti**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Odabir kvantizacije za različiti hardver**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Platforma za AI na rubu poduzeća

### Arhitektura razine poduzeća

Microsoft Foundry Local predstavlja sveobuhvatno rješenje za poduzeća dizajnirano posebno za produkcijske AI implementacije na rubu s dubokom integracijom u Microsoftov ekosustav.

**Osnova temeljena na ONNX-u**: Izgrađena na industrijskom standardu ONNX Runtime, Foundry Local pruža optimizirane performanse na raznim hardverskim arhitekturama. Platforma koristi integraciju Windows ML-a za nativnu optimizaciju na Windowsu uz održavanje kompatibilnosti između platformi.

**Izvrsno ubrzanje hardvera**: Foundry Local ima inteligentno otkrivanje i optimizaciju hardvera na CPU-ima, GPU-ima i NPU-ima. Duboka suradnja s proizvođačima hardvera (AMD, Intel, NVIDIA, Qualcomm) osigurava optimalne performanse na konfiguracijama hardvera za poduzeća.

### Napredno iskustvo za programere

**Pristup putem više sučelja**: Foundry Local pruža sveobuhvatna razvojna sučelja uključujući moćan CLI za upravljanje modelima i implementaciju, SDK-ove za više jezika (Python, NodeJS) za nativnu integraciju i RESTful API-je s kompatibilnošću s OpenAI-jem za jednostavnu migraciju.

**Integracija s Visual Studiom**: Platforma se besprijekorno integrira s AI Toolkitom za VS Code, pružajući alate za konverziju modela, kvantizaciju i optimizaciju unutar razvojnog okruženja. Ova integracija ubrzava razvojne tijekove rada i smanjuje složenost implementacije.

**Cjevovod za optimizaciju modela**: Integracija s Microsoft Olive omogućuje sofisticirane tijekove rada za optimizaciju modela, uključujući dinamičku kvantizaciju, optimizaciju grafova i podešavanje specifično za hardver. Mogućnosti konverzije putem oblaka kroz Azure ML pružaju skalabilnu optimizaciju za velike modele.

### Strategije implementacije u produkciji

**Instalacija i konfiguracija**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Operacije upravljanja modelima**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Napredna konfiguracija implementacije**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Integracija u ekosustav poduzeća

**Sigurnost i usklađenost**: Foundry Local pruža sigurnosne značajke razine poduzeća uključujući kontrolu pristupa temeljenog na ulogama, zapisivanje revizije, izvješćivanje o usklađenosti i šifrirano pohranjivanje modela. Integracija s Microsoftovom sigurnosnom infrastrukturom osigurava pridržavanje sigurnosnih politika poduzeća.

**Ugrađene AI usluge**: Platforma nudi gotove AI mogućnosti uključujući Phi Silica za lokalnu obradu jezika, AI Imaging za poboljšanje i analizu slika te specijalizirane API-je za uobičajene AI zadatke u poduzeću.

## Komparativna analiza: Ollama vs Foundry Local

### Usporedba tehničke arhitekture

| **Aspekt** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Format modela** | GGUF (putem llama.cpp) | ONNX (putem ONNX Runtime) |
| **Fokus platforme** | Univerzalna kompatibilnost | Optimizacija za Windows/poduzeća |
| **Integracija hardvera** | Opća podrška za GPU/CPU | Duboka podrška za Windows ML, NPU |
| **Optimizacija** | Kvantizacija putem llama.cpp | Microsoft Olive + ONNX Runtime |
| **Značajke za poduzeća** | Vođeno zajednicom | Razina poduzeća s SLA-ovima |

### Karakteristike performansi

**Prednosti performansi Ollame**:
- Izvrsne performanse CPU-a zahvaljujući optimizaciji llama.cpp
- Dosljedno ponašanje na različitim platformama i hardveru
- Učinkovito korištenje memorije uz inteligentno učitavanje modela
- Brzo pokretanje za razvojne i testne scenarije

**Prednosti performansi Foundry Local**:
- Superiorno korištenje NPU-a na modernom Windows hardveru
- Optimizirano ubrzanje GPU-a kroz partnerstva s proizvođačima
- Praćenje i optimizacija performansi razine poduzeća
- Skalabilne mogućnosti implementacije za produkcijska okruženja

### Analiza iskustva za programere

**Iskustvo programera s Ollamom**:
- Minimalni zahtjevi za postavku uz trenutnu produktivnost
- Intuitivno sučelje naredbenog retka za sve operacije
- Opsežna podrška zajednice i dokumentacija
- Fleksibilna prilagodba putem Modelfiles

**Iskustvo programera s Foundry Local**:
- Sveobuhvatna integracija IDE-a s ekosustavom Visual Studio
- Razvojni tijekovi rada razine poduzeća s značajkama za suradnju timova
- Profesionalni kanali podrške uz Microsoftovu podršku
- Napredni alati za otklanjanje pogrešaka i optimizaciju

### Optimizacija slučajeva upotrebe

**Odaberite Ollamu kada**:
- Razvijate aplikacije koje rade na više platformi i zahtijevaju dosljedno ponašanje
- Prioritet je transparentnost otvorenog koda i doprinosi zajednice
- Radite s ograničenim resursima ili proračunskim ograničenjima
- Gradite eksperimentalne ili istraživačke aplikacije
- Potrebna je široka kompatibilnost modela s različitim arhitekturama

**Odaberite Foundry Local kada**:
- Implementirate aplikacije za poduzeća s strogim zahtjevima za performansama
- Koristite optimizacije hardvera specifične za Windows (NPU, Windows ML)
- Potrebna je podrška za poduzeća, SLA-ovi i značajke usklađenosti
- Gradite produkcijske aplikacije s integracijom u Microsoftov ekosustav
- Potrebni su napredni alati za optimizaciju i profesionalni razvojni tijekovi rada

## Napredne strategije implementacije

### Obrasci za kontejneriziranu implementaciju

**Kontejnerizacija Ollame**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Implementacija Foundry Local za poduzeća**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Tehnike optimizacije performansi

**Strategije optimizacije Ollame**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Optimizacija Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Sigurnosni i regulatorni aspekti

### Implementacija sigurnosti u poduzeću

**Najbolje prakse sigurnosti za Ollamu**:
- Izolacija mreže s pravilima vatrozida i VPN pristupom
- Autentifikacija putem integracije reverznog proxyja
- Provjera integriteta modela i sigurna distribucija modela
- Zapisivanje revizije za pristup API-ju i operacije modela

**Sigurnost Foundry Local za poduzeća**:
- Ugrađena kontrola pristupa temeljenog na ulogama s integracijom Active Directoryja
- Sveobuhvatni tragovi revizije s izvješćivanjem o usklađenosti
- Šifrirano pohranjivanje modela i sigurna implementacija modela
- Integracija s Microsoftovom sigurnosnom infrastrukturom

### Usklađenost i regulatorni zahtjevi

Obje platforme podržavaju regulatornu usklađenost kroz:
- Kontrole rezidencije podataka koje osiguravaju lokalnu obradu
- Zapisivanje revizije za zahtjeve regulatornog izvješćivanja
- Kontrole pristupa za rukovanje osjetljivim podacima
- Šifriranje u mirovanju i tijekom prijenosa za zaštitu podataka

## Najbolje prakse za implementaciju u produkciji

### Praćenje i preglednost

**Ključne metrike za praćenje**:
- Latencija i propusnost izvođenja modela
- Korištenje resursa (CPU, GPU, memorija)
- Vrijeme odgovora API-ja i stope pogrešaka
- Točnost modela i odstupanje performansi

**Implementacija praćenja**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### Kontinuirana integracija i implementacija

**Integracija CI/CD cjevovoda**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## Budući trendovi i razmatranja

### Novi trendovi u tehnologiji

Lokalni krajolik implementacije SLM-ova nastavlja se razvijati s nekoliko ključnih trendova:

**Napredne arhitekture modela**: Pojavljuju se sljedeće generacije SLM-ova s poboljšanom učinkovitošću i omjerima sposobnosti, uključujući modele s mješavinom stručnjaka za dinamičko skaliranje i specijalizirane arhitekture za implementaciju na rubu.

**Integracija hardvera**: Dublja integracija sa specijaliziranim AI hardverom uključujući NPU-ove, prilagođeni silicij i akceleratore za računalstvo na rubu pružit će poboljšane performanse.

**Evolucija ekosustava**: Napori za standardizaciju između platformi za implementaciju i poboljšana interoperabilnost između različitih okvira pojednostavit će implementacije na više platformi.

### Obrasci usvajanja u industriji

**Usvajanje u poduzećima**: Povećano usvajanje u poduzećima potaknuto zahtjevima za privatnošću, optimizacijom troškova i potrebama za usklađenošću s propisima. Vladini i obrambeni sektori posebno su usmjereni na implementacije u izoliranim mrežama.

**Globalna razmatranja**: Međunarodni zahtjevi za suverenitet podataka potiču usvajanje lokalnih implementacija, posebno u regijama sa strogim propisima o zaštiti podataka.

## Izazovi i razmatranja

### Tehnički izazovi

**Zahtjevi za infrastrukturu**: Lokalna implementacija zahtijeva pažljivo planiranje kapaciteta i odabir hardvera. Organizacije moraju balansirati zahtjeve za performansama s ograničenjima troškova uz osiguranje skalabilnosti za rastuće radne opterećenje.

**🔧 Održavanje i ažuriranja**: Redovita ažuriranja modela, sigurnosne zakrpe i optimizacija performansi zahtijevaju posvećene resurse i stručnost. Automatizirani cjevovodi implementacije

---

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden pomoću AI usluge za prevođenje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za ključne informacije preporučuje se profesionalni prijevod od strane čovjeka. Ne preuzimamo odgovornost za nesporazume ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.