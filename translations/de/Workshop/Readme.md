<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8b994c57f1207012e4d7f58b7c0d1ae7",
  "translation_date": "2025-10-17T09:05:35+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "de"
}
-->
# EdgeAI f√ºr Anf√§nger - Workshop

> **Praktischer Lernpfad f√ºr die Entwicklung produktionsreifer Edge-AI-Anwendungen**
>
> Beherrschen Sie die lokale KI-Bereitstellung mit Microsoft Foundry Local ‚Äì von der ersten Chat-Komplettierung bis zur Multi-Agenten-Orchestrierung in 6 aufeinander aufbauenden Sitzungen.

---

## üéØ Einf√ºhrung

Willkommen zum **EdgeAI f√ºr Anf√§nger Workshop** ‚Äì Ihrem praktischen Leitfaden f√ºr die Entwicklung intelligenter Anwendungen, die vollst√§ndig auf lokaler Hardware laufen. Dieser Workshop verwandelt theoretische Edge-AI-Konzepte in praktische F√§higkeiten durch zunehmend anspruchsvollere √úbungen mit Microsoft Foundry Local und Small Language Models (SLMs).

### Warum dieser Workshop?

**Die Edge-AI-Revolution ist da**

Weltweit wechseln Organisationen von cloudbasierter KI zu Edge Computing aus drei entscheidenden Gr√ºnden:

1. **Datenschutz & Compliance** ‚Äì Verarbeitung sensibler Daten lokal ohne √úbertragung in die Cloud (HIPAA, DSGVO, Finanzvorschriften)
2. **Leistung** ‚Äì Eliminierung von Netzwerklatenz (50-500ms lokal vs. 500-2000ms Cloud-Roundtrip)
3. **Kostenkontrolle** ‚Äì Wegfall von API-Kosten pro Token und Skalierung ohne Cloud-Ausgaben

**Aber Edge AI ist anders**

Das Ausf√ºhren von KI vor Ort erfordert neue F√§higkeiten:
- Modellauswahl und -optimierung f√ºr ressourcenbeschr√§nkte Umgebungen
- Lokales Servicemanagement und Hardwarebeschleunigung
- Prompt-Engineering f√ºr kleinere Modelle
- Produktionsbereitstellungsmuster f√ºr Edge-Ger√§te

**Dieser Workshop vermittelt diese F√§higkeiten**

In 6 fokussierten Sitzungen (~3 Stunden insgesamt) entwickeln Sie sich von "Hello World" bis zur Bereitstellung produktionsreifer Multi-Agenten-Systeme ‚Äì alles l√§uft lokal auf Ihrem Rechner.

---

## üìö Lernziele

Nach Abschluss dieses Workshops k√∂nnen Sie:

### Kernkompetenzen
1. **Lokale KI-Dienste bereitstellen und verwalten**
   - Microsoft Foundry Local installieren und konfigurieren
   - Geeignete Modelle f√ºr Edge-Bereitstellungen ausw√§hlen
   - Modelllebenszyklus verwalten (herunterladen, laden, zwischenspeichern)
   - Ressourcenverbrauch √ºberwachen und Leistung optimieren

2. **KI-gest√ºtzte Anwendungen entwickeln**
   - OpenAI-kompatible Chat-Komplettierungen lokal implementieren
   - Effektive Prompts f√ºr Small Language Models entwerfen
   - Streaming-Antworten f√ºr bessere Benutzererfahrung handhaben
   - Lokale Modelle in bestehende Anwendungen integrieren

3. **RAG-Systeme (Retrieval Augmented Generation) erstellen**
   - Semantische Suche mit Embeddings aufbauen
   - LLM-Antworten in dom√§nenspezifisches Wissen einbetten
   - RAG-Qualit√§t mit branchen√ºblichen Metriken bewerten
   - Vom Prototyp zur Produktion skalieren

4. **Modellleistung optimieren**
   - Mehrere Modelle f√ºr Ihren Anwendungsfall benchmarken
   - Latenz, Durchsatz und Zeit bis zum ersten Token messen
   - Optimale Modelle basierend auf Geschwindigkeits-/Qualit√§tskompromissen ausw√§hlen
   - SLM- vs. LLM-Abw√§gungen in realen Szenarien vergleichen

5. **Multi-Agenten-Systeme orchestrieren**
   - Spezialisierte Agenten f√ºr verschiedene Aufgaben entwerfen
   - Agentenspeicher und Kontextmanagement implementieren
   - Agenten in komplexen Workflows koordinieren
   - Anfragen intelligent √ºber mehrere Modelle routen

6. **Produktionsreife L√∂sungen bereitstellen**
   - Fehlerbehandlung und Wiederholungslogik implementieren
   - Token-Nutzung und Systemressourcen √ºberwachen
   - Skalierbare Architekturen mit Modell-als-Tool-Mustern erstellen
   - Migrationspfade von Edge zu Hybrid (Edge + Cloud) planen

---

## üéì Lernergebnisse

### Was Sie erstellen werden

Am Ende dieses Workshops haben Sie Folgendes erstellt:

| Sitzung | Ergebnis | Gezeigte F√§higkeiten |
|---------|----------|-----------------------|
| **1** | Chat-Anwendung mit Streaming | Service-Setup, grundlegende Komplettierungen, Streaming-UX |
| **2** | RAG-System mit Bewertung | Embeddings, semantische Suche, Qualit√§tsmetriken |
| **3** | Multi-Modell-Benchmark-Suite | Leistungsbewertung, Modellvergleich |
| **4** | SLM- vs. LLM-Vergleich | Kompromissanalyse, Optimierungsstrategien |
| **5** | Multi-Agenten-Orchestrator | Agentendesign, Speicherverwaltung, Koordination |
| **6** | Intelligentes Routingsystem | Intent-Erkennung, Modellauswahl, Skalierbarkeit |

### Kompetenzmatrix

| F√§higkeitsniveau | Sitzung 1-2 | Sitzung 3-4 | Sitzung 5-6 |
|------------------|-------------|-------------|-------------|
| **Anf√§nger** | ‚úÖ Setup & Grundlagen | ‚ö†Ô∏è Herausfordernd | ‚ùå Zu fortgeschritten |
| **Fortgeschritten** | ‚úÖ Schnelle √úberpr√ºfung | ‚úÖ Kernlernen | ‚ö†Ô∏è Stretch-Ziele |
| **Experte** | ‚úÖ Leicht zu bew√§ltigen | ‚úÖ Verfeinerung | ‚úÖ Produktionsmuster |

### Karrieref√§higkeiten

**Nach diesem Workshop sind Sie bereit:**

‚úÖ **Datenschutzorientierte Anwendungen zu entwickeln**
- Gesundheitsanwendungen, die PHI/PII lokal verarbeiten
- Finanzdienstleistungen mit Compliance-Anforderungen
- Regierungssysteme mit Anforderungen an Datenhoheit

‚úÖ **F√ºr Edge-Umgebungen zu optimieren**
- IoT-Ger√§te mit begrenzten Ressourcen
- Offline-fokussierte mobile Anwendungen
- Echtzeitsysteme mit niedriger Latenz

‚úÖ **Intelligente Architekturen zu entwerfen**
- Multi-Agenten-Systeme f√ºr komplexe Workflows
- Hybride Edge-Cloud-Bereitstellungen
- Kostenoptimierte KI-Infrastruktur

‚úÖ **Edge-AI-Initiativen zu leiten**
- Machbarkeit von Edge-AI f√ºr Projekte bewerten
- Geeignete Modelle und Frameworks ausw√§hlen
- Skalierbare lokale KI-L√∂sungen entwerfen

---

## üó∫Ô∏è Workshop-Struktur

### Sitzungs√ºbersicht (6 Sitzungen √ó 30 Minuten = 3 Stunden)

| Sitzung | Thema | Fokus | Dauer |
|---------|-------|-------|-------|
| **1** | Einstieg in Foundry Local | Installation, Validierung, erste Komplettierungen | 30 min |
| **2** | KI-L√∂sungen mit RAG erstellen | Prompt-Engineering, Embeddings, Bewertung | 30 min |
| **3** | Open-Source-Modelle | Modellentdeckung, Benchmarking, Auswahl | 30 min |
| **4** | Cutting-Edge-Modelle | SLM vs. LLM, Optimierung, Frameworks | 30 min |
| **5** | KI-gest√ºtzte Agenten | Agentendesign, Orchestrierung, Speicher | 30 min |
| **6** | Modelle als Werkzeuge | Routing, Verkettung, Skalierungsstrategien | 30 min |

---

## üöÄ Schnellstart

### Voraussetzungen

**Systemanforderungen:**
- **OS**: Windows 10/11, macOS 11+ oder Linux (Ubuntu 20.04+)
- **RAM**: Mindestens 8GB, empfohlen 16GB+
- **Speicherplatz**: 10GB+ freier Speicher f√ºr Modelle
- **CPU**: Moderner Prozessor mit AVX2-Unterst√ºtzung
- **GPU** (optional): CUDA-kompatibel oder Qualcomm NPU f√ºr Beschleunigung

**Softwareanforderungen:**
- **Python 3.8+** ([Download](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Installationsanleitung](../../../Workshop))
- **Git** ([Download](https://git-scm.com/downloads))
- **Visual Studio Code** (empfohlen) ([Download](https://code.visualstudio.com/))

### Einrichtung in 3 Schritten

#### 1. Foundry Local installieren

**Windows:**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Installation √ºberpr√ºfen:**
```bash
foundry --version
foundry service status
```

**Stellen Sie sicher, dass Azure AI Foundry Local mit einem festen Port l√§uft**

```bash
# Set FoundryLocal to use port 58123 (default)
foundry service set --port 58123 --show

# Or use a different port
foundry service set --port 58000 --show
```

**√úberpr√ºfung der Funktionalit√§t:**
```bash
# Check service status
foundry service status

# Test the endpoint
curl http://127.0.0.1:58123/v1/models
```
**Verf√ºgbare Modelle finden**
Um zu sehen, welche Modelle in Ihrer Foundry Local-Instanz verf√ºgbar sind, k√∂nnen Sie den Models-Endpunkt abfragen:

```bash
# cmd/bash/powershell
foundry model list
```

Verwendung des Web-Endpunkts 

```bash
# Windows PowerShell
powershell -Command "Invoke-RestMethod -Uri 'http://127.0.0.1:58123/v1/models' -Method Get"

# Or using curl (if available)
curl http://127.0.0.1:58123/v1/models
```

#### 2. Repository klonen & Abh√§ngigkeiten installieren

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Ihr erstes Beispiel ausf√ºhren

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ Erfolg!** Sie sollten eine Streaming-Antwort zu Edge AI sehen.

---

## üì¶ Workshop-Ressourcen

### Python-Beispiele

Progressive praktische Beispiele, die jedes Konzept demonstrieren:

| Sitzung | Beispiel | Beschreibung | Laufzeit |
|---------|----------|--------------|----------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Basis- & Streaming-Chat | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG mit Embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | RAG-Qualit√§tsbewertung | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Multi-Modell-Benchmarking | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | SLM vs. LLM-Vergleich | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Multi-Agenten-System | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Intent-basiertes Routing | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Multi-Step-Pipeline | ~60s |

### Jupyter-Notebooks

Interaktive Erkundung mit Erkl√§rungen und Visualisierungen:

| Sitzung | Notebook | Beschreibung | Schwierigkeitsgrad |
|---------|----------|--------------|--------------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | Chat-Grundlagen & Streaming | ‚≠ê Anf√§nger |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | RAG-System erstellen | ‚≠ê‚≠ê Fortgeschritten |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | RAG-Qualit√§t bewerten | ‚≠ê‚≠ê Fortgeschritten |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Modell-Benchmarking | ‚≠ê‚≠ê Fortgeschritten |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Modellvergleich | ‚≠ê‚≠ê Fortgeschritten |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Agenten-Orchestrierung | ‚≠ê‚≠ê‚≠ê Experte |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Intent-Routing | ‚≠ê‚≠ê‚≠ê Experte |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Pipeline-Orchestrierung | ‚≠ê‚≠ê‚≠ê Experte |

### Dokumentation

Umfassende Anleitungen und Referenzen:

| Dokument | Beschreibung | Verwendung |
|----------|--------------|------------|
| [QUICK_START.md](./QUICK_START.md) | Schnellstart-Anleitung | Start von Grund auf |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Befehls- & API-Spickzettel | Schnelle Antworten |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | SDK-Muster & Beispiele | Code schreiben |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Anleitung zu Umgebungsvariablen | Beispiele konfigurieren |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Neueste Verbesserungen der Beispiele | √Ñnderungen verstehen |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Migrationsanleitung | Code aktualisieren |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | H√§ufige Probleme & L√∂sungen | Probleme beheben |

---

## üéì Empfehlungen f√ºr den Lernpfad

### F√ºr Anf√§nger (3-4 Stunden)
1. ‚úÖ Sitzung 1: Einstieg (Fokus auf Setup und grundlegenden Chat)
2. ‚úÖ Sitzung 2: RAG-Grundlagen (Bewertung zun√§chst √ºberspringen)
3. ‚úÖ Sitzung 3: Einfaches Benchmarking (nur 2 Modelle)
4. ‚è≠Ô∏è Sitzungen 4-6 vorerst √ºberspringen
5. üîÑ Zu Sitzungen 4-6 zur√ºckkehren, nachdem die erste Anwendung erstellt wurde

### F√ºr fortgeschrittene Entwickler (3 Stunden)
1. ‚ö° Sitzung 1: Schnelle Setup-Validierung
2. ‚úÖ Sitzung 2: Vollst√§ndige RAG-Pipeline mit Bewertung
3. ‚úÖ Sitzung 3: Vollst√§ndige Benchmarking-Suite
4. ‚úÖ Sitzung 4: Modelloptimierung
5. ‚úÖ Sitzungen 5-6: Fokus auf Architekturmustern

### F√ºr Experten (2-3 Stunden)
1. ‚ö° Sitzungen 1-3: Schnelle √úberpr√ºfung und Validierung
2. ‚úÖ Sitzung 4: Optimierungs-Deep-Dive
3. ‚úÖ Sitzung 5: Multi-Agenten-Architektur
4. ‚úÖ Sitzung 6: Produktionsmuster und Skalierung
5. üöÄ Erweiterung: Eigene Routing-Logik und hybride Bereitstellungen erstellen

---

## Workshop-Sitzungspaket (Fokussierte 30‚ÄëMinuten-Labs)

Wenn Sie dem kompakten 6-Sitzungs-Workshop-Format folgen, verwenden Sie diese speziellen Leitf√§den (jeder erg√§nzt die umfassenderen Moduldokumente oben):

| Workshop-Sitzung | Leitfaden | Kernfokus |
|------------------|-----------|-----------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Installation, Validierung, Ausf√ºhrung von phi & GPT-OSS-20B, Beschleunigung |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Prompt-Engineering, RAG-Muster, CSV- & Dokumenten-Einbettung, Migration |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Hugging Face-Integration, Benchmarking, Modellauswahl |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, ONNX-Beschleunigung |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | Agentenrollen, Speicher, Tools, Orchestrierung |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Routing, Verkettung, Skalierungspfad zu Azure |

Jede Sitzungsdatei enth√§lt: Zusammenfassung, Lernziele, 30-min√ºtigen Demo-Ablauf, Starterprojekt, Validierungs-Checkliste, Fehlerbehebung und Verweise auf das offizielle Foundry Local Python SDK.

### Beispielskripte

Workshop-Abh√§ngigkeiten installieren (Windows):

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux:

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Falls der Foundry Local-Dienst auf einer anderen (Windows-)Maschine oder VM von macOS ausgef√ºhrt wird, exportieren Sie den Endpunkt:

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Sitzung | Skript(e) | Beschreibung |
|---------|-----------|-------------|
| 1 | `samples/session01/chat_bootstrap.py` | Bootstrap-Dienst & Streaming-Chat |
| 2 | `samples/session02/rag_pipeline.py` | Minimaler RAG (In-Memory-Embeddings) |
|   | `samples/session02/rag_eval_ragas.py` | RAG-Auswertung mit Ragas-Metriken |
| 3 | `samples/session03/benchmark_oss_models.py` | Benchmarking von Multi-Modell-Latenz & Durchsatz |
| 4 | `samples/session04/model_compare.py` | Vergleich SLM vs LLM (Latenz & Beispielausgabe) |
| 5 | `samples/session05/agents_orchestrator.py` | Zwei-Agenten-Forschung ‚Üí Redaktionelle Pipeline |
| 6 | `samples/session06/models_router.py` | Intent-basiertes Routing-Demo |
|   | `samples/session06/models_pipeline.py` | Multi-Step-Plan/Execute/Refine-Kette |

### Umgebungsvariablen (Gemeinsam f√ºr alle Beispiele)

| Variable | Zweck | Beispiel |
|----------|-------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Standardalias f√ºr ein einzelnes Modell f√ºr einfache Beispiele | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | Explizites SLM vs gr√∂√üeres Modell f√ºr Vergleich | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Komma-separierte Liste von Aliases f√ºr Benchmarking | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | Benchmark-Wiederholungen pro Modell | `3` |
| `BENCH_PROMPT` | Prompt, der im Benchmarking verwendet wird | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Sentence-Transformers-Embedding-Modell | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Testabfrage f√ºr RAG-Pipeline √ºberschreiben | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Abfrage f√ºr Agenten-Pipeline √ºberschreiben | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Modellalias f√ºr Forschungsagenten | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Modellalias f√ºr Editor-Agenten (kann unterschiedlich sein) | `gpt-oss-20b` |
| `SHOW_USAGE` | Wenn `1`, druckt Token-Nutzung pro Abschluss | `1` |
| `RETRY_ON_FAIL` | Wenn `1`, einmaliger Wiederholungsversuch bei vor√ºbergehenden Chat-Fehlern | `1` |
| `RETRY_BACKOFF` | Sekunden, die vor Wiederholung gewartet werden | `1.0` |

Falls eine Variable nicht gesetzt ist, greifen die Skripte auf sinnvolle Standardwerte zur√ºck. F√ºr Demos mit einem einzigen Modell ben√∂tigen Sie normalerweise nur `FOUNDRY_LOCAL_ALIAS`.

### Hilfsmodul

Alle Beispiele teilen jetzt ein Hilfsmodul `samples/workshop_utils.py`, das Folgendes bietet:

* Zwischengespeicherte Erstellung von `FoundryLocalManager` + OpenAI-Client
* `chat_once()`-Helfer mit optionalem Wiederholungsversuch + Nutzungsanzeige
* Einfache Token-Nutzungsberichte (aktivieren √ºber `SHOW_USAGE=1`)

Dies reduziert Duplikationen und hebt Best Practices f√ºr effiziente lokale Modellorchestrierung hervor.

## Optionale Verbesserungen (Sitzungs√ºbergreifend)

| Thema | Verbesserung | Sitzungen | Env / Umschalter |
|-------|-------------|----------|------------------|
| Determinismus | Feste Temperatur + stabile Prompt-Sets | 1‚Äì6 | Setzen Sie `temperature=0`, `top_p=1` |
| Token-Nutzungsanzeige | Konsistente Kosten-/Effizienzvermittlung | 1‚Äì6 | `SHOW_USAGE=1` |
| Streaming des ersten Tokens | Wahrgenommene Latenzmetrik | 1,3,4,6 | `BENCH_STREAM=1` (Benchmark) |
| Wiederholungsresilienz | Handhabt vor√ºbergehende Kaltstartprobleme | Alle | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Multi-Modell-Agenten | Heterogene Rollenspezialisierung | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Adaptives Routing | Intent + Kostenheuristiken | 6 | Router mit Eskalationslogik erweitern |
| Vektorspeicher | Langfristiges semantisches Erinnern | 2,5,6 | FAISS/Chroma-Embedding-Index integrieren |
| Trace-Export | Auditierung & Auswertung | 2,5,6 | JSON-Linien pro Schritt anh√§ngen |
| Qualit√§tsrubriken | Qualitative Nachverfolgung | 3‚Äì6 | Sekund√§re Bewertungs-Prompts |
| Smoke-Tests | Schnelle Validierung vor Workshop | Alle | `python Workshop/tests/smoke.py` |

### Deterministischer Schnellstart

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Erwarten Sie stabile Token-Zahlen bei wiederholten identischen Eingaben.

### RAG-Auswertung (Sitzung 2)

Verwenden Sie `rag_eval_ragas.py`, um Antwortrelevanz, Glaubw√ºrdigkeit und Kontextpr√§zision auf einem kleinen synthetischen Datensatz zu berechnen:

```powershell
python samples/session02/rag_eval_ragas.py
```

Erweitern Sie dies, indem Sie ein gr√∂√üeres JSONL mit Fragen, Kontexten und Ground Truths bereitstellen und dann in ein Hugging Face `Dataset` konvertieren.

## CLI-Befehlsgenauigkeits-Anhang

Der Workshop verwendet absichtlich nur derzeit dokumentierte / stabile Foundry Local CLI-Befehle.

### Referenzierte stabile Befehle

| Kategorie | Befehl | Zweck |
|-----------|--------|-------|
| Kern | `foundry --version` | Installierte Version anzeigen |
| Kern | `foundry init` | Konfiguration initialisieren |
| Dienst | `foundry service start` | Lokalen Dienst starten (falls nicht automatisch) |
| Dienst | `foundry status` | Dienststatus anzeigen |
| Modelle | `foundry model list` | Katalog / verf√ºgbare Modelle auflisten |
| Modelle | `foundry model download <alias>` | Modellgewichte in den Cache herunterladen |
| Modelle | `foundry model run <alias>` | Modell lokal starten (laden); mit `--prompt` f√ºr Einmal-Antwort kombinieren |
| Modelle | `foundry model unload <alias>` / `foundry model stop <alias>` | Modell aus dem Speicher entladen (falls unterst√ºtzt) |
| Cache | `foundry cache list` | Zwischengespeicherte (heruntergeladene) Modelle auflisten |
| System | `foundry system info` | Snapshot der Hardware- & Beschleunigungsf√§higkeiten |
| System | `foundry system gpu-info` | GPU-Diagnoseinformationen |
| Konfiguration | `foundry config list` | Aktuelle Konfigurationswerte anzeigen |
| Konfiguration | `foundry config set <key> <value>` | Konfiguration aktualisieren |

### Einmal-Prompt-Muster

Anstelle eines veralteten `model chat`-Unterbefehls verwenden Sie:

```powershell
foundry model run <alias> --prompt "Your question here"
```

Dies f√ºhrt einen einzigen Prompt-/Antwortzyklus aus und beendet dann.

### Entfernte / Vermeidete Muster

| Veraltet / Undokumentiert | Ersatz / Anleitung |
|---------------------------|--------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Verwenden Sie einfach `foundry model list` + aktuelle Aktivit√§t / Logs |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Verwenden Sie Benchmark-Python-Skript + OS-Tools (Task-Manager / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking & Telemetrie

- Latenz, p95, Tokens/Sek.: `samples/session03/benchmark_oss_models.py`
- Erste-Token-Latenz (Streaming): Setzen Sie `BENCH_STREAM=1`
- Ressourcennutzung: OS-Monitore (Task-Manager, Aktivit√§tsmonitor, `nvidia-smi`) + `foundry system info`.

Sobald neue CLI-Telemetrie-Befehle stabilisiert sind, k√∂nnen sie mit minimalen √Ñnderungen in die Sitzungs-Markdowns integriert werden.

### Automatisierte Lint-√úberpr√ºfung

Ein automatisierter Linter verhindert die Wiedereinf√ºhrung veralteter CLI-Muster innerhalb von Codebl√∂cken in Markdown-Dateien:

Skript: `Workshop/scripts/lint_markdown_cli.py`

Veraltete Muster werden innerhalb von Code-Fences blockiert.

Empfohlene Ersatzmuster:
| Veraltet | Ersatz |
|----------|--------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Benchmark-Skript + Systemtools |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Lokal ausf√ºhren:
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

GitHub Action: `.github/workflows/markdown-cli-lint.yml` wird bei jedem Push & PR ausgef√ºhrt.

Optionaler Pre-Commit-Hook:
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Schnelle CLI ‚Üí SDK-Migrationstabelle

| Aufgabe | CLI-Einzeiler | SDK (Python) √Ñquivalent | Hinweise |
|---------|---------------|-------------------------|---------|
| Ein Modell einmal ausf√ºhren (Prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | SDK bootstrapped Dienst & Caching automatisch |
| Modell herunterladen (Cache) | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # l√∂st Download/Laden aus` | Manager w√§hlt die beste Variante, falls Alias auf mehrere Builds verweist |
| Katalog auflisten | `foundry model list` | `# verwenden Sie Manager f√ºr jeden Alias oder pflegen Sie bekannte Liste` | CLI aggregiert; SDK derzeit pro Alias-Initialisierung |
| Zwischengespeicherte Modelle auflisten | `foundry cache list` | `manager.list_cached_models()` | Nach Manager-Init (beliebiger Alias) |
| GPU-Beschleunigung aktivieren | `foundry config set compute.onnx.enable_gpu true` | `# CLI-Aktion; SDK geht davon aus, dass Konfiguration bereits angewendet wurde` | Konfiguration ist externer Nebeneffekt |
| Endpunkt-URL abrufen | (implizit) | `manager.endpoint` | Wird verwendet, um OpenAI-kompatiblen Client zu erstellen |
| Ein Modell aufw√§rmen | `foundry model run <alias>` dann erster Prompt | `chat_once(alias, messages=[...])` (Utility) | Utilities handhaben anf√§ngliche Kaltlatenz-Aufw√§rmung |
| Latenz messen | `python benchmark_oss_models.py` | `import benchmark_oss_models` (oder neues Exporter-Skript) | Skript bevorzugen f√ºr konsistente Metriken |
| Modell stoppen / entladen | `foundry model unload <alias>` | (Nicht verf√ºgbar ‚Äì Dienst / Prozess neu starten) | Normalerweise nicht erforderlich f√ºr Workshop-Ablauf |
| Token-Nutzung abrufen | (Ausgabe anzeigen) | `resp.usage.total_tokens` | Wird bereitgestellt, wenn Backend Nutzungsobjekt zur√ºckgibt |

## Benchmark-Markdown-Export

Verwenden Sie das Skript `Workshop/scripts/export_benchmark_markdown.py`, um einen frischen Benchmark auszuf√ºhren (gleiche Logik wie `samples/session03/benchmark_oss_models.py`) und eine GitHub-freundliche Markdown-Tabelle plus rohes JSON zu erstellen.

### Beispiel

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Generierte Dateien:
| Datei | Inhalt |
|-------|--------|
| `benchmark_report.md` | Markdown-Tabelle + Interpretationshinweise |
| `benchmark_report.json` | Rohmetrik-Array (f√ºr Differenzierung / Trendverfolgung) |

Setzen Sie `BENCH_STREAM=1` in der Umgebung, um die Latenz des ersten Tokens einzuschlie√üen, falls unterst√ºtzt.

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.