<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T10:43:38+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "de"
}
-->
# Abschnitt 3: Microsoft Olive Optimierungssuite

## Inhaltsverzeichnis
1. [Einführung](../../../Module04)
2. [Was ist Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Schnellstartanleitung](../../../Module04)
5. [Beispiel: Qwen3 in ONNX INT4 konvertieren](../../../Module04)
6. [Erweiterte Nutzung](../../../Module04)
7. [Olive-Rezepte-Repository](../../../Module04)
8. [Best Practices](../../../Module04)
9. [Fehlerbehebung](../../../Module04)
10. [Zusätzliche Ressourcen](../../../Module04)

## Einführung

Microsoft Olive ist ein leistungsstarkes, benutzerfreundliches hardwarebewusstes Modelloptimierungstool, das den Prozess der Optimierung von Machine-Learning-Modellen für die Bereitstellung auf verschiedenen Hardwareplattformen vereinfacht. Egal, ob Sie CPUs, GPUs oder spezialisierte KI-Beschleuniger anvisieren, Olive hilft Ihnen, optimale Leistung zu erzielen, während die Modellgenauigkeit erhalten bleibt.

## Was ist Microsoft Olive?

Olive ist ein einfach zu bedienendes, hardwarebewusstes Modelloptimierungstool, das branchenführende Techniken in den Bereichen Modellkompression, Optimierung und Kompilierung kombiniert. Es arbeitet mit ONNX Runtime als End-to-End-Lösung für Inferenzoptimierung.

### Hauptmerkmale

- **Hardwarebewusste Optimierung**: Wählt automatisch die besten Optimierungstechniken für Ihre Zielhardware aus
- **40+ integrierte Optimierungskomponenten**: Umfasst Modellkompression, Quantisierung, Graphoptimierung und mehr
- **Einfache CLI-Schnittstelle**: Einfache Befehle für gängige Optimierungsaufgaben
- **Multi-Framework-Unterstützung**: Funktioniert mit PyTorch, Hugging Face-Modellen und ONNX
- **Unterstützung beliebter Modelle**: Olive kann beliebte Modellarchitekturen wie Llama, Phi, Qwen, Gemma usw. automatisch optimieren

### Vorteile

- **Reduzierte Entwicklungszeit**: Keine Notwendigkeit, verschiedene Optimierungstechniken manuell auszuprobieren
- **Leistungssteigerungen**: Signifikante Geschwindigkeitsverbesserungen (bis zu 6x in einigen Fällen)
- **Plattformübergreifende Bereitstellung**: Optimierte Modelle funktionieren auf verschiedenen Hardware- und Betriebssystemen
- **Erhaltene Genauigkeit**: Optimierungen bewahren die Modellqualität und verbessern gleichzeitig die Leistung

## Installation

### Voraussetzungen

- Python 3.8 oder höher
- pip-Paketmanager
- Virtuelle Umgebung (empfohlen)

### Grundinstallation

Erstellen und aktivieren Sie eine virtuelle Umgebung:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installieren Sie Olive mit Auto-Optimierungsfunktionen:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Optionale Abhängigkeiten

Olive bietet verschiedene optionale Abhängigkeiten für zusätzliche Funktionen:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Installation überprüfen

```bash
olive --help
```

Wenn erfolgreich, sollte die Olive CLI-Hilfenachricht angezeigt werden.

## Schnellstartanleitung

### Ihre erste Optimierung

Optimieren wir ein kleines Sprachmodell mit der Auto-Optimierungsfunktion von Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Was dieser Befehl macht

Der Optimierungsprozess umfasst: Abrufen des Modells aus dem lokalen Cache, Erfassen des ONNX-Graphen und Speichern der Gewichte in einer ONNX-Datendatei, Optimieren des ONNX-Graphen und Quantisieren des Modells auf int4 mit der RTN-Methode.

### Erklärung der Befehlsparameter

- `--model_name_or_path`: Hugging Face-Modellkennung oder lokaler Pfad
- `--output_path`: Verzeichnis, in dem das optimierte Modell gespeichert wird
- `--device`: Zielgerät (cpu, gpu)
- `--provider`: Ausführungsanbieter (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI für Inferenz verwenden
- `--precision`: Quantisierungspräzision (int4, int8, fp16)
- `--log_level`: Protokollierungsdetails (0=minimal, 1=ausführlich)

## Beispiel: Qwen3 in ONNX INT4 konvertieren

Basierend auf dem bereitgestellten Hugging Face-Beispiel unter [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) können Sie ein Qwen3-Modell wie folgt optimieren:

### Schritt 1: Modell herunterladen (optional)

Um die Downloadzeit zu minimieren, cachen Sie nur die wesentlichen Dateien:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Schritt 2: Qwen3-Modell optimieren

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Schritt 3: Optimiertes Modell testen

Erstellen Sie ein einfaches Python-Skript, um Ihr optimiertes Modell zu testen:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktur der Ausgabe

Nach der Optimierung enthält Ihr Ausgabeverzeichnis:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Erweiterte Nutzung

### Konfigurationsdateien

Für komplexere Optimierungsabläufe können Sie JSON-Konfigurationsdateien verwenden:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Ausführen mit Konfiguration:

```bash
olive run --config config.json
```

### GPU-Optimierung

Für CUDA-GPU-Optimierung:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Für DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Feinabstimmung mit Olive

Olive unterstützt auch die Feinabstimmung von Modellen:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best Practices

### 1. Modellauswahl
- Beginnen Sie mit kleineren Modellen für Tests (z. B. 0,5B-7B Parameter)
- Stellen Sie sicher, dass Ihre Zielmodellarchitektur von Olive unterstützt wird

### 2. Hardwareüberlegungen
- Passen Sie Ihr Optimierungsziel an Ihre Bereitstellungshardware an
- Verwenden Sie GPU-Optimierung, wenn Sie CUDA-kompatible Hardware haben
- Ziehen Sie DirectML für Windows-Maschinen mit integrierter Grafik in Betracht

### 3. Präzisionsauswahl
- **INT4**: Maximale Kompression, leichte Genauigkeitsverluste
- **INT8**: Gute Balance zwischen Größe und Genauigkeit
- **FP16**: Minimale Genauigkeitsverluste, moderate Größenreduzierung

### 4. Testen und Validieren
- Testen Sie optimierte Modelle immer mit Ihren spezifischen Anwendungsfällen
- Vergleichen Sie Leistungskennzahlen (Latenz, Durchsatz, Genauigkeit)
- Verwenden Sie repräsentative Eingabedaten für die Bewertung

### 5. Iterative Optimierung
- Beginnen Sie mit Auto-Optimierung für schnelle Ergebnisse
- Verwenden Sie Konfigurationsdateien für eine feinere Steuerung
- Experimentieren Sie mit verschiedenen Optimierungsschritten

## Fehlerbehebung

### Häufige Probleme

#### 1. Installationsprobleme
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-Probleme
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Speicherprobleme
- Verwenden Sie kleinere Batchgrößen während der Optimierung
- Versuchen Sie zuerst Quantisierung mit höherer Präzision (int8 statt int4)
- Stellen Sie sicher, dass ausreichend Speicherplatz für das Modell-Caching vorhanden ist

#### 4. Modellladefehler
- Überprüfen Sie den Modellpfad und die Zugriffsberechtigungen
- Prüfen Sie, ob das Modell `trust_remote_code=True` benötigt
- Stellen Sie sicher, dass alle erforderlichen Modelfiles heruntergeladen wurden

### Hilfe erhalten

- **Dokumentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Beispiele**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive-Rezepte-Repository

### Einführung in Olive-Rezepte

Das [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)-Repository ergänzt das Haupt-Toolkit von Olive, indem es eine umfassende Sammlung von einsatzbereiten Optimierungsrezepten für beliebte KI-Modelle bereitstellt. Dieses Repository dient als praktische Referenz sowohl für die Optimierung öffentlich verfügbarer Modelle als auch für die Erstellung von Optimierungsabläufen für proprietäre Modelle.

### Hauptmerkmale

- **100+ vorgefertigte Rezepte**: Einsatzbereite Optimierungskonfigurationen für beliebte Modelle
- **Multi-Architektur-Unterstützung**: Umfasst Transformermodelle, Vision-Modelle und multimodale Architekturen
- **Hardware-spezifische Optimierungen**: Rezepte, die auf CPU, GPU und spezialisierte Beschleuniger zugeschnitten sind
- **Beliebte Modellfamilien**: Beinhaltet Phi, Llama, Qwen, Gemma, Mistral und viele mehr

### Unterstützte Modellfamilien

Das Repository enthält Optimierungsrezepte für:

#### Sprachmodelle
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5-Serie (0.5B bis 14B)
- **Google Gemma**: Verschiedene Gemma-Modellkonfigurationen
- **Mistral AI**: Mistral-7B-Serie
- **DeepSeek**: R1-Distill-Serie-Modelle

#### Vision- und multimodale Modelle
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP-Modelle**: Verschiedene CLIP-ViT-Konfigurationen
- **ResNet**: ResNet-50-Optimierungen
- **Vision Transformers**: ViT-base-patch16-224

#### Spezialisierte Modelle
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Basis- und mehrsprachige Varianten
- **Sentence Transformers**: all-MiniLM-L6-v2

### Verwendung von Olive-Rezepten

#### Methode 1: Spezifisches Rezept klonen

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Methode 2: Rezept als Vorlage verwenden

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Rezeptstruktur

Jedes Rezeptverzeichnis enthält typischerweise:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Beispiel: Verwendung des Phi-4-mini-Rezepts

Verwenden wir das Phi-4-mini-Rezept als Beispiel:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Die Konfigurationsdatei enthält typischerweise:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Rezepte anpassen

#### Zielhardware ändern

Um die Zielhardware zu ändern, aktualisieren Sie den Abschnitt `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimierungsparameter anpassen

Ändern Sie den Abschnitt `passes` für unterschiedliche Optimierungsstufen:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Eigenes Rezept erstellen

1. **Mit einem ähnlichen Modell beginnen**: Finden Sie ein Rezept für ein Modell mit ähnlicher Architektur
2. **Modellkonfiguration aktualisieren**: Ändern Sie den Modellnamen/Pfad in der Konfiguration
3. **Parameter anpassen**: Optimierungsparameter nach Bedarf ändern
4. **Testen und validieren**: Optimierung durchführen und Ergebnisse validieren
5. **Beitrag leisten**: Erwägen Sie, Ihr Rezept zum Repository beizutragen

### Vorteile der Verwendung von Rezepten

#### 1. **Bewährte Konfigurationen**
- Getestete Optimierungseinstellungen für spezifische Modelle
- Vermeidet Trial-and-Error bei der Suche nach optimalen Parametern

#### 2. **Hardware-spezifische Abstimmung**
- Voroptimiert für verschiedene Ausführungsanbieter
- Einsatzbereite Konfigurationen für CPU-, GPU- und NPU-Ziele

#### 3. **Umfassende Abdeckung**
- Unterstützt die beliebtesten Open-Source-Modelle
- Regelmäßige Updates mit neuen Modellveröffentlichungen

#### 4. **Community-Beiträge**
- Kollaborative Entwicklung mit der KI-Community
- Geteiltes Wissen und Best Practices

### Beitrag zu Olive-Rezepten

Wenn Sie ein Modell optimiert haben, das im Repository nicht abgedeckt ist:

1. **Repository forken**: Erstellen Sie einen eigenen Fork von olive-recipes
2. **Rezeptverzeichnis erstellen**: Fügen Sie ein neues Verzeichnis für Ihr Modell hinzu
3. **Konfiguration hinzufügen**: Fügen Sie olive_config.json und unterstützende Dateien hinzu
4. **Nutzung dokumentieren**: Stellen Sie eine klare README mit Anweisungen bereit
5. **Pull-Request einreichen**: Beitrag zur Community leisten

### Leistungsbenchmarks

Viele Rezepte enthalten Leistungsbenchmarks, die zeigen:
- **Latenzverbesserungen**: Typische 2-6x Geschwindigkeitssteigerung gegenüber der Basisversion
- **Speicherreduzierung**: 50-75% weniger Speicherverbrauch durch Quantisierung
- **Genauigkeitserhaltung**: 95-99% Genauigkeit bleibt erhalten

### Integration mit KI-Toolkits

Die Rezepte arbeiten nahtlos mit:
- **VS Code AI Toolkit**: Direkte Integration für Modelloptimierung
- **Azure Machine Learning**: Cloud-basierte Optimierungsabläufe
- **ONNX Runtime**: Optimierte Inferenzbereitstellung

## Zusätzliche Ressourcen

### Offizielle Links
- **GitHub-Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive-Rezepte-Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime-Dokumentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face-Beispiel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community-Beispiele
- **Jupyter Notebooks**: Verfügbar im Olive GitHub-Repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code-Erweiterung**: Überblick über das AI Toolkit für VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogbeiträge**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Verwandte Tools
- **ONNX Runtime**: Hochleistungs-Inferenz-Engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Quelle vieler kompatibler Modelle — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloud-basierte Optimierungsabläufe — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Was kommt als Nächstes?

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.