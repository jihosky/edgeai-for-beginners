<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T12:23:46+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "br"
}
-->
# Seção 3: Microsoft Olive Optimization Suite

## Índice
1. [Introdução](../../../Module04)
2. [O que é Microsoft Olive?](../../../Module04)
3. [Instalação](../../../Module04)
4. [Guia Rápido](../../../Module04)
5. [Exemplo: Convertendo Qwen3 para ONNX INT4](../../../Module04)
6. [Uso Avançado](../../../Module04)
7. [Repositório de Receitas Olive](../../../Module04)
8. [Melhores Práticas](../../../Module04)
9. [Solução de Problemas](../../../Module04)
10. [Recursos Adicionais](../../../Module04)

## Introdução

Microsoft Olive é uma poderosa e fácil ferramenta de otimização de modelos, consciente do hardware, que simplifica o processo de otimização de modelos de aprendizado de máquina para implantação em diferentes plataformas de hardware. Seja para CPUs, GPUs ou aceleradores de IA especializados, Olive ajuda você a alcançar desempenho ideal enquanto mantém a precisão do modelo.

## O que é Microsoft Olive?

Olive é uma ferramenta de otimização de modelos, consciente do hardware, que combina técnicas líderes da indústria em compressão, otimização e compilação de modelos. Funciona com ONNX Runtime como uma solução de otimização de inferência de ponta a ponta.

### Principais Recursos

- **Otimização Consciente do Hardware**: Seleciona automaticamente as melhores técnicas de otimização para o hardware alvo
- **40+ Componentes de Otimização Integrados**: Abrange compressão de modelos, quantização, otimização de gráficos e mais
- **Interface CLI Fácil**: Comandos simples para tarefas comuns de otimização
- **Suporte Multi-Framework**: Funciona com PyTorch, modelos Hugging Face e ONNX
- **Suporte a Modelos Populares**: Olive pode otimizar automaticamente arquiteturas de modelos populares como Llama, Phi, Qwen, Gemma, etc., sem complicações

### Benefícios

- **Redução do Tempo de Desenvolvimento**: Sem necessidade de experimentar manualmente diferentes técnicas de otimização
- **Ganhos de Desempenho**: Melhorias significativas de velocidade (até 6x em alguns casos)
- **Implantação Multiplataforma**: Modelos otimizados funcionam em diferentes hardwares e sistemas operacionais
- **Manutenção da Precisão**: As otimizações preservam a qualidade do modelo enquanto melhoram o desempenho

## Instalação

### Pré-requisitos

- Python 3.8 ou superior
- Gerenciador de pacotes pip
- Ambiente virtual (recomendado)

### Instalação Básica

Crie e ative um ambiente virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instale Olive com recursos de auto-otimização:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependências Opcionais

Olive oferece várias dependências opcionais para recursos adicionais:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verificar Instalação

```bash
olive --help
```

Se bem-sucedido, você verá a mensagem de ajuda do CLI do Olive.

## Guia Rápido

### Sua Primeira Otimização

Vamos otimizar um pequeno modelo de linguagem usando o recurso de auto-otimização do Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### O que Este Comando Faz

O processo de otimização envolve: adquirir o modelo do cache local, capturar o Gráfico ONNX e armazenar os pesos em um arquivo de dados ONNX, otimizar o Gráfico ONNX e quantizar o modelo para int4 usando o método RTN.

### Explicação dos Parâmetros do Comando

- `--model_name_or_path`: Identificador do modelo Hugging Face ou caminho local
- `--output_path`: Diretório onde o modelo otimizado será salvo
- `--device`: Dispositivo alvo (cpu, gpu)
- `--provider`: Provedor de execução (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Usar ONNX Runtime Generate AI para inferência
- `--precision`: Precisão de quantização (int4, int8, fp16)
- `--log_level`: Verbosidade do log (0=minimal, 1=verbose)

## Exemplo: Convertendo Qwen3 para ONNX INT4

Com base no exemplo fornecido pelo Hugging Face em [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), veja como otimizar um modelo Qwen3:

### Etapa 1: Baixar Modelo (Opcional)

Para minimizar o tempo de download, faça o cache apenas dos arquivos essenciais:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Etapa 2: Otimizar Modelo Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Etapa 3: Testar o Modelo Otimizado

Crie um script Python simples para testar seu modelo otimizado:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Estrutura de Saída

Após a otimização, seu diretório de saída conterá:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Uso Avançado

### Arquivos de Configuração

Para fluxos de trabalho de otimização mais complexos, você pode usar arquivos de configuração JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Execute com configuração:

```bash
olive run --config config.json
```

### Otimização para GPU

Para otimização CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Para DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ajuste Fino com Olive

Olive também suporta ajuste fino de modelos:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Melhores Práticas

### 1. Seleção de Modelos
- Comece com modelos menores para testes (ex.: 0.5B-7B parâmetros)
- Certifique-se de que a arquitetura do modelo alvo é suportada pelo Olive

### 2. Considerações de Hardware
- Combine seu alvo de otimização com o hardware de implantação
- Use otimização para GPU se tiver hardware compatível com CUDA
- Considere DirectML para máquinas Windows com gráficos integrados

### 3. Seleção de Precisão
- **INT4**: Máxima compressão, leve perda de precisão
- **INT8**: Bom equilíbrio entre tamanho e precisão
- **FP16**: Mínima perda de precisão, redução moderada de tamanho

### 4. Teste e Validação
- Sempre teste modelos otimizados com seus casos de uso específicos
- Compare métricas de desempenho (latência, throughput, precisão)
- Use dados de entrada representativos para avaliação

### 5. Otimização Iterativa
- Comece com auto-otimização para resultados rápidos
- Use arquivos de configuração para controle detalhado
- Experimente diferentes passes de otimização

## Solução de Problemas

### Problemas Comuns

#### 1. Problemas de Instalação
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemas com CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemas de Memória
- Use tamanhos de lote menores durante a otimização
- Tente quantização com maior precisão primeiro (int8 em vez de int4)
- Certifique-se de ter espaço suficiente em disco para o cache do modelo

#### 4. Erros de Carregamento de Modelo
- Verifique o caminho do modelo e as permissões de acesso
- Confirme se o modelo requer `trust_remote_code=True`
- Certifique-se de que todos os arquivos necessários do modelo foram baixados

### Obter Ajuda

- **Documentação**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemas no GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exemplos**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repositório de Receitas Olive

### Introdução às Receitas Olive

O repositório [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) complementa a principal ferramenta Olive, fornecendo uma coleção abrangente de receitas de otimização prontas para uso para modelos de IA populares. Este repositório serve como uma referência prática tanto para otimizar modelos disponíveis publicamente quanto para criar fluxos de trabalho de otimização para modelos proprietários.

### Principais Recursos

- **100+ Receitas Pré-construídas**: Configurações de otimização prontas para uso para modelos populares
- **Suporte Multi-Arquitetura**: Abrange modelos de transformadores, modelos de visão e arquiteturas multimodais
- **Otimizações Específicas de Hardware**: Receitas adaptadas para CPU, GPU e aceleradores especializados
- **Famílias de Modelos Populares**: Inclui Phi, Llama, Qwen, Gemma, Mistral e muitos outros

### Famílias de Modelos Suportadas

O repositório inclui receitas de otimização para:

#### Modelos de Linguagem
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, série Qwen2.5 (0.5B a 14B)
- **Google Gemma**: Várias configurações de modelos Gemma
- **Mistral AI**: Série Mistral-7B
- **DeepSeek**: Modelos da série R1-Distill

#### Modelos de Visão e Multimodais
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Modelos CLIP**: Várias configurações CLIP-ViT
- **ResNet**: Otimizações ResNet-50
- **Transformadores de Visão**: ViT-base-patch16-224

#### Modelos Especializados
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Variantes base e multilíngue
- **Transformadores de Sentenças**: all-MiniLM-L6-v2

### Usando Receitas Olive

#### Método 1: Clonar Receita Específica

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Método 2: Usar Receita como Modelo

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Estrutura da Receita

Cada diretório de receita geralmente contém:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Exemplo: Usando Receita Phi-4-mini

Vamos usar a receita Phi-4-mini como exemplo:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

O arquivo de configuração geralmente inclui:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Personalizando Receitas

#### Modificando Hardware Alvo

Para alterar o hardware alvo, atualize a seção `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Ajustando Parâmetros de Otimização

Modifique a seção `passes` para diferentes níveis de otimização:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Criando Sua Própria Receita

1. **Comece com um Modelo Similar**: Encontre uma receita para um modelo com arquitetura semelhante
2. **Atualize a Configuração do Modelo**: Altere o nome/caminho do modelo na configuração
3. **Ajuste os Parâmetros**: Modifique os parâmetros de otimização conforme necessário
4. **Teste e Valide**: Execute a otimização e valide os resultados
5. **Contribua de Volta**: Considere contribuir com sua receita para o repositório

### Benefícios de Usar Receitas

#### 1. **Configurações Comprovadas**
- Configurações de otimização testadas para modelos específicos
- Evita tentativa e erro ao encontrar parâmetros ideais

#### 2. **Ajuste Específico de Hardware**
- Pré-otimizado para diferentes provedores de execução
- Configurações prontas para uso para alvos CPU, GPU e NPU

#### 3. **Cobertura Abrangente**
- Suporta os modelos open-source mais populares
- Atualizações regulares com novos lançamentos de modelos

#### 4. **Contribuições da Comunidade**
- Desenvolvimento colaborativo com a comunidade de IA
- Compartilhamento de conhecimento e melhores práticas

### Contribuindo para Receitas Olive

Se você otimizou um modelo que não está coberto no repositório:

1. **Faça um Fork do Repositório**: Crie seu próprio fork do olive-recipes
2. **Crie um Diretório de Receita**: Adicione um novo diretório para seu modelo
3. **Inclua Configuração**: Adicione olive_config.json e arquivos de suporte
4. **Documente o Uso**: Forneça README claro com instruções
5. **Envie um Pull Request**: Contribua de volta para a comunidade

### Benchmarks de Desempenho

Muitas receitas incluem benchmarks de desempenho mostrando:
- **Melhorias de Latência**: Típico aumento de velocidade de 2-6x em relação ao baseline
- **Redução de Memória**: Redução de uso de memória de 50-75% com quantização
- **Preservação de Precisão**: Preservação de 95-99% da precisão

### Integração com Ferramentas de IA

As receitas funcionam perfeitamente com:
- **VS Code AI Toolkit**: Integração direta para otimização de modelos
- **Azure Machine Learning**: Fluxos de trabalho de otimização baseados em nuvem
- **ONNX Runtime**: Implantação de inferência otimizada

## Recursos Adicionais

### Links Oficiais
- **Repositório GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repositório de Receitas Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Documentação ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Exemplo Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Exemplos da Comunidade
- **Jupyter Notebooks**: Disponíveis no repositório GitHub do Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Extensão VS Code**: Visão geral do AI Toolkit para VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Postagens no Blog**: Blog de Código Aberto da Microsoft — https://opensource.microsoft.com/blog/

### Ferramentas Relacionadas
- **ONNX Runtime**: Motor de inferência de alto desempenho — https://onnxruntime.ai/
- **Transformers Hugging Face**: Fonte de muitos modelos compatíveis — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Fluxos de trabalho de otimização baseados em nuvem — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Próximos passos

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Aviso Legal**:  
Este documento foi traduzido utilizando o serviço de tradução por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informações críticas, recomenda-se a tradução profissional humana. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações incorretas decorrentes do uso desta tradução.