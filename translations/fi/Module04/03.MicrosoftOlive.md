<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T13:19:57+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fi"
}
-->
# Osa 3: Microsoft Olive Optimization Suite

## Sisällysluettelo
1. [Johdanto](../../../Module04)
2. [Mikä on Microsoft Olive?](../../../Module04)
3. [Asennus](../../../Module04)
4. [Pikaopas](../../../Module04)
5. [Esimerkki: Qwen3-mallin muuntaminen ONNX INT4 -muotoon](../../../Module04)
6. [Edistyneet käyttömahdollisuudet](../../../Module04)
7. [Olive Recipes -arkisto](../../../Module04)
8. [Parhaat käytännöt](../../../Module04)
9. [Vianmääritys](../../../Module04)
10. [Lisäresurssit](../../../Module04)

## Johdanto

Microsoft Olive on tehokas ja helppokäyttöinen laitteistotietoinen mallin optimointityökalu, joka yksinkertaistaa koneoppimismallien optimointia eri laitteistoalustoille. Olitpa kohdistamassa CPU:ta, GPU:ta tai erikoistuneita AI-kiihdyttimiä, Olive auttaa saavuttamaan parhaan suorituskyvyn säilyttäen samalla mallin tarkkuuden.

## Mikä on Microsoft Olive?

Olive on helppokäyttöinen laitteistotietoinen mallin optimointityökalu, joka yhdistää alan johtavia tekniikoita mallin pakkaamiseen, optimointiin ja kääntämiseen. Se toimii ONNX Runtime -alustalla tarjoten kokonaisvaltaisen optimointiratkaisun inferenssille.

### Keskeiset ominaisuudet

- **Laitteistotietoinen optimointi**: Valitsee automaattisesti parhaat optimointitekniikat kohdelaitteistolle
- **Yli 40 sisäänrakennettua optimointikomponenttia**: Sisältää mallin pakkaamisen, kvantisoinnin, graafin optimoinnin ja paljon muuta
- **Helppo CLI-käyttöliittymä**: Yksinkertaiset komennot yleisiin optimointitehtäviin
- **Monikehys-tuki**: Yhteensopiva PyTorchin, Hugging Facen mallien ja ONNX:n kanssa
- **Suosittujen mallien tuki**: Olive voi automaattisesti optimoida suosittuja mallirakenteita, kuten Llama, Phi, Qwen, Gemma jne.

### Hyödyt

- **Lyhyempi kehitysaika**: Ei tarvetta kokeilla manuaalisesti eri optimointitekniikoita
- **Suorituskyvyn parannukset**: Merkittäviä nopeusparannuksia (jopa 6x joissakin tapauksissa)
- **Monialustainen käyttöönotto**: Optimoidut mallit toimivat eri laitteistoilla ja käyttöjärjestelmillä
- **Säilytetty tarkkuus**: Optimoinnit säilyttävät mallin laadun parantaen samalla suorituskykyä

## Asennus

### Esivaatimukset

- Python 3.8 tai uudempi
- pip-pakettien hallinta
- Virtuaalinen ympäristö (suositeltu)

### Perusasennus

Luo ja aktivoi virtuaalinen ympäristö:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Asenna Olive automaattisten optimointiominaisuuksien kanssa:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valinnaiset riippuvuudet

Olive tarjoaa erilaisia valinnaisia riippuvuuksia lisäominaisuuksille:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Asennuksen tarkistus

```bash
olive --help
```

Jos asennus onnistuu, näet Olive CLI:n ohjeviestin.

## Pikaopas

### Ensimmäinen optimointi

Optimoidaan pieni kielimalli Oliven automaattisen optimointiominaisuuden avulla:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mitä tämä komento tekee

Optimointiprosessi sisältää: mallin hankkimisen paikallisesta välimuistista, ONNX-graafin tallentamisen ja painojen säilyttämisen ONNX-tiedostossa, ONNX-graafin optimoinnin ja mallin kvantisoinnin int4-muotoon RTN-menetelmällä.

### Komentoparametrien selitys

- `--model_name_or_path`: Hugging Facen mallin tunniste tai paikallinen polku
- `--output_path`: Hakemisto, johon optimoitu malli tallennetaan
- `--device`: Kohdelaitteisto (cpu, gpu)
- `--provider`: Suoritusalusta (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Käytä ONNX Runtime Generate AI -ominaisuutta inferenssissä
- `--precision`: Kvantisoinnin tarkkuus (int4, int8, fp16)
- `--log_level`: Lokien yksityiskohtaisuus (0=minimaalinen, 1=laaja)

## Esimerkki: Qwen3-mallin muuntaminen ONNX INT4 -muotoon

Perustuen Hugging Facen esimerkkiin [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), tässä ohjeet Qwen3-mallin optimointiin:

### Vaihe 1: Lataa malli (valinnainen)

Vähennä latausaikaa välimuistittamalla vain olennaiset tiedostot:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Vaihe 2: Optimoi Qwen3-malli

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Vaihe 3: Testaa optimoitu malli

Luo yksinkertainen Python-skripti optimoidun mallin testaamiseen:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Tulostusrakenne

Optimoinnin jälkeen tulostushakemisto sisältää:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Edistyneet käyttömahdollisuudet

### Konfiguraatiotiedostot

Monimutkaisempia optimointityönkulkuja varten voit käyttää JSON-konfiguraatiotiedostoja:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Suorita konfiguraation avulla:

```bash
olive run --config config.json
```

### GPU-optimointi

CUDA GPU -optimointia varten:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) -optimointia varten:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mallien hienosäätö Olivella

Olive tukee myös mallien hienosäätöä:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Parhaat käytännöt

### 1. Mallin valinta
- Aloita pienemmistä malleista testaukseen (esim. 0.5B-7B parametrit)
- Varmista, että kohdemallin arkkitehtuuri on Oliven tukema

### 2. Laitteistoharkinnat
- Sovita optimointitavoite käyttöönoton laitteistoon
- Käytä GPU-optimointia, jos sinulla on CUDA-yhteensopiva laitteisto
- Harkitse DirectML:ää Windows-koneille, joissa on integroitu grafiikka

### 3. Tarkkuuden valinta
- **INT4**: Maksimaalinen pakkaus, pieni tarkkuuden menetys
- **INT8**: Hyvä tasapaino koon ja tarkkuuden välillä
- **FP16**: Vähäinen tarkkuuden menetys, kohtuullinen koon pienennys

### 4. Testaus ja validointi
- Testaa aina optimoidut mallit omilla käyttötapauksillasi
- Vertaa suorituskykymittareita (viive, läpimeno, tarkkuus)
- Käytä edustavaa syöttödataa arviointiin

### 5. Iteratiivinen optimointi
- Aloita automaattisella optimoinnilla nopeiden tulosten saamiseksi
- Käytä konfiguraatiotiedostoja tarkempaan hallintaan
- Kokeile eri optimointivaiheita

## Vianmääritys

### Yleiset ongelmat

#### 1. Asennusongelmat
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU-ongelmat
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Muistiongelmat
- Käytä pienempiä eräkokoja optimoinnin aikana
- Kokeile kvantisointia korkeammalla tarkkuudella ensin (int8 int4:n sijaan)
- Varmista riittävä levytila mallin välimuistille

#### 4. Mallin latausvirheet
- Varmista mallin polku ja käyttöoikeudet
- Tarkista, vaatiiko malli `trust_remote_code=True`
- Varmista, että kaikki tarvittavat mallin tiedostot on ladattu

### Apua ongelmiin

- **Dokumentaatio**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub-ongelmat**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Esimerkit**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes -arkisto

### Johdatus Olive Recipes -arkistoon

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) -arkisto täydentää Oliven päätyökalua tarjoamalla kattavan kokoelman valmiita optimointireseptejä suosituimmille AI-malleille. Tämä arkisto toimii käytännön viitteenä sekä julkisten mallien optimointiin että omien mallien optimointityönkulkujen luomiseen.

### Keskeiset ominaisuudet

- **Yli 100 valmista reseptiä**: Valmiita optimointikonfiguraatioita suosituimmille malleille
- **Moniarkkitehtuurituki**: Kattaa transformer-mallit, visiomallit ja multimodaaliset arkkitehtuurit
- **Laitteistokohtaiset optimoinnit**: Reseptit räätälöity CPU:lle, GPU:lle ja erikoiskiihdyttimille
- **Suosittujen malliperheiden tuki**: Sisältää Phi, Llama, Qwen, Gemma, Mistral ja paljon muuta

### Tuetut malliperheet

Arkisto sisältää optimointireseptejä seuraaville:

#### Kielimallit
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5-sarja (0.5B–14B)
- **Google Gemma**: Erilaiset Gemma-mallikonfiguraatiot
- **Mistral AI**: Mistral-7B-sarja
- **DeepSeek**: R1-Distill-sarjan mallit

#### Visiomallit ja multimodaaliset mallit
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP-mallit**: Erilaiset CLIP-ViT-konfiguraatiot
- **ResNet**: ResNet-50-optimoinnit
- **Vision Transformers**: ViT-base-patch16-224

#### Erikoismallit
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Perus- ja monikieliset versiot
- **Lauseenmuuntimet**: all-MiniLM-L6-v2

### Olive Recipes -reseptien käyttö

#### Menetelmä 1: Tietyn reseptin kloonaus

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Menetelmä 2: Reseptin käyttö mallina

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Reseptin rakenne

Jokainen reseptihakemisto sisältää yleensä:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Esimerkki: Phi-4-mini-reseptin käyttö

Käytetään Phi-4-mini-reseptiä esimerkkinä:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfiguraatiotiedosto sisältää yleensä:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Reseptien muokkaaminen

#### Kohdelaitteiston muuttaminen

Päivitä `systems`-osio kohdelaitteiston vaihtamiseksi:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimointiparametrien säätäminen

Muokkaa `passes`-osiota eri optimointitasojen saavuttamiseksi:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Oman reseptin luominen

1. **Aloita samankaltaisesta mallista**: Etsi resepti mallille, jonka arkkitehtuuri on samankaltainen
2. **Päivitä mallikonfiguraatio**: Vaihda mallin nimi/polku konfiguraatiossa
3. **Säädä parametreja**: Muokkaa optimointiparametreja tarpeen mukaan
4. **Testaa ja validoi**: Suorita optimointi ja validoi tulokset
5. **Osallistu takaisin**: Harkitse reseptisi jakamista yhteisölle

### Reseptien käytön hyödyt

#### 1. **Todistetut konfiguraatiot**
- Testatut optimointiasetukset tiettyjä malleja varten
- Välttää optimaalisten parametrien etsimisen kokeilemalla

#### 2. **Laitteistokohtainen hienosäätö**
- Esivalmiiksi optimoitu eri suoritusalustoille
- Valmiit konfiguraatiot CPU-, GPU- ja NPU-kohteille

#### 3. **Kattava valikoima**
- Tukee suosituimpia avoimen lähdekoodin malleja
- Säännölliset päivitykset uusilla mallijulkaisuilla

#### 4. **Yhteisön panos**
- Yhteistyöhön perustuva kehitys AI-yhteisön kanssa
- Jaettu tieto ja parhaat käytännöt

### Osallistuminen Olive Recipes -arkistoon

Jos olet optimoinut mallin, jota arkisto ei kata:

1. **Haarauta arkisto**: Luo oma haarautus olive-recipes-arkistosta
2. **Luo reseptihakemisto**: Lisää uusi hakemisto mallillesi
3. **Lisää konfiguraatio**: Lisää olive_config.json ja tukitiedostot
4. **Dokumentoi käyttö**: Tarjoa selkeä README-ohjeistus
5. **Lähetä Pull Request**: Osallistu takaisin yhteisölle

### Suorituskykyvertailut

Monet reseptit sisältävät suorituskykyvertailuja, jotka osoittavat:
- **Viiveen parannukset**: Tyypillisesti 2–6x nopeutusta verrattuna lähtötasoon
- **Muistin vähennys**: 50–75 % muistin käytön väheneminen kvantisoinnilla
- **Tarkkuuden säilyminen**: 95–99 % tarkkuuden säilyminen

### Integraatio AI-työkalujen kanssa

Reseptit toimivat saumattomasti:
- **VS Code AI Toolkit**: Suora integraatio mallin optimointiin
- **Azure Machine Learning**: Pilvipohjaiset optimointityönkulut
- **ONNX Runtime**: Optimoitu inferenssin käyttöönotto

## Lisäresurssit

### Viralliset linkit
- **GitHub-arkisto**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes -arkisto**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime -dokumentaatio**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face -esimerkki**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Yhteisön esimerkit
- **Jupyter-muistikirjat**: Saatavilla Oliven GitHub-arkistossa — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code -laajennus**: AI Toolkit for VS Code -yleiskatsaus — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogikirjoitukset**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Liittyvät työkalut
- **ONNX Runtime**: Korkean suorituskyvyn inferenssimoottori — https://onnxruntime.ai/
- **Hugging Face Transformers**: Lähde monille yhteensopiville malleille — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Pilvipohjaiset optimointityönkulut — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Mitä seuraavaksi

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.