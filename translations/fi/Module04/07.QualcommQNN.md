<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:55:17+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "fi"
}
-->
# Osa 7: Qualcomm QNN (Qualcomm Neural Network) Optimointipaketti

## Sisällysluettelo
1. [Johdanto](../../../Module04)
2. [Mikä on Qualcomm QNN?](../../../Module04)
3. [Asennus](../../../Module04)
4. [Pikaopas](../../../Module04)
5. [Esimerkki: Mallien muuntaminen ja optimointi QNN:llä](../../../Module04)
6. [Edistynyt käyttö](../../../Module04)
7. [Parhaat käytännöt](../../../Module04)
8. [Vianetsintä](../../../Module04)
9. [Lisäresurssit](../../../Module04)

## Johdanto

Qualcomm QNN (Qualcomm Neural Network) on kattava tekoälyn inferenssikehys, joka on suunniteltu hyödyntämään täysimääräisesti Qualcommin AI-laitteistokiihdyttimiä, kuten Hexagon NPU:ta, Adreno GPU:ta ja Kryo CPU:ta. Olipa kohteena mobiililaitteet, reunalaskentaympäristöt tai autoteollisuuden järjestelmät, QNN tarjoaa optimoituja inferenssitoimintoja, jotka hyödyntävät Qualcommin erikoistuneita AI-prosessointiyksiköitä maksimaalisen suorituskyvyn ja energiatehokkuuden saavuttamiseksi.

## Mikä on Qualcomm QNN?

Qualcomm QNN on yhtenäinen tekoälyn inferenssikehys, joka mahdollistaa kehittäjien AI-mallien tehokkaan käyttöönoton Qualcommin heterogeenisessä laskenta-arkkitehtuurissa. Se tarjoaa yhtenäisen ohjelmointirajapinnan Hexagon NPU:n (Neural Processing Unit), Adreno GPU:n ja Kryo CPU:n käyttöön, valiten automaattisesti parhaan prosessointiyksikön eri mallikerroksille ja -operaatioille.

### Keskeiset ominaisuudet

- **Heterogeeninen laskenta**: Yhtenäinen pääsy NPU:hun, GPU:hun ja CPU:hun automaattisella työnjaolla
- **Laitteistotietoinen optimointi**: Erikoistuneet optimoinnit Qualcomm Snapdragon -alustoille
- **Kvantisointituki**: Kehittyneet INT8-, INT16- ja sekatarkkuuden kvantisointitekniikat
- **Mallimuunnostyökalut**: Suora tuki TensorFlow-, PyTorch-, ONNX- ja Caffe-malleille
- **Reuna-AI optimoitu**: Suunniteltu erityisesti mobiili- ja reunasovelluksiin energiatehokkuutta painottaen

### Hyödyt

- **Maksimaalinen suorituskyky**: Hyödynnä erikoistunutta AI-laitteistoa jopa 15x suorituskyvyn parannuksiin
- **Energiatehokkuus**: Optimoitu mobiili- ja akkukäyttöisille laitteille älykkäällä virranhallinnalla
- **Matala viive**: Laitteistokiihdytetty inferenssi minimaalisella viiveellä reaaliaikaisiin sovelluksiin
- **Skaalautuva käyttöönotto**: Älypuhelimista autoteollisuuden alustoihin Qualcommin ekosysteemissä
- **Valmis tuotantoon**: Testattu kehys, jota käytetään miljoonissa laitteissa

## Asennus

### Esivaatimukset

- Qualcomm QNN SDK (vaatii rekisteröitymisen Qualcommille)
- Python 3.7 tai uudempi
- Yhteensopiva Qualcomm-laitteisto tai simulaattori
- Android NDK (mobiilikäyttöönottoa varten)
- Linux- tai Windows-kehitysympäristö

### QNN SDK:n asennus

1. **Rekisteröidy ja lataa**: Käy Qualcomm Developer Network -sivustolla rekisteröitymässä ja lataamassa QNN SDK
2. **Pura SDK**: Pura QNN SDK kehityskansioosi
3. **Aseta ympäristömuuttujat**: Määritä polut QNN-työkaluille ja kirjastoille

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python-ympäristön asennus

Luo ja aktivoi virtuaaliympäristö:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Asenna tarvittavat Python-paketit:

```bash
pip install numpy tensorflow torch onnx
```

### Asennuksen tarkistus

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Jos onnistuu, näet kunkin QNN-työkalun ohjetiedot.

## Pikaopas

### Ensimmäinen mallimuunnos

Muunnetaan yksinkertainen PyTorch-malli Qualcomm-laitteistolle:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Muunna ONNX QNN-muotoon

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Luo QNN-mallikirjasto

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Mitä tämä prosessi tekee

Optimointityönkulku sisältää: alkuperäisen mallin muuntamisen ONNX-muotoon, ONNX:n kääntämisen QNN-välimuotoon, laitteistokohtaiset optimoinnit ja käännetyn mallikirjaston luomisen käyttöönottoa varten.

### Keskeiset parametrit

- `--input_network`: Lähde-ONNX-mallitiedosto
- `--output_path`: Generoitu C++-lähdetiedosto
- `--input_dim`: Syötteen tensorin mitat optimointia varten
- `--quantization_overrides`: Mukautettu kvantisointiasetus
- `-t x86_64-linux-clang`: Kohdearkkitehtuuri ja kääntäjä

## Esimerkki: Mallien muuntaminen ja optimointi QNN:llä

### Vaihe 1: Edistynyt mallimuunnos kvantisoinnilla

Näin sovelletaan mukautettua kvantisointia muunnoksen aikana:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Muunna mukautetulla kvantisoinnilla:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Vaihe 2: Monitaustainen optimointi

Konfiguroi heterogeeninen suoritus NPU:n, GPU:n ja CPU:n välillä:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Vaihe 3: Luo kontekstibinaari käyttöönottoa varten

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Vaihe 4: Inferenssi QNN-ajonaikaisella

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Tulostusrakenne

Optimoinnin jälkeen käyttöönottohakemistosi sisältää:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Edistynyt käyttö

### Mukautettu taustakonfiguraatio

Konfiguroi erityiset taustaoptimoinnit:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dynaaminen kvantisointi

Sovella kvantisointia ajonaikaisesti paremman tarkkuuden saavuttamiseksi:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Suorituskykyprofiili

Seuraa suorituskykyä eri taustoilla:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automaattinen taustavalinta

Toteuta älykäs taustavalinta mallin ominaisuuksien perusteella:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Parhaat käytännöt

### 1. Malliarkkitehtuurin optimointi
- **Kerrosfuusio**: Yhdistä operaatioita, kuten Conv+BatchNorm+ReLU, paremman NPU-hyödyn saavuttamiseksi
- **Syvyyskohtaiset erottavat konvoluutiot**: Suosi näitä tavallisten konvoluutioiden sijaan mobiilikäyttöön
- **Kvantisointiin sopivat suunnitelmat**: Käytä ReLU-aktivointeja ja vältä operaatioita, jotka eivät kvantisoidu hyvin

### 2. Kvantisointistrategia
- **Jälkikoulutuksen kvantisointi**: Aloita tällä nopeaa käyttöönottoa varten
- **Kalibrointidatasetti**: Käytä edustavaa dataa, joka kattaa kaikki syötevariaatiot
- **Sekatarkkuus**: Käytä INT8 useimmille kerroksille, pidä kriittiset kerrokset korkeammassa tarkkuudessa

### 3. Taustavalinnan ohjeet
- **NPU (HTP)**: Paras CNN-työkuormille, kvantisoiduille malleille ja virtaa säästäville sovelluksille
- **GPU**: Optimaalinen laskentaintensiivisille operaatioille, suuremmille malleille ja FP16-tarkkuudelle
- **CPU**: Varavaihtoehto tukemattomille operaatioille ja virheenkorjaukseen

### 4. Suorituskyvyn optimointi
- **Eräkoko**: Käytä eräkokoa 1 reaaliaikaisiin sovelluksiin, suurempia eriä läpimenon parantamiseksi
- **Syötteen esikäsittely**: Minimoi datan kopiointi ja muunnosviiveet
- **Kontekstin uudelleenkäyttö**: Esikäännä kontekstit välttääksesi ajonaikaisen käännösviiveen

### 5. Muistin hallinta
- **Tensorin allokointi**: Käytä staattista allokointia, kun mahdollista, välttääksesi ajonaikaisia viiveitä
- **Muistialtaat**: Toteuta mukautettuja muistialtaita usein allokoiduille tensoreille
- **Puskurin uudelleenkäyttö**: Käytä syöte-/tulospuskureita uudelleen inferenssikutsujen välillä

### 6. Virran optimointi
- **Suorituskykymoodit**: Käytä sopivia suorituskykymoodeja lämpötilarajoitusten mukaan
- **Dynaaminen taajuusskaalaus**: Anna järjestelmän skaalata taajuutta työkuorman mukaan
- **Lepotilan hallinta**: Vapauta resurssit asianmukaisesti, kun niitä ei käytetä

## Vianetsintä

### Yleiset ongelmat

#### 1. SDK:n asennusongelmat
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Mallimuunnosvirheet
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Kvantisointiongelmat
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Suorituskykyongelmat
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Muistiongelmat
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Taustayhteensopivuus
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Suorituskyvyn vianetsintä

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Apua saatavilla

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN-dokumentaatio**: Saatavilla SDK-paketissa
- **Yhteisöfoorumit**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Tekninen tuki**: Qualcommin kehittäjäportaalin kautta

## Lisäresurssit

### Viralliset linkit
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon-alustat**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Kehittäjäportaali**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Oppimisresurssit
- **Aloitusopas**: Saatavilla QNN SDK -dokumentaatiossa
- **Mallizoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimointiopas**: SDK-dokumentaatio sisältää kattavat optimointiohjeet
- **Videotutoriaalit**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integrointityökalut
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Esivalmistetut mallit Qualcomm-laitteistolle
- **Android Neural Networks API**: Integrointi Android NNAPI:n kanssa
- **TensorFlow Lite Delegate**: Qualcomm-delegaatti TFLite:lle

### Suorituskykyvertailut
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Yhteisön esimerkit
- **Esimerkkisovellukset**: Saatavilla QNN SDK:n esimerkkihakemistossa
- **GitHub-repositoriot**: Yhteisön tuottamat esimerkit ja työkalut
- **Tekniset blogit**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Liittyvät työkalut
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Kehittyneet kvantisointi- ja pakkaustekniikat
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Vertailu- ja varakäyttöönottoa varten
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Monialustainen inferenssimoottori

### Laitteiston tekniset tiedot
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon-alustat**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Mitä seuraavaksi

Jatka Edge AI -matkaasi tutkimalla [Moduuli 5: SLMOps ja tuotantokäyttöönotto](../Module05/README.md), jossa opit pienten kielimallien elinkaaren hallinnan operatiivisista näkökohdista.

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäinen asiakirja sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Tärkeissä tiedoissa suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.