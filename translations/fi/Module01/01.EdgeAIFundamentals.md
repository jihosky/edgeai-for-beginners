<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:50:13+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "fi"
}
-->
# Osa 1: EdgeAI:n perusteet

EdgeAI edustaa merkittävää muutosta tekoälyn käyttöönotossa, tuoden tekoälyominaisuudet suoraan reunalaitteisiin sen sijaan, että ne perustuisi pelkästään pilvipohjaiseen käsittelyyn. On tärkeää ymmärtää, miten EdgeAI mahdollistaa paikallisen tekoälykäsittelyn resurssirajoitteisilla laitteilla samalla kun se säilyttää kohtuullisen suorituskyvyn ja käsittelee haasteita, kuten yksityisyyttä, viivettä ja offline-ominaisuuksia.

## Johdanto

Tässä oppitunnissa tutustumme EdgeAI:hin ja sen peruskäsitteisiin. Käymme läpi perinteisen tekoälykäsittelyn paradigman, reunalaskennan haasteet, keskeiset teknologiat, jotka mahdollistavat EdgeAI:n, sekä käytännön sovelluksia eri toimialoilla.

## Oppimistavoitteet

Oppitunnin päätteeksi osaat:

- Ymmärtää perinteisen pilvipohjaisen tekoälyn ja EdgeAI:n lähestymistapojen erot.
- Tunnistaa keskeiset teknologiat, jotka mahdollistavat tekoälykäsittelyn reunalaitteilla.
- Tunnistaa EdgeAI:n toteutusten hyödyt ja rajoitukset.
- Soveltaa EdgeAI-tietämystä todellisiin tilanteisiin ja käyttötapauksiin.

## Perinteisen tekoälykäsittelyn paradigma

Perinteisesti generatiiviset tekoälysovellukset luottavat suorituskykyiseen laskentainfrastruktuuriin suurten kielimallien (LLM) tehokkaaseen käyttöön. Organisaatiot yleensä ottavat nämä mallit käyttöön GPU-klustereilla pilviympäristöissä ja käyttävät niiden ominaisuuksia API-rajapintojen kautta.

Tämä keskitetty malli toimii hyvin monissa sovelluksissa, mutta sillä on luontaisia rajoituksia reunalaskennan tilanteissa. Perinteinen lähestymistapa sisältää käyttäjän kyselyiden lähettämisen etäpalvelimille, niiden käsittelyn tehokkaalla laitteistolla ja tulosten palauttamisen internetin kautta. Vaikka tämä menetelmä tarjoaa pääsyn huipputeknisiin malleihin, se luo riippuvuuksia internet-yhteydestä, aiheuttaa viivehuolia ja herättää yksityisyyskysymyksiä, kun arkaluontoisia tietoja täytyy lähettää ulkoisille palvelimille.

On olemassa joitakin keskeisiä käsitteitä, jotka meidän täytyy ymmärtää työskennellessämme perinteisten tekoälykäsittelyn paradigmojen kanssa, nimittäin:

- **☁️ Pilvipohjainen käsittely**: Tekoälymallit toimivat tehokkaalla palvelininfrastruktuurilla, jossa on korkeat laskentaresurssit.
- **🔌 API-pohjainen käyttö**: Sovellukset käyttävät tekoälyominaisuuksia etä-API-kutsujen kautta paikallisen käsittelyn sijaan.
- **🎛️ Keskitetty mallien hallinta**: Mallit ylläpidetään ja päivitetään keskitetysti, mikä varmistaa johdonmukaisuuden mutta vaatii verkkoyhteyttä.
- **📈 Resurssien skaalautuvuus**: Pilvi-infrastruktuuri voi dynaamisesti skaalautua käsittelemään vaihtelevia laskentatarpeita.

## Reunalaskennan haasteet

Reunalaitteet, kuten kannettavat tietokoneet, matkapuhelimet ja esineiden internetin (IoT) laitteet, kuten Raspberry Pi ja NVIDIA Orin Nano, asettavat ainutlaatuisia laskennallisia rajoitteita. Näillä laitteilla on yleensä vähemmän laskentatehoa, muistia ja energiavarantoja verrattuna datakeskusten infrastruktuuriin.

Perinteisten LLM-mallien käyttö tällaisilla laitteilla on historiallisesti ollut haastavaa näiden laitteistojen rajoitusten vuoksi. Kuitenkin tarve reunalaskennalle on kasvanut yhä tärkeämmäksi eri tilanteissa. Mieti tilanteita, joissa internet-yhteys on epäluotettava tai puuttuu kokonaan, kuten etäisillä teollisuusalueilla, liikenteessä olevissa ajoneuvoissa tai alueilla, joilla verkkoyhteys on heikko. Lisäksi sovellukset, jotka vaativat korkeita turvallisuusstandardeja, kuten lääketieteelliset laitteet, finanssijärjestelmät tai valtion sovellukset, saattavat tarvita arkaluontoisten tietojen käsittelyä paikallisesti yksityisyyden ja vaatimustenmukaisuuden ylläpitämiseksi.

### Keskeiset reunalaskennan rajoitteet

Reunalaskentaympäristöt kohtaavat useita perustavanlaatuisia rajoitteita, joita perinteiset pilvipohjaiset tekoälyratkaisut eivät kohtaa:

- **Rajoitettu laskentateho**: Reunalaitteilla on yleensä vähemmän prosessoriytimiä ja alhaisemmat kellotaajuudet verrattuna palvelintason laitteistoon.
- **Muistirajoitteet**: Käytettävissä oleva RAM-muisti ja tallennuskapasiteetti ovat merkittävästi pienemmät reunalaitteilla.
- **Energian rajoitukset**: Akkuvirralla toimivien laitteiden on tasapainotettava suorituskyky ja energiankulutus pitkän käyttöajan varmistamiseksi.
- **Lämpöhallinta**: Kompaktit muodot rajoittavat jäähdytysmahdollisuuksia, mikä vaikuttaa jatkuvaan suorituskykyyn kuormituksen alla.

## Mikä on EdgeAI?

### Käsite: EdgeAI määritelty

EdgeAI tarkoittaa tekoälyalgoritmien käyttöönottoa ja suorittamista suoraan reunalaitteilla—fyysisillä laitteilla, jotka sijaitsevat verkon "reunalla", lähellä sitä paikkaa, jossa dataa tuotetaan ja kerätään. Näitä laitteita ovat muun muassa älypuhelimet, IoT-anturit, älykamerat, autonomiset ajoneuvot, puettavat laitteet ja teollisuuslaitteet. Toisin kuin perinteiset tekoälyjärjestelmät, jotka luottavat pilvipalvelimiin käsittelyssä, EdgeAI tuo älykkyyden suoraan datan lähteelle.

Perusajatuksena EdgeAI:ssa on tekoälykäsittelyn hajauttaminen, siirtäen sen pois keskitetystä datakeskuksesta ja jakamalla se laajalle laiteverkostolle, joka muodostaa digitaalisen ekosysteemimme. Tämä edustaa perustavanlaatuista arkkitehtuurimuutosta siinä, miten tekoälyjärjestelmät suunnitellaan ja otetaan käyttöön.

EdgeAI:n keskeiset käsitteelliset pilarit ovat:

- **Läheisyyskäsittely**: Laskenta tapahtuu fyysisesti lähellä datan alkuperää.
- **Hajautettu älykkyys**: Päätöksentekokyky jakautuu useille laitteille.
- **Tietosuvereniteetti**: Tiedot pysyvät paikallisessa hallinnassa, eivätkä usein koskaan poistu laitteesta.
- **Autonominen toiminta**: Laitteet voivat toimia älykkäästi ilman jatkuvaa yhteyttä.
- **Upotettu tekoäly**: Älykkyys tulee osaksi jokapäiväisten laitteiden ominaisuuksia.

### EdgeAI-arkkitehtuurin visualisointi

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

EdgeAI edustaa merkittävää muutosta tekoälyn käyttöönotossa, tuoden tekoälyominaisuudet suoraan reunalaitteisiin sen sijaan, että ne perustuisi pelkästään pilvipohjaiseen käsittelyyn. Tämä lähestymistapa mahdollistaa tekoälymallien toiminnan paikallisesti laitteilla, joilla on rajalliset laskentaresurssit, tarjoten reaaliaikaisia ennustetoimintoja ilman jatkuvaa internet-yhteyttä.

EdgeAI kattaa erilaisia teknologioita ja tekniikoita, jotka on suunniteltu tekemään tekoälymalleista tehokkaampia ja soveltuvia resurssirajoitteisille laitteille. Tavoitteena on säilyttää kohtuullinen suorituskyky samalla kun merkittävästi vähennetään tekoälymallien laskenta- ja muistivaatimuksia.

Tarkastellaan peruslähestymistapoja, jotka mahdollistavat EdgeAI:n toteutukset eri laitetyypeillä ja käyttötapauksissa.

### EdgeAI:n keskeiset periaatteet

EdgeAI perustuu useisiin perusperiaatteisiin, jotka erottavat sen perinteisestä pilvipohjaisesta tekoälystä:

- **Paikallinen käsittely**: Tekoälykäsittely tapahtuu suoraan reunalaitteella ilman ulkoista yhteyttä.
- **Resurssien optimointi**: Mallit optimoidaan erityisesti kohdelaitteiden laitteistorajoituksia varten.
- **Reaaliaikainen suorituskyky**: Käsittely tapahtuu minimaalisella viiveellä ajankohtaisissa sovelluksissa.
- **Yksityisyys suunnittelussa**: Arkaluontoiset tiedot pysyvät laitteessa, mikä parantaa turvallisuutta ja vaatimustenmukaisuutta.

## Keskeiset teknologiat, jotka mahdollistavat EdgeAI:n

### Mallien kvantisointi

Yksi tärkeimmistä tekniikoista EdgeAI:ssa on mallien kvantisointi. Tämä prosessi sisältää malliparametrien tarkkuuden vähentämisen, tyypillisesti 32-bittisistä liukuluvuista 8-bittisiin kokonaislukuihin tai jopa matalamman tarkkuuden muotoihin. Vaikka tämä tarkkuuden vähentäminen saattaa vaikuttaa huolestuttavalta, tutkimukset ovat osoittaneet, että monet tekoälymallit voivat säilyttää suorituskykynsä jopa merkittävästi pienemmällä tarkkuudella.

Kvantisointi toimii kartoittamalla liukulukuarvojen alue pienempään joukkoon diskreettejä arvoja. Esimerkiksi sen sijaan, että käytettäisiin 32 bittiä kunkin parametrin edustamiseen, kvantisointi saattaa käyttää vain 8 bittiä, mikä johtaa 4x pienempään muistivaatimukseen ja usein nopeampiin käsittelyaikoihin.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

Eri kvantisointitekniikoita ovat:

- **Jälkikoulutuskvantisointi (PTQ)**: Sovelletaan mallin koulutuksen jälkeen ilman uudelleenkoulutusta.
- **Kvantisointitietoinen koulutus (QAT)**: Sisällyttää kvantisointivaikutukset koulutuksen aikana paremman tarkkuuden saavuttamiseksi.
- **Dynaaminen kvantisointi**: Kvantisoi painot int8-muotoon, mutta laskee aktivoinnit dynaamisesti.
- **Staattinen kvantisointi**: Esilaskentaa kaikki kvantisointiparametrit sekä painoille että aktivoinneille.

EdgeAI-toteutuksissa sopivan kvantisointistrategian valinta riippuu mallin arkkitehtuurista, suorituskykyvaatimuksista ja kohdelaitteen laitteistokyvyistä.

### Mallien pakkaus ja optimointi

Kvantisoinnin lisäksi erilaiset pakkaustekniikat auttavat vähentämään mallin kokoa ja laskentavaatimuksia. Näitä ovat:

**Karsinta**: Tämä tekniikka poistaa tarpeettomia yhteyksiä tai neuroneita neuroverkoista. Tunnistamalla ja eliminoimalla parametrit, jotka vaikuttavat vähän mallin suorituskykyyn, karsinta voi merkittävästi pienentää mallin kokoa säilyttäen tarkkuuden.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Tietojen tislaus**: Tämä lähestymistapa sisältää pienemmän "oppilasmallin" kouluttamisen jäljittelemään suuremman "opettajamallin" käyttäytymistä. Oppilasmalli oppii lähentämään opettajan tuottamia tuloksia, usein saavuttaen samanlaisen suorituskyvyn merkittävästi vähemmillä parametreilla.

**Mallin arkkitehtuurin optimointi**: Tutkijat ovat kehittäneet erikoistuneita arkkitehtuureja, jotka on suunniteltu erityisesti reunakäyttöön, kuten MobileNets, EfficientNets ja muut kevyet arkkitehtuurit, jotka tasapainottavat suorituskyvyn ja laskentatehokkuuden.

### Pienet kielimallit (SLM)

Nouseva trendi EdgeAI:ssa on pienten kielimallien (SLM) kehittäminen. Nämä mallit on suunniteltu alusta alkaen kompakteiksi ja tehokkaiksi samalla kun ne tarjoavat merkittäviä luonnollisen kielen ominaisuuksia. SLM:t saavuttavat tämän huolellisilla arkkitehtuurivalinnoilla, tehokkailla koulutustekniikoilla ja keskittymällä tiettyihin toimialoihin tai tehtäviin.

Toisin kuin perinteiset lähestymistavat, jotka sisältävät suurten mallien pakkaamisen, SLM:t koulutetaan usein pienemmillä tietoaineistoilla ja optimoiduilla arkkitehtuureilla, jotka on suunniteltu erityisesti reunakäyttöön. Tämä lähestymistapa voi tuottaa malleja, jotka eivät ole vain pienempiä, vaan myös tehokkaampia tietyissä käyttötapauksissa.

## Laitteistokiihdytys EdgeAI:lle

Modernit reunalaitteet sisältävät yhä enemmän erikoistunutta laitteistoa, joka on suunniteltu kiihdyttämään tekoälytehtäviä:

### Neuroprosessoriyksiköt (NPU:t)

NPU:t ovat erikoistuneita prosessoreita, jotka on suunniteltu erityisesti neuroverkkojen laskentaan. Nämä sirut voivat suorittaa tekoälykäsittelytehtäviä paljon tehokkaammin kuin perinteiset CPU:t, usein pienemmällä energiankulutuksella. Monet modernit älypuhelimet, kannettavat tietokoneet ja IoT-laitteet sisältävät nyt NPU:ita mahdollistamaan laitekohtaisen tekoälykäsittelyn.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

Laitteet, joissa on NPU:t, sisältävät:

- **Apple**: A-sarjan ja M-sarjan sirut Neural Enginellä
- **Qualcomm**: Snapdragon-prosessorit Hexagon DSP/NPU:lla
- **Samsung**: Exynos-prosessorit NPU:lla
- **Intel**: Movidius VPU:t ja Habana Labs -kiihdyttimet
- **Microsoft**: Windows Copilot+ PC:t NPU:illa

### 🎮 GPU-kiihdytys

Vaikka reunalaitteilla ei ehkä ole datakeskusten tehokkaita GPU:ita, monilla on silti integroituja tai erillisiä GPU:ita, jotka voivat kiihdyttää tekoälytehtäviä. Modernit mobiili-GPU:t ja integroidut grafiikkaprosessorit voivat tarjota merkittäviä suorituskykyparannuksia tekoälykäsittelytehtäviin.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU-optimointi

Jopa pelkästään CPU:ta käyttävät laitteet voivat hyötyä EdgeAI:sta optimoitujen toteutusten avulla. Modernit CPU:t sisältävät erikoistuneita ohjeita tekoälytehtäviä varten, ja ohjelmistokehykset on kehitetty maksimoimaan CPU:n suorituskyky tekoälykäsittelyssä.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

EdgeAI:n parissa työskenteleville ohjelmistosuunnittelijoille näiden laitteistokiihdytysvaihtoehtojen hyödyntämisen ymmärtäminen on kriittistä kohdelaitteiden käsittelysuorituskyvyn ja energiatehokkuuden optimoimiseksi.

## EdgeAI:n hyödyt

### Yksityisyys ja turvallisuus

Yksi merkittävimmistä EdgeAI:n eduista on parantunut yksityisyys ja turvallisuus. Käsittelemällä dataa paikallisesti laitteessa arkaluontoiset tiedot eivät koskaan poistu käyttäjän hallinnasta. Tämä on erityisen tärkeää sovelluksille, jotka käsittelevät henkilökohtaisia tietoja, lääketieteellistä tietoa tai luottamuksellisia liiketoimintatietoja.

### Vähentynyt viive

EdgeAI poistaa tarpeen lähettää dataa etäpalvelimille käsittelyä varten, mikä vähentää merkittävästi viivettä. Tämä on ratkaisevan tärkeää reaaliaikaisissa sovelluksissa, kuten autonomisissa ajoneuvoissa, teollisuusautomaatiota tai interaktiivisissa sovelluksissa, joissa tarvitaan välittömiä vastauksia.

### Offline-ominaisuus

EdgeAI mahdollistaa tekoälytoiminnallisuuden jopa silloin, kun internet-yhteys ei ole käytettävissä. Tämä on arvokasta sovelluksille etäisillä alueilla, matkustamisen aikana tai tilanteissa, joissa verkkoyhteyden luotettavuus on huolenaihe.

### Kustannustehokkuus

Vähentämällä riippuvuutta pilvipohjaisista tekoälypalveluista EdgeAI voi auttaa vähentämään käyttökustannuksia, erityisesti sovelluksissa, joissa käyttömäärät ovat suuria. Organisaatiot voivat välttää jatkuvat API-kustannukset ja vähentää kaistanleveysvaatimuksia.

### Skaalaut
- [02: EdgeAI-sovellukset](02.RealWorldCaseStudies.md)

---

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty käyttämällä tekoälypohjaista käännöspalvelua [Co-op Translator](https://github.com/Azure/co-op-translator). Vaikka pyrimme tarkkuuteen, huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulisi pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmiskäännöstä. Emme ole vastuussa väärinkäsityksistä tai virhetulkinnoista, jotka johtuvat tämän käännöksen käytöstä.