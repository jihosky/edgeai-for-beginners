<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T10:32:36+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "fr"
}
-->
# Section 03 - Intégration du protocole de contexte de modèle (MCP)

## Introduction au MCP (Protocole de Contexte de Modèle)

Le Protocole de Contexte de Modèle (MCP) est une norme open-source permettant de connecter des applications d'IA à des systèmes externes. Grâce au MCP, des applications d'IA comme Claude ou ChatGPT peuvent se connecter à des sources de données (par exemple, fichiers locaux, bases de données), des outils (par exemple, moteurs de recherche, calculatrices) et des flux de travail (par exemple, invites spécialisées), leur permettant d'accéder à des informations clés et d'exécuter des tâches.

Pensez au MCP comme à un **port USB-C pour les applications d'IA**. Tout comme l'USB-C offre une manière standardisée de connecter des appareils électroniques, le MCP fournit une manière standardisée de connecter des applications d'IA à des systèmes externes.

### Que permet le MCP ?

Le MCP débloque des capacités puissantes pour les applications d'IA :

- **Assistants IA personnalisés** : Les agents peuvent accéder à votre Google Calendar et Notion, agissant comme un assistant IA plus personnalisé.
- **Génération avancée de code** : Claude Code peut créer une application web complète à partir d'un design Figma.
- **Intégration de données d'entreprise** : Les chatbots d'entreprise peuvent se connecter à plusieurs bases de données au sein d'une organisation, permettant aux utilisateurs d'analyser des données via des conversations.
- **Flux de travail créatifs** : Les modèles d'IA peuvent créer des designs 3D sur Blender et les imprimer avec une imprimante 3D.
- **Accès à des informations en temps réel** : Se connecter à des sources de données externes pour obtenir des informations actualisées.
- **Opérations complexes en plusieurs étapes** : Exécuter des flux de travail sophistiqués combinant plusieurs outils et systèmes.

### Pourquoi le MCP est-il important ?

Le MCP offre des avantages à travers tout l'écosystème :

**Pour les développeurs** : Le MCP réduit le temps et la complexité de développement lors de la création ou de l'intégration d'une application ou d'un agent d'IA.

**Pour les applications d'IA** : Le MCP donne accès à un écosystème de sources de données, d'outils et d'applications qui améliorent les capacités et l'expérience utilisateur.

**Pour les utilisateurs finaux** : Le MCP permet des applications ou agents d'IA plus performants, capables d'accéder à vos données et d'agir en votre nom lorsque nécessaire.

## Modèles de langage réduits (SLMs) dans le MCP

Les modèles de langage réduits représentent une approche efficace pour le déploiement de l'IA, offrant plusieurs avantages :

### Avantages des SLMs
- **Efficacité des ressources** : Moins de besoins en calcul.
- **Temps de réponse rapides** : Latence réduite pour les applications en temps réel.
- **Rentabilité** : Besoins d'infrastructure minimaux.
- **Confidentialité** : Peut fonctionner localement sans transmission de données.
- **Personnalisation** : Plus facile à ajuster pour des domaines spécifiques.

### Pourquoi les SLMs fonctionnent bien avec le MCP

Les SLMs associés au MCP créent une combinaison puissante où les capacités de raisonnement du modèle sont renforcées par des outils externes, compensant leur nombre réduit de paramètres par des fonctionnalités améliorées.

## Aperçu du SDK Python MCP

Le SDK Python MCP fournit la base pour créer des applications compatibles avec le MCP. Le SDK inclut :

- **Bibliothèques client** : Pour se connecter aux serveurs MCP.
- **Cadre serveur** : Pour créer des serveurs MCP personnalisés.
- **Gestionnaires de protocole** : Pour gérer les communications.
- **Intégration d'outils** : Pour exécuter des fonctions externes.

## Implémentation pratique : Client MCP Phi-4

Explorons une implémentation réelle utilisant le mini modèle Phi-4 de Microsoft intégré aux capacités MCP.

### Aperçu de l'architecture MCP

Le MCP suit une **architecture client-serveur** où un hôte MCP (une application d'IA comme Claude Code ou Claude Desktop) établit des connexions avec un ou plusieurs serveurs MCP. L'hôte MCP accomplit cela en créant un client MCP pour chaque serveur MCP.

#### Participants clés

- **Hôte MCP** : L'application d'IA qui coordonne et gère un ou plusieurs clients MCP.
- **Client MCP** : Un composant qui maintient une connexion avec un serveur MCP et obtient un contexte du serveur MCP pour que l'hôte MCP puisse l'utiliser.
- **Serveur MCP** : Un programme qui fournit un contexte aux clients MCP.

#### Architecture à deux couches

Le MCP se compose de deux couches distinctes :

**Couche de données** : Définit le protocole basé sur JSON-RPC pour la communication client-serveur, incluant :
- Gestion du cycle de vie (initialisation de la connexion, négociation des capacités).
- Primitives principales (outils, ressources, invites).
- Fonctionnalités client (échantillonnage, sollicitation, journalisation).
- Fonctionnalités utilitaires (notifications, suivi de progression).

**Couche de transport** : Définit les mécanismes et canaux de communication :
- **Transport STDIO** : Utilise les flux d'entrée/sortie standard pour les processus locaux (performance optimale, pas de surcharge réseau).
- **Transport HTTP en streaming** : Utilise HTTP POST avec des événements envoyés par le serveur pour les serveurs distants (prend en charge l'authentification HTTP standard).

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### Primitives principales du MCP

Le MCP définit des primitives qui spécifient les types d'informations contextuelles pouvant être partagées avec les applications d'IA et la gamme d'actions pouvant être effectuées.

#### Primitives serveur

Le MCP définit trois primitives principales que les serveurs peuvent exposer :

**Outils** : Fonctions exécutables que les applications d'IA peuvent invoquer pour effectuer des actions.
- Exemples : opérations sur fichiers, appels API, requêtes de base de données.
- Méthodes : `tools/list`, `tools/call`.
- Prise en charge de la découverte et de l'exécution dynamiques.

**Ressources** : Sources de données fournissant des informations contextuelles aux applications d'IA.
- Exemples : contenu de fichiers, enregistrements de bases de données, réponses API.
- Méthodes : `resources/list`, `resources/read`.
- Permet l'accès à des données structurées.

**Invites** : Modèles réutilisables qui aident à structurer les interactions avec les modèles de langage.
- Exemples : invites système, exemples few-shot.
- Méthodes : `prompts/list`, `prompts/get`.
- Standardise les modèles d'interaction avec l'IA.

#### Primitives client

Le MCP définit également des primitives que les clients peuvent exposer pour permettre des interactions plus riches :

**Échantillonnage** : Permet aux serveurs de demander des complétions de modèles de langage à l'application d'IA du client.
- Méthode : `sampling/complete`.
- Permet le développement de serveurs indépendants des modèles.
- Fournit un accès au modèle de langage de l'hôte.

**Solicitation** : Permet aux serveurs de demander des informations supplémentaires aux utilisateurs.
- Méthode : `elicitation/request`.
- Permet l'interaction et la confirmation utilisateur.
- Prend en charge la collecte dynamique d'informations.

**Journalisation** : Permet aux serveurs d'envoyer des messages de journalisation aux clients.
- Utilisé pour le débogage et la surveillance.
- Fournit une visibilité sur les opérations du serveur.

### Cycle de vie du protocole MCP

#### Initialisation et négociation des capacités

Le MCP est un protocole avec état qui nécessite une gestion du cycle de vie. Le processus d'initialisation remplit plusieurs objectifs critiques :

1. **Négociation de la version du protocole** : Assure que le client et le serveur utilisent des versions compatibles du protocole (par exemple, "2025-06-18").
2. **Découverte des capacités** : Chaque partie déclare les fonctionnalités et primitives prises en charge.
3. **Échange d'identité** : Fournit des informations d'identification et de version.

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Découverte et exécution des outils

Après l'initialisation, les clients peuvent découvrir et exécuter des outils :

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Notifications en temps réel

Le MCP prend en charge les notifications en temps réel pour les mises à jour dynamiques :

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## Premiers pas : Guide étape par étape

### Étape 1 : Configuration de l'environnement

Installez les dépendances requises :
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Étape 2 : Configuration de base

Configurez vos variables d'environnement :
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Étape 3 : Exécution de votre premier client MCP

**Configuration de base Ollama :**
```bash
python ghmodel_mcp_demo.py
```

**Utilisation du backend vLLM :**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Connexion avec événements envoyés par le serveur :**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Serveur MCP personnalisé :**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Étape 4 : Utilisation programmatique

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Fonctionnalités avancées

### Prise en charge multi-backend

L'implémentation prend en charge les backends Ollama et vLLM, vous permettant de choisir en fonction de vos besoins :

- **Ollama** : Idéal pour le développement et les tests locaux.
- **vLLM** : Optimisé pour la production et les scénarios à haut débit.

### Protocoles de connexion flexibles

Deux modes de connexion sont pris en charge :

**Mode STDIO** : Communication directe entre processus.
- Latence réduite.
- Adapté aux outils locaux.
- Configuration simple.

**Mode SSE** : Streaming basé sur HTTP.
- Compatible réseau.
- Idéal pour les systèmes distribués.
- Mises à jour en temps réel.

### Capacités d'intégration d'outils

Le système peut s'intégrer à divers outils :
- Automatisation web (Playwright).
- Opérations sur fichiers.
- Interactions API.
- Commandes système.
- Fonctions personnalisées.

## Gestion des erreurs et bonnes pratiques

### Gestion complète des erreurs

L'implémentation inclut une gestion robuste des erreurs pour :

**Erreurs de connexion :**
- Pannes du serveur MCP.
- Délais d'attente réseau.
- Problèmes de connectivité.

**Erreurs d'exécution des outils :**
- Outils manquants.
- Validation des paramètres.
- Échecs d'exécution.

**Erreurs de traitement des réponses :**
- Problèmes de parsing JSON.
- Incohérences de format.
- Anomalies dans les réponses des modèles de langage.

### Bonnes pratiques

1. **Gestion des ressources** : Utilisez des gestionnaires de contexte asynchrones.
2. **Gestion des erreurs** : Implémentez des blocs try-catch complets.
3. **Journalisation** : Activez des niveaux de journalisation appropriés.
4. **Sécurité** : Validez les entrées et nettoyez les sorties.
5. **Performance** : Utilisez le pooling de connexions et la mise en cache.

## Applications réelles

### Automatisation web
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Traitement des données
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Intégration API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optimisation des performances

### Gestion de la mémoire
- Gestion efficace de l'historique des messages.
- Nettoyage approprié des ressources.
- Pooling de connexions.

### Optimisation réseau
- Opérations HTTP asynchrones.
- Délais configurables.
- Récupération d'erreurs en douceur.

### Traitement concurrent
- I/O non bloquante.
- Exécution parallèle des outils.
- Modèles asynchrones efficaces.

## Considérations de sécurité

### Protection des données
- Gestion sécurisée des clés API.
- Validation des entrées.
- Nettoyage des sorties.

### Sécurité réseau
- Prise en charge HTTPS.
- Points de terminaison locaux par défaut.
- Gestion sécurisée des jetons.

### Sécurité d'exécution
- Filtrage des outils.
- Environnements isolés.
- Journalisation des audits.

## Écosystème et développement MCP

### Portée du projet MCP

L'écosystème du Protocole de Contexte de Modèle inclut plusieurs composants clés :

- **[Spécification MCP](https://modelcontextprotocol.io/specification/latest)** : Spécification officielle décrivant les exigences d'implémentation pour les clients et serveurs.
- **[SDKs MCP](https://modelcontextprotocol.io/docs/sdk)** : SDKs pour différents langages de programmation implémentant le MCP.
- **Outils de développement MCP** : Outils pour développer des serveurs et clients MCP, incluant le [MCP Inspector](https://github.com/modelcontextprotocol/inspector).
- **[Implémentations de serveurs de référence MCP](https://github.com/modelcontextprotocol/servers)** : Implémentations de référence des serveurs MCP.

### Premiers pas avec le développement MCP

Pour commencer à développer avec le MCP :

**Créer des serveurs** : [Créez des serveurs MCP](https://modelcontextprotocol.io/docs/develop/build-server) pour exposer vos données et outils.

**Créer des clients** : [Développez des applications](https://modelcontextprotocol.io/docs/develop/build-client) qui se connectent aux serveurs MCP.

**Apprendre les concepts** : [Comprenez les concepts clés](https://modelcontextprotocol.io/docs/learn/architecture) et l'architecture du MCP.

## Conclusion

Les SLMs intégrés au MCP représentent un changement de paradigme dans le développement des applications d'IA. En combinant l'efficacité des modèles réduits avec la puissance des outils externes, les développeurs peuvent créer des systèmes intelligents à la fois économes en ressources et très performants.

Le Protocole de Contexte de Modèle fournit une manière standardisée de connecter les applications d'IA à des systèmes externes, tout comme l'USB-C offre une norme universelle de connexion pour les appareils électroniques. Cette standardisation permet :

- **Intégration transparente** : Connecter des modèles d'IA à des sources de données et outils variés.
- **Croissance de l'écosystème** : Construire une fois, utiliser sur plusieurs applications d'IA.
- **Capacités améliorées** : Augmenter les fonctionnalités des SLMs grâce à des outils externes.
- **Mises à jour en temps réel** : Soutenir des applications d'IA dynamiques et réactives.

Points clés :
- Le MCP est une norme ouverte qui relie les applications d'IA et les systèmes externes.
- Le protocole prend en charge les outils, ressources et invites comme primitives principales.
- Les notifications en temps réel permettent des applications dynamiques et réactives.
- Une gestion appropriée du cycle de vie et des erreurs est essentielle pour une utilisation en production.
- L'écosystème offre des SDKs complets et des outils de développement.

## Références et lectures complémentaires

### Documentation officielle du MCP

- **[Site officiel du Protocole de Contexte de Modèle](https://modelcontextprotocol.io/)** - Documentation complète et spécifications.
- **[Guide de démarrage MCP](https://modelcontextprotocol.io/docs/getting-started/intro)** - Introduction et concepts clés.
- **[Aperçu de l'architecture MCP](https://modelcontextprotocol.io/docs/learn/architecture)** - Architecture technique détaillée.
- **[Spécification MCP](https://modelcontextprotocol.io/specification/latest)** - Spécification officielle du protocole.
- **[Documentation des SDKs MCP](https://modelcontextprotocol.io/docs/sdk)** - Guides des SDKs spécifiques aux langages.

### Ressources de développement

- **[MCP pour débutants](https://aka.ms/mcp-for-beginners)** - Guide complet pour débuter avec le Protocole de Contexte de Modèle.
- **[Organisation GitHub MCP](https://github.com/modelcontextprotocol)** - Dépôts officiels et exemples.
- **[Dépôt des serveurs MCP](https://github.com/modelcontextprotocol/servers)** - Implémentations de serveurs de référence.
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - Outil de développement et de débogage.
- **[Guide de création de serveurs MCP](https://modelcontextprotocol.io/docs/develop/build-server)** - Tutoriel de développement de serveurs.
- **[Guide de création de clients MCP](https://modelcontextprotocol.io/docs/develop/build-client)** - Tutoriel de développement de clients.

### Modèles de langage réduits et IA en périphérie

- **[Modèles Phi de Microsoft](https://aka.ms/phicookbook)** - Famille de modèles Phi.
- **[Documentation Foundry Local](https://github.com/microsoft/Foundry-Local)** - Runtime IA en périphérie de Microsoft.
- **[Documentation Ollama](https://ollama.ai/docs)** - Plateforme de déploiement de LLM local
- **[Documentation vLLM](https://docs.vllm.ai/)** - Service de LLM haute performance

### Normes techniques et protocoles

- **[Spécification JSON-RPC 2.0](https://www.jsonrpc.org/)** - Protocole RPC sous-jacent utilisé par MCP
- **[JSON Schema](https://json-schema.org/)** - Norme de définition de schéma pour les outils MCP
- **[Spécification OpenAPI](https://swagger.io/specification/)** - Norme de documentation API
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Norme web pour les mises à jour en temps réel

### Développement d'agents IA

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Développement d'agents prêt pour la production
- **[Documentation LangChain](https://docs.langchain.com/)** - Cadre d'intégration pour agents et outils
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - SDK d'orchestration IA de Microsoft

### Rapports et recherches sur l'industrie

- **[Annonce du protocole Model Context par Anthropic](https://www.anthropic.com/news/model-context-protocol)** - Introduction originale du MCP
- **[Enquête sur les petits modèles linguistiques](https://arxiv.org/abs/2410.20011)** - Enquête académique sur la recherche SLM
- **[Analyse du marché de l'Edge AI](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Tendances et prévisions de l'industrie
- **[Meilleures pratiques pour le développement d'agents IA](https://arxiv.org/abs/2309.02427)** - Recherche sur les architectures d'agents

Cette section fournit les bases pour créer vos propres applications MCP alimentées par SLM, ouvrant des possibilités d'automatisation, de traitement des données et d'intégration de systèmes intelligents.

## ➡️ Et après

- [Module 7. Exemples d'Edge AI](../Module07/README.md)

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction professionnelle humaine. Nous ne sommes pas responsables des malentendus ou des interprétations erronées résultant de l'utilisation de cette traduction.