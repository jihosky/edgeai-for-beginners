<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T10:33:07+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "fr"
}
-->
# Section 3 : Suite d'Optimisation Microsoft Olive

## Table des Matières
1. [Introduction](../../../Module04)
2. [Qu'est-ce que Microsoft Olive ?](../../../Module04)
3. [Installation](../../../Module04)
4. [Guide de démarrage rapide](../../../Module04)
5. [Exemple : Conversion de Qwen3 en ONNX INT4](../../../Module04)
6. [Utilisation avancée](../../../Module04)
7. [Répertoire des recettes Olive](../../../Module04)
8. [Bonnes pratiques](../../../Module04)
9. [Dépannage](../../../Module04)
10. [Ressources supplémentaires](../../../Module04)

## Introduction

Microsoft Olive est un outil puissant et facile à utiliser pour l'optimisation des modèles, conçu pour être conscient des spécificités matérielles. Il simplifie le processus d'optimisation des modèles d'apprentissage automatique pour leur déploiement sur différentes plateformes matérielles. Que vous cibliez des CPU, GPU ou des accélérateurs IA spécialisés, Olive vous aide à atteindre des performances optimales tout en préservant la précision du modèle.

## Qu'est-ce que Microsoft Olive ?

Olive est un outil d'optimisation des modèles, conscient des spécificités matérielles, qui intègre des techniques de pointe dans les domaines de la compression, de l'optimisation et de la compilation des modèles. Il fonctionne avec ONNX Runtime comme solution d'optimisation de bout en bout pour l'inférence.

### Fonctionnalités principales

- **Optimisation consciente du matériel** : Sélectionne automatiquement les meilleures techniques d'optimisation pour votre matériel cible
- **Plus de 40 composants d'optimisation intégrés** : Inclut la compression des modèles, la quantification, l'optimisation des graphes, et plus encore
- **Interface CLI facile** : Commandes simples pour les tâches d'optimisation courantes
- **Support multi-framework** : Compatible avec PyTorch, les modèles Hugging Face et ONNX
- **Support des modèles populaires** : Olive peut optimiser automatiquement des architectures de modèles populaires comme Llama, Phi, Qwen, Gemma, etc., sans configuration supplémentaire

### Avantages

- **Réduction du temps de développement** : Élimine le besoin d'expérimenter manuellement différentes techniques d'optimisation
- **Amélioration des performances** : Gains significatifs en vitesse (jusqu'à 6x dans certains cas)
- **Déploiement multiplateforme** : Les modèles optimisés fonctionnent sur différents matériels et systèmes d'exploitation
- **Précision maintenue** : Les optimisations préservent la qualité du modèle tout en améliorant les performances

## Installation

### Prérequis

- Python 3.8 ou supérieur
- Gestionnaire de paquets pip
- Environnement virtuel (recommandé)

### Installation de base

Créez et activez un environnement virtuel :

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installez Olive avec les fonctionnalités d'auto-optimisation :

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dépendances optionnelles

Olive propose diverses dépendances optionnelles pour des fonctionnalités supplémentaires :

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Vérification de l'installation

```bash
olive --help
```

Si l'installation est réussie, vous devriez voir le message d'aide de la CLI Olive.

## Guide de démarrage rapide

### Votre première optimisation

Optimisons un petit modèle de langage en utilisant la fonctionnalité d'auto-optimisation d'Olive :

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ce que fait cette commande

Le processus d'optimisation comprend : l'acquisition du modèle depuis le cache local, la capture du graphe ONNX et le stockage des poids dans un fichier de données ONNX, l'optimisation du graphe ONNX, et la quantification du modèle en int4 en utilisant la méthode RTN.

### Explication des paramètres de commande

- `--model_name_or_path` : Identifiant du modèle Hugging Face ou chemin local
- `--output_path` : Répertoire où le modèle optimisé sera sauvegardé
- `--device` : Matériel cible (cpu, gpu)
- `--provider` : Fournisseur d'exécution (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai` : Utiliser ONNX Runtime Generate AI pour l'inférence
- `--precision` : Précision de quantification (int4, int8, fp16)
- `--log_level` : Niveau de verbosité des journaux (0=minimal, 1=verbeux)

## Exemple : Conversion de Qwen3 en ONNX INT4

Basé sur l'exemple fourni par Hugging Face à [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), voici comment optimiser un modèle Qwen3 :

### Étape 1 : Télécharger le modèle (optionnel)

Pour minimiser le temps de téléchargement, mettez en cache uniquement les fichiers essentiels :

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Étape 2 : Optimiser le modèle Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Étape 3 : Tester le modèle optimisé

Créez un script Python simple pour tester votre modèle optimisé :

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Structure de sortie

Après optimisation, votre répertoire de sortie contiendra :

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Utilisation avancée

### Fichiers de configuration

Pour des workflows d'optimisation plus complexes, vous pouvez utiliser des fichiers de configuration JSON :

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Exécutez avec la configuration :

```bash
olive run --config config.json
```

### Optimisation GPU

Pour l'optimisation CUDA GPU :

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pour DirectML (Windows) :

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Affinage avec Olive

Olive prend également en charge l'affinage des modèles :

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Bonnes pratiques

### 1. Sélection du modèle
- Commencez par des modèles plus petits pour les tests (par ex., 0.5B-7B paramètres)
- Assurez-vous que l'architecture de votre modèle cible est prise en charge par Olive

### 2. Considérations matérielles
- Adaptez votre cible d'optimisation au matériel de déploiement
- Utilisez l'optimisation GPU si vous disposez de matériel compatible CUDA
- Envisagez DirectML pour les machines Windows avec graphiques intégrés

### 3. Sélection de la précision
- **INT4** : Compression maximale, légère perte de précision
- **INT8** : Bon équilibre entre taille et précision
- **FP16** : Perte de précision minimale, réduction modérée de la taille

### 4. Tests et validation
- Testez toujours les modèles optimisés avec vos cas d'utilisation spécifiques
- Comparez les métriques de performance (latence, débit, précision)
- Utilisez des données d'entrée représentatives pour l'évaluation

### 5. Optimisation itérative
- Commencez par l'auto-optimisation pour des résultats rapides
- Utilisez des fichiers de configuration pour un contrôle précis
- Expérimentez avec différents passes d'optimisation

## Dépannage

### Problèmes courants

#### 1. Problèmes d'installation
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problèmes CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problèmes de mémoire
- Utilisez des tailles de lot plus petites pendant l'optimisation
- Essayez la quantification avec une précision plus élevée d'abord (int8 au lieu de int4)
- Assurez-vous d'avoir suffisamment d'espace disque pour la mise en cache des modèles

#### 4. Erreurs de chargement de modèle
- Vérifiez le chemin du modèle et les permissions d'accès
- Vérifiez si le modèle nécessite `trust_remote_code=True`
- Assurez-vous que tous les fichiers nécessaires du modèle sont téléchargés

### Obtenir de l'aide

- **Documentation** : [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problèmes GitHub** : [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exemples** : [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Répertoire des recettes Olive

### Introduction aux recettes Olive

Le répertoire [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) complète l'outil principal Olive en fournissant une collection complète de recettes d'optimisation prêtes à l'emploi pour les modèles d'IA populaires. Ce répertoire sert de référence pratique pour optimiser des modèles disponibles publiquement et créer des workflows d'optimisation pour des modèles propriétaires.

### Fonctionnalités principales

- **100+ Recettes préconstruites** : Configurations d'optimisation prêtes à l'emploi pour des modèles populaires
- **Support multi-architecture** : Inclut des modèles transformateurs, des modèles de vision et des architectures multimodales
- **Optimisations spécifiques au matériel** : Recettes adaptées pour CPU, GPU et accélérateurs spécialisés
- **Familles de modèles populaires** : Inclut Phi, Llama, Qwen, Gemma, Mistral, et bien d'autres

### Familles de modèles prises en charge

Le répertoire inclut des recettes d'optimisation pour :

#### Modèles de langage
- **Microsoft Phi** : Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama** : Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen** : Qwen1.5-7B, Qwen2-7B, séries Qwen2.5 (0.5B à 14B)
- **Google Gemma** : Diverses configurations de modèles Gemma
- **Mistral AI** : Série Mistral-7B
- **DeepSeek** : Modèles de la série R1-Distill

#### Modèles de vision et multimodaux
- **Stable Diffusion** : v1.4, XL-base-1.0
- **Modèles CLIP** : Diverses configurations CLIP-ViT
- **ResNet** : Optimisations pour ResNet-50
- **Transformateurs de vision** : ViT-base-patch16-224

#### Modèles spécialisés
- **Whisper** : OpenAI Whisper-large-v3
- **BERT** : Variantes de base et multilingues
- **Transformateurs de phrases** : all-MiniLM-L6-v2

### Utilisation des recettes Olive

#### Méthode 1 : Cloner une recette spécifique

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Méthode 2 : Utiliser une recette comme modèle

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Structure des recettes

Chaque répertoire de recette contient généralement :

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Exemple : Utilisation de la recette Phi-4-mini

Utilisons la recette Phi-4-mini comme exemple :

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Le fichier de configuration inclut généralement :

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Personnalisation des recettes

#### Modification du matériel cible

Pour changer le matériel cible, mettez à jour la section `systems` :

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Ajustement des paramètres d'optimisation

Modifiez la section `passes` pour différents niveaux d'optimisation :

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Création de votre propre recette

1. **Commencez avec un modèle similaire** : Trouvez une recette pour un modèle avec une architecture similaire
2. **Mettez à jour la configuration du modèle** : Changez le nom/chemin du modèle dans la configuration
3. **Ajustez les paramètres** : Modifiez les paramètres d'optimisation selon vos besoins
4. **Testez et validez** : Exécutez l'optimisation et validez les résultats
5. **Contribuez au projet** : Envisagez de contribuer votre recette au répertoire

### Avantages de l'utilisation des recettes

#### 1. **Configurations éprouvées**
- Paramètres d'optimisation testés pour des modèles spécifiques
- Évite les essais-erreurs pour trouver les paramètres optimaux

#### 2. **Réglage spécifique au matériel**
- Pré-optimisé pour différents fournisseurs d'exécution
- Configurations prêtes à l'emploi pour les cibles CPU, GPU et NPU

#### 3. **Couverture complète**
- Supporte les modèles open-source les plus populaires
- Mises à jour régulières avec les nouvelles versions de modèles

#### 4. **Contributions communautaires**
- Développement collaboratif avec la communauté IA
- Partage de connaissances et de bonnes pratiques

### Contribution aux recettes Olive

Si vous avez optimisé un modèle non couvert dans le répertoire :

1. **Forkez le répertoire** : Créez votre propre fork de olive-recipes
2. **Créez un répertoire de recette** : Ajoutez un nouveau répertoire pour votre modèle
3. **Incluez la configuration** : Ajoutez olive_config.json et les fichiers de support
4. **Documentez l'utilisation** : Fournissez un README clair avec des instructions
5. **Soumettez une Pull Request** : Contribuez à la communauté

### Benchmarks de performance

De nombreuses recettes incluent des benchmarks de performance montrant :
- **Améliorations de latence** : Typiquement 2-6x plus rapide que la base
- **Réduction de mémoire** : Réduction de 50-75% de l'utilisation mémoire avec la quantification
- **Préservation de la précision** : 95-99% de précision conservée

### Intégration avec les outils IA

Les recettes fonctionnent parfaitement avec :
- **VS Code AI Toolkit** : Intégration directe pour l'optimisation des modèles
- **Azure Machine Learning** : Workflows d'optimisation basés sur le cloud
- **ONNX Runtime** : Déploiement optimisé pour l'inférence

## Ressources supplémentaires

### Liens officiels
- **Répertoire GitHub** : [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Répertoire des recettes Olive** : [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Documentation ONNX Runtime** : [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Exemple Hugging Face** : [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Exemples communautaires
- **Notebooks Jupyter** : Disponibles dans le répertoire GitHub Olive — https://github.com/microsoft/Olive/tree/main/examples
- **Extension VS Code** : Vue d'ensemble de l'AI Toolkit pour VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Articles de blog** : Blog Open Source de Microsoft — https://opensource.microsoft.com/blog/

### Outils associés
- **ONNX Runtime** : Moteur d'inférence haute performance — https://onnxruntime.ai/
- **Transformers Hugging Face** : Source de nombreux modèles compatibles — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning** : Workflows d'optimisation basés sur le cloud — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Et après ?

- [04 : Suite d'Optimisation OpenVINO Toolkit](./04.openvino.md)

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interprétations erronées résultant de l'utilisation de cette traduction.