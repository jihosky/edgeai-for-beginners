<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "45923ada94573fee7c82cc4f0c3bb344",
  "translation_date": "2025-10-28T20:02:09+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "fr"
}
-->
# EdgeAI pour D√©butants - Atelier

> **Parcours d'apprentissage pratique pour cr√©er des applications d'IA √† la pointe de la technologie pr√™tes pour la production**
>
> Ma√Ætrisez le d√©ploiement local de l'IA avec Microsoft Foundry Local, depuis la premi√®re compl√©tion de chat jusqu'√† l'orchestration multi-agents en 6 sessions progressives.

---

## üéØ Introduction

Bienvenue √† l'**Atelier EdgeAI pour D√©butants** - votre guide pratique pour cr√©er des applications intelligentes fonctionnant enti√®rement sur du mat√©riel local. Cet atelier transforme les concepts th√©oriques de l'Edge AI en comp√©tences pratiques √† travers des exercices progressifs utilisant Microsoft Foundry Local et des Small Language Models (SLMs).

### Pourquoi cet atelier ?

**La r√©volution Edge AI est en marche**

Les organisations du monde entier passent de l'IA d√©pendante du cloud √† l'informatique en p√©riph√©rie pour trois raisons essentielles :

1. **Confidentialit√© & Conformit√©** - Traitez les donn√©es sensibles localement sans transmission dans le cloud (HIPAA, RGPD, r√©glementations financi√®res)
2. **Performance** - √âliminez la latence r√©seau (50-500ms local contre 500-2000ms aller-retour dans le cloud)
3. **Ma√Ætrise des co√ªts** - Supprimez les co√ªts par jeton des API et √©voluez sans frais li√©s au cloud

**Mais l'Edge AI est diff√©rent**

Faire fonctionner l'IA sur site n√©cessite de nouvelles comp√©tences :
- S√©lection et optimisation des mod√®les pour des contraintes de ressources
- Gestion des services locaux et acc√©l√©ration mat√©rielle
- Conception de prompts pour des mod√®les plus petits
- Mod√®les de d√©ploiement en production pour les appareils en p√©riph√©rie

**Cet atelier vous apporte ces comp√©tences**

En 6 sessions cibl√©es (~3 heures au total), vous passerez de "Hello World" au d√©ploiement de syst√®mes multi-agents pr√™ts pour la production - le tout fonctionnant localement sur votre machine.

---

## üìö Objectifs d'apprentissage

En compl√©tant cet atelier, vous serez capable de :

### Comp√©tences de base
1. **D√©ployer et g√©rer des services d'IA locaux**
   - Installer et configurer Microsoft Foundry Local
   - S√©lectionner les mod√®les appropri√©s pour un d√©ploiement en p√©riph√©rie
   - G√©rer le cycle de vie des mod√®les (t√©l√©chargement, chargement, mise en cache)
   - Surveiller l'utilisation des ressources et optimiser les performances

2. **Cr√©er des applications aliment√©es par l'IA**
   - Impl√©menter des compl√©tions de chat compatibles OpenAI localement
   - Concevoir des prompts efficaces pour les Small Language Models
   - G√©rer les r√©ponses en streaming pour une meilleure exp√©rience utilisateur
   - Int√©grer des mod√®les locaux dans des applications existantes

3. **Cr√©er des syst√®mes RAG (Retrieval Augmented Generation)**
   - Construire une recherche s√©mantique avec des embeddings
   - Ancrer les r√©ponses des LLM dans des connaissances sp√©cifiques au domaine
   - √âvaluer la qualit√© des RAG avec des m√©triques standard de l'industrie
   - Passer du prototype √† la production

4. **Optimiser les performances des mod√®les**
   - √âvaluer plusieurs mod√®les pour votre cas d'utilisation
   - Mesurer la latence, le d√©bit et le temps du premier jeton
   - S√©lectionner les mod√®les optimaux en fonction des compromis vitesse/qualit√©
   - Comparer les compromis SLM vs LLM dans des sc√©narios r√©els

5. **Orchestrer des syst√®mes multi-agents**
   - Concevoir des agents sp√©cialis√©s pour diff√©rentes t√¢ches
   - Impl√©menter la m√©moire des agents et la gestion du contexte
   - Coordonner les agents dans des flux de travail complexes
   - Acheminer intelligemment les requ√™tes entre plusieurs mod√®les

6. **D√©ployer des solutions pr√™tes pour la production**
   - Impl√©menter la gestion des erreurs et la logique de reprise
   - Surveiller l'utilisation des jetons et les ressources syst√®me
   - Construire des architectures √©volutives avec des mod√®les comme outils
   - Planifier des chemins de migration de la p√©riph√©rie vers l'hybride (p√©riph√©rie + cloud)

---

## üéì R√©sultats d'apprentissage

### Ce que vous allez construire

√Ä la fin de cet atelier, vous aurez cr√©√© :

| Session | Livrable | Comp√©tences d√©montr√©es |
|---------|----------|-------------------------|
| **1** | Application de chat avec streaming | Configuration du service, compl√©tions de base, UX en streaming |
| **2** | Syst√®me RAG avec √©valuation | Embeddings, recherche s√©mantique, m√©triques de qualit√© |
| **3** | Suite de benchmarks multi-mod√®les | Mesure des performances, comparaison des mod√®les |
| **4** | Comparateur SLM vs LLM | Analyse des compromis, strat√©gies d'optimisation |
| **5** | Orchestrateur multi-agents | Conception d'agents, gestion de la m√©moire, coordination |
| **6** | Syst√®me de routage intelligent | D√©tection d'intention, s√©lection de mod√®les, √©volutivit√© |

### Matrice de comp√©tences

| Niveau de comp√©tence | Session 1-2 | Session 3-4 | Session 5-6 |
|-----------------------|-------------|-------------|-------------|
| **D√©butant** | ‚úÖ Configuration & bases | ‚ö†Ô∏è D√©fi | ‚ùå Trop avanc√© |
| **Interm√©diaire** | ‚úÖ R√©vision rapide | ‚úÖ Apprentissage cl√© | ‚ö†Ô∏è Objectifs ambitieux |
| **Avanc√©** | ‚úÖ Facile | ‚úÖ Affinement | ‚úÖ Mod√®les de production |

### Comp√©tences pr√™tes pour le march√©

**Apr√®s cet atelier, vous serez pr√™t √† :**

‚úÖ **Cr√©er des applications ax√©es sur la confidentialit√©**
- Applications de sant√© traitant des donn√©es PHI/PII localement
- Services financiers avec exigences de conformit√©
- Syst√®mes gouvernementaux avec besoins de souverainet√© des donn√©es

‚úÖ **Optimiser pour les environnements en p√©riph√©rie**
- Appareils IoT avec ressources limit√©es
- Applications mobiles fonctionnant hors ligne
- Syst√®mes en temps r√©el √† faible latence

‚úÖ **Concevoir des architectures intelligentes**
- Syst√®mes multi-agents pour des flux de travail complexes
- D√©ploiements hybrides p√©riph√©rie-cloud
- Infrastructures IA optimis√©es en termes de co√ªts

‚úÖ **Diriger des initiatives Edge AI**
- √âvaluer la faisabilit√© de l'Edge AI pour des projets
- S√©lectionner les mod√®les et frameworks appropri√©s
- Concevoir des solutions d'IA locales √©volutives

---

## üó∫Ô∏è Structure de l'atelier

### Aper√ßu des sessions (6 sessions √ó 30 minutes = 3 heures)

| Session | Sujet | Focus | Dur√©e |
|---------|-------|-------|-------|
| **1** | Premiers pas avec Foundry Local | Installation, validation, premi√®res compl√©tions | 30 min |
| **2** | Construire des solutions IA avec RAG | Conception de prompts, embeddings, √©valuation | 30 min |
| **3** | Mod√®les open source | D√©couverte, benchmarking, s√©lection | 30 min |
| **4** | Mod√®les de pointe | SLM vs LLM, optimisation, frameworks | 30 min |
| **5** | Agents aliment√©s par l'IA | Conception d'agents, orchestration, m√©moire | 30 min |
| **6** | Mod√®les comme outils | Routage, encha√Ænement, strat√©gies d'√©volutivit√© | 30 min |

---

## üöÄ D√©marrage rapide

### Pr√©requis

**Configuration syst√®me :**
- **OS** : Windows 10/11, macOS 11+ ou Linux (Ubuntu 20.04+)
- **RAM** : 8 Go minimum, 16 Go+ recommand√©
- **Stockage** : 10 Go+ d'espace libre pour les mod√®les
- **CPU** : Processeur moderne avec support AVX2
- **GPU** (optionnel) : Compatible CUDA ou NPU Qualcomm pour l'acc√©l√©ration

**Logiciels requis :**
- **Python 3.8+** ([T√©l√©charger](https://www.python.org/downloads/))
- **Microsoft Foundry Local** ([Guide d'installation](../../../Workshop))
- **Git** ([T√©l√©charger](https://git-scm.com/downloads))
- **Visual Studio Code** (recommand√©) ([T√©l√©charger](https://code.visualstudio.com/))

### Configuration en 3 √©tapes

#### 1. Installer Foundry Local

**Windows :**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS :**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**V√©rifier l'installation :**
```bash
foundry --version
foundry service status
```

**Assurez-vous que Azure AI Foundry Local fonctionne avec un port fixe**

```bash
# Set FoundryLocal to use port 58123 (default)
foundry service set --port 58123 --show

# Or use a different port
foundry service set --port 58000 --show
```

**V√©rifiez son fonctionnement :**
```bash
# Check service status
foundry service status

# Test the endpoint
curl http://127.0.0.1:58123/v1/models
```
**Recherche des mod√®les disponibles**
Pour voir quels mod√®les sont disponibles dans votre instance Foundry Local, vous pouvez interroger le point de terminaison des mod√®les :

```bash
# cmd/bash/powershell
foundry model list
```

Utilisation du point de terminaison Web 

```bash
# Windows PowerShell
powershell -Command "Invoke-RestMethod -Uri 'http://127.0.0.1:58123/v1/models' -Method Get"

# Or using curl (if available)
curl http://127.0.0.1:58123/v1/models
```

#### 2. Cloner le d√©p√¥t & installer les d√©pendances

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Ex√©cuter votre premier exemple

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples
python -m session01.chat_bootstrap "What is edge AI?"
```

**‚úÖ Succ√®s !** Vous devriez voir une r√©ponse en streaming sur l'Edge AI.

---

## üì¶ Ressources de l'atelier

### Exemples Python

Exemples pratiques progressifs d√©montrant chaque concept :

| Session | Exemple | Description | Dur√©e d'ex√©cution |
|---------|---------|-------------|-------------------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Chat de base & streaming | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG avec embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | √âvaluation de la qualit√© RAG | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Benchmarking multi-mod√®les | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | Comparaison SLM vs LLM | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Syst√®me multi-agents | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Routage bas√© sur l'intention | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Pipeline multi-√©tapes | ~60s |

### Notebooks Jupyter

Exploration interactive avec explications et visualisations :

| Session | Notebook | Description | Difficult√© |
|---------|----------|-------------|------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | Bases du chat & streaming | ‚≠ê D√©butant |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Construire un syst√®me RAG | ‚≠ê‚≠ê Interm√©diaire |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | √âvaluer la qualit√© RAG | ‚≠ê‚≠ê Interm√©diaire |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Benchmarking des mod√®les | ‚≠ê‚≠ê Interm√©diaire |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Comparaison des mod√®les | ‚≠ê‚≠ê Interm√©diaire |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Orchestration d'agents | ‚≠ê‚≠ê‚≠ê Avanc√© |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Routage bas√© sur l'intention | ‚≠ê‚≠ê‚≠ê Avanc√© |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Orchestration de pipeline | ‚≠ê‚≠ê‚≠ê Avanc√© |

### Documentation

Guides et r√©f√©rences complets :

| Document | Description | √Ä utiliser quand |
|----------|-------------|------------------|
| [QUICK_START.md](./QUICK_START.md) | Guide de configuration rapide | D√©buter √† partir de z√©ro |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Fiche de r√©f√©rence commandes & API | Besoin de r√©ponses rapides |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | Mod√®les et exemples SDK | √âcrire du code |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Guide des variables d'environnement | Configurer les exemples |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | Am√©liorations r√©centes des exemples | Comprendre les changements |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Guide de migration | Mettre √† jour le code |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Probl√®mes courants & solutions | R√©soudre les probl√®mes |

---

## üéì Recommandations pour le parcours d'apprentissage

### Pour les d√©butants (3-4 heures)
1. ‚úÖ Session 1 : Premiers pas (concentrez-vous sur la configuration et le chat de base)
2. ‚úÖ Session 2 : Bases du RAG (ignorez l'√©valuation au d√©but)
3. ‚úÖ Session 3 : Benchmarking simple (2 mod√®les seulement)
4. ‚è≠Ô∏è Ignorez les sessions 4-6 pour l'instant
5. üîÑ Revenez aux sessions 4-6 apr√®s avoir cr√©√© votre premi√®re application

### Pour les d√©veloppeurs interm√©diaires (3 heures)
1. ‚ö° Session 1 : Validation rapide de la configuration
2. ‚úÖ Session 2 : Pipeline RAG complet avec √©valuation
3. ‚úÖ Session 3 : Suite compl√®te de benchmarking
4. ‚úÖ Session 4 : Optimisation des mod√®les
5. ‚úÖ Sessions 5-6 : Concentrez-vous sur les mod√®les d'architecture

### Pour les praticiens avanc√©s (2-3 heures)
1. ‚ö° Sessions 1-3 : R√©vision rapide et validation
2. ‚úÖ Session 4 : Approfondissement de l'optimisation
3. ‚úÖ Session 5 : Architecture multi-agents
4. ‚úÖ Session 6 : Mod√®les de production et √©volutivit√©
5. üöÄ Extension : Construisez une logique de routage personnalis√©e et des d√©ploiements hybrides

---

## Pack de sessions d'atelier (laboratoires concentr√©s de 30 minutes)

Si vous suivez le format condens√© de l'atelier en 6 sessions, utilisez ces guides d√©di√©s (chacun correspond et compl√®te les modules plus larges ci-dessus) :

| Session d'atelier | Guide | Focus principal |
|-------------------|-------|-----------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Installation, validation, ex√©cution de phi & GPT-OSS-20B, acc√©l√©ration |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Conception de prompts, mod√®les RAG, ancrage de documents & CSV, migration |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Int√©gration Hugging Face, benchmarking, s√©lection de mod√®les |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, acc√©l√©ration ONNX |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | R√¥les des agents, m√©moire, outils, orchestration |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Routage, encha√Ænement, mise √† l'√©chelle vers Azure |

Chaque fichier de session inclut : r√©sum√©, objectifs d'apprentissage, d√©monstration de 30 minutes, projet de d√©marrage, liste de validation, d√©pannage et r√©f√©rences au SDK Python Foundry Local officiel.

### Scripts d'exemple

Installer les d√©pendances de l'atelier (Windows) :

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux :

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Si le service Foundry Local est ex√©cut√© sur une autre machine (Windows) ou VM depuis macOS, exportez l'endpoint :

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Session | Script(s) | Description |
|---------|-----------|-------------|
| 1 | `samples/session01/chat_bootstrap.py` | Service de d√©marrage & chat en streaming |
| 2 | `samples/session02/rag_pipeline.py` | RAG minimal (embeddings en m√©moire) |
|   | `samples/session02/rag_eval_ragas.py` | √âvaluation RAG avec m√©triques ragas |
| 3 | `samples/session03/benchmark_oss_models.py` | Benchmarking multi-mod√®le : latence & d√©bit |
| 4 | `samples/session04/model_compare.py` | Comparaison SLM vs LLM (latence & exemples de sortie) |
| 5 | `samples/session05/agents_orchestrator.py` | Pipeline de recherche ‚Üí √©ditorial √† deux agents |
| 6 | `samples/session06/models_router.py` | D√©mo de routage bas√© sur l'intention |
|   | `samples/session06/models_pipeline.py` | Cha√Æne planifier/ex√©cuter/affiner en plusieurs √©tapes |

### Variables d'environnement (communes √† tous les exemples)

| Variable | Objectif | Exemple |
|----------|----------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Alias de mod√®le unique par d√©faut pour les exemples de base | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | SLM explicite vs mod√®le plus grand pour comparaison | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Liste d'alias √† tester en benchmark | `qwen2.5-0.5b,mistral-7b` |
| `BENCH_ROUNDS` | R√©p√©titions de benchmark par mod√®le | `3` |
| `BENCH_PROMPT` | Prompt utilis√© pour le benchmark | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Mod√®le d'embedding sentence-transformers | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Remplace la requ√™te de test pour le pipeline RAG | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Remplace la requ√™te du pipeline d'agents | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Alias du mod√®le pour l'agent de recherche | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Alias du mod√®le pour l'agent √©diteur (peut diff√©rer) | `gpt-oss-20b` |
| `SHOW_USAGE` | Si `1`, affiche l'utilisation des tokens par compl√©tion | `1` |
| `RETRY_ON_FAIL` | Si `1`, r√©essaie une fois en cas d'erreur de chat transitoire | `1` |
| `RETRY_BACKOFF` | Secondes √† attendre avant de r√©essayer | `1.0` |

Si une variable n'est pas d√©finie, les scripts utilisent des valeurs par d√©faut raisonnables. Pour les d√©monstrations avec un seul mod√®le, vous n'avez g√©n√©ralement besoin que de `FOUNDRY_LOCAL_ALIAS`.

### Module utilitaire

Tous les exemples partagent d√©sormais un utilitaire `samples/workshop_utils.py` qui fournit :

* Cr√©ation mise en cache de `FoundryLocalManager` + client OpenAI
* Fonction d'aide `chat_once()` avec option de r√©essai + affichage de l'utilisation
* Rapport simple sur l'utilisation des tokens (activ√© via `SHOW_USAGE=1`)

Cela r√©duit la duplication et met en avant les meilleures pratiques pour une orchestration efficace des mod√®les locaux.

## Am√©liorations optionnelles (inter-sessions)

| Th√®me | Am√©lioration | Sessions | Env / Basculer |
|-------|-------------|----------|----------------|
| D√©terminisme | Temp√©rature fixe + ensembles de prompts stables | 1‚Äì6 | D√©finir `temperature=0`, `top_p=1` |
| Visibilit√© de l'utilisation des tokens | Enseignement coh√©rent des co√ªts/efficacit√© | 1‚Äì6 | `SHOW_USAGE=1` |
| Premier token en streaming | M√©trique de latence per√ßue | 1,3,4,6 | `BENCH_STREAM=1` (benchmark) |
| R√©silience au r√©essai | G√®re les d√©marrages √† froid transitoires | Tous | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Agents multi-mod√®les | Sp√©cialisation des r√¥les h√©t√©rog√®nes | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Routage adaptatif | Intention + heuristiques de co√ªt | 6 | √âtendre le routeur avec une logique d'escalade |
| M√©moire vectorielle | Rappel s√©mantique √† long terme | 2,5,6 | Int√©grer un index d'embedding FAISS/Chroma |
| Exportation de traces | Audit & √©valuation | 2,5,6 | Ajouter des lignes JSON par √©tape |
| Rubriques de qualit√© | Suivi qualitatif | 3‚Äì6 | Prompts de notation secondaire |
| Tests rapides | Validation rapide avant atelier | Tous | `python Workshop/tests/smoke.py` |

### D√©marrage rapide d√©terministe

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Attendez-vous √† des comptes de tokens stables pour des entr√©es identiques r√©p√©t√©es.

### √âvaluation RAG (Session 2)

Utilisez `rag_eval_ragas.py` pour calculer la pertinence des r√©ponses, leur fid√©lit√© et la pr√©cision contextuelle sur un petit ensemble de donn√©es synth√©tiques :

```powershell
cd Workshop/samples
python -m session02.rag_eval_ragas
```

√âtendez en fournissant un JSONL plus grand de questions, contextes et v√©rit√©s terrain, puis convertissez-le en un `Dataset` Hugging Face.

## Annexe sur la pr√©cision des commandes CLI

L'atelier utilise uniquement des commandes CLI Foundry Local document√©es et stables.

### Commandes stables r√©f√©renc√©es

| Cat√©gorie | Commande | Objectif |
|-----------|----------|----------|
| Noyau | `foundry --version` | Affiche la version install√©e |
| Noyau | `foundry init` | Initialise la configuration |
| Service | `foundry service start` | D√©marre le service local (si non automatique) |
| Service | `foundry status` | Affiche l'√©tat du service |
| Mod√®les | `foundry model list` | Liste le catalogue / les mod√®les disponibles |
| Mod√®les | `foundry model download <alias>` | T√©l√©charge les poids du mod√®le dans le cache |
| Mod√®les | `foundry model run <alias>` | Lance (charge) un mod√®le localement ; √† combiner avec `--prompt` pour un one-shot |
| Mod√®les | `foundry model unload <alias>` / `foundry model stop <alias>` | D√©charge un mod√®le de la m√©moire (si pris en charge) |
| Cache | `foundry cache list` | Liste les mod√®les mis en cache (t√©l√©charg√©s) |
| Syst√®me | `foundry system info` | Instantan√© des capacit√©s mat√©rielles & d'acc√©l√©ration |
| Syst√®me | `foundry system gpu-info` | Infos de diagnostic GPU |
| Config | `foundry config list` | Affiche les valeurs de configuration actuelles |
| Config | `foundry config set <key> <value>` | Met √† jour la configuration |

### Mod√®le de prompt one-shot

Au lieu de la sous-commande obsol√®te `model chat`, utilisez :

```powershell
foundry model run <alias> --prompt "Your question here"
```

Cela ex√©cute un cycle unique de prompt/r√©ponse, puis se termine.

### Mod√®les supprim√©s / √©vit√©s

| Obsol√®te / Non document√© | Remplacement / Conseils |
|--------------------------|-------------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Utilisez simplement `foundry model list` + activit√© r√©cente / journaux |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Utilisez le script de benchmark Python + outils OS (Gestionnaire des t√¢ches / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking & T√©l√©m√©trie

- Latence, p95, tokens/sec : `samples/session03/benchmark_oss_models.py`
- Latence du premier token (streaming) : d√©finissez `BENCH_STREAM=1`
- Utilisation des ressources : moniteurs OS (Gestionnaire des t√¢ches, Moniteur d'activit√©, `nvidia-smi`) + `foundry system info`.

√Ä mesure que de nouvelles commandes de t√©l√©m√©trie CLI se stabilisent en amont, elles peuvent √™tre int√©gr√©es avec des modifications minimales aux fichiers markdown des sessions.

### Garde de lint automatis√©e

Un linter automatis√© emp√™che la r√©introduction de mod√®les CLI obsol√®tes dans les blocs de code des fichiers markdown :

Script : `Workshop/scripts/lint_markdown_cli.py`

Les mod√®les obsol√®tes sont bloqu√©s dans les blocs de code.

Remplacements recommand√©s :
| Obsol√®te | Remplacement |
|----------|--------------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Script de benchmark + outils syst√®me |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Ex√©cuter localement :
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

Action GitHub : `.github/workflows/markdown-cli-lint.yml` s'ex√©cute √† chaque push & PR.

Hook pr√©-commit optionnel :
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Tableau de migration rapide CLI ‚Üí SDK

| T√¢che | Ligne de commande CLI | √âquivalent SDK (Python) | Remarques |
|-------|-----------------------|-------------------------|-----------|
| Ex√©cuter un mod√®le une fois (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | Le SDK initialise automatiquement le service & le cache |
| T√©l√©charger (mettre en cache) un mod√®le | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # d√©clenche le t√©l√©chargement/chargement` | Le gestionnaire choisit la meilleure variante si l'alias correspond √† plusieurs versions |
| Lister le catalogue | `foundry model list` | `# utiliser le gestionnaire pour chaque alias ou maintenir une liste connue` | Le CLI agr√®ge ; le SDK actuellement par instanciation d'alias |
| Lister les mod√®les en cache | `foundry cache list` | `manager.list_cached_models()` | Apr√®s l'initialisation du gestionnaire (n'importe quel alias) |
| Activer l'acc√©l√©ration GPU | `foundry config set compute.onnx.enable_gpu true` | `# Action CLI ; le SDK suppose que la configuration est d√©j√† appliqu√©e` | La configuration est un effet secondaire externe |
| Obtenir l'URL de l'endpoint | (implicite) | `manager.endpoint` | Utilis√© pour cr√©er un client compatible OpenAI |
| Pr√©parer un mod√®le | `foundry model run <alias>` puis premier prompt | `chat_once(alias, messages=[...])` (utilitaire) | Les utilitaires g√®rent le pr√©chauffage initial de la latence √† froid |
| Mesurer la latence | `python -m session03.benchmark_oss_models` | `import benchmark_oss_models` (ou nouveau script d'exportation) | Pr√©f√©rez le script pour des m√©triques coh√©rentes |
| Arr√™ter / d√©charger un mod√®le | `foundry model unload <alias>` | (Non expos√© ‚Äì red√©marrer le service / processus) | G√©n√©ralement non n√©cessaire pour le flux de l'atelier |
| R√©cup√©rer l'utilisation des tokens | (voir la sortie) | `resp.usage.total_tokens` | Fourni si le backend renvoie un objet d'utilisation |

## Exportation Markdown de Benchmark

Utilisez le script `Workshop/scripts/export_benchmark_markdown.py` pour ex√©cuter un benchmark r√©cent (m√™me logique que `samples/session03/benchmark_oss_models.py`) et g√©n√©rer un tableau Markdown compatible avec GitHub ainsi qu'un JSON brut.

### Exemple

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Fichiers g√©n√©r√©s :
| Fichier | Contenu |
|---------|---------|
| `benchmark_report.md` | Tableau Markdown + conseils d'interpr√©tation |
| `benchmark_report.json` | Tableau brut des m√©triques (pour comparaison / suivi des tendances) |

D√©finissez `BENCH_STREAM=1` dans l'environnement pour inclure la latence du premier token si pris en charge.

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.