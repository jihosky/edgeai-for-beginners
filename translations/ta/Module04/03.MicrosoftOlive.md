<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T15:33:10+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ta"
}
-->
# பகுதி 3 : மைக்ரோசாஃப்ட் ஒலிவ் ஆப்டிமைசேஷன் சுயிட்

## உள்ளடக்க அட்டவணை
1. [அறிமுகம்](../../../Module04)
2. [மைக்ரோசாஃப்ட் ஒலிவ் என்றால் என்ன?](../../../Module04)
3. [நிறுவல்](../../../Module04)
4. [விரைவான தொடக்க வழிகாட்டி](../../../Module04)
5. [உதாரணம்: Qwen3 ஐ ONNX INT4 ஆக மாற்றுதல்](../../../Module04)
6. [மேம்பட்ட பயன்பாடு](../../../Module04)
7. [ஒலிவ் ரெசிபி களஞ்சியம்](../../../Module04)
8. [சிறந்த நடைமுறைகள்](../../../Module04)
9. [சிக்கல்களை சரிசெய்தல்](../../../Module04)
10. [கூடுதல் வளங்கள்](../../../Module04)

## அறிமுகம்

மைக்ரோசாஃப்ட் ஒலிவ் என்பது சக்திவாய்ந்த, எளிதாக பயன்படுத்தக்கூடிய ஹார்ட்வேரை கருத்தில் கொண்ட மாடல் ஆப்டிமைசேஷன் கருவியாகும், இது இயந்திர கற்றல் மாடல்களை பல்வேறு ஹார்ட்வேர தளங்களில் பயன்படுத்துவதற்கான ஆப்டிமைசேஷன் செயல்முறையை எளிதாக்குகிறது. நீங்கள் CPU, GPU அல்லது சிறப்பு AI ஆக்சிலரேட்டர்களை இலக்காகக் கொண்டிருந்தாலும், ஒலிவ் மாடல் துல்லியத்தை பராமரிக்கும்போது சிறந்த செயல்திறனை அடைய உதவுகிறது.

## மைக்ரோசாஃப்ட் ஒலிவ் என்றால் என்ன?

ஒலிவ் என்பது எளிதாக பயன்படுத்தக்கூடிய ஹார்ட்வேரை கருத்தில் கொண்ட மாடல் ஆப்டிமைசேஷன் கருவியாகும், இது மாடல் சுருக்கம், ஆப்டிமைசேஷன் மற்றும் தொகுப்பு ஆகியவற்றில் முன்னணி தொழில்நுட்பங்களை ஒருங்கிணைக்கிறது. இது ONNX Runtime உடன் E2E தீர்வாக செயல்படுகிறது.

### முக்கிய அம்சங்கள்

- **ஹார்ட்வேரை கருத்தில் கொண்ட ஆப்டிமைசேஷன்**: உங்கள் இலக்கு ஹார்ட்வேருக்கான சிறந்த ஆப்டிமைசேஷன் தொழில்நுட்பங்களை தானாகவே தேர்ந்தெடுக்கிறது
- **40+ உள்ளமைக்கப்பட்ட ஆப்டிமைசேஷன் கூறுகள்**: மாடல் சுருக்கம், குவாண்டைசேஷன், கிராஃப் ஆப்டிமைசேஷன் மற்றும் பலவற்றை உள்ளடக்கியது
- **எளிய CLI இடைமுகம்**: பொதுவான ஆப்டிமைசேஷன் பணிகளுக்கான எளிய கட்டளைகள்
- **பல-கட்டமைப்பு ஆதரவு**: PyTorch, Hugging Face மாடல்கள் மற்றும் ONNX உடன் வேலை செய்கிறது
- **பிரபலமான மாடல் ஆதரவு**: Llama, Phi, Qwen, Gemma போன்ற பிரபலமான மாடல் கட்டமைப்புகளை தானாகவே ஆப்டிமைசே செய்ய ஒலிவ் உதவுகிறது

### நன்மைகள்

- **வளர்ச்சி நேரத்தை குறைத்தல்**: பல்வேறு ஆப்டிமைசேஷன் தொழில்நுட்பங்களை கையால் சோதிக்க தேவையில்லை
- **செயல்திறன் மேம்பாடுகள்**: குறிப்பிடத்தக்க வேக மேம்பாடுகள் (சில நேரங்களில் 6x வரை)
- **குறுக்குவழி பயன்பாட்டு தளம்**: ஆப்டிமைசே செய்யப்பட்ட மாடல்கள் பல்வேறு ஹார்ட்வேர்களிலும் இயக்க முறைமைகளிலும் வேலை செய்கின்றன
- **துல்லியத்தை பராமரித்தல்**: ஆப்டிமைசேஷன்கள் மாடல் தரத்தை மேம்படுத்தும்போது பராமரிக்கின்றன

## நிறுவல்

### முன் தேவைகள்

- Python 3.8 அல்லது அதற்கு மேல்
- pip பேக்கேஜ் மேலாளர்
- மெய்நிகர் சூழல் (பரிந்துரைக்கப்படுகிறது)

### அடிப்படை நிறுவல்

மெய்நிகர் சூழலை உருவாக்கி, செயல்படுத்தவும்:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ஆட்டோ-ஆப்டிமைசேஷன் அம்சங்களுடன் ஒலிவை நிறுவவும்:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### விருப்ப சார்புகள்

ஒலிவ் கூடுதல் அம்சங்களுக்கான பல்வேறு விருப்ப சார்புகளை வழங்குகிறது:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### நிறுவலை சரிபார்க்கவும்

```bash
olive --help
```

வெற்றிகரமாக இருந்தால், ஒலிவ் CLI உதவி செய்தியை நீங்கள் காணலாம்.

## விரைவான தொடக்க வழிகாட்டி

### உங்கள் முதல் ஆப்டிமைசேஷன்

ஒலிவின் ஆட்டோ-ஆப்டிமைசேஷன் அம்சத்தைப் பயன்படுத்தி ஒரு சிறிய மொழி மாடலை ஆப்டிமைசே செய்யலாம்:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### இந்த கட்டளையின் செயல்பாடு

ஆப்டிமைசேஷன் செயல்முறை: மாடலை உள்ளூர் கேஷ் மூலம் பெறுதல், ONNX கிராஃப்பை பிடித்தல் மற்றும் ONNX தரவுக் கோப்பில் எடைகளை சேமித்தல், ONNX கிராஃப்பை ஆப்டிமைசே செய்தல் மற்றும் RTN முறையைப் பயன்படுத்தி மாடலை int4 ஆக குவாண்டைசே செய்தல் ஆகியவற்றை உள்ளடக்கியது.

### கட்டளை அளவுருக்கள் விளக்கம்

- `--model_name_or_path`: Hugging Face மாடல் அடையாளம் அல்லது உள்ளூர் பாதை
- `--output_path`: ஆப்டிமைசே செய்யப்பட்ட மாடல் சேமிக்கப்படும் கோப்பகம்
- `--device`: இலக்கு சாதனம் (cpu, gpu)
- `--provider`: செயல்பாட்டு வழங்குநர் (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: inference க்கான ONNX Runtime Generate AI ஐப் பயன்படுத்தவும்
- `--precision`: குவாண்டைசேஷன் துல்லியம் (int4, int8, fp16)
- `--log_level`: பதிவு விவரக்குறிப்பு (0=minimal, 1=verbose)

## உதாரணம்: Qwen3 ஐ ONNX INT4 ஆக மாற்றுதல்

Hugging Face உதாரணத்தின் அடிப்படையில் [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), Qwen3 மாடலை எப்படி ஆப்டிமைசே செய்வது என்பதைப் பார்ப்போம்:

### படி 1: மாடலை பதிவிறக்குதல் (விருப்பம்)

பதிவிறக்க நேரத்தை குறைக்க, முக்கிய கோப்புகளை மட்டுமே கேஷ் செய்யவும்:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### படி 2: Qwen3 மாடலை ஆப்டிமைசே செய்தல்

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### படி 3: ஆப்டிமைசே செய்யப்பட்ட மாடலை சோதிக்கவும்

உங்கள் ஆப்டிமைசே செய்யப்பட்ட மாடலை சோதிக்க ஒரு எளிய Python ஸ்கிரிப்டை உருவாக்கவும்:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### வெளியீட்டு அமைப்பு

ஆப்டிமைசேஷன் முடிந்த பிறகு, உங்கள் வெளியீட்டு கோப்பகம் கீழ்கண்டவற்றை கொண்டிருக்கும்:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## மேம்பட்ட பயன்பாடு

### கட்டமைப்பு கோப்புகள்

மிகவும் சிக்கலான ஆப்டிமைசேஷன் பணிகளுக்கு, JSON கட்டமைப்பு கோப்புகளைப் பயன்படுத்தலாம்:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

கட்டமைப்புடன் இயக்கவும்:

```bash
olive run --config config.json
```

### GPU ஆப்டிமைசேஷன்

CUDA GPU ஆப்டிமைசேஷனுக்கு:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) க்கானது:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ஒலிவுடன் நுணுக்கமாக அமைத்தல்

ஒலிவ் மாடல்களை நுணுக்கமாக அமைக்கவும் ஆதரிக்கிறது:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## சிறந்த நடைமுறைகள்

### 1. மாடல் தேர்வு
- சோதனைக்கான சிறிய மாடல்களுடன் தொடங்கவும் (எ.கா., 0.5B-7B அளவுகள்)
- உங்கள் இலக்கு மாடல் கட்டமைப்பு ஒலிவ் ஆதரவைப் பெறுவதை உறுதிப்படுத்தவும்

### 2. ஹார்ட்வேரை கருத்தில் கொள்ளுதல்
- உங்கள் ஆப்டிமைசேஷன் இலக்கை உங்கள் பயன்பாட்டு ஹார்ட்வேருடன் பொருந்துமாறு அமைக்கவும்
- CUDA-இயங்கும் ஹார்ட்வேரில் GPU ஆப்டிமைசேஷனைப் பயன்படுத்தவும்
- Windows இயந்திரங்களுக்கு DirectML ஐ பரிந்துரைக்கவும்

### 3. துல்லியத் தேர்வு
- **INT4**: அதிகபட்ச சுருக்கம், சிறிய துல்லிய இழப்பு
- **INT8**: அளவு மற்றும் துல்லியத்தின் நல்ல சமநிலை
- **FP16**: குறைந்த துல்லிய இழப்பு, மிதமான அளவு குறைப்பு

### 4. சோதனை மற்றும் சரிபார்ப்பு
- உங்கள் குறிப்பிட்ட பயன்பாடுகளுடன் ஆப்டிமைசே செய்யப்பட்ட மாடல்களை எப்போதும் சோதிக்கவும்
- செயல்திறன் அளவுகோல்களை ஒப்பிடவும் (தாமதம், துரிதம், துல்லியம்)
- மதிப்பீட்டுக்கான பிரதிநிதி உள்ளீட்டு தரவுகளைப் பயன்படுத்தவும்

### 5. மீண்டும் மீண்டும் ஆப்டிமைசேஷன்
- விரைவான முடிவுகளுக்காக ஆட்டோ-ஆப்டிமைசேஷனுடன் தொடங்கவும்
- நுணுக்கமான கட்டுப்பாட்டுக்கான கட்டமைப்பு கோப்புகளைப் பயன்படுத்தவும்
- பல்வேறு ஆப்டிமைசேஷன் பாஸ்களுடன் பரிசோதிக்கவும்

## சிக்கல்களை சரிசெய்தல்

### பொதுவான சிக்கல்கள்

#### 1. நிறுவல் சிக்கல்கள்
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU சிக்கல்கள்
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. நினைவக சிக்கல்கள்
- ஆப்டிமைசேஷனின் போது சிறிய தொகுதி அளவுகளைப் பயன்படுத்தவும்
- முதலில் அதிக துல்லியத்துடன் குவாண்டைசேஷனை முயற்சிக்கவும் (int8 int4 இற்கு பதிலாக)
- மாடல் கேஷிங் க்கான போதுமான டிஸ்க் இடத்தை உறுதிப்படுத்தவும்

#### 4. மாடல் ஏற்றுதல் பிழைகள்
- மாடல் பாதை மற்றும் அணுகல் அனுமதிகளை சரிபார்க்கவும்
- மாடல் `trust_remote_code=True` ஐ தேவைப்படுகிறதா என்பதை சரிபார்க்கவும்
- தேவையான அனைத்து மாடல் கோப்புகளும் பதிவிறக்கப்பட்டுள்ளதா என்பதை உறுதிப்படுத்தவும்

### உதவி பெறுதல்

- **ஆவணங்கள்**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub சிக்கல்கள்**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **உதாரணங்கள்**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## ஒலிவ் ரெசிபி களஞ்சியம்

### ஒலிவ் ரெசிபிகளின் அறிமுகம்

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) களஞ்சியம் முக்கிய AI மாடல்களுக்கான தயாராக பயன்படுத்தக்கூடிய ஆப்டிமைசேஷன் ரெசிபிகளின் விரிவான தொகுப்பை வழங்குவதன் மூலம் ஒலிவ் கருவியைப் पूர்த்துகிறது. இந்த களஞ்சியம் பொதுவாக கிடைக்கும் மாடல்களை ஆப்டிமைசே செய்வதற்கும், சொந்த மாடல்களுக்கான ஆப்டிமைசேஷன் பணிகளை உருவாக்குவதற்கும் ஒரு நடைமுறை குறிப்பாக செயல்படுகிறது.

### முக்கிய அம்சங்கள்

- **100+ முன்-உருவாக்கப்பட்ட ரெசிபிகள்**: பிரபலமான மாடல்களுக்கான தயாராக பயன்படுத்தக்கூடிய ஆப்டிமைசேஷன் கட்டமைப்புகள்
- **பல கட்டமைப்பு ஆதரவு**: மாற்றம் மாடல்கள், பார்வை மாடல்கள் மற்றும் பல்வேறு கட்டமைப்புகளை உள்ளடக்கியது
- **ஹார்ட்வேர-குறிப்பான ஆப்டிமைசேஷன்கள்**: CPU, GPU மற்றும் சிறப்பு ஆக்சிலரேட்டர்களுக்கான ரெசிபிகள்
- **பிரபலமான மாடல் குடும்பங்கள்**: Phi, Llama, Qwen, Gemma, Mistral மற்றும் பலவற்றை உள்ளடக்கியது

### ஆதரவு மாடல் குடும்பங்கள்

களஞ்சியம் ஆப்டிமைசேஷன் ரெசிபிகளை வழங்குகிறது:

#### மொழி மாடல்கள்
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 தொடர் (0.5B முதல் 14B வரை)
- **Google Gemma**: பல்வேறு Gemma மாடல் கட்டமைப்புகள்
- **Mistral AI**: Mistral-7B தொடர்
- **DeepSeek**: R1-Distill தொடர் மாடல்கள்

#### பார்வை மற்றும் பல்வேறு மாடல்கள்
- **ஸ்டேபிள் டிஃப்யூஷன்**: v1.4, XL-base-1.0
- **CLIP மாடல்கள்**: பல CLIP-ViT கட்டமைப்புகள்
- **ResNet**: ResNet-50 ஆப்டிமைசேஷன்கள்
- **விஷன் டிரான்ஸ்ஃபார்மர்கள்**: ViT-base-patch16-224

#### சிறப்பு மாடல்கள்
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: அடிப்படை மற்றும் பன்மொழி மாறுபாடுகள்
- **Sentence Transformers**: all-MiniLM-L6-v2

### ஒலிவ் ரெசிபிகளைப் பயன்படுத்துதல்

#### முறை 1: குறிப்பிட்ட ரெசிபியை கிளோன் செய்யவும்

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### முறை 2: ரெசிபியை மாதிரியாகப் பயன்படுத்தவும்

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### ரெசிபி அமைப்பு

ஒவ்வொரு ரெசிபி கோப்பகமும் பொதுவாகக் கொண்டிருக்கும்:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### உதாரணம்: Phi-4-mini ரெசிபியைப் பயன்படுத்துதல்

Phi-4-mini ரெசிபியை உதாரணமாகப் பயன்படுத்தலாம்:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

கட்டமைப்பு கோப்பு பொதுவாகக் கொண்டிருக்கும்:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### ரெசிபிகளை தனிப்பயனாக்குதல்

#### இலக்கு ஹார்ட்வேரை மாற்றுதல்

`systems` பிரிவை புதுப்பிக்கவும்:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### ஆப்டிமைசேஷன் அளவுருக்களை சரிசெய்தல்

வெவ்வேறு ஆப்டிமைசேஷன் நிலைகளுக்கு `passes` பிரிவை மாற்றவும்:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### உங்கள் சொந்த ரெசிபியை உருவாக்குதல்

1. **சமமான மாடலுடன் தொடங்கவும்**: ஒரே கட்டமைப்புடன் மாடலுக்கான ரெசிபியை கண்டறியவும்
2. **மாடல் கட்டமைப்பை புதுப்பிக்கவும்**: கட்டமைப்பில் மாடல் பெயர்/பாதையை மாற்றவும்
3. **அளவுருக்களை சரிசெய்யவும்**: தேவையான ஆப்டிமைசேஷன் அளவுருக்களை மாற்றவும்
4. **சோதனை மற்றும் சரிபார்ப்பு**: ஆப்டிமைசேஷனை இயக்கி முடிவுகளை சரிபார்க்கவும்
5. **திரும்ப பங்களிக்கவும்**: உங்கள் ரெசிபியை களஞ்சியத்திற்கு பங்களிக்க பரிந்துரைக்கவும்

### ரெசிபிகளைப் பயன்படுத்துவதன் நன்மைகள்

#### 1. **சோதிக்கப்பட்ட கட்டமைப்புகள்**
- குறிப்பிட்ட மாடல்களுக்கு சோதிக்கப்பட்ட ஆப்டிமைசேஷன் அமைப்புகள்
- சிறந்த அளவுருக்களை கண்டறிய முயற்சிகளைத் தவிர்க்கிறது

#### 2. **ஹார்ட்வேர-குறிப்பான அமைப்பு**
- வெவ்வேறு செயல்பாட்டு வழங்குநர்களுக்கு முன்-ஆப்டிமைசே செய்யப்பட்டவை
- CPU, GPU மற்றும் NPU இலக்குகளுக்கான தயாராக பயன்படுத்தக்கூடிய அமைப்புகள்

#### 3. **விரிவான கவரேஜ்**
- மிகவும் பிரபலமான திறந்த மூல மாடல்களை ஆதரிக்கிறது
- புதிய மாடல் வெளியீடுகளுடன் வழக்கமான புதுப்பிப்புகள்

#### 4. **சமூக பங்களிப்புகள்**
- AI சமூகத்துடன் ஒத்துழைப்பு வளர்ச்சி
- பகிரப்பட்ட அறிவு மற்றும் சிறந்த நடைமுறைகள்

### ஒலிவ் ரெசிபிகளுக்கு பங்களித்தல்

களஞ்சியத்தில் உள்ளடங்காத மாடலை நீங்கள் ஆப்டிமைசே செய்திருந்தால்:

1. **களஞ்சியத்தை கிளோன் செய்யவும்**: olive-recipes க்கு உங்கள் சொந்த கிளோனை உருவாக்கவும்
2. **ரெசிபி கோப்பகத்தை உருவாக்கவும்**: உங்கள் ம

---

**குறிப்பு**:  
இந்த ஆவணம் AI மொழிபெயர்ப்பு சேவை [Co-op Translator](https://github.com/Azure/co-op-translator) பயன்படுத்தி மொழிபெயர்க்கப்பட்டுள்ளது. எங்களது துல்லியத்திற்கான முயற்சிகள் இருந்தாலும், தானியங்கி மொழிபெயர்ப்புகளில் பிழைகள் அல்லது தவறுகள் இருக்கக்கூடும் என்பதை கவனத்தில் கொள்ளவும். அதன் தாய்மொழியில் உள்ள மூல ஆவணம் அதிகாரப்பூர்வ ஆதாரமாக கருதப்பட வேண்டும். முக்கியமான தகவல்களுக்கு, தொழில்முறை மனித மொழிபெயர்ப்பு பரிந்துரைக்கப்படுகிறது. இந்த மொழிபெயர்ப்பைப் பயன்படுத்துவதால் ஏற்படும் எந்த தவறான புரிதல்கள் அல்லது தவறான விளக்கங்களுக்கு நாங்கள் பொறுப்பல்ல.