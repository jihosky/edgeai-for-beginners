<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T14:06:06+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "hu"
}
-->
# AI √ºgyn√∂k√∂k √©s Kis Nyelvi Modellek: √Åtfog√≥ √∫tmutat√≥

## Bevezet√©s

Ebben az oktat√≥anyagban az AI √ºgyn√∂k√∂ket √©s Kis Nyelvi Modelleket (SLM-eket) vizsg√°ljuk meg, valamint azok fejlett megval√≥s√≠t√°si strat√©gi√°it √©lvonalbeli sz√°m√≠t√°si k√∂rnyezetekben. √Åttekintj√ºk az √ºgyn√∂ki AI alapfogalmait, az SLM optimaliz√°l√°si technik√°kat, a gyakorlatban alkalmazhat√≥ telep√≠t√©si strat√©gi√°kat er≈ëforr√°s-korl√°tozott eszk√∂z√∂kre, valamint a Microsoft Agent Framework-√∂t, amely lehet≈ëv√© teszi termel√©sre k√©sz √ºgyn√∂ki rendszerek l√©trehoz√°s√°t.

A mesters√©ges intelligencia ter√ºlete 2025-ben paradigmav√°lt√°son megy kereszt√ºl. M√≠g 2023 a chatbotok √©ve volt, 2024-ben a copilotok robban√°sszer≈± n√∂veked√©s√©t tapasztaltuk, 2025 az AI √ºgyn√∂k√∂k√© ‚Äî intelligens rendszerek√©, amelyek gondolkodnak, √©rvelnek, terveznek, eszk√∂z√∂ket haszn√°lnak, √©s minim√°lis emberi beavatkoz√°ssal hajtanak v√©gre feladatokat, egyre ink√°bb hat√©kony Kis Nyelvi Modellek √°ltal t√°mogatva. A Microsoft Agent Framework vezet≈ë megold√°sk√©nt jelenik meg ezeknek az intelligens rendszereknek offline, √©lvonalbeli k√©pess√©gekkel t√∂rt√©n≈ë l√©trehoz√°s√°ra.

## Tanul√°si c√©lok

Az oktat√≥anyag v√©g√©re k√©pes leszel:

- ü§ñ Meg√©rteni az AI √ºgyn√∂k√∂k √©s √ºgyn√∂ki rendszerek alapfogalmait
- üî¨ Azonos√≠tani a Kis Nyelvi Modellek el≈ënyeit a Nagy Nyelvi Modellekkel szemben √ºgyn√∂ki alkalmaz√°sokban
- üöÄ Megtanulni fejlett SLM telep√≠t√©si strat√©gi√°kat √©lvonalbeli sz√°m√≠t√°si k√∂rnyezetekhez
- üì± Gyakorlatban alkalmazhat√≥ SLM-alap√∫ √ºgyn√∂k√∂ket megval√≥s√≠tani val√≥s alkalmaz√°sokhoz
- üèóÔ∏è Termel√©sre k√©sz √ºgyn√∂k√∂ket √©p√≠teni a Microsoft Agent Framework seg√≠ts√©g√©vel
- üåê Offline √©lvonalbeli √ºgyn√∂k√∂ket telep√≠teni helyi LLM √©s SLM integr√°ci√≥val
- üîß Integr√°lni a Microsoft Agent Framework-√∂t a Foundry Local-lal √©lvonalbeli telep√≠t√©shez

## Az AI √ºgyn√∂k√∂k meg√©rt√©se: Alapok √©s oszt√°lyoz√°s

### Meghat√°roz√°s √©s alapfogalmak

A mesters√©ges intelligencia (AI) √ºgyn√∂k olyan rendszer vagy program, amely k√©pes √∂n√°ll√≥an feladatokat v√©grehajtani egy felhaszn√°l√≥ vagy m√°s rendszer nev√©ben, mik√∂zben megtervezi a munkafolyamat√°t √©s haszn√°lja a rendelkez√©sre √°ll√≥ eszk√∂z√∂ket. A hagyom√°nyos AI-val ellent√©tben, amely csak v√°laszol a k√©rd√©seidre, egy √ºgyn√∂k √∂n√°ll√≥an cselekedhet c√©lok el√©r√©se √©rdek√©ben.

### √úgyn√∂k oszt√°lyoz√°si keretrendszer

Az √ºgyn√∂k hat√°rainak meg√©rt√©se seg√≠t a megfelel≈ë √ºgyn√∂kt√≠pusok kiv√°laszt√°s√°ban k√ºl√∂nb√∂z≈ë sz√°m√≠t√°si forgat√≥k√∂nyvekhez:

- **üî¨ Egyszer≈± reflex √ºgyn√∂k√∂k**: Szab√°lyalap√∫ rendszerek, amelyek azonnali √©rz√©kel√©sekre reag√°lnak (termoszt√°tok, alapvet≈ë automatiz√°l√°s)
- **üì± Modellalap√∫ √ºgyn√∂k√∂k**: Rendszerek, amelyek bels≈ë √°llapotot √©s mem√≥ri√°t tartanak fenn (robotporsz√≠v√≥k, navig√°ci√≥s rendszerek)
- **‚öñÔ∏è C√©lalap√∫ √ºgyn√∂k√∂k**: Rendszerek, amelyek terveket k√©sz√≠tenek √©s v√©grehajtj√°k a c√©lok el√©r√©s√©hez sz√ºks√©ges l√©p√©seket (√∫tvonaltervez≈ëk, feladat√ºtemez≈ëk)
- **üß† Tanul√≥ √ºgyn√∂k√∂k**: Adapt√≠v rendszerek, amelyek id≈ëvel jav√≠tj√°k teljes√≠tm√©ny√ºket (aj√°nl√≥rendszerek, szem√©lyre szabott asszisztensek)

### Az AI √ºgyn√∂k√∂k kulcsfontoss√°g√∫ el≈ënyei

Az AI √ºgyn√∂k√∂k sz√°mos alapvet≈ë el≈ënyt k√≠n√°lnak, amelyek ide√°liss√° teszik ≈ëket √©lvonalbeli sz√°m√≠t√°si alkalmaz√°sokhoz:

**M≈±k√∂d√©si auton√≥mia**: Az √ºgyn√∂k√∂k f√ºggetlen feladatv√©grehajt√°st biztos√≠tanak √°lland√≥ emberi fel√ºgyelet n√©lk√ºl, ami ide√°lis val√≥s idej≈± alkalmaz√°sokhoz. Minim√°lis fel√ºgyeletet ig√©nyelnek, mik√∂zben adapt√≠v viselked√©st tartanak fenn, lehet≈ëv√© t√©ve az er≈ëforr√°s-korl√°tozott eszk√∂z√∂k√∂n t√∂rt√©n≈ë telep√≠t√©st cs√∂kkentett m≈±k√∂d√©si k√∂lts√©gekkel.

**Telep√≠t√©si rugalmass√°g**: Ezek a rendszerek lehet≈ëv√© teszik az eszk√∂z√∂n t√∂rt√©n≈ë AI k√©pess√©geket internetkapcsolat n√©lk√ºl, n√∂velik a mag√°n√©let v√©delm√©t √©s biztons√°g√°t helyi feldolgoz√°ssal, testreszabhat√≥k domain-specifikus alkalmaz√°sokhoz, √©s alkalmasak k√ºl√∂nb√∂z≈ë √©lvonalbeli sz√°m√≠t√°si k√∂rnyezetekhez.

**K√∂lts√©ghat√©konys√°g**: Az √ºgyn√∂ki rendszerek k√∂lts√©ghat√©kony telep√≠t√©st k√≠n√°lnak a felh≈ëalap√∫ megold√°sokhoz k√©pest, cs√∂kkentett m≈±k√∂d√©si k√∂lts√©gekkel √©s alacsonyabb s√°vsz√©less√©g-ig√©nnyel √©lvonalbeli alkalmaz√°sokhoz.

## Fejlett Kis Nyelvi Modell strat√©gi√°k

### SLM (Kis Nyelvi Modell) alapok

A Kis Nyelvi Modell (SLM) olyan nyelvi modell, amely elf√©r egy √°ltal√°nos fogyaszt√≥i elektronikai eszk√∂z√∂n, √©s elegend≈ëen alacsony k√©sleltet√©ssel v√©gez k√∂vetkeztet√©seket, hogy praktikus legyen egy felhaszn√°l√≥ √ºgyn√∂ki k√©r√©seinek kiszolg√°l√°s√°ra. Gyakorlatilag az SLM-ek √°ltal√°ban kevesebb mint 10 milli√°rd param√©terrel rendelkeznek.

**Form√°tum felfedez√©si funkci√≥k**: Az SLM-ek fejlett t√°mogat√°st k√≠n√°lnak k√ºl√∂nb√∂z≈ë kvant√°l√°si szintekhez, platformok k√∂z√∂tti kompatibilit√°shoz, val√≥s idej≈± teljes√≠tm√©nyoptimaliz√°l√°shoz √©s √©lvonalbeli telep√≠t√©si k√©pess√©gekhez. A felhaszn√°l√≥k fokozott adatv√©delmet √©rhetnek el helyi feldolgoz√°ssal √©s WebGPU t√°mogat√°ssal b√∂ng√©sz≈ëalap√∫ telep√≠t√©shez.

**Kvant√°l√°si szint gy≈±jtem√©nyek**: N√©pszer≈± SLM form√°tumok k√∂z√© tartozik a Q4_K_M mobilalkalmaz√°sokhoz kiegyens√∫lyozott t√∂m√∂r√≠t√©ssel, a Q5_K_S sorozat min≈ës√©gk√∂zpont√∫ √©lvonalbeli telep√≠t√©shez, a Q8_0 k√∂zel eredeti pontoss√°ggal er≈ëteljes √©lvonalbeli eszk√∂z√∂k√∂n, valamint k√≠s√©rleti form√°tumok, mint a Q2_K ultra-alacsony er≈ëforr√°s forgat√≥k√∂nyvekhez.

### GGUF (√Åltal√°nos GGML Univerz√°lis Form√°tum) SLM telep√≠t√©shez

A GGUF az els≈ëdleges form√°tum a kvant√°lt SLM-ek CPU √©s √©lvonalbeli eszk√∂z√∂k√∂n t√∂rt√©n≈ë telep√≠t√©s√©hez, kifejezetten √ºgyn√∂ki alkalmaz√°sokhoz optimaliz√°lva:

**√úgyn√∂k-optimaliz√°lt funkci√≥k**: A form√°tum √°tfog√≥ er≈ëforr√°sokat biztos√≠t az SLM konverzi√≥hoz √©s telep√≠t√©shez, fejlett t√°mogat√°ssal eszk√∂zhaszn√°lathoz, struktur√°lt kimenet gener√°l√°shoz √©s t√∂bbfordul√≥s besz√©lget√©sekhez. A platformok k√∂z√∂tti kompatibilit√°s biztos√≠tja az √ºgyn√∂k√∂k k√∂vetkezetes viselked√©s√©t k√ºl√∂nb√∂z≈ë √©lvonalbeli eszk√∂z√∂k√∂n.

**Teljes√≠tm√©nyoptimaliz√°l√°s**: A GGUF hat√©kony mem√≥riahaszn√°latot tesz lehet≈ëv√© az √ºgyn√∂ki munkafolyamatokhoz, t√°mogatja a dinamikus modellbet√∂lt√©st t√∂bb √ºgyn√∂ki rendszerekhez, √©s optimaliz√°lt k√∂vetkeztet√©st biztos√≠t val√≥s idej≈± √ºgyn√∂ki interakci√≥khoz.

### √âlvonalra optimaliz√°lt SLM keretrendszerek

#### Llama.cpp optimaliz√°l√°s √ºgyn√∂k√∂kh√∂z

A Llama.cpp korszer≈± kvant√°l√°si technik√°kat k√≠n√°l, kifejezetten √ºgyn√∂ki SLM telep√≠t√©shez optimaliz√°lva:

**√úgyn√∂k-specifikus kvant√°l√°s**: A keretrendszer t√°mogatja a Q4_0 (optim√°lis mobil √ºgyn√∂ki telep√≠t√©shez 75%-os m√©retcs√∂kkent√©ssel), Q5_1 (kiegyens√∫lyozott min≈ës√©g-t√∂m√∂r√≠t√©s √©lvonalbeli k√∂vetkeztet√©si √ºgyn√∂k√∂kh√∂z) √©s Q8_0 (k√∂zel eredeti min≈ës√©g termel√©si √ºgyn√∂ki rendszerekhez). Fejlett form√°tumok lehet≈ëv√© teszik ultra-t√∂m√∂r√≠tett √ºgyn√∂k√∂ket extr√©m √©lvonalbeli forgat√≥k√∂nyvekhez.

**Megval√≥s√≠t√°si el≈ëny√∂k**: CPU-optimaliz√°lt k√∂vetkeztet√©s SIMD gyors√≠t√°ssal mem√≥riahat√©kony √ºgyn√∂ki v√©grehajt√°st biztos√≠t. A platformok k√∂z√∂tti kompatibilit√°s x86, ARM √©s Apple Silicon architekt√∫r√°kon univerz√°lis √ºgyn√∂ki telep√≠t√©si k√©pess√©geket tesz lehet≈ëv√©.

#### Apple MLX keretrendszer SLM √ºgyn√∂k√∂kh√∂z

Az Apple MLX nat√≠v optimaliz√°l√°st biztos√≠t, kifejezetten SLM-alap√∫ √ºgyn√∂k√∂kh√∂z Apple Silicon eszk√∂z√∂k√∂n:

**Apple Silicon √ºgyn√∂k optimaliz√°l√°s**: A keretrendszer egys√©ges mem√≥riaarchitekt√∫r√°t haszn√°l Metal Performance Shaders integr√°ci√≥val, automatikus vegyes pontoss√°got √ºgyn√∂ki k√∂vetkeztet√©shez, √©s optimaliz√°lt mem√≥ria-s√°vsz√©less√©get t√∂bb √ºgyn√∂ki rendszerekhez. Az SLM √ºgyn√∂k√∂k kiv√©teles teljes√≠tm√©nyt mutatnak M-sorozat√∫ chipeken.

**Fejleszt√©si funkci√≥k**: Python √©s Swift API t√°mogat√°s √ºgyn√∂k-specifikus optimaliz√°l√°sokkal, automatikus differenci√°l√°s √ºgyn√∂ki tanul√°shoz, √©s z√∂kken≈ëmentes integr√°ci√≥ az Apple fejleszt≈ëi eszk√∂z√∂kkel √°tfog√≥ √ºgyn√∂ki fejleszt√©si k√∂rnyezeteket biztos√≠t.

#### ONNX Runtime t√∂bbplatformos SLM √ºgyn√∂k√∂kh√∂z

Az ONNX Runtime univerz√°lis k√∂vetkeztet√©si motort biztos√≠t, amely lehet≈ëv√© teszi az SLM √ºgyn√∂k√∂k k√∂vetkezetes futtat√°s√°t k√ºl√∂nb√∂z≈ë hardverplatformokon √©s oper√°ci√≥s rendszereken:

**Univerz√°lis telep√≠t√©s**: Az ONNX Runtime biztos√≠tja az SLM √ºgyn√∂k√∂k k√∂vetkezetes viselked√©s√©t Windows, Linux, macOS, iOS √©s Android platformokon. Ez a t√∂bbplatformos kompatibilit√°s lehet≈ëv√© teszi a fejleszt≈ëk sz√°m√°ra, hogy egyszer √≠rjanak √©s mindenhol telep√≠tsenek, jelent≈ësen cs√∂kkentve a fejleszt√©si √©s karbantart√°si k√∂lts√©geket t√∂bbplatformos alkalmaz√°sokhoz.

**Hardvergyors√≠t√°si opci√≥k**: A keretrendszer optimaliz√°lt v√©grehajt√°si szolg√°ltat√≥kat k√≠n√°l k√ºl√∂nb√∂z≈ë hardverkonfigur√°ci√≥khoz, bele√©rtve a CPU-t (Intel, AMD, ARM), GPU-t (NVIDIA CUDA, AMD ROCm) √©s speci√°lis gyors√≠t√≥kat (Intel VPU, Qualcomm NPU). Az SLM √ºgyn√∂k√∂k automatikusan kihaszn√°lhatj√°k a legjobb el√©rhet≈ë hardvert k√≥dv√°ltoztat√°s n√©lk√ºl.

**Termel√©sre k√©sz funkci√≥k**: Az ONNX Runtime v√°llalati szint≈± funkci√≥kat k√≠n√°l, amelyek elengedhetetlenek a termel√©si √ºgyn√∂ki telep√≠t√©shez, bele√©rtve a grafikus optimaliz√°l√°st a gyorsabb k√∂vetkeztet√©shez, mem√≥ria-kezel√©st er≈ëforr√°s-korl√°tozott k√∂rnyezetekhez, √©s √°tfog√≥ profiloz√≥ eszk√∂z√∂ket a teljes√≠tm√©ny elemz√©s√©hez. A keretrendszer t√°mogatja mind a Python, mind a C++ API-kat rugalmas integr√°ci√≥hoz.

## SLM vs LLM √ºgyn√∂ki rendszerekben: Fejlett √∂sszehasonl√≠t√°s

### Az SLM el≈ënyei √ºgyn√∂ki alkalmaz√°sokban

**M≈±k√∂d√©si hat√©konys√°g**: Az SLM-ek 10-30√ó k√∂lts√©gcs√∂kkent√©st biztos√≠tanak az LLM-ekhez k√©pest √ºgyn√∂ki feladatokhoz, lehet≈ëv√© t√©ve val√≥s idej≈± √ºgyn√∂ki v√°laszokat nagy l√©pt√©kben. Gyorsabb k√∂vetkeztet√©si id≈ëket k√≠n√°lnak a cs√∂kkentett sz√°m√≠t√°si komplexit√°s miatt, ami ide√°liss√° teszi ≈ëket interakt√≠v √ºgyn√∂ki alkalmaz√°sokhoz.

**√âlvonalbeli telep√≠t√©si k√©pess√©gek**: Az SLM-ek lehet≈ëv√© teszik az eszk√∂z√∂n t√∂rt√©n≈ë √ºgyn√∂ki v√©grehajt√°st internetf√ºgg≈ës√©g n√©lk√ºl, fokozott adatv√©delmet helyi √ºgyn√∂ki feldolgoz√°ssal, √©s testreszabhat√≥k domain-specifikus √ºgyn√∂ki alkalmaz√°sokhoz, amelyek alkalmasak k√ºl√∂nb√∂z≈ë √©lvonalbeli sz√°m√≠t√°si k√∂rnyezetekhez.

**√úgyn√∂k-specifikus optimaliz√°l√°s**: Az SLM-ek kiv√°l√≥an alkalmasak eszk√∂zhaszn√°latra, struktur√°lt kimenet gener√°l√°sra √©s rutinszer≈± d√∂nt√©shozatali munkafolyamatokra, amelyek a tipikus √ºgyn√∂ki feladatok 70-80%-√°t teszik ki.

### Mikor haszn√°ljunk SLM-eket vs LLM-eket √ºgyn√∂ki rendszerekben

**T√∂k√©letes az SLM-ekhez**:
- **Ism√©tl≈ëd≈ë √ºgyn√∂ki feladatok**: Adatbevitel, ≈±rlapkit√∂lt√©s, rutinszer≈± API-h√≠v√°sok
- **Eszk√∂zintegr√°ci√≥**: Adatb√°zis-lek√©rdez√©sek, f√°jlm≈±veletek, rendszerinterakci√≥k
- **Struktur√°lt munkafolyamatok**: El≈ëre meghat√°rozott √ºgyn√∂ki folyamatok k√∂vet√©se
- **Domain-specifikus √ºgyn√∂k√∂k**: √úgyf√©lszolg√°lat, √ºtemez√©s, alapvet≈ë elemz√©s
- **Helyi feldolgoz√°s**: Adatv√©delmi szempontb√≥l √©rz√©keny √ºgyn√∂ki m≈±veletek

**Jobb az LLM-ekhez**:
- **Komplex √©rvel√©s**: √öj probl√©m√°k megold√°sa, strat√©giai tervez√©s
- **Nyitott v√©g≈± besz√©lget√©sek**: √Åltal√°nos cseveg√©s, kreat√≠v megbesz√©l√©sek
- **Sz√©lesk√∂r≈± tud√°sfeladatok**: Kutat√°s, amely kiterjedt √°ltal√°nos tud√°st ig√©nyel
- **√öj helyzetek**: Teljesen √∫j √ºgyn√∂ki forgat√≥k√∂nyvek kezel√©se

### Hibrid √ºgyn√∂ki architekt√∫ra

Az optim√°lis megk√∂zel√≠t√©s az SLM-ek √©s LLM-ek kombin√°l√°sa heterog√©n √ºgyn√∂ki rendszerekben:

**Intelligens √ºgyn√∂ki orkestr√°ci√≥**:
1. **SLM els≈ëdleges**: A rutin √ºgyn√∂ki feladatok 70-80%-√°nak helyi kezel√©se
2. **LLM sz√ºks√©g eset√©n**: Bonyolult lek√©rdez√©sek ir√°ny√≠t√°sa felh≈ëalap√∫ nagyobb modellekhez
3. **Speci√°lis SLM-ek**: K√ºl√∂nb√∂z≈ë kis modellek k√ºl√∂nb√∂z≈ë √ºgyn√∂ki ter√ºletekhez
4. **K√∂lts√©goptimaliz√°l√°s**: Dr√°ga LLM-h√≠v√°sok minimaliz√°l√°sa intelligens ir√°ny√≠t√°ssal

## Termel√©si SLM √ºgyn√∂ki telep√≠t√©si strat√©gi√°k

### Foundry Local: V√°llalati szint≈± √©lvonalbeli AI futtat√≥k√∂rnyezet

A Foundry Local (https://github.com/microsoft/foundry-local) a Microsoft z√°szl√≥shaj√≥ megold√°sa Kis Nyelvi Modellek termel√©si √©lvonalbeli k√∂rnyezetekben t√∂rt√©n≈ë telep√≠t√©s√©hez. Teljes futtat√≥k√∂rnyezetet biztos√≠t, kifejezetten SLM-alap√∫ √ºgyn√∂k√∂kh√∂z, v√°llalati szint≈± funkci√≥kkal √©s z√∂kken≈ëmentes integr√°ci√≥s k√©pess√©gekkel.

**Alapvet≈ë architekt√∫ra √©s funkci√≥k**:
- **OpenAI-kompatibilis API**: Teljes kompatibilit√°s az OpenAI SDK-val √©s az Agent Framework integr√°ci√≥kkal
- **Automatikus hardveroptimaliz√°l√°s**: Intelligens modellv√°ltozat kiv√°l
- Microsoft Agent Framework integr√°ci√≥ tesztel√©se  
- Offline m≈±k√∂d√©si k√©pess√©gek ellen≈ërz√©se  
- √Åt√°ll√°si forgat√≥k√∂nyvek √©s hibakezel√©s tesztel√©se  
- √úgyn√∂k√∂k teljes k√∂r≈± munkafolyamatainak valid√°l√°sa  

**√ñsszehasonl√≠t√°s a Foundry Local megold√°ssal**:

| Funkci√≥ | Foundry Local | Ollama |
|---------|---------------|--------|
| **C√©lfelhaszn√°l√°si eset** | V√°llalati termel√©s | Fejleszt√©s √©s k√∂z√∂ss√©g |
| **Modellek √∂kosziszt√©m√°ja** | Microsoft √°ltal √∂ssze√°ll√≠tott | Kiterjedt k√∂z√∂ss√©gi |
| **Hardver optimaliz√°ci√≥** | Automatikus (CUDA/NPU/CPU) | Manu√°lis konfigur√°ci√≥ |
| **V√°llalati funkci√≥k** | Be√©p√≠tett monitoroz√°s, biztons√°g | K√∂z√∂ss√©gi eszk√∂z√∂k |
| **Telep√≠t√©si komplexit√°s** | Egyszer≈± (winget install) | Egyszer≈± (curl install) |
| **API kompatibilit√°s** | OpenAI + kiterjeszt√©sek | OpenAI standard |
| **T√°mogat√°s** | Microsoft hivatalos | K√∂z√∂ss√©g √°ltal vez√©relt |
| **Legjobb felhaszn√°l√°si ter√ºlet** | Termel√©si √ºgyn√∂k√∂k | Protot√≠pusok, kutat√°s |

**Mikor v√°lasszuk az Ollama-t**:  
- **Fejleszt√©s √©s protot√≠pus k√©sz√≠t√©s**: Gyors k√≠s√©rletez√©s k√ºl√∂nb√∂z≈ë modellekkel  
- **K√∂z√∂ss√©gi modellek**: Hozz√°f√©r√©s a leg√∫jabb k√∂z√∂ss√©gi modellekhez  
- **Oktat√°si c√©lok**: AI √ºgyn√∂kfejleszt√©s tanul√°sa √©s tan√≠t√°sa  
- **Kutat√°si projektek**: Akad√©miai kutat√°s, amelyhez v√°ltozatos modellek sz√ºks√©gesek  
- **Egyedi modellek**: Saj√°t finomhangolt modellek √©p√≠t√©se √©s tesztel√©se  

### VLLM: Nagy teljes√≠tm√©ny≈± SLM √ºgyn√∂k√∂k inferenci√°ja  

A VLLM (Very Large Language Model inference) egy nagy √°tereszt≈ëk√©pess√©g≈±, mem√≥riahat√©kony inferencia motor, amelyet kifejezetten termel√©si SLM telep√≠t√©sekhez optimaliz√°ltak. M√≠g a Foundry Local az egyszer≈± haszn√°latra √∂sszpontos√≠t, az Ollama a k√∂z√∂ss√©gi modelleket helyezi el≈ët√©rbe, a VLLM pedig a maxim√°lis √°tereszt≈ëk√©pess√©get √©s hat√©kony er≈ëforr√°s-felhaszn√°l√°st ig√©nyl≈ë nagy teljes√≠tm√©ny≈± forgat√≥k√∂nyvekben jeleskedik.

**F≈ë architekt√∫ra √©s funkci√≥k**:  
- **PagedAttention**: Forradalmi mem√≥ria kezel√©s az hat√©kony figyelem sz√°m√≠t√°s √©rdek√©ben  
- **Dinamikus csoportos√≠t√°s**: Intelligens k√©r√©s csoportos√≠t√°s az optim√°lis √°tereszt≈ëk√©pess√©g√©rt  
- **GPU optimaliz√°ci√≥**: Fejlett CUDA kernel √©s tensor p√°rhuzamos√≠t√°s t√°mogat√°s  
- **OpenAI kompatibilit√°s**: Teljes API kompatibilit√°s a z√∂kken≈ëmentes integr√°ci√≥ √©rdek√©ben  
- **Spekulat√≠v dek√≥dol√°s**: Fejlett inferencia gyors√≠t√°si technik√°k  
- **Kvant√°l√°s t√°mogat√°s**: INT4, INT8 √©s FP16 kvant√°l√°s a mem√≥riahat√©konys√°g √©rdek√©ben  

#### Telep√≠t√©s √©s be√°ll√≠t√°s  

**Telep√≠t√©si lehet≈ës√©gek**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Gyors kezd√©s √ºgyn√∂kfejleszt√©shez**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### √úgyn√∂kkeret integr√°ci√≥  

**VLLM a Microsoft Agent Framework-kel**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**Nagy √°tereszt≈ëk√©pess√©g≈± t√∂bb√ºgyn√∂k√∂s be√°ll√≠t√°s**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### Termel√©si telep√≠t√©si mint√°k  

**V√°llalati VLLM termel√©si szolg√°ltat√°s**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### V√°llalati funkci√≥k √©s monitoroz√°s  

**Fejlett VLLM teljes√≠tm√©ny monitoroz√°s**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### Fejlett konfigur√°ci√≥ √©s optimaliz√°ci√≥  

**Termel√©si VLLM konfigur√°ci√≥s sablonok**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Termel√©si telep√≠t√©si ellen≈ërz≈ëlista VLLM-hez**:  

‚úÖ **Hardver optimaliz√°ci√≥**:  
- Tensor p√°rhuzamos√≠t√°s konfigur√°l√°sa t√∂bb GPU-s be√°ll√≠t√°sokhoz  
- Kvant√°l√°s enged√©lyez√©se (AWQ/GPTQ) a mem√≥riahat√©konys√°g √©rdek√©ben  
- Optim√°lis GPU mem√≥ria kihaszn√°l√°s be√°ll√≠t√°sa (85-95%)  
- Megfelel≈ë csoportm√©retek konfigur√°l√°sa az √°tereszt≈ëk√©pess√©g √©rdek√©ben  

‚úÖ **Teljes√≠tm√©ny hangol√°s**:  
- Prefix caching enged√©lyez√©se ism√©tl≈ëd≈ë lek√©rdez√©sekhez  
- Chunked prefill konfigur√°l√°sa hossz√∫ szekvenci√°khoz  
- Spekulat√≠v dek√≥dol√°s be√°ll√≠t√°sa a gyorsabb inferenci√°hoz  
- Max_num_seqs optimaliz√°l√°sa a hardver alapj√°n  

‚úÖ **Termel√©si funkci√≥k**:  
- Eg√©szs√©g√ºgyi monitoroz√°s √©s metrik√°k gy≈±jt√©se  
- Automatikus √∫jraind√≠t√°s √©s √°t√°ll√°s konfigur√°l√°sa  
- K√©r√©s sorba√°ll√≠t√°s √©s terhel√©seloszt√°s megval√≥s√≠t√°sa  
- √Åtfog√≥ napl√≥z√°s √©s riaszt√°sok be√°ll√≠t√°sa  

‚úÖ **Biztons√°g √©s megb√≠zhat√≥s√°g**:  
- T≈±zfal szab√°lyok √©s hozz√°f√©r√©s-vez√©rl√©s konfigur√°l√°sa  
- API sebess√©gkorl√°toz√°s √©s hiteles√≠t√©s be√°ll√≠t√°sa  
- K√≠m√©letes le√°ll√≠t√°s √©s tiszt√≠t√°s megval√≥s√≠t√°sa  
- Biztons√°gi ment√©s √©s katasztr√≥fa helyre√°ll√≠t√°s konfigur√°l√°sa  

‚úÖ **Integr√°ci√≥s tesztel√©s**:  
- Microsoft Agent Framework integr√°ci√≥ tesztel√©se  
- Nagy √°tereszt≈ëk√©pess√©g≈± forgat√≥k√∂nyvek valid√°l√°sa  
- √Åt√°ll√°si √©s helyre√°ll√≠t√°si elj√°r√°sok tesztel√©se  
- Teljes√≠tm√©ny benchmarkol√°sa terhel√©s alatt  

**√ñsszehasonl√≠t√°s m√°s megold√°sokkal**:

| Funkci√≥ | VLLM | Foundry Local | Ollama |
|---------|------|---------------|--------|
| **C√©lfelhaszn√°l√°si eset** | Nagy √°tereszt≈ëk√©pess√©g≈± termel√©s | V√°llalati egyszer≈±s√©g | Fejleszt√©s √©s k√∂z√∂ss√©g |
| **Teljes√≠tm√©ny** | Maxim√°lis √°tereszt≈ëk√©pess√©g | Kiegyens√∫lyozott | J√≥ |
| **Mem√≥riahat√©konys√°g** | PagedAttention optimaliz√°ci√≥ | Automatikus optimaliz√°ci√≥ | Standard |
| **Be√°ll√≠t√°si komplexit√°s** | Magas (sok param√©ter) | Alacsony (automatikus) | Alacsony (egyszer≈±) |
| **Sk√°l√°zhat√≥s√°g** | Kiv√°l√≥ (tensor/pipeline p√°rhuzamos√≠t√°s) | J√≥ | Korl√°tozott |
| **Kvant√°l√°s** | Fejlett (AWQ, GPTQ, FP8) | Automatikus | Standard GGUF |
| **V√°llalati funkci√≥k** | Egyedi megval√≥s√≠t√°s sz√ºks√©ges | Be√©p√≠tett | K√∂z√∂ss√©gi eszk√∂z√∂k |
| **Legjobb felhaszn√°l√°si ter√ºlet** | Nagy sk√°l√°j√∫ termel√©si √ºgyn√∂k√∂k | V√°llalati termel√©s | Fejleszt√©s |

**Mikor v√°lasszuk a VLLM-et**:  
- **Nagy √°tereszt≈ëk√©pess√©g ig√©nyek**: T√∂bb sz√°z k√©r√©s feldolgoz√°sa m√°sodpercenk√©nt  
- **Nagy sk√°l√°j√∫ telep√≠t√©sek**: T√∂bb GPU-s, t√∂bb csom√≥pontos telep√≠t√©sek  
- **Teljes√≠tm√©ny kritikus**: M√°sodperc alatti v√°laszid≈ëk nagy sk√°l√°n  
- **Fejlett optimaliz√°ci√≥**: Egyedi kvant√°l√°s √©s csoportos√≠t√°s sz√ºks√©gess√©ge  
- **Er≈ëforr√°s-hat√©konys√°g**: Dr√°ga GPU hardver maxim√°lis kihaszn√°l√°sa  

## Val√≥s SLM √ºgyn√∂k alkalmaz√°sok  

### √úgyf√©lszolg√°lati SLM √ºgyn√∂k√∂k  
- **SLM k√©pess√©gek**: Fi√≥klek√©rdez√©sek, jelsz√≥ vissza√°ll√≠t√°sok, rendel√©si √°llapot ellen≈ërz√©sek  
- **K√∂lts√©gel≈ëny√∂k**: 10x cs√∂kkent√©s az inferencia k√∂lts√©gekben az LLM √ºgyn√∂k√∂kh√∂z k√©pest  
- **Teljes√≠tm√©ny**: Gyorsabb v√°laszid≈ëk, k√∂vetkezetes min≈ës√©g rutink√©rd√©sekhez  

### √úzleti folyamatokat t√°mogat√≥ SLM √ºgyn√∂k√∂k  
- **Sz√°mlafeldolgoz√≥ √ºgyn√∂k√∂k**: Adatok kinyer√©se, inform√°ci√≥ valid√°l√°sa, j√≥v√°hagy√°sra tov√°bb√≠t√°s  
- **Email kezel≈ë √ºgyn√∂k√∂k**: Kategoriz√°l√°s, priorit√°s meghat√°roz√°s, v√°laszok automatikus meg√≠r√°sa  
- **√útemez≈ë √ºgyn√∂k√∂k**: Tal√°lkoz√≥k koordin√°l√°sa, napt√°rak kezel√©se, eml√©keztet≈ëk k√ºld√©se  

### Szem√©lyes SLM digit√°lis asszisztensek  
- **Feladatkezel≈ë √ºgyn√∂k√∂k**: Teend≈ëlist√°k l√©trehoz√°sa, friss√≠t√©se, hat√©kony szervez√©se  
- **Inform√°ci√≥gy≈±jt≈ë √ºgyn√∂k√∂k**: T√©m√°k kutat√°sa, eredm√©nyek √∂sszefoglal√°sa helyben  
- **Kommunik√°ci√≥s √ºgyn√∂k√∂k**: Email-ek, √ºzenetek, k√∂z√∂ss√©gi m√©dia posztok meg√≠r√°sa priv√°t m√≥don  

### Kereskedelmi √©s p√©nz√ºgyi SLM √ºgyn√∂k√∂k  
- **Piacfigyel≈ë √ºgyn√∂k√∂k**: √Årak k√∂vet√©se, trendek azonos√≠t√°sa val√≥s id≈ëben  
- **Jelent√©sk√©sz√≠t≈ë √ºgyn√∂k√∂k**: Napi/heti √∂sszefoglal√≥k automatikus l√©trehoz√°sa  
- **Kock√°zat√©rt√©kel≈ë √ºgyn√∂k√∂k**: Portf√≥li√≥ poz√≠ci√≥k √©rt√©kel√©se helyi adatok alapj√°n  

### Eg√©szs√©g√ºgyi t√°mogat√°st ny√∫jt√≥ SLM √ºgyn√∂k√∂k  
- **Beteg√ºtemez≈ë √ºgyn√∂k√∂k**: Id≈ëpontok koordin√°l√°sa, automatikus eml√©keztet≈ëk k√ºld√©se  
- **Dokument√°ci√≥s √ºgyn√∂k√∂k**: Orvosi √∂sszefoglal√≥k, jelent√©sek helyi gener√°l√°sa  
- **Receptkezel≈ë √ºgyn√∂k√∂k**: Ut√°nt√∂lt√©sek nyomon k√∂vet√©se, interakci√≥k ellen≈ërz√©se priv√°t m√≥don  

## Microsoft Agent Framework: Termel√©sre k√©sz √ºgyn√∂kfejleszt√©s  

### √Åttekint√©s √©s architekt√∫ra  

A Microsoft Agent Framework egy √°tfog√≥, v√°llalati szint≈± platformot biztos√≠t AI √ºgyn√∂k√∂k √©p√≠t√©s√©hez, telep√≠t√©s√©hez √©s kezel√©s√©hez, amelyek k√©pesek felh≈ëben √©s offline edge k√∂rnyezetekben is m≈±k√∂dni. A keretet kifejezetten √∫gy tervezt√©k, hogy z√∂kken≈ëmentesen m≈±k√∂dj√∂n Small Language Model-ekkel √©s edge computing forgat√≥k√∂nyvekkel, ide√°lis v√°laszt√°sk√©nt szolg√°lva adatv√©delmi szempontb√≥l √©rz√©keny √©s er≈ëforr√°s-korl√°tozott telep√≠t√©sekhez.  

**F≈ë keretkomponensek**:  
- **√úgyn√∂k futtat√≥k√∂rnyezet**: K√∂nny≈± v√©grehajt√°si k√∂rnyezet, amelyet edge eszk√∂z√∂kre optimaliz√°ltak  
- **Eszk√∂z integr√°ci√≥s rendszer**: B≈ëv√≠thet≈ë plugin architekt√∫ra k√ºls≈ë szolg√°ltat√°sok √©s API-k csatlakoztat√°s√°hoz  
- **√Ållapotkezel√©s**: Tart√≥s √ºgyn√∂k mem√≥ria √©s kontextus kezel√©s a munkamenetek k√∂z√∂tt  
- **Biztons√°gi r√©teg**: Be√©p√≠tett biztons√°gi vez√©rl≈ëk v√°llalati telep√≠t√©shez  
- **Orchestration Engine**: T√∂bb √ºgyn√∂k koordin√°ci√≥ja √©s munkafolyamat kezel√©se  

### Kulcsfontoss√°g√∫ funkci√≥k edge telep√≠t√©shez  

**Offline-First Architekt√∫ra**: A Microsoft Agent Framework offline-first elvekkel k√©sz√ºlt, lehet≈ëv√© t√©ve az √ºgyn√∂k√∂k hat√©kony m≈±k√∂d√©s√©t √°lland√≥ internetkapcsolat n√©lk√ºl. Ez mag√°ban foglalja a helyi modell inferenci√°t, gyors√≠t√≥t√°razott tud√°sb√°zisokat, offline eszk√∂z v√©grehajt√°st √©s fokozatos degrad√°ci√≥t, amikor a felh≈ëszolg√°ltat√°sok nem √©rhet≈ëk el.  

**Er≈ëforr√°s optimaliz√°ci√≥**: A keret intelligens er≈ëforr√°s-kezel√©st biztos√≠t automatikus mem√≥ria optimaliz√°ci√≥val SLM-ekhez, CPU/GPU terhel√©seloszt√°ssal edge eszk√∂z√∂kh√∂z, adapt√≠v modellv√°laszt√°ssal a rendelkez√©sre √°ll√≥ er≈ëforr√°sok alapj√°n, √©s energiahat√©kony inferencia mint√°kkal mobil telep√≠t√©shez.  

**Biztons√°g √©s adatv√©delem**: V√°llalati szint≈± biztons√°gi funkci√≥k, bele√©rtve a helyi adatfeldolgoz√°st az adatv√©delem √©rdek√©ben, titkos√≠tott √ºgyn√∂k kommunik√°ci√≥s csatorn√°kat, szerepk√∂r alap√∫ hozz√°f√©r√©s-vez√©rl√©st az √ºgyn√∂k k√©pess√©geihez, √©s audit napl√≥z√°st a megfelel≈ës√©gi k√∂vetelm√©nyekhez.  

### Integr√°ci√≥ a Foundry Local megold√°ssal  

A Microsoft Agent Framework z√∂kken≈ëmentesen integr√°l√≥dik a Foundry Local megold√°ssal, hogy teljes edge AI megold√°st ny√∫jtson:  

**Automatikus modell felfedez√©s**: A keret automatikusan felismeri √©s csatlakozik a Foundry Local p√©ld√°nyokhoz, felfedezi a rendelkez√©sre √°ll√≥ SLM modelleket, √©s kiv√°lasztja az optim√°lis modelleket az √ºgyn√∂k k√∂vetelm√©nyei √©s hardver k√©pess√©gei alapj√°n.  

**Dinamikus modell bet√∂lt√©s**: Az √ºgyn√∂k√∂k dinamikusan t√∂lthetnek be k√ºl√∂nb√∂z≈ë SLM-eket specifikus feladatokhoz, lehet≈ëv√© t√©ve t√∂bbmodell≈± √ºgyn√∂krendszerek l√©trehoz√°s√°t, ahol k√ºl√∂nb√∂z≈ë modellek kezelik a k√ºl√∂nb√∂z≈ë t√≠pus√∫ k√©r√©seket, √©s automatikus √°t√°ll√°s t√∂rt√©nik a modellek k√∂z√∂tt a rendelkez√©sre √°ll√°s √©s teljes√≠tm√©ny alapj√°n.  

**Teljes√≠tm√©ny optimaliz√°ci√≥**: Integr√°lt gyors√≠t√≥t√°raz√°si mechanizmusok cs√∂kkentik a modell bet√∂lt√©si id≈ëket, kapcsolat pooling optimaliz√°lja az API h√≠v√°sokat a Foundry Local megold√°shoz, √©s intelligens csoportos√≠t√°s jav√≠tja az √°tereszt≈ëk√©pess√©get t√∂bb √ºgyn√∂k k√©r√©s eset√©n.  

### √úgyn√∂k√∂k √©p√≠t√©se a Microsoft Agent Framework seg√≠ts√©g√©vel  

#### √úgyn√∂k defin√≠ci√≥ √©s konfigur√°ci√≥  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### Eszk√∂z integr√°ci√≥ edge forgat√≥k√∂nyvekhez  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### T√∂bb√ºgyn√∂k√∂s orchestration  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### Fejlett edge telep√≠t√©si mint√°k  

#### Hierarchikus √ºgyn√∂k architekt√∫ra  

**Helyi √ºgyn√∂k klaszterek**: T√∂bb specializ√°lt SLM √ºgyn√∂k telep√≠t√©se edge eszk√∂z√∂kre, mindegyik optimaliz√°lva specifikus feladatokra. K√∂nny≈± modellek, mint p√©ld√°ul Qwen2.5-0.5B egyszer≈± ir√°ny√≠t√°shoz √©s √ºtemez√©shez, k√∂zepes modellek, mint Phi-4-Mini √ºgyf√©lszolg√°lathoz √©s dokument√°ci√≥hoz, √©s nagyobb modellek komplex √©rvel√©shez, amikor az er≈ëforr√°sok lehet≈ëv√© teszik.  

**Edge-to-Cloud koordin√°ci√≥**: Intelligens eszkal√°ci√≥s mint√°k megval√≥s√≠t√°sa, ahol a helyi √ºgyn√∂k√∂k kezelik a rutinfeladatokat, a felh≈ë √ºgyn√∂k√∂k komplex √©rvel√©st biztos√≠tanak, amikor a kapcsolat lehet≈ëv√© teszi, √©s z√∂kken≈ëmentes √°tad√°s az edge √©s felh≈ë feldolgoz√°s k√∂z√∂tt a folytonoss√°g fenntart√°sa √©rdek√©ben.  

#### Telep√≠t√©si konfigur√°ci√≥k  

**Egy eszk√∂z telep√≠t√©se**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**Elosztott edge telep√≠t√©s**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### Teljes√≠tm√©ny optimaliz√°ci√≥ edge √ºgyn√∂k√∂kh√∂z  

#### Modell kiv√°laszt√°si strat√©gi√°k  

**Feladat alap√∫ modell hozz√°rendel√©s**: A Microsoft Agent Framework lehet≈ëv√© teszi az intelligens modellv√°laszt√°st a feladat komplexit√°sa √©s k√∂vetelm√©nyei alapj√°n:  

- **Egyszer≈± feladatok** (Q&A, ir√°ny√≠t√°s): Qwen2.5-0.5B (500MB, <100ms v√°laszid≈ë)  
- **K√∂zepes feladatok** (√ºgyf√©lszolg√°lat, √ºtemez√©s): Phi-4-Mini (2.4GB, 200-500ms v√°laszid≈ë)  
- **Komplex feladatok** (technikai elemz√©s, tervez√©s): Phi-4 (7GB, 1-3s v√°laszid≈ë, amikor az
**Keretrendszer kiv√°laszt√°sa √ºgyn√∂k√∂k telep√≠t√©s√©hez**: V√°lasszon optimaliz√°ci√≥s keretrendszereket a c√©lhardver √©s az √ºgyn√∂k k√∂vetelm√©nyei alapj√°n. Haszn√°lja a Llama.cpp-t CPU-optimaliz√°lt √ºgyn√∂k√∂k telep√≠t√©s√©hez, az Apple MLX-et Apple Silicon √ºgyn√∂kalkalmaz√°sokhoz, √©s az ONNX-et platformf√ºggetlen √ºgyn√∂k√∂k kompatibilit√°s√°hoz.

## Gyakorlati SLM √ºgyn√∂k konverzi√≥ √©s felhaszn√°l√°si esetek

### Val√≥s √ºgyn√∂k telep√≠t√©si forgat√≥k√∂nyvek

**Mobil √ºgyn√∂kalkalmaz√°sok**: A Q4_K form√°tumok kiv√°l√≥an alkalmasak okostelefonos √ºgyn√∂kalkalmaz√°sokhoz minim√°lis mem√≥riaig√©nnyel, m√≠g a Q8_0 kiegyens√∫lyozott teljes√≠tm√©nyt ny√∫jt t√°blag√©pes √ºgyn√∂krendszerekhez. A Q5_K form√°tumok kiemelked≈ë min≈ës√©get biztos√≠tanak mobil produktivit√°si √ºgyn√∂k√∂kh√∂z.

**Asztali √©s edge √ºgyn√∂k sz√°m√≠t√°stechnika**: A Q5_K optim√°lis teljes√≠tm√©nyt ny√∫jt asztali √ºgyn√∂kalkalmaz√°sokhoz, a Q8_0 magas min≈ës√©g≈± k√∂vetkeztet√©st biztos√≠t munka√°llom√°s √ºgyn√∂ki k√∂rnyezetekhez, m√≠g a Q4_K hat√©kony feldolgoz√°st tesz lehet≈ëv√© edge √ºgyn√∂k eszk√∂z√∂k√∂n.

**Kutat√°si √©s k√≠s√©rleti √ºgyn√∂k√∂k**: Fejlett kvant√°l√°si form√°tumok lehet≈ëv√© teszik az ultra-alacsony precizit√°s√∫ √ºgyn√∂k k√∂vetkeztet√©s vizsg√°lat√°t akad√©miai kutat√°sokhoz √©s er≈ëforr√°s-korl√°tos k√≠s√©rleti √ºgyn√∂kalkalmaz√°sokhoz.

### SLM √ºgyn√∂k teljes√≠tm√©ny benchmarkok

**√úgyn√∂k k√∂vetkeztet√©si sebess√©g**: A Q4_K a leggyorsabb √ºgyn√∂k v√°laszid≈ëket √©r el mobil CPU-kon, a Q5_K kiegyens√∫lyozott sebess√©g-min≈ës√©g ar√°nyt biztos√≠t √°ltal√°nos √ºgyn√∂kalkalmaz√°sokhoz, a Q8_0 kiv√°l√≥ min≈ës√©get ny√∫jt √∂sszetett √ºgyn√∂kfeladatokhoz, m√≠g a k√≠s√©rleti form√°tumok maxim√°lis √°tereszt≈ëk√©pess√©get biztos√≠tanak speci√°lis √ºgyn√∂k hardverekhez.

**√úgyn√∂k mem√≥riaig√©nyek**: Az √ºgyn√∂k√∂k kvant√°l√°si szintjei Q2_K-t√≥l (500 MB alatt kis √ºgyn√∂kmodellekhez) Q8_0-ig (az eredeti m√©ret k√∂r√ºlbel√ºl 50%-a) terjednek, k√≠s√©rleti konfigur√°ci√≥kkal, amelyek maxim√°lis t√∂m√∂r√≠t√©st √©rnek el er≈ëforr√°s-korl√°tos √ºgyn√∂ki k√∂rnyezetekhez.

## Kih√≠v√°sok √©s megfontol√°sok SLM √ºgyn√∂k√∂kn√©l

### Teljes√≠tm√©ny kompromisszumok az √ºgyn√∂krendszerekben

Az SLM √ºgyn√∂k√∂k telep√≠t√©se gondos m√©rlegel√©st ig√©nyel a modellm√©ret, az √ºgyn√∂k v√°laszsebess√©ge √©s a kimeneti min≈ës√©g k√∂z√∂tt. M√≠g a Q4_K kiv√©teles sebess√©get √©s hat√©konys√°got k√≠n√°l mobil √ºgyn√∂k√∂kh√∂z, a Q8_0 kiv√°l√≥ min≈ës√©get biztos√≠t √∂sszetett √ºgyn√∂kfeladatokhoz. A Q5_K k√∂z√©putat k√©pvisel, amely a legt√∂bb √°ltal√°nos √ºgyn√∂kalkalmaz√°shoz megfelel≈ë.

### Hardverkompatibilit√°s SLM √ºgyn√∂k√∂kh√∂z

K√ºl√∂nb√∂z≈ë edge eszk√∂z√∂k elt√©r≈ë k√©pess√©gekkel rendelkeznek az SLM √ºgyn√∂k√∂k telep√≠t√©s√©hez. A Q4_K hat√©konyan fut egyszer≈± processzorokon egyszer≈± √ºgyn√∂k√∂kh√∂z, a Q5_K m√©rs√©kelt sz√°m√≠t√°si er≈ëforr√°sokat ig√©nyel kiegyens√∫lyozott √ºgyn√∂ki teljes√≠tm√©nyhez, m√≠g a Q8_0 magasabb kateg√≥ri√°s hardverek el≈ënyeit √©lvezi fejlett √ºgyn√∂ki k√©pess√©gekhez.

### Biztons√°g √©s adatv√©delem SLM √ºgyn√∂krendszerekben

M√≠g az SLM √ºgyn√∂k√∂k helyi feldolgoz√°st tesznek lehet≈ëv√© a fokozott adatv√©delem √©rdek√©ben, megfelel≈ë biztons√°gi int√©zked√©seket kell bevezetni az √ºgyn√∂kmodellek √©s adatok v√©delm√©re edge k√∂rnyezetekben. Ez k√ºl√∂n√∂sen fontos, amikor nagy precizit√°s√∫ √ºgyn√∂kform√°tumokat telep√≠tenek v√°llalati k√∂rnyezetekben, vagy t√∂m√∂r√≠tett √ºgyn√∂kform√°tumokat √©rz√©keny adatokat kezel≈ë alkalmaz√°sokban.

## J√∂v≈ëbeli trendek az SLM √ºgyn√∂kfejleszt√©sben

Az SLM √ºgyn√∂k√∂k ter√ºlete folyamatosan fejl≈ëdik a t√∂m√∂r√≠t√©si technik√°k, optimaliz√°ci√≥s m√≥dszerek √©s edge telep√≠t√©si strat√©gi√°k el≈ërehalad√°s√°val. A j√∂v≈ëbeli fejleszt√©sek k√∂z√© tartoznak a hat√©konyabb kvant√°l√°si algoritmusok az √ºgyn√∂kmodellekhez, jobb t√∂m√∂r√≠t√©si m√≥dszerek az √ºgyn√∂k munkafolyamatokhoz, √©s jobb integr√°ci√≥ az edge hardvergyors√≠t√≥kkal az √ºgyn√∂kfeldolgoz√°shoz.

**Piaci el≈ërejelz√©sek az SLM √ºgyn√∂k√∂kh√∂z**: A legfrissebb kutat√°sok szerint az √ºgyn√∂k-alap√∫ automatiz√°ci√≥ 2027-re ak√°r 40‚Äì60%-kal cs√∂kkentheti az ism√©tl≈ëd≈ë kognit√≠v feladatokat a v√°llalati munkafolyamatokban, az SLM-ek vezetve ezt az √°talakul√°st k√∂lts√©ghat√©konys√°guk √©s telep√≠t√©si rugalmass√°guk miatt.

**Technol√≥giai trendek az SLM √ºgyn√∂k√∂kn√©l**:
- **Speci√°lis SLM √ºgyn√∂k√∂k**: Adott feladatokra √©s ipar√°gakra k√©pzett domain-specifikus modellek
- **Edge √ºgyn√∂k sz√°m√≠t√°stechnika**: Fejlettebb eszk√∂z√∂n fut√≥ √ºgyn√∂ki k√©pess√©gek jav√≠tott adatv√©delemmel √©s cs√∂kkentett k√©sleltet√©ssel
- **√úgyn√∂k√∂k koordin√°ci√≥ja**: Jobb egy√ºttm≈±k√∂d√©s t√∂bb SLM √ºgyn√∂k k√∂z√∂tt dinamikus √∫tvonalv√°laszt√°ssal √©s terhel√©seloszt√°ssal
- **Demokratiz√°ci√≥**: Az SLM rugalmass√°ga sz√©lesebb k√∂r≈± r√©szv√©telt tesz lehet≈ëv√© az √ºgyn√∂kfejleszt√©sben szervezetek k√∂z√∂tt

## Els≈ë l√©p√©sek az SLM √ºgyn√∂k√∂kkel

### 1. l√©p√©s: Microsoft Agent Framework k√∂rnyezet be√°ll√≠t√°sa

**F√ºgg≈ës√©gek telep√≠t√©se**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Foundry Local inicializ√°l√°sa**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### 2. l√©p√©s: V√°lassza ki az SLM-et √ºgyn√∂kalkalmaz√°sokhoz
N√©pszer≈± opci√≥k a Microsoft Agent Framework sz√°m√°ra:
- **Microsoft Phi-4 Mini (3.8B)**: Kiv√°l√≥ √°ltal√°nos √ºgyn√∂kfeladatokhoz kiegyens√∫lyozott teljes√≠tm√©nnyel
- **Qwen2.5-0.5B (0.5B)**: Ultra-hat√©kony egyszer≈± √∫tvonalv√°laszt√°si √©s oszt√°lyoz√°si √ºgyn√∂k√∂kh√∂z
- **Qwen2.5-Coder-0.5B (0.5B)**: K√≥dhoz kapcsol√≥d√≥ √ºgyn√∂kfeladatokra specializ√°lt
- **Phi-4 (7B)**: Fejlett √©rvel√©s √∂sszetett edge forgat√≥k√∂nyvekhez, ha az er≈ëforr√°sok engedik

### 3. l√©p√©s: Hozza l√©tre els≈ë √ºgyn√∂k√©t a Microsoft Agent Framework seg√≠ts√©g√©vel

**Alapvet≈ë √ºgyn√∂k be√°ll√≠t√°s**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### 4. l√©p√©s: Hat√°rozza meg az √ºgyn√∂k hat√≥k√∂r√©t √©s k√∂vetelm√©nyeit
Kezdje f√≥kusz√°lt, j√≥l meghat√°rozott √ºgyn√∂kalkalmaz√°sokkal a Microsoft Agent Framework seg√≠ts√©g√©vel:
- **Egyetlen ter√ºletre specializ√°lt √ºgyn√∂k√∂k**: √úgyf√©lszolg√°lat VAGY √ºtemez√©s VAGY kutat√°s
- **Egy√©rtelm≈± √ºgyn√∂ki c√©lok**: Specifikus, m√©rhet≈ë c√©lok az √ºgyn√∂k teljes√≠tm√©ny√©hez
- **Korl√°tozott eszk√∂zintegr√°ci√≥**: Maximum 3-5 eszk√∂z az els≈ë √ºgyn√∂k telep√≠t√©s√©hez
- **Meghat√°rozott √ºgyn√∂ki hat√°rok**: Egy√©rtelm≈± eszkal√°ci√≥s utak √∂sszetett forgat√≥k√∂nyvekhez
- **Edge-els≈ë tervez√©s**: Offline funkcionalit√°s √©s helyi feldolgoz√°s priorit√°sa

### 5. l√©p√©s: Edge telep√≠t√©s megval√≥s√≠t√°sa a Microsoft Agent Framework seg√≠ts√©g√©vel

**Er≈ëforr√°s konfigur√°ci√≥**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Biztons√°gi int√©zked√©sek telep√≠t√©se edge √ºgyn√∂k√∂kh√∂z**:
- **Helyi bemeneti valid√°ci√≥**: K√©r√©sek ellen≈ërz√©se felh≈ëf√ºgg≈ës√©g n√©lk√ºl
- **Offline kimeneti sz≈±r√©s**: Biztos√≠tsa, hogy a v√°laszok helyi min≈ës√©gi szabv√°nyoknak megfeleljenek
- **Edge biztons√°gi kontrollok**: Biztons√°g megval√≥s√≠t√°sa internetkapcsolat n√©lk√ºl
- **Helyi monitoroz√°s**: Teljes√≠tm√©ny nyomon k√∂vet√©se √©s probl√©m√°k jelz√©se edge telemetria seg√≠ts√©g√©vel

### 6. l√©p√©s: Edge √ºgyn√∂k teljes√≠tm√©ny√©nek m√©r√©se √©s optimaliz√°l√°sa
- **√úgyn√∂k feladat teljes√≠t√©si ar√°nyok**: Sikeress√©gi ar√°nyok monitoroz√°sa offline forgat√≥k√∂nyvekben
- **√úgyn√∂k v√°laszid≈ëk**: Biztos√≠tsa a m√°sodperc alatti v√°laszid≈ëket edge telep√≠t√©shez
- **Er≈ëforr√°s kihaszn√°l√°s**: Mem√≥ria-, CPU- √©s akkumul√°torhaszn√°lat nyomon k√∂vet√©se edge eszk√∂z√∂k√∂n
- **K√∂lts√©ghat√©konys√°g**: Edge telep√≠t√©si k√∂lts√©gek √∂sszehasonl√≠t√°sa felh≈ëalap√∫ alternat√≠v√°kkal
- **Offline megb√≠zhat√≥s√°g**: √úgyn√∂k teljes√≠tm√©ny√©nek m√©r√©se h√°l√≥zati kimarad√°sok eset√©n

## Kulcsfontoss√°g√∫ tanuls√°gok az SLM √ºgyn√∂k√∂k megval√≥s√≠t√°s√°hoz

1. **Az SLM-ek elegend≈ëek az √ºgyn√∂k√∂kh√∂z**: A legt√∂bb √ºgyn√∂kfeladathoz a kis modellek ugyanolyan j√≥l teljes√≠tenek, mint a nagyok, mik√∂zben jelent≈ës el≈ëny√∂ket k√≠n√°lnak
2. **K√∂lts√©ghat√©konys√°g az √ºgyn√∂k√∂kn√©l**: Az SLM √ºgyn√∂k√∂k futtat√°sa 10-30x olcs√≥bb, ami gazdas√°gilag √©letk√©pess√© teszi ≈ëket sz√©les k√∂r≈± telep√≠t√©shez
3. **Specializ√°ci√≥ m≈±k√∂dik az √ºgyn√∂k√∂kn√©l**: A finomhangolt SLM-ek gyakran fel√ºlm√∫lj√°k az √°ltal√°nos c√©l√∫ LLM-eket specifikus √ºgyn√∂kalkalmaz√°sokban
4. **Hibrid √ºgyn√∂k architekt√∫ra**: Haszn√°ljon SLM-eket rutinszer≈± √ºgyn√∂kfeladatokhoz, LLM-eket √∂sszetett √©rvel√©shez, ha sz√ºks√©ges
5. **Microsoft Agent Framework lehet≈ëv√© teszi a termel√©si telep√≠t√©st**: V√°llalati szint≈± eszk√∂z√∂ket biztos√≠t az edge √ºgyn√∂k√∂k √©p√≠t√©s√©hez, telep√≠t√©s√©hez √©s kezel√©s√©hez
6. **Edge-els≈ë tervez√©si elvek**: Offline-k√©pes √ºgyn√∂k√∂k helyi feldolgoz√°ssal biztos√≠tj√°k az adatv√©delmet √©s megb√≠zhat√≥s√°got
7. **Foundry Local integr√°ci√≥**: Z√∂kken≈ëmentes kapcsolat a Microsoft Agent Framework √©s a helyi modellk√∂vetkeztet√©s k√∂z√∂tt
8. **A j√∂v≈ë az SLM √ºgyn√∂k√∂k√©**: Kis nyelvi modellek termel√©si keretrendszerekkel az √ºgyn√∂ki mesters√©ges intelligencia j√∂v≈ëj√©t jelentik, lehet≈ëv√© t√©ve a demokratiz√°lt √©s hat√©kony √ºgyn√∂ktelep√≠t√©st

## Hivatkoz√°sok √©s tov√°bbi olvasm√°nyok

### Alapvet≈ë kutat√°si cikkek √©s publik√°ci√≥k

#### AI √ºgyn√∂k√∂k √©s √ºgyn√∂ki rendszerek
- **"Language Agents as Optimizable Graphs"** (2024) - Alapvet≈ë kutat√°s az √ºgyn√∂k architekt√∫r√°r√≥l √©s optimaliz√°ci√≥s strat√©gi√°kr√≥l
  - Szerz≈ëk: Wenyue Hua, Lishan Yang, et al.
  - Link: https://arxiv.org/abs/2402.16823
  - F≈ëbb meg√°llap√≠t√°sok: Grafikon-alap√∫ √ºgyn√∂ktervez√©s √©s optimaliz√°ci√≥s strat√©gi√°k

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Szerz≈ëk: Zhiheng Xi, Wenxiang Chen, et al.
  - Link: https://arxiv.org/abs/2309.07864
  - F≈ëbb meg√°llap√≠t√°sok: √Åtfog√≥ √°ttekint√©s az LLM-alap√∫ √ºgyn√∂k√∂k k√©pess√©geir≈ël √©s alkalmaz√°sair√≥l

- **"Cognitive Architectures for Language Agents"** (2024)
  - Szerz≈ëk: Theodore Sumers, Shunyu Yao, et al.
  - Link: https://arxiv.org/abs/2309.02427
  - F≈ëbb meg√°llap√≠t√°sok: Kognit√≠v keretrendszerek intelligens √ºgyn√∂k√∂k tervez√©s√©hez

#### Kis nyelvi modellek √©s optimaliz√°ci√≥
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Szerz≈ëk: Microsoft Research Team
  - Link: https://arxiv.org/abs/2404.14219
  - F≈ëbb meg√°llap√≠t√°sok: SLM tervez√©si elvek √©s mobil telep√≠t√©si strat√©gi√°k

- **"Qwen2.5 Technical Report"** (2024)
  - Szerz≈ëk: Alibaba Cloud Team
  - Link: https://arxiv.org/abs/2407.10671
  - F≈ëbb meg√°llap√≠t√°sok: Fejlett SLM k√©pz√©si technik√°k √©s teljes√≠tm√©nyoptimaliz√°ci√≥

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Szerz≈ëk: Peiyuan Zhang, Guangtao Zeng, et al.
  - Link: https://arxiv.org/abs/2401.02385
  - F≈ëbb meg√°llap√≠t√°sok: Ultra-kompakt modelltervez√©s √©s k√©pz√©si hat√©konys√°g

### Hivatalos dokument√°ci√≥ √©s keretrendszerek

#### Microsoft Agent Framework
- **Hivatalos dokument√°ci√≥**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub Repository**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Els≈ëdleges Repository**: https://github.com/microsoft/foundry-local
- **Dokument√°ci√≥**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **F≈ë Repository**: https://github.com/vllm-project/vllm
- **Dokument√°ci√≥**: https://docs.vllm.ai/


#### Ollama
- **Hivatalos weboldal**: https://ollama.ai/
- **GitHub Repository**: https://github.com/ollama/ollama

### Modell optimaliz√°ci√≥s keretrendszerek

#### Llama.cpp
- **Repository**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokument√°ci√≥**: https://microsoft.github.io/Olive/
- **GitHub Repository**: https://github.com/microsoft/Olive

#### OpenVINO
- **Hivatalos oldal**: https://docs.openvino.ai/

#### Apple MLX
- **Repository**: https://github.com/ml-explore/mlx

### Ipar√°gi jelent√©sek √©s piaci elemz√©sek

#### AI √ºgyn√∂k piackutat√°s
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Link: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - F≈ëbb meg√°llap√≠t√°sok: Piaci trendek √©s v√°llalati alkalmaz√°si mint√°k

#### Technikai benchmarkok

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Link: https://mlcommons.org/en/inference-edge/
  - F≈ëbb meg√°llap√≠t√°sok: Szabv√°nyos√≠tott teljes√≠tm√©nymetrik√°k edge telep√≠t√©shez

---

**Felel≈ëss√©g kiz√°r√°sa**:  
Ez a dokumentum az AI ford√≠t√°si szolg√°ltat√°s [Co-op Translator](https://github.com/Azure/co-op-translator) seg√≠ts√©g√©vel lett leford√≠tva. B√°r t√∂reksz√ºnk a pontoss√°gra, k√©rj√ºk, vegye figyelembe, hogy az automatikus ford√≠t√°sok hib√°kat vagy pontatlans√°gokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelv√©n tekintend≈ë hiteles forr√°snak. Fontos inform√°ci√≥k eset√©n javasolt professzion√°lis emberi ford√≠t√°st ig√©nybe venni. Nem v√°llalunk felel≈ëss√©get semmilyen f√©lre√©rt√©s√©rt vagy t√©ves √©rtelmez√©s√©rt, amely a ford√≠t√°s haszn√°lat√°b√≥l eredhet.