<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "be25052ac4c842765e7f6f7eb4d7dcc5",
  "translation_date": "2025-10-20T09:56:04+00:00",
  "source_file": "Module01/01.EdgeAIFundamentals.md",
  "language_code": "hu"
}
-->
# 1. szakasz: EdgeAI alapjai

Az EdgeAI az mesterséges intelligencia (MI) telepítésének paradigmaváltását jelenti, amely az MI képességeket közvetlenül az edge eszközökre hozza, ahelyett, hogy kizárólag a felhőalapú feldolgozásra támaszkodna. Fontos megérteni, hogyan teszi lehetővé az EdgeAI a helyi MI feldolgozást erőforrás-korlátozott eszközökön, miközben fenntartja az elfogadható teljesítményt, és olyan kihívásokkal foglalkozik, mint a magánélet, a késleltetés és az offline képességek.

## Bevezetés

Ebben a leckében az EdgeAI-t és annak alapvető fogalmait fogjuk megvizsgálni. Áttekintjük a hagyományos MI számítási paradigmát, az edge számítás kihívásait, az EdgeAI-t lehetővé tevő kulcstechnológiákat, valamint gyakorlati alkalmazásokat különböző iparágakban.

## Tanulási célok

A lecke végére képes leszel:

- Megérteni a különbséget a hagyományos felhőalapú MI és az EdgeAI megközelítések között.
- Azonosítani azokat a kulcstechnológiákat, amelyek lehetővé teszik az MI feldolgozást az edge eszközökön.
- Felismerni az EdgeAI megvalósítások előnyeit és korlátait.
- Az EdgeAI-val kapcsolatos tudást alkalmazni valós helyzetekben és felhasználási esetekben.

## A hagyományos MI számítási paradigma megértése

Hagyományosan a generatív MI alkalmazások nagy teljesítményű számítási infrastruktúrára támaszkodnak, hogy hatékonyan futtassák a nagy nyelvi modelleket (LLM-eket). A szervezetek általában ezeket a modelleket GPU klasztereken telepítik felhőalapú környezetekben, és API interfészeken keresztül férnek hozzá képességeikhez.

Ez a központosított modell számos alkalmazás esetében jól működik, de inherens korlátokkal rendelkezik, amikor edge számítási helyzetekről van szó. A hagyományos megközelítés magában foglalja a felhasználói lekérdezések távoli szerverekre történő küldését, azok feldolgozását nagy teljesítményű hardverekkel, majd az eredmények interneten keresztüli visszaküldését. Bár ez a módszer hozzáférést biztosít a legmodernebb modellekhez, internetkapcsolati függőségeket hoz létre, késleltetési problémákat okoz, és adatvédelmi aggályokat vet fel, amikor érzékeny adatokat kell külső szerverekre továbbítani.

Néhány alapvető fogalmat meg kell értenünk, amikor a hagyományos MI számítási paradigmákkal dolgozunk, nevezetesen:

- **☁️ Felhőalapú feldolgozás**: Az MI modellek nagy teljesítményű szerver infrastruktúrán futnak, magas számítási erőforrásokkal.
- **🔌 API-alapú hozzáférés**: Az alkalmazások távoli API-hívásokon keresztül férnek hozzá az MI képességekhez, nem helyi feldolgozással.
- **🎛️ Központosított modellkezelés**: A modelleket központilag tartják karban és frissítik, biztosítva a konzisztenciát, de hálózati kapcsolatot igényelve.
- **📈 Erőforrás-skálázhatóság**: A felhőinfrastruktúra dinamikusan skálázható a változó számítási igények kezelésére.

## Az edge számítás kihívása

Az edge eszközök, mint például laptopok, mobiltelefonok és az Internet of Things (IoT) eszközök, mint a Raspberry Pi és az NVIDIA Orin Nano, egyedi számítási korlátokat mutatnak. Ezek az eszközök általában korlátozott feldolgozási teljesítménnyel, memóriával és energiaforrásokkal rendelkeznek az adatközponti infrastruktúrához képest.

A hagyományos LLM-ek futtatása ilyen eszközökön történelmileg kihívást jelentett ezeknek a hardverkorlátoknak köszönhetően. Az edge MI feldolgozás iránti igény azonban egyre fontosabbá vált különböző helyzetekben. Gondoljunk olyan helyzetekre, ahol az internetkapcsolat megbízhatatlan vagy nem elérhető, például távoli ipari helyszíneken, úton lévő járművekben vagy gyenge hálózati lefedettségű területeken. Ezenkívül az olyan alkalmazások, amelyek magas biztonsági szabványokat igényelnek, mint például orvosi eszközök, pénzügyi rendszerek vagy kormányzati alkalmazások, helyben kell feldolgozniuk az érzékeny adatokat a magánélet és a megfelelőség fenntartása érdekében.

### Az edge számítás alapvető korlátai

Az edge számítási környezetek számos alapvető korláttal szembesülnek, amelyeket a hagyományos felhőalapú MI megoldások nem tapasztalnak:

- **Korlátozott feldolgozási teljesítmény**: Az edge eszközök általában kevesebb CPU maggal és alacsonyabb órajellel rendelkeznek, mint a szerver szintű hardverek.
- **Memóriakorlátok**: Az elérhető RAM és tárolókapacitás jelentősen csökkent az edge eszközökön.
- **Energiakorlátok**: Az akkumulátorról működő eszközöknek egyensúlyozniuk kell a teljesítményt és az energiafogyasztást a hosszabb működés érdekében.
- **Hőkezelés**: A kompakt formatervezés korlátozza a hűtési képességeket, ami befolyásolja a terhelés alatti folyamatos teljesítményt.

## Mi az EdgeAI?

### Fogalom: Az EdgeAI meghatározása

Az EdgeAI az mesterséges intelligencia algoritmusok telepítését és végrehajtását jelenti közvetlenül az edge eszközökön – a hálózat "peremén" található fizikai hardveren, közel ahhoz a helyhez, ahol az adatokat előállítják és gyűjtik. Ezek az eszközök közé tartoznak az okostelefonok, IoT érzékelők, okoskamerák, önvezető járművek, viselhető eszközök és ipari berendezések. A hagyományos MI rendszerekkel ellentétben, amelyek a feldolgozáshoz felhőszerverekre támaszkodnak, az EdgeAI az intelligenciát közvetlenül az adatforráshoz hozza.

Az EdgeAI lényege az MI feldolgozás decentralizálása, amely a központosított adatközpontoktól távolodik, és elosztja azt a digitális ökoszisztémát alkotó eszközök széles hálózatán. Ez alapvető architekturális változást jelent az MI rendszerek tervezésében és telepítésében.

Az EdgeAI kulcsfontosságú fogalmi pillérei a következők:

- **Közelségi feldolgozás**: A számítás fizikailag közel történik az adatok keletkezési helyéhez.
- **Decentralizált intelligencia**: A döntéshozatali képességek több eszköz között oszlanak meg.
- **Adatszuverenitás**: Az információ helyi ellenőrzés alatt marad, gyakran soha nem hagyja el az eszközt.
- **Autonóm működés**: Az eszközök intelligensen működhetnek állandó kapcsolat nélkül.
- **Beágyazott MI**: Az intelligencia a mindennapi eszközök alapvető képességévé válik.

### Az EdgeAI architektúra vizualizációja

```
┌─────────────────────────────────────────────────────────────────────┐
│                         TRADITIONAL AI ARCHITECTURE                 │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌──────────────┐   Data Transfer  ┌───────────────┐   API Response   ┌───────────┐
│ Edge Devices ├─────────────────>│ Cloud Servers │────────────────> │ End Users │
└──────────────┘                  └───────────────┘                  └───────────┘
     Data                         Model Inference                     Results
   Collection                     High Latency
                                  High Bandwidth
                                  Privacy Concerns
                               
┌─────────────────────────────────────────────────────────────────────┐
│                            EDGE AI ARCHITECTURE                     │
└─────────────────────────────────────────────────────────────────────┘
                                  │
┌────────────────────────────────────────────────────┐   Direct Response  ┌───────────┐
│              Edge Devices with Embedded AI         │───────────────────>│ End Users │
│  ┌─────────┐  ┌───────────────┐  ┌──────────────┐  │                    └───────────┘
│  │ Sensors │─>│ SLM Inference │─>│ Local Action │  │
│  └─────────┘  └───────────────┘  └──────────────┘  │
└────────────────────────────────────────────────────┘
     Data         Low Latency       Immediate
   Collection     Processing        Response
                  No Data Transfer
                  Enhanced Privacy
```

Az EdgeAI az mesterséges intelligencia telepítésének paradigmaváltását jelenti, amely az MI képességeket közvetlenül az edge eszközökre hozza, ahelyett, hogy kizárólag a felhőalapú feldolgozásra támaszkodna. Ez a megközelítés lehetővé teszi, hogy az MI modellek helyben fussanak korlátozott számítási erőforrásokkal rendelkező eszközökön, valós idejű következtetési képességeket biztosítva állandó internetkapcsolat nélkül.

Az EdgeAI különböző technológiákat és technikákat foglal magában, amelyek célja az MI modellek hatékonyabbá tétele és az erőforrás-korlátozott eszközökön történő telepítésre való alkalmassá tétele. A cél az, hogy ésszerű teljesítményt tartsunk fenn, miközben jelentősen csökkentjük az MI modellek számítási és memóriaigényeit.

Nézzük meg az alapvető megközelítéseket, amelyek lehetővé teszik az EdgeAI megvalósítását különböző eszköztípusok és felhasználási esetek között.

### Az EdgeAI alapelvei

Az EdgeAI több alapvető elvre épül, amelyek megkülönböztetik a hagyományos felhőalapú MI-től:

- **Helyi feldolgozás**: Az MI következtetés közvetlenül az edge eszközön történik, külső kapcsolat nélkül.
- **Erőforrás-optimalizálás**: A modelleket kifejezetten a cél eszközök hardverkorlátaihoz optimalizálják.
- **Valós idejű teljesítmény**: A feldolgozás minimális késleltetéssel történik az időérzékeny alkalmazásokhoz.
- **Adatvédelem alapú tervezés**: Az érzékeny adatok az eszközön maradnak, növelve a biztonságot és a megfelelőséget.

## Az EdgeAI-t lehetővé tevő kulcstechnológiák

### Modell kvantálás

Az EdgeAI egyik legfontosabb technikája a modell kvantálás. Ez a folyamat magában foglalja a modell paramétereinek pontosságának csökkentését, általában 32 bites lebegőpontos számokról 8 bites egész számokra vagy még alacsonyabb pontosságú formátumokra. Bár ez a pontosságcsökkentés aggasztónak tűnhet, a kutatások kimutatták, hogy sok MI modell teljesítménye jelentősen csökkentett pontosság mellett is fenntartható.

A kvantálás úgy működik, hogy a lebegőpontos értékek tartományát kisebb diszkrét értékek halmazára térképezi. Például ahelyett, hogy minden paramétert 32 biten ábrázolnánk, a kvantálás csak 8 bitet használhat, ami 4-szeres memóriaigény-csökkenést eredményez, és gyakran gyorsabb következtetési időt tesz lehetővé.

```python
# Example: PyTorch model quantization 
import torch

# Load a pre-trained model
model = torch.load('large_model.pth')

# Quantize the model to INT8
quantized_model = torch.quantization.quantize_dynamic(
    model,  # model to quantize
    {torch.nn.Linear, torch.nn.Conv2d},  # layers to quantize
    dtype=torch.qint8  # quantization data type
)

# Save the quantized model
torch.save(quantized_model, 'quantized_model.pth')

# Memory usage comparison
original_size = model.size()
quantized_size = quantized_model.size()
print(f"Memory reduction: {original_size / quantized_size:.2f}x")
```

A különböző kvantálási technikák közé tartozik:

- **Utólagos kvantálás (PTQ)**: A modell tanítása után alkalmazzák, újratanítás nélkül.
- **Kvantálás-tudatos tanítás (QAT)**: A tanítás során figyelembe veszi a kvantálás hatásait a jobb pontosság érdekében.
- **Dinamikus kvantálás**: A súlyokat int8 formátumba kvantálja, de az aktivációkat dinamikusan számítja.
- **Statikus kvantálás**: Előre kiszámítja az összes kvantálási paramétert a súlyokhoz és az aktivációkhoz.

Az EdgeAI telepítésekhez a megfelelő kvantálási stratégia kiválasztása a konkrét modellarchitektúrától, teljesítménykövetelményektől és a cél eszköz hardverkapacitásaitól függ.

### Modell tömörítés és optimalizálás

A kvantáláson túl különböző tömörítési technikák segítenek csökkenteni a modell méretét és számítási igényeit. Ezek közé tartozik:

**Pruning**: Ez a technika eltávolítja a felesleges kapcsolatokat vagy neuronokat a neurális hálózatokból. Azoknak a paramétereknek az azonosításával és eltávolításával, amelyek kevéssé járulnak hozzá a modell teljesítményéhez, a pruning jelentősen csökkentheti a modell méretét, miközben megőrzi a pontosságot.

```python
# Example: Neural network pruning in TensorFlow
import tensorflow as tf
import tensorflow_model_optimization as tfmot

# Define the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Apply pruning during training
pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(
    initial_sparsity=0.0,
    final_sparsity=0.5,  # 50% of connections will be pruned
    begin_step=0,
    end_step=10000
)

pruned_model = tfmot.sparsity.keras.prune_low_magnitude(
    model, pruning_schedule=pruning_schedule
)

# Compile the pruned model
pruned_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train the model while pruning
pruned_model.fit(x_train, y_train, epochs=10)

# Convert to a smaller model for deployment
final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)
```

**Tudás desztilláció**: Ez a megközelítés magában foglalja egy kisebb "diák" modell tanítását, hogy utánozza egy nagyobb "tanár" modell viselkedését. A diák modell megtanulja közelíteni a tanár kimeneteit, gyakran hasonló teljesítményt érve el lényegesen kevesebb paraméterrel.

**Modellarchitektúra optimalizálás**: A kutatók kifejezetten az edge telepítésre tervezett speciális architektúrákat fejlesztettek ki, mint például a MobileNets, EfficientNets és más könnyű architektúrák, amelyek egyensúlyozzák a teljesítményt a számítási hatékonysággal.

### Kis nyelvi modellek (SLM-ek)

Az EdgeAI egyik feltörekvő trendje a kis nyelvi modellek (SLM-ek) fejlesztése. Ezeket a modelleket alapvetően úgy tervezték, hogy kompaktak és hatékonyak legyenek, miközben továbbra is jelentős természetes nyelvi képességeket nyújtanak. Az SLM-ek ezt gondos architekturális választásokkal, hatékony tanítási technikákkal és specifikus területekre vagy feladatokra összpontosított tanítással érik el.

A hagyományos megközelítésekkel ellentétben, amelyek nagy modellek tömörítését foglalják magukban, az SLM-eket gyakran kisebb adathalmazokkal és kifejezetten az edge telepítésre optimalizált architektúrákkal tanítják. Ez a megközelítés olyan modelleket eredményezhet, amelyek nemcsak kisebbek, hanem hatékonyabbak is bizonyos felhasználási esetekben.

## Hardvergyorsítás az EdgeAI számára

A modern edge eszközök egyre inkább tartalmaznak speciális hardvert, amely az MI munkaterhelések gyorsítására szolgál:

### Neurális feldolgozó egységek (NPUs)

Az NPUs olyan speciális processzorok, amelyeket kifejezetten neurális hálózati számításokra terveztek. Ezek a chipek sokkal hatékonyabban tudják elvégezni az MI következtetési feladatokat, mint a hagyományos CPU-k, gyakran alacsonyabb energiafogyasztással. Számos modern okostelefon, laptop és IoT eszköz már tartalmaz NPUs-t, hogy lehetővé tegye az eszközön történő MI feldolgozást.

```csharp
// Example: Using Windows ML to target NPU acceleration in C#
using Microsoft.ML.OnnxRuntime;

// Create session options with NPU provider
var sessionOptions = new SessionOptions();
sessionOptions.AppendExecutionProvider_DmlExecutionProvider();  // DirectML for NPU

// Load the ONNX model
using var session = new InferenceSession("model.onnx", sessionOptions);

// Create input tensor
var inputTensor = new DenseTensor<float>(new[] { 1, 3, 224, 224 });  // Example input shape
var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor("input", inputTensor) };

// Run inference with NPU acceleration
using var results = session.Run(inputs);
var output = results.First().AsTensor<float>();

// Process output
Console.WriteLine($"Inference result: {output[0]}");
```

NPUs-t tartalmazó eszközök:

- **Apple**: A-sorozatú és M-sorozatú chipek Neural Engine-nel
- **Qualcomm**: Snapdragon processzorok Hexagon DSP/NPU-val
- **Samsung**: Exynos processzorok NPU-val
- **Intel**: Movidius VPUs és Habana Labs gyorsítók
- **Microsoft**: Windows Copilot+ PC-k NPUs-val

### 🎮 GPU gyorsítás

Bár az edge eszközök nem rendelkeznek az adatközpontokban található erőteljes GPU-kkal, sok eszköz mégis tartalmaz integrált vagy dedikált GPU-kat, amelyek gyorsíthatják az MI munkaterheléseket. A modern mobil GPU-k és integrált grafikus processzorok jelentős teljesítményjavulást nyújthatnak az MI következtetési feladatokhoz.

```python
# Example: Using TensorRT for GPU acceleration on edge devices
import tensorrt as trt
import numpy as np

# Create TensorRT logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('model.onnx', 'rb') as model:
    parser.parse(model.read())

# Configure builder
config = builder.create_builder_config()
config.max_workspace_size = 1 << 28  # 256 MiB
config.set_flag(trt.BuilderFlag.FP16)  # Use FP16 precision for edge GPU

# Build and serialize engine
engine = builder.build_engine(network, config)
with open('model.trt', 'wb') as f:
    f.write(engine.serialize())
```

### CPU optimalizálás

Még a csak CPU-val rendelkező eszközök is profitálhatnak az EdgeAI-ból optimalizált megvalósítások révén. A modern CPU-k speciális utasításokat tartalmaznak az MI munkaterhelésekhez, és olyan szoftverkeretrendszerek készültek, amelyek maximalizálják a CPU teljesítményét az MI következtetéshez.

```bash
# Example: Using XNNPACK with TFLite for optimized CPU inference
# Compilation command with XNNPACK acceleration enabled

bazel build -c opt --copt=-O3 --copt=-march=native \
  tensorflow/lite/delegates/xnnpack:libxnnpack_delegate.so

# Convert TensorFlow model to TFLite with optimization
tflite_convert \
  --keras_model_file=model.h5 \
  --output_file=model.tflite \
  --inference_type=FLOAT \
  --experimental_new_converter \
  --experimental_new_quantizer
```

Az EdgeAI-val dolgozó szoftvermérnökök számára kritikus fontosságú megérteni, hogyan lehet kihasznál
- [02: EdgeAI Alkalmazások](02.RealWorldCaseStudies.md)

---

**Felelősség kizárása**:  
Ez a dokumentum az [Co-op Translator](https://github.com/Azure/co-op-translator) AI fordítási szolgáltatás segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Fontos információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget semmilyen félreértésért vagy téves értelmezésért, amely a fordítás használatából eredhet.