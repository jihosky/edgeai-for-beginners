<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:58:28+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "hu"
}
-->
# 7. szakasz: Qualcomm QNN (Qualcomm Neural Network) Optimalizációs csomag

## Tartalomjegyzék
1. [Bevezetés](../../../Module04)
2. [Mi az a Qualcomm QNN?](../../../Module04)
3. [Telepítés](../../../Module04)
4. [Gyors kezdési útmutató](../../../Module04)
5. [Példa: Modellek konvertálása és optimalizálása QNN-nel](../../../Module04)
6. [Haladó használat](../../../Module04)
7. [Legjobb gyakorlatok](../../../Module04)
8. [Hibaelhárítás](../../../Module04)
9. [További források](../../../Module04)

## Bevezetés

A Qualcomm QNN (Qualcomm Neural Network) egy átfogó AI következtetési keretrendszer, amelyet a Qualcomm AI hardvergyorsítók, például a Hexagon NPU, Adreno GPU és Kryo CPU teljes potenciáljának kiaknázására terveztek. Legyen szó mobil eszközökről, edge computing platformokról vagy autóipari rendszerekről, a QNN optimalizált következtetési képességeket biztosít, amelyek kihasználják a Qualcomm speciális AI feldolgozó egységeit a maximális teljesítmény és energiahatékonyság érdekében.

## Mi az a Qualcomm QNN?

A Qualcomm QNN egy egységes AI következtetési keretrendszer, amely lehetővé teszi a fejlesztők számára, hogy hatékonyan telepítsenek AI modelleket a Qualcomm heterogén számítási architektúráján. Egységes programozási interfészt biztosít a Hexagon NPU (Neural Processing Unit), Adreno GPU és Kryo CPU eléréséhez, automatikusan kiválasztva az optimális feldolgozó egységet a különböző modellrétegek és műveletek számára.

### Főbb jellemzők

- **Heterogén számítás**: Egységes hozzáférés az NPU-hoz, GPU-hoz és CPU-hoz automatikus munkaterhelés-elosztással
- **Hardver-tudatos optimalizáció**: Speciális optimalizációk a Qualcomm Snapdragon platformokhoz
- **Kvantálás támogatása**: Fejlett INT8, INT16 és vegyes precíziós kvantálási technikák
- **Modellek konvertálási eszközei**: Közvetlen támogatás TensorFlow, PyTorch, ONNX és Caffe modellekhez
- **Edge AI optimalizált**: Kifejezetten mobil és edge telepítési forgatókönyvekhez tervezve, energiahatékonyságra összpontosítva

### Előnyök

- **Maximális teljesítmény**: Speciális AI hardverek kihasználása akár 15x teljesítményjavulásért
- **Energiahatékonyság**: Optimalizált mobil és akkumulátoros eszközökhöz intelligens energiafelügyelettel
- **Alacsony késleltetés**: Hardvergyorsított következtetés minimális többletterheléssel valós idejű alkalmazásokhoz
- **Skálázható telepítés**: Okostelefonoktól az autóipari platformokig a Qualcomm ökoszisztémájában
- **Gyártásra kész**: Harcedzett keretrendszer, amelyet milliók telepített eszközben használnak

## Telepítés

### Előfeltételek

- Qualcomm QNN SDK (regisztráció szükséges a Qualcomm-nál)
- Python 3.7 vagy újabb
- Kompatibilis Qualcomm hardver vagy szimulátor
- Android NDK (mobil telepítéshez)
- Linux vagy Windows fejlesztési környezet

### QNN SDK beállítása

1. **Regisztráció és letöltés**: Látogasson el a Qualcomm Developer Network oldalára, hogy regisztráljon és letöltse a QNN SDK-t
2. **SDK kicsomagolása**: Csomagolja ki a QNN SDK-t a fejlesztési könyvtárába
3. **Környezeti változók beállítása**: Konfigurálja a QNN eszközök és könyvtárak elérési útjait

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python környezet beállítása

Hozzon létre és aktiváljon egy virtuális környezetet:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Telepítse a szükséges Python csomagokat:

```bash
pip install numpy tensorflow torch onnx
```

### Telepítés ellenőrzése

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Ha sikeres, minden QNN eszköz súgó információját látnia kell.

## Gyors kezdési útmutató

### Az első modell konvertálása

Konvertáljunk egy egyszerű PyTorch modellt, hogy Qualcomm hardveren fusson:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### ONNX konvertálása QNN formátumba

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### QNN modellkönyvtár generálása

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Mit csinál ez a folyamat?

Az optimalizációs munkafolyamat magában foglalja: az eredeti modell ONNX formátumba való konvertálását, az ONNX QNN köztes reprezentációvá való fordítását, hardver-specifikus optimalizációk alkalmazását, és egy lefordított modellkönyvtár generálását telepítéshez.

### Kulcsfontosságú paraméterek magyarázata

- `--input_network`: Forrás ONNX modell fájl
- `--output_path`: Generált C++ forrásfájl
- `--input_dim`: Bemeneti tensor dimenziók optimalizációhoz
- `--quantization_overrides`: Egyedi kvantálási konfiguráció
- `-t x86_64-linux-clang`: Célarchitektúra és fordító

## Példa: Modellek konvertálása és optimalizálása QNN-nel

### 1. lépés: Haladó modellkonvertálás kvantálással

Így alkalmazhat egyedi kvantálást a konvertálás során:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konvertálás egyedi kvantálással:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### 2. lépés: Több backend optimalizáció

Konfigurálás heterogén végrehajtásra NPU, GPU és CPU között:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### 3. lépés: Kontextus bináris létrehozása telepítéshez

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### 4. lépés: Következtetés QNN futtatókörnyezettel

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Kimeneti struktúra

Az optimalizáció után a telepítési könyvtár tartalmazni fogja:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Haladó használat

### Egyedi backend konfiguráció

Specifikus backend optimalizációk konfigurálása:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dinamikus kvantálás

Kvantálás alkalmazása futásidőben a jobb pontosság érdekében:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Teljesítményprofilozás

Teljesítmény monitorozása különböző backendeken:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatikus backend kiválasztás

Intelligens backend kiválasztás megvalósítása a modell jellemzői alapján:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Legjobb gyakorlatok

### 1. Modellarchitektúra optimalizáció
- **Rétegfúzió**: Kombinálja a műveleteket, mint például Conv+BatchNorm+ReLU a jobb NPU kihasználás érdekében
- **Mélységi szétválasztott konvolúciók**: Ezeket részesítse előnyben a standard konvolúciók helyett mobil telepítéshez
- **Kvantálás-barát tervezés**: Használjon ReLU aktivációkat, és kerülje azokat a műveleteket, amelyek nem kvantálhatók jól

### 2. Kvantálási stratégia
- **Utólagos kvantálás**: Kezdje ezzel a gyors telepítéshez
- **Kalibrációs adatállomány**: Használjon reprezentatív adatokat, amelyek lefedik az összes bemeneti variációt
- **Vegyes precízió**: Használjon INT8-at a legtöbb réteghez, tartsa a kritikus rétegeket magasabb precízióban

### 3. Backend kiválasztási irányelvek
- **NPU (HTP)**: Legjobb CNN munkaterhelésekhez, kvantált modellekhez és energiaérzékeny alkalmazásokhoz
- **GPU**: Optimális számításigényes műveletekhez, nagyobb modellekhez és FP16 precízióhoz
- **CPU**: Tartalék az nem támogatott műveletekhez és hibakereséshez

### 4. Teljesítményoptimalizáció
- **Batch méret**: Használjon batch méretet 1 valós idejű alkalmazásokhoz, nagyobb batch-eket áteresztőképességhez
- **Bemeneti előfeldolgozás**: Minimalizálja az adatmásolási és konverziós többletterhelést
- **Kontextus újrahasználása**: Előre fordítsa a kontextusokat, hogy elkerülje a futásidejű fordítási többletterhelést

### 5. Memóriakezelés
- **Tensor allokáció**: Használjon statikus allokációt, amikor csak lehetséges, hogy elkerülje a futásidejű többletterhelést
- **Memóriapoolok**: Valósítson meg egyedi memóriapoolokat gyakran allokált tensorokhoz
- **Buffer újrahasználása**: Használja újra a bemeneti/kimeneti buffereket a következtetési hívások között

### 6. Energiaoptimalizáció
- **Teljesítménymódok**: Használjon megfelelő teljesítménymódokat a hőmérsékleti korlátok alapján
- **Dinamikus frekvencia skálázás**: Engedje meg a rendszernek, hogy a frekvenciát a munkaterhelés alapján skálázza
- **Tétlen állapot kezelése**: Megfelelően engedje el az erőforrásokat, amikor nem használja őket

## Hibaelhárítás

### Gyakori problémák

#### 1. SDK telepítési problémák
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Modellkonvertálási hibák
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Kvantálási problémák
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Teljesítményproblémák
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Memóriaproblémák
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Backend kompatibilitás
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Teljesítmény hibakeresés

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Segítség kérése

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN dokumentáció**: Elérhető az SDK csomagban
- **Közösségi fórumok**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Technikai támogatás**: A Qualcomm fejlesztői portálon keresztül

## További források

### Hivatalos linkek
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon platformok**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Fejlesztői portál**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Tanulási források
- **Kezdő útmutató**: Elérhető a QNN SDK dokumentációban
- **Modellek gyűjteménye**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimalizációs útmutató**: Az SDK dokumentáció átfogó optimalizációs irányelveket tartalmaz
- **Videós oktatóanyagok**: [Qualcomm Developer YouTube csatorna](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integrációs eszközök
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Előre optimalizált modellek Qualcomm hardverhez
- **Android Neural Networks API**: Integráció az Android NNAPI-val
- **TensorFlow Lite Delegate**: Qualcomm delegált TFLite-hez

### Teljesítményértékelések
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Közösségi példák
- **Mintaprogramok**: Elérhetők a QNN SDK példák könyvtárában
- **GitHub tárolók**: Közösség által hozzájárult példák és eszközök
- **Technikai blogok**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Kapcsolódó eszközök
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Fejlett kvantálási és tömörítési technikák
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Összehasonlításhoz és tartalék telepítéshez
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Keresztplatformos következtetési motor

### Hardver specifikációk
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon platformok**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Mi következik?

Folytassa az Edge AI felfedezését a [5. modul: SLMOps és gyártási telepítés](../Module05/README.md) megismerésével, hogy többet tudjon meg a Kis Nyelvi Modellek életciklusának operatív aspektusairól.

---

**Felelősség kizárása**:  
Ez a dokumentum az AI fordítási szolgáltatás [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget semmilyen félreértésért vagy téves értelmezésért, amely a fordítás használatából eredhet.