<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T14:08:58+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "hu"
}
-->
# 3. szakasz: Microsoft Olive Optimalizációs Eszközkészlet

## Tartalomjegyzék
1. [Bevezetés](../../../Module04)
2. [Mi az a Microsoft Olive?](../../../Module04)
3. [Telepítés](../../../Module04)
4. [Gyors kezdési útmutató](../../../Module04)
5. [Példa: Qwen3 átalakítása ONNX INT4 formátumba](../../../Module04)
6. [Haladó használat](../../../Module04)
7. [Olive receptek tárháza](../../../Module04)
8. [Legjobb gyakorlatok](../../../Module04)
9. [Hibaelhárítás](../../../Module04)
10. [További források](../../../Module04)

## Bevezetés

A Microsoft Olive egy erőteljes, könnyen használható, hardverre optimalizált modelloptimalizációs eszközkészlet, amely leegyszerűsíti a gépi tanulási modellek különböző hardverplatformokra történő telepítésének optimalizálását. Legyen szó CPU-ról, GPU-ról vagy speciális AI gyorsítóról, az Olive segít elérni az optimális teljesítményt a modell pontosságának megőrzése mellett.

## Mi az a Microsoft Olive?

Az Olive egy könnyen használható, hardverre optimalizált modelloptimalizációs eszköz, amely iparágvezető technikákat ötvöz a modellkompresszió, optimalizáció és fordítás terén. Az ONNX Runtime-mal működik, mint egy E2E (end-to-end) inferencia optimalizációs megoldás.

### Főbb jellemzők

- **Hardverre optimalizált megoldás**: Automatikusan kiválasztja a legjobb optimalizációs technikákat a célhardverhez
- **40+ beépített optimalizációs komponens**: Modellkompresszió, kvantálás, grafikus optimalizáció és egyéb funkciók
- **Egyszerű CLI felület**: Könnyen használható parancsok a gyakori optimalizációs feladatokhoz
- **Több keretrendszer támogatása**: Működik PyTorch, Hugging Face modellekkel és ONNX-szel
- **Népszerű modellek támogatása**: Az Olive automatikusan optimalizálja a népszerű modellek architektúráit, mint például Llama, Phi, Qwen, Gemma stb.

### Előnyök

- **Fejlesztési idő csökkentése**: Nem szükséges manuálisan kísérletezni különböző optimalizációs technikákkal
- **Teljesítményjavulás**: Jelentős sebességnövekedés (akár 6x bizonyos esetekben)
- **Platformfüggetlen telepítés**: Az optimalizált modellek különböző hardvereken és operációs rendszereken működnek
- **Pontosság megőrzése**: Az optimalizációk megőrzik a modell minőségét, miközben javítják a teljesítményt

## Telepítés

### Előfeltételek

- Python 3.8 vagy újabb
- pip csomagkezelő
- Virtuális környezet (ajánlott)

### Alapvető telepítés

Hozzon létre és aktiváljon egy virtuális környezetet:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Telepítse az Olive-t automatikus optimalizációs funkciókkal:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Opcionális függőségek

Az Olive különféle opcionális függőségeket kínál további funkciókhoz:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Telepítés ellenőrzése

```bash
olive --help
```

Sikeres telepítés esetén az Olive CLI súgó üzenetét kell látnia.

## Gyors kezdési útmutató

### Az első optimalizáció

Optimalizáljunk egy kis nyelvi modellt az Olive automatikus optimalizációs funkciójával:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Mit csinál ez a parancs?

Az optimalizációs folyamat magában foglalja: a modell helyi gyorsítótárból történő beszerzését, az ONNX grafikon rögzítését és a súlyok ONNX adatfájlba történő mentését, az ONNX grafikon optimalizálását, valamint a modell int4 formátumba történő kvantálását az RTN módszerrel.

### Parancsparaméterek magyarázata

- `--model_name_or_path`: Hugging Face modellazonosító vagy helyi elérési út
- `--output_path`: Az optimalizált modell mentésére szolgáló könyvtár
- `--device`: Célhardver (cpu, gpu)
- `--provider`: Végrehajtási szolgáltató (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI használata inferenciához
- `--precision`: Kvantálási pontosság (int4, int8, fp16)
- `--log_level`: Naplózási részletesség (0=minimális, 1=részletes)

## Példa: Qwen3 átalakítása ONNX INT4 formátumba

A Hugging Face példája alapján: [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), így optimalizálhatunk egy Qwen3 modellt:

### 1. lépés: Modell letöltése (opcionális)

A letöltési idő minimalizálása érdekében csak a szükséges fájlokat gyorsítótárazza:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### 2. lépés: Qwen3 modell optimalizálása

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### 3. lépés: Az optimalizált modell tesztelése

Hozzon létre egy egyszerű Python szkriptet az optimalizált modell teszteléséhez:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Kimeneti struktúra

Az optimalizáció után a kimeneti könyvtár tartalmazza:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Haladó használat

### Konfigurációs fájlok

Összetettebb optimalizációs munkafolyamatokhoz JSON konfigurációs fájlokat használhat:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Futtatás konfigurációval:

```bash
olive run --config config.json
```

### GPU optimalizáció

CUDA GPU optimalizációhoz:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) esetén:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finomhangolás Olive-val

Az Olive támogatja a modellek finomhangolását is:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Legjobb gyakorlatok

### 1. Modellválasztás
- Kezdje kisebb modellekkel teszteléshez (pl. 0.5B-7B paraméterek)
- Győződjön meg róla, hogy a célmodell architektúrája támogatott az Olive által

### 2. Hardveres szempontok
- Igazítsa az optimalizációs célt a telepítési hardverhez
- Használjon GPU optimalizációt, ha CUDA-kompatibilis hardverrel rendelkezik
- Fontolja meg a DirectML használatát Windows gépeken integrált grafikával

### 3. Pontosság kiválasztása
- **INT4**: Maximális kompresszió, enyhe pontosságvesztés
- **INT8**: Jó egyensúly méret és pontosság között
- **FP16**: Minimális pontosságvesztés, mérsékelt méretcsökkentés

### 4. Tesztelés és validáció
- Mindig tesztelje az optimalizált modelleket saját specifikus felhasználási esetekkel
- Hasonlítsa össze a teljesítménymutatókat (késleltetés, átbocsátás, pontosság)
- Használjon reprezentatív bemeneti adatokat az értékeléshez

### 5. Iteratív optimalizáció
- Kezdje automatikus optimalizációval gyors eredmények érdekében
- Használjon konfigurációs fájlokat a finomhangolt vezérléshez
- Kísérletezzen különböző optimalizációs lépésekkel

## Hibaelhárítás

### Gyakori problémák

#### 1. Telepítési problémák
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU problémák
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Memóriaproblémák
- Használjon kisebb batch méreteket az optimalizáció során
- Próbálja ki a kvantálást magasabb pontossággal először (int8 az int4 helyett)
- Biztosítson elegendő lemezterületet a modell gyorsítótárazásához

#### 4. Modell betöltési hibák
- Ellenőrizze a modell elérési útját és hozzáférési jogosultságait
- Ellenőrizze, hogy a modell igényli-e a `trust_remote_code=True` beállítást
- Győződjön meg róla, hogy minden szükséges modellfájl le van töltve

### Segítség kérése

- **Dokumentáció**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub hibák**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Példák**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive receptek tárháza

### Bevezetés az Olive receptekbe

A [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) tárház kiegészíti a fő Olive eszközkészletet, és átfogó gyűjteményt kínál népszerű AI modellek optimalizációs receptjeiből. Ez a tárház gyakorlati referenciaként szolgál mind nyilvánosan elérhető modellek optimalizálásához, mind saját modellek optimalizációs munkafolyamatainak létrehozásához.

### Főbb jellemzők

- **100+ előre elkészített recept**: Kész optimalizációs konfigurációk népszerű modellekhez
- **Több architektúra támogatása**: Transformer modellek, vizuális modellek és multimodális architektúrák
- **Hardver-specifikus optimalizációk**: CPU-ra, GPU-ra és speciális gyorsítókra szabott receptek
- **Népszerű modellcsaládok**: Phi, Llama, Qwen, Gemma, Mistral és sok más

### Támogatott modellcsaládok

A tárház optimalizációs recepteket tartalmaz az alábbiakhoz:

#### Nyelvi modellek
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 sorozat (0.5B-tól 14B-ig)
- **Google Gemma**: Különböző Gemma modellkonfigurációk
- **Mistral AI**: Mistral-7B sorozat
- **DeepSeek**: R1-Distill sorozat modellek

#### Vizuális és multimodális modellek
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP modellek**: Különböző CLIP-ViT konfigurációk
- **ResNet**: ResNet-50 optimalizációk
- **Vision Transformers**: ViT-base-patch16-224

#### Speciális modellek
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Alap és többnyelvű változatok
- **Sentence Transformers**: all-MiniLM-L6-v2

### Olive receptek használata

#### 1. módszer: Specifikus recept klónozása

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### 2. módszer: Recept sablonként való használata

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Recept struktúra

Minden receptkönyvtár általában tartalmazza:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Példa: Phi-4-mini recept használata

Használjuk a Phi-4-mini receptet példaként:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

A konfigurációs fájl általában tartalmazza:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Receptek testreszabása

#### Célhardver módosítása

A célhardver megváltoztatásához frissítse a `systems` szekciót:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Optimalizációs paraméterek módosítása

Módosítsa a `passes` szekciót különböző optimalizációs szintekhez:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Saját recept létrehozása

1. **Hasonló modellel kezdje**: Keressen egy receptet hasonló architektúrájú modellhez
2. **Modell konfiguráció frissítése**: Módosítsa a modell nevét/elérési útját a konfigurációban
3. **Paraméterek módosítása**: Igazítsa az optimalizációs paramétereket igényei szerint
4. **Tesztelés és validáció**: Futtassa az optimalizációt és validálja az eredményeket
5. **Hozzájárulás**: Fontolja meg, hogy hozzájáruljon receptjével a tárházhoz

### Receptek használatának előnyei

#### 1. **Bevált konfigurációk**
- Tesztelt optimalizációs beállítások specifikus modellekhez
- Elkerüli a próbálgatásos módszert az optimális paraméterek megtalálásában

#### 2. **Hardver-specifikus hangolás**
- Előre optimalizált különböző végrehajtási szolgáltatókhoz
- Kész konfigurációk CPU, GPU és NPU célokra

#### 3. **Átfogó lefedettség**
- Támogatja a legnépszerűbb nyílt forráskódú modelleket
- Rendszeres frissítések új modellkiadásokkal

#### 4. **Közösségi hozzájárulások**
- Együttműködő fejlesztés az AI közösséggel
- Megosztott tudás és legjobb gyakorlatok

### Hozzájárulás az Olive receptekhez

Ha optimalizált egy modellt, amelyet a tárház nem fed le:

1. **Forkolja a tárházat**: Hozzon létre saját forkot az olive-recipes tárházból
2. **Receptkönyvtár létrehozása**: Adjon hozzá egy új könyvtárat a modelljéhez
3. **Konfiguráció hozzáadása**: Adja hozzá az olive_config.json-t és a támogató fájlokat
4. **Használati dokumentáció**: Készítsen egyértelmű README-t az utasításokkal
5. **Pull Request beküldése**: Járuljon hozzá a közösséghez

### Teljesítményértékelések

Sok recept tartalmaz teljesítményértékeléseket, amelyek bemutatják:
- **Késleltetés javulása**: Általában 2-6x gyorsulás az alaphoz képest
- **Memóriacsökkentés**: 50-75% memóriahasználat csökkenés kvantálással
- **Pontosság megőrzése**: 95-99% pontosság megőrzés

### Integráció AI eszközkészlettel

A receptek zökkenőmentesen működnek:
- **VS Code AI Toolkit**: Közvetlen integráció modelloptimalizációhoz
- **Azure Machine Learning**: Felhőalapú optimalizációs munkafolyamatok
- **ONNX Runtime**: Optimalizált inferencia telepítés

## További források

### Hivatalos linkek
- **GitHub tárház**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive receptek tárháza**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime dokumentáció**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face példa**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Közösségi példák
- **Jupyter Notebookok**: Elérhetők

---

**Felelősség kizárása**:  
Ez a dokumentum az AI fordítási szolgáltatás [Co-op Translator](https://github.com/Azure/co-op-translator) segítségével lett lefordítva. Bár törekszünk a pontosságra, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális emberi fordítást igénybe venni. Nem vállalunk felelősséget semmilyen félreértésért vagy téves értelmezésért, amely a fordítás használatából eredhet.