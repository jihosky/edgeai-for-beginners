<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:45:37+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "ar"
}
-->
# القسم 7: مجموعة تحسين Qualcomm QNN (شبكة Qualcomm العصبية)

## جدول المحتويات
1. [المقدمة](../../../Module04)
2. [ما هو Qualcomm QNN؟](../../../Module04)
3. [التثبيت](../../../Module04)
4. [دليل البدء السريع](../../../Module04)
5. [مثال: تحويل وتحسين النماذج باستخدام QNN](../../../Module04)
6. [الاستخدام المتقدم](../../../Module04)
7. [أفضل الممارسات](../../../Module04)
8. [استكشاف الأخطاء وإصلاحها](../../../Module04)
9. [موارد إضافية](../../../Module04)

## المقدمة

Qualcomm QNN (شبكة Qualcomm العصبية) هو إطار شامل للاستدلال بالذكاء الاصطناعي مصمم لاستغلال الإمكانيات الكاملة لمسرعات الذكاء الاصطناعي من Qualcomm، بما في ذلك Hexagon NPU وAdreno GPU وKryo CPU. سواء كنت تستهدف الأجهزة المحمولة أو منصات الحوسبة الطرفية أو أنظمة السيارات، يوفر QNN قدرات استدلال محسّنة تستفيد من وحدات معالجة الذكاء الاصطناعي المتخصصة من Qualcomm لتحقيق أقصى أداء وكفاءة في استهلاك الطاقة.

## ما هو Qualcomm QNN؟

Qualcomm QNN هو إطار موحد للاستدلال بالذكاء الاصطناعي يمكّن المطورين من نشر نماذج الذكاء الاصطناعي بكفاءة عبر بنية الحوسبة المتنوعة من Qualcomm. يوفر واجهة برمجة موحدة للوصول إلى Hexagon NPU (وحدة معالجة الشبكة العصبية) وAdreno GPU وKryo CPU، مع اختيار تلقائي للوحدة المثلى لمعالجة طبقات النموذج والعمليات المختلفة.

### الميزات الرئيسية

- **الحوسبة المتنوعة**: الوصول الموحد إلى NPU وGPU وCPU مع توزيع تلقائي للأعباء
- **تحسينات متوافقة مع الأجهزة**: تحسينات متخصصة لمنصات Snapdragon من Qualcomm
- **دعم التكميم**: تقنيات تكميم متقدمة مثل INT8 وINT16 والدقة المختلطة
- **أدوات تحويل النماذج**: دعم مباشر لنماذج TensorFlow وPyTorch وONNX وCaffe
- **محسّن للذكاء الاصطناعي الطرفي**: مصمم خصيصًا للسيناريوهات الطرفية والمحمولة مع التركيز على كفاءة الطاقة

### الفوائد

- **أقصى أداء**: الاستفادة من أجهزة الذكاء الاصطناعي المتخصصة لتحسين الأداء حتى 15 ضعفًا
- **كفاءة الطاقة**: مصمم للأجهزة المحمولة التي تعمل بالبطاريات مع إدارة ذكية للطاقة
- **زمن استجابة منخفض**: استدلال مسرّع بالأجهزة مع أقل قدر من التأخير للتطبيقات الفورية
- **نشر قابل للتوسع**: من الهواتف الذكية إلى منصات السيارات عبر نظام Qualcomm البيئي
- **جاهز للإنتاج**: إطار عمل مجرب ومستخدم في ملايين الأجهزة المنتشرة

## التثبيت

### المتطلبات الأساسية

- مجموعة أدوات Qualcomm QNN SDK (تتطلب التسجيل مع Qualcomm)
- Python 3.7 أو أعلى
- أجهزة Qualcomm المتوافقة أو المحاكي
- Android NDK (للنشر على الأجهزة المحمولة)
- بيئة تطوير على Linux أو Windows

### إعداد QNN SDK

1. **التسجيل والتنزيل**: قم بزيارة شبكة مطوري Qualcomm للتسجيل وتنزيل QNN SDK
2. **استخراج SDK**: فك ضغط QNN SDK في دليل التطوير الخاص بك
3. **تعيين متغيرات البيئة**: قم بتكوين المسارات لأدوات ومكتبات QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### إعداد بيئة Python

قم بإنشاء وتفعيل بيئة افتراضية:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

قم بتثبيت حزم Python المطلوبة:

```bash
pip install numpy tensorflow torch onnx
```

### التحقق من التثبيت

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

إذا نجح التثبيت، يجب أن ترى معلومات المساعدة لكل أداة من أدوات QNN.

## دليل البدء السريع

### تحويل النموذج الأول الخاص بك

لنقم بتحويل نموذج PyTorch بسيط ليعمل على أجهزة Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### تحويل ONNX إلى صيغة QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### إنشاء مكتبة نموذج QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### ماذا يفعل هذا الإجراء؟

يتضمن سير العمل الخاص بالتحسين: تحويل النموذج الأصلي إلى صيغة ONNX، ترجمة ONNX إلى تمثيل وسيط لـ QNN، تطبيق تحسينات خاصة بالأجهزة، وإنشاء مكتبة نموذج مجمعة للنشر.

### شرح المعلمات الرئيسية

- `--input_network`: ملف نموذج ONNX المصدر
- `--output_path`: ملف مصدر C++ الناتج
- `--input_dim`: أبعاد المصفوفة المدخلة للتحسين
- `--quantization_overrides`: تكوين تكميم مخصص
- `-t x86_64-linux-clang`: بنية الهدف والمترجم

## مثال: تحويل وتحسين النماذج باستخدام QNN

### الخطوة 1: تحويل النموذج المتقدم مع التكميم

إليك كيفية تطبيق تكميم مخصص أثناء التحويل:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

قم بالتحويل مع التكميم المخصص:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### الخطوة 2: تحسين متعدد الخلفيات

قم بتكوين التنفيذ المتنوع عبر NPU وGPU وCPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### الخطوة 3: إنشاء ملف ثنائي للنشر

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### الخطوة 4: الاستدلال باستخدام وقت تشغيل QNN

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### هيكل الإخراج

بعد التحسين، سيحتوي دليل النشر الخاص بك على:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## الاستخدام المتقدم

### تكوين الخلفية المخصص

قم بتكوين تحسينات خلفية محددة:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### التكميم الديناميكي

قم بتطبيق التكميم أثناء وقت التشغيل لتحسين الدقة:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### تحليل الأداء

راقب الأداء عبر الخلفيات المختلفة:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### اختيار الخلفية التلقائي

قم بتنفيذ اختيار الخلفية الذكي بناءً على خصائص النموذج:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## أفضل الممارسات

### 1. تحسين بنية النموذج
- **دمج الطبقات**: دمج العمليات مثل Conv+BatchNorm+ReLU لتحسين استخدام NPU
- **التلافيف القابلة للفصل عموديًا**: فضل هذه على التلافيف القياسية للنشر على الأجهزة المحمولة
- **تصاميم متوافقة مع التكميم**: استخدم تنشيطات ReLU وتجنب العمليات التي لا تتكمم جيدًا

### 2. استراتيجية التكميم
- **التكميم بعد التدريب**: ابدأ بهذا للنشر السريع
- **مجموعة بيانات المعايرة**: استخدم بيانات تمثيلية تغطي جميع التغيرات المدخلة
- **الدقة المختلطة**: استخدم INT8 لمعظم الطبقات، واحتفظ بالطبقات الحرجة بدقة أعلى

### 3. إرشادات اختيار الخلفية
- **NPU (HTP)**: الأفضل لأعباء عمل CNN والنماذج المكممة والتطبيقات الحساسة للطاقة
- **GPU**: الأمثل للعمليات كثيفة الحوسبة والنماذج الكبيرة ودقة FP16
- **CPU**: الحل البديل للعمليات غير المدعومة وتصحيح الأخطاء

### 4. تحسين الأداء
- **حجم الدفعة**: استخدم حجم دفعة 1 للتطبيقات الفورية، دفعات أكبر لزيادة الإنتاجية
- **معالجة المدخلات**: قلل من نسخ البيانات وتحويلها
- **إعادة استخدام السياق**: قم بتهيئة السياقات مسبقًا لتجنب تكاليف التهيئة أثناء وقت التشغيل

### 5. إدارة الذاكرة
- **تخصيص المصفوفات**: استخدم التخصيص الثابت عندما يكون ذلك ممكنًا لتجنب تكاليف وقت التشغيل
- **مجموعات الذاكرة**: قم بتنفيذ مجموعات ذاكرة مخصصة للمصفوفات التي يتم تخصيصها بشكل متكرر
- **إعادة استخدام المخازن المؤقتة**: أعد استخدام مخازن الإدخال/الإخراج عبر مكالمات الاستدلال

### 6. تحسين الطاقة
- **أنماط الأداء**: استخدم أنماط الأداء المناسبة بناءً على قيود الحرارة
- **تدرج التردد الديناميكي**: اسمح للنظام بتغيير التردد بناءً على عبء العمل
- **إدارة حالة الخمول**: قم بإطلاق الموارد بشكل صحيح عندما لا تكون قيد الاستخدام

## استكشاف الأخطاء وإصلاحها

### المشكلات الشائعة

#### 1. مشاكل تثبيت SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. أخطاء تحويل النموذج
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. مشاكل التكميم
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. مشاكل الأداء
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. مشاكل الذاكرة
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. توافق الخلفية
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### تصحيح الأداء

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### الحصول على المساعدة

- **شبكة مطوري Qualcomm**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **وثائق QNN**: متوفرة في حزمة SDK
- **منتديات المجتمع**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **الدعم الفني**: عبر بوابة مطوري Qualcomm

## موارد إضافية

### الروابط الرسمية
- **مركز Qualcomm للذكاء الاصطناعي**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **منصات Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **بوابة المطورين**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **محرك الذكاء الاصطناعي**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### موارد التعلم
- **دليل البدء**: متوفر في وثائق QNN SDK
- **حديقة النماذج**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **دليل التحسين**: تتضمن وثائق SDK إرشادات شاملة للتحسين
- **دروس الفيديو**: [قناة Qualcomm Developer على YouTube](https://www.youtube.com/c/QualcommDeveloperNetwork)

### أدوات التكامل
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: نماذج محسّنة مسبقًا لأجهزة Qualcomm
- **واجهة الشبكات العصبية لنظام Android**: التكامل مع Android NNAPI
- **TensorFlow Lite Delegate**: مندوب Qualcomm لـ TFLite

### معايير الأداء
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **أبحاث Qualcomm للذكاء الاصطناعي**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### أمثلة المجتمع
- **تطبيقات نموذجية**: متوفرة في دليل أمثلة QNN SDK
- **مستودعات GitHub**: أمثلة وأدوات مقدمة من المجتمع
- **مدونات تقنية**: [مدونة مطوري Qualcomm](https://developer.qualcomm.com/blog)

### الأدوات ذات الصلة
- **أداة كفاءة نماذج الذكاء الاصطناعي من Qualcomm (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - تقنيات التكميم والضغط المتقدمة
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - للمقارنة والنشر البديل
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - محرك استدلال متعدد المنصات

### مواصفات الأجهزة
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **منصات Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ ما التالي

واصل رحلتك في الذكاء الاصطناعي الطرفي من خلال استكشاف [الوحدة 5: SLMOps ونشر الإنتاج](../Module05/README.md) لتتعرف على الجوانب التشغيلية لإدارة دورة حياة نماذج اللغة الصغيرة.

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالترجمة البشرية الاحترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.