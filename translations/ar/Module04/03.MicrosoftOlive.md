<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T10:53:26+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ar"
}
-->
# القسم 3: مجموعة تحسين Microsoft Olive

## جدول المحتويات
1. [المقدمة](../../../Module04)
2. [ما هو Microsoft Olive؟](../../../Module04)
3. [التثبيت](../../../Module04)
4. [دليل البدء السريع](../../../Module04)
5. [مثال: تحويل Qwen3 إلى ONNX INT4](../../../Module04)
6. [الاستخدام المتقدم](../../../Module04)
7. [مستودع وصفات Olive](../../../Module04)
8. [أفضل الممارسات](../../../Module04)
9. [استكشاف الأخطاء وإصلاحها](../../../Module04)
10. [موارد إضافية](../../../Module04)

## المقدمة

Microsoft Olive هي أداة قوية وسهلة الاستخدام لتحسين النماذج مع مراعاة الأجهزة، حيث تبسط عملية تحسين نماذج التعلم الآلي لنشرها عبر منصات الأجهزة المختلفة. سواء كنت تستهدف وحدات المعالجة المركزية (CPU)، أو وحدات معالجة الرسومات (GPU)، أو مسرعات الذكاء الاصطناعي المتخصصة، فإن Olive تساعدك على تحقيق الأداء الأمثل مع الحفاظ على دقة النموذج.

## ما هو Microsoft Olive؟

Olive هي أداة تحسين نماذج سهلة الاستخدام تأخذ في الاعتبار الأجهزة وتجمع بين تقنيات رائدة في الصناعة مثل ضغط النماذج، والتحسين، والتجميع. تعمل مع ONNX Runtime كحل شامل لتحسين الاستدلال.

### الميزات الرئيسية

- **تحسين مراعي للأجهزة**: يختار تلقائيًا أفضل تقنيات التحسين للأجهزة المستهدفة.
- **أكثر من 40 مكون تحسين مدمج**: يشمل ضغط النماذج، التكميم، تحسين الرسوم البيانية، والمزيد.
- **واجهة CLI سهلة**: أوامر بسيطة للمهام الشائعة في التحسين.
- **دعم متعدد الأطر**: يعمل مع PyTorch، نماذج Hugging Face، و ONNX.
- **دعم النماذج الشهيرة**: يمكن لـ Olive تحسين بنى النماذج الشهيرة مثل Llama، Phi، Qwen، Gemma، وغيرها تلقائيًا.

### الفوائد

- **تقليل وقت التطوير**: لا حاجة لتجربة تقنيات التحسين المختلفة يدويًا.
- **تحسين الأداء**: تحسينات كبيرة في السرعة (حتى 6 أضعاف في بعض الحالات).
- **النشر عبر المنصات**: النماذج المحسنة تعمل عبر أجهزة وأنظمة تشغيل مختلفة.
- **الحفاظ على الدقة**: التحسينات تحافظ على جودة النموذج مع تحسين الأداء.

## التثبيت

### المتطلبات الأساسية

- Python 3.8 أو أعلى
- مدير الحزم pip
- بيئة افتراضية (موصى بها)

### التثبيت الأساسي

قم بإنشاء وتفعيل بيئة افتراضية:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

قم بتثبيت Olive مع ميزات التحسين التلقائي:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### التبعيات الاختيارية

يوفر Olive تبعيات اختيارية لميزات إضافية:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### التحقق من التثبيت

```bash
olive --help
```

إذا نجح التثبيت، يجب أن ترى رسالة مساعدة CLI الخاصة بـ Olive.

## دليل البدء السريع

### أول عملية تحسين لك

لنقم بتحسين نموذج لغة صغير باستخدام ميزة التحسين التلقائي لـ Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ماذا يفعل هذا الأمر؟

تشمل عملية التحسين: الحصول على النموذج من ذاكرة التخزين المحلية، التقاط الرسم البياني ONNX وتخزين الأوزان في ملف بيانات ONNX، تحسين الرسم البياني ONNX، وتكميم النموذج إلى int4 باستخدام طريقة RTN.

### شرح معلمات الأمر

- `--model_name_or_path`: معرف نموذج Hugging Face أو المسار المحلي
- `--output_path`: الدليل الذي سيتم حفظ النموذج المحسن فيه
- `--device`: الجهاز المستهدف (cpu، gpu)
- `--provider`: مزود التنفيذ (CPUExecutionProvider، CUDAExecutionProvider، DmlExecutionProvider)
- `--use_ort_genai`: استخدام ONNX Runtime Generate AI للاستدلال
- `--precision`: دقة التكميم (int4، int8، fp16)
- `--log_level`: مستوى تفصيل السجل (0=الحد الأدنى، 1=تفصيلي)

## مثال: تحويل Qwen3 إلى ONNX INT4

استنادًا إلى المثال المقدم من Hugging Face في [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)، إليك كيفية تحسين نموذج Qwen3:

### الخطوة 1: تنزيل النموذج (اختياري)

لتقليل وقت التنزيل، قم بتخزين الملفات الأساسية فقط:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### الخطوة 2: تحسين نموذج Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### الخطوة 3: اختبار النموذج المحسن

قم بإنشاء برنامج Python بسيط لاختبار النموذج المحسن:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### هيكل الإخراج

بعد التحسين، سيحتوي دليل الإخراج الخاص بك على:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## الاستخدام المتقدم

### ملفات التكوين

للحصول على سير عمل تحسين أكثر تعقيدًا، يمكنك استخدام ملفات تكوين JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

قم بالتشغيل باستخدام التكوين:

```bash
olive run --config config.json
```

### تحسين GPU

لتحسين CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

لـ DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### التخصيص باستخدام Olive

يدعم Olive أيضًا تخصيص النماذج:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## أفضل الممارسات

### 1. اختيار النموذج
- ابدأ بالنماذج الصغيرة للاختبار (مثل 0.5B-7B معلمات)
- تأكد من أن بنية النموذج المستهدف مدعومة من Olive

### 2. اعتبارات الأجهزة
- طابق هدف التحسين مع أجهزة النشر الخاصة بك
- استخدم تحسين GPU إذا كان لديك أجهزة متوافقة مع CUDA
- ضع في اعتبارك DirectML لأجهزة Windows مع الرسومات المدمجة

### 3. اختيار الدقة
- **INT4**: أقصى ضغط، فقدان طفيف في الدقة
- **INT8**: توازن جيد بين الحجم والدقة
- **FP16**: فقدان دقة ضئيل، تقليل متوسط الحجم

### 4. الاختبار والتحقق
- اختبر دائمًا النماذج المحسنة مع حالات الاستخدام الخاصة بك
- قارن مقاييس الأداء (زمن الاستجابة، الإنتاجية، الدقة)
- استخدم بيانات إدخال تمثيلية للتقييم

### 5. التحسين التكراري
- ابدأ بالتحسين التلقائي للحصول على نتائج سريعة
- استخدم ملفات التكوين للتحكم الدقيق
- جرب تمريرات تحسين مختلفة

## استكشاف الأخطاء وإصلاحها

### المشكلات الشائعة

#### 1. مشاكل التثبيت
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. مشاكل CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. مشاكل الذاكرة
- استخدم أحجام دفعات أصغر أثناء التحسين
- جرب التكميم بدقة أعلى أولاً (int8 بدلاً من int4)
- تأكد من وجود مساحة كافية على القرص لتخزين النموذج

#### 4. أخطاء تحميل النموذج
- تحقق من مسار النموذج وأذونات الوصول
- تحقق مما إذا كان النموذج يتطلب `trust_remote_code=True`
- تأكد من تنزيل جميع ملفات النموذج المطلوبة

### الحصول على المساعدة

- **التوثيق**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **مشكلات GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **أمثلة**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## مستودع وصفات Olive

### مقدمة عن وصفات Olive

مستودع [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) يكمل أداة Olive الرئيسية من خلال توفير مجموعة شاملة من وصفات التحسين الجاهزة للاستخدام لنماذج الذكاء الاصطناعي الشهيرة. يعمل هذا المستودع كمرجع عملي لتحسين النماذج المتاحة علنًا وإنشاء سير عمل تحسين للنماذج الخاصة.

### الميزات الرئيسية

- **أكثر من 100 وصفة جاهزة**: تكوينات تحسين جاهزة للاستخدام للنماذج الشهيرة
- **دعم متعدد البنى**: يشمل نماذج المحولات، نماذج الرؤية، والهياكل متعددة الوسائط
- **تحسينات خاصة بالأجهزة**: وصفات مخصصة لوحدات المعالجة المركزية، وحدات معالجة الرسومات، والمسرعات المتخصصة
- **عائلات النماذج الشهيرة**: تشمل Phi، Llama، Qwen، Gemma، Mistral، والعديد غيرها

### عائلات النماذج المدعومة

يتضمن المستودع وصفات تحسين لـ:

#### نماذج اللغة
- **Microsoft Phi**: Phi-3-mini، Phi-3.5-mini، Phi-4-mini، Phi-4-reasoning
- **Meta Llama**: Llama-2-7b، Llama-3.1-8B، Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B، Qwen2-7B، سلسلة Qwen2.5 (0.5B إلى 14B)
- **Google Gemma**: تكوينات نماذج Gemma المختلفة
- **Mistral AI**: سلسلة Mistral-7B
- **DeepSeek**: نماذج سلسلة R1-Distill

#### نماذج الرؤية والنماذج متعددة الوسائط
- **Stable Diffusion**: v1.4، XL-base-1.0
- **نماذج CLIP**: تكوينات CLIP-ViT المختلفة
- **ResNet**: تحسينات ResNet-50
- **محولات الرؤية**: ViT-base-patch16-224

#### النماذج المتخصصة
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: المتغيرات الأساسية ومتعددة اللغات
- **محولات الجمل**: all-MiniLM-L6-v2

### استخدام وصفات Olive

#### الطريقة 1: استنساخ وصفة محددة

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### الطريقة 2: استخدام الوصفة كقالب

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### هيكل الوصفة

عادةً ما يحتوي كل دليل وصفة على:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### مثال: استخدام وصفة Phi-4-mini

لنستخدم وصفة Phi-4-mini كمثال:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

عادةً ما يتضمن ملف التكوين:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### تخصيص الوصفات

#### تعديل الأجهزة المستهدفة

لتغيير الأجهزة المستهدفة، قم بتحديث قسم `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### ضبط معلمات التحسين

قم بتعديل قسم `passes` لمستويات تحسين مختلفة:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### إنشاء وصفة خاصة بك

1. **ابدأ بنموذج مشابه**: ابحث عن وصفة لنموذج ذو بنية مشابهة
2. **تحديث تكوين النموذج**: قم بتغيير اسم النموذج/المسار في التكوين
3. **ضبط المعلمات**: قم بتعديل معلمات التحسين حسب الحاجة
4. **اختبار والتحقق**: قم بتشغيل التحسين والتحقق من النتائج
5. **المساهمة مرة أخرى**: فكر في المساهمة بوصفتك في المستودع

### فوائد استخدام الوصفات

#### 1. **تكوينات مثبتة**
- إعدادات تحسين تم اختبارها لنماذج محددة
- تجنب التجربة والخطأ في العثور على المعلمات المثلى

#### 2. **ضبط خاص بالأجهزة**
- تحسين مسبق لمزودي التنفيذ المختلفين
- تكوينات جاهزة للاستخدام لوحدات المعالجة المركزية، وحدات معالجة الرسومات، وأهداف NPU

#### 3. **تغطية شاملة**
- يدعم النماذج المفتوحة المصدر الأكثر شهرة
- تحديثات منتظمة مع إصدارات النماذج الجديدة

#### 4. **مساهمات المجتمع**
- تطوير تعاوني مع مجتمع الذكاء الاصطناعي
- مشاركة المعرفة وأفضل الممارسات

### المساهمة في وصفات Olive

إذا قمت بتحسين نموذج غير مغطى في المستودع:

1. **استنساخ المستودع**: قم بإنشاء نسخة خاصة بك من olive-recipes
2. **إنشاء دليل الوصفة**: أضف دليلًا جديدًا لنموذجك
3. **إضافة التكوين**: أضف olive_config.json والملفات الداعمة
4. **توثيق الاستخدام**: قدم README واضحًا مع التعليمات
5. **إرسال طلب سحب**: ساهم مرة أخرى في المجتمع

### معايير الأداء

تتضمن العديد من الوصفات معايير أداء تظهر:
- **تحسين زمن الاستجابة**: تسريع نموذجي من 2 إلى 6 أضعاف مقارنة بالأساس
- **تقليل الذاكرة**: تقليل استخدام الذاكرة بنسبة 50-75% مع التكميم
- **الحفاظ على الدقة**: الحفاظ على دقة بنسبة 95-99%

### التكامل مع أدوات الذكاء الاصطناعي

تعمل الوصفات بسلاسة مع:
- **VS Code AI Toolkit**: تكامل مباشر لتحسين النماذج
- **Azure Machine Learning**: سير عمل تحسين قائم على السحابة
- **ONNX Runtime**: نشر استدلال محسن

## موارد إضافية

### الروابط الرسمية
- **مستودع GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **مستودع وصفات Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **توثيق ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **مثال Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### أمثلة المجتمع
- **دفاتر Jupyter**: متوفرة في مستودع Olive GitHub — https://github.com/microsoft/Olive/tree/main/examples
- **إضافة VS Code**: نظرة عامة على AI Toolkit لـ VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **مقالات المدونة**: مدونة Microsoft Open Source — https://opensource.microsoft.com/blog/

### الأدوات ذات الصلة
- **ONNX Runtime**: محرك استدلال عالي الأداء — https://onnxruntime.ai/
- **Hugging Face Transformers**: مصدر العديد من النماذج المتوافقة — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: سير عمل تحسين قائم على السحابة — https://learn.microsoft.com/azure/machine-learning/

## ➡️ ما التالي

- [04: مجموعة تحسين OpenVINO Toolkit](./04.openvino.md)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الرسمي. للحصول على معلومات حاسمة، يُوصى بالترجمة البشرية الاحترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة تنشأ عن استخدام هذه الترجمة.