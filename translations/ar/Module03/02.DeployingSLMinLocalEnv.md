<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:09:37+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "ar"
}
-->
# القسم 2: نشر البيئة المحلية - حلول تحافظ على الخصوصية

يمثل نشر نماذج اللغة الصغيرة (SLMs) محليًا تحولًا جذريًا نحو حلول الذكاء الاصطناعي التي تحافظ على الخصوصية وتكون فعالة من حيث التكلفة. يستعرض هذا الدليل الشامل إطارين قويين - Ollama وMicrosoft Foundry Local - يمكّنان المطورين من استغلال الإمكانيات الكاملة لنماذج SLM مع الحفاظ على السيطرة الكاملة على بيئة النشر.

## المقدمة

في هذه الدرس، سنستكشف استراتيجيات النشر المتقدمة لنماذج اللغة الصغيرة في البيئات المحلية. سنغطي المفاهيم الأساسية لنشر الذكاء الاصطناعي محليًا، ونستعرض منصتين رائدتين (Ollama وMicrosoft Foundry Local)، ونقدم إرشادات عملية لتنفيذ حلول جاهزة للإنتاج.

## أهداف التعلم

بنهاية هذا الدرس، ستكون قادرًا على:

- فهم بنية وفوائد أطر نشر نماذج SLM المحلية.
- تنفيذ عمليات نشر جاهزة للإنتاج باستخدام Ollama وMicrosoft Foundry Local.
- مقارنة واختيار المنصة المناسبة بناءً على المتطلبات والقيود المحددة.
- تحسين عمليات النشر المحلية من حيث الأداء والأمان وقابلية التوسع.

## فهم بنية نشر نماذج SLM المحلية

يمثل نشر نماذج SLM محليًا تحولًا أساسيًا من خدمات الذكاء الاصطناعي المعتمدة على السحابة إلى حلول تحافظ على الخصوصية وتعمل داخل المؤسسة. يتيح هذا النهج للمؤسسات السيطرة الكاملة على بنيتها التحتية للذكاء الاصطناعي مع ضمان سيادة البيانات واستقلالية العمليات.

### تصنيفات أطر النشر

فهم النهج المختلفة للنشر يساعد في اختيار الاستراتيجية المناسبة للاستخدامات المحددة:

- **موجه للتطوير**: إعداد مبسط للتجارب والنماذج الأولية.
- **مستوى المؤسسة**: حلول جاهزة للإنتاج مع قدرات تكامل للمؤسسات.
- **متعدد المنصات**: توافق عالمي عبر أنظمة التشغيل المختلفة والأجهزة.

### المزايا الرئيسية لنشر نماذج SLM محليًا

يوفر نشر نماذج SLM محليًا العديد من المزايا الأساسية التي تجعله مثاليًا للتطبيقات الحساسة للخصوصية والمؤسسات:

**الخصوصية والأمان**: يضمن المعالجة المحلية عدم مغادرة البيانات الحساسة لبنية المؤسسة، مما يتيح الامتثال لـ GDPR، HIPAA، وغيرها من المتطلبات التنظيمية. يمكن تنفيذ عمليات نشر معزولة للبيئات المصنفة، بينما توفر سجلات التدقيق الكاملة إشرافًا أمنيًا.

**فعالية التكلفة**: القضاء على نماذج التسعير لكل رمز يقلل بشكل كبير من تكاليف التشغيل. متطلبات النطاق الترددي المنخفضة والاعتماد الأقل على السحابة توفر هياكل تكلفة متوقعة لميزانية المؤسسة.

**الأداء والموثوقية**: أوقات استنتاج أسرع بدون تأخير الشبكة تمكن التطبيقات في الوقت الفعلي. تضمن الوظائف غير المتصلة بالإنترنت التشغيل المستمر بغض النظر عن الاتصال بالإنترنت، بينما يوفر تحسين الموارد المحلية أداءً ثابتًا.

## Ollama: منصة نشر محلية عالمية

### البنية الأساسية والفلسفة

تم تصميم Ollama كمنصة عالمية سهلة الاستخدام للمطورين، تتيح نشر نماذج LLM محليًا عبر تكوينات الأجهزة وأنظمة التشغيل المختلفة.

**الأساس التقني**: يعتمد Ollama على إطار العمل القوي llama.cpp، ويستخدم تنسيق النموذج GGUF الفعال لتحقيق الأداء الأمثل. يضمن التوافق عبر المنصات سلوكًا متسقًا عبر بيئات Windows وmacOS وLinux، بينما يعمل إدارة الموارد الذكية على تحسين استخدام وحدة المعالجة المركزية ووحدة معالجة الرسومات والذاكرة.

**فلسفة التصميم**: يركز Ollama على البساطة دون التضحية بالوظائف، حيث يوفر نشرًا بدون إعداد لتحقيق الإنتاجية الفورية. تحافظ المنصة على توافق واسع مع النماذج مع توفير واجهات برمجة تطبيقات متسقة عبر هياكل النماذج المختلفة.

### الميزات والقدرات المتقدمة

**إدارة النماذج الممتازة**: يوفر Ollama إدارة شاملة لدورة حياة النماذج مع السحب التلقائي، التخزين المؤقت، وإدارة الإصدارات. تدعم المنصة نظامًا بيئيًا واسعًا للنماذج بما في ذلك Llama 3.2، Google Gemma 2، Microsoft Phi-4، Qwen 2.5، DeepSeek، Mistral، ونماذج التضمين المتخصصة.

**التخصيص عبر ملفات النماذج**: يمكن للمستخدمين المتقدمين إنشاء تكوينات نماذج مخصصة مع معلمات محددة، مطالبات النظام، وتعديلات السلوك. يتيح ذلك تحسينات خاصة بالمجال ومتطلبات التطبيقات المتخصصة.

**تحسين الأداء**: يكتشف Ollama تلقائيًا ويستخدم تسريع الأجهزة المتاح بما في ذلك NVIDIA CUDA، Apple Metal، وOpenCL. يضمن إدارة الذاكرة الذكية استخدام الموارد الأمثل عبر تكوينات الأجهزة المختلفة.

### استراتيجيات التنفيذ للإنتاج

**التثبيت والإعداد**: يوفر Ollama تثبيتًا مبسطًا عبر المنصات من خلال المثبتات الأصلية، ومديري الحزم (WinGet، Homebrew، APT)، وحاويات Docker للنشر المعبأ.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**الأوامر والعمليات الأساسية**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**التكوين المتقدم**: تتيح ملفات النماذج تخصيصًا متطورًا لمتطلبات المؤسسات:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### أمثلة على تكامل المطورين

**تكامل واجهة برمجة تطبيقات Python**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**تكامل JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**استخدام واجهة برمجة التطبيقات RESTful مع cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### تحسين الأداء وضبطه

**تكوين الذاكرة والخيوط**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**اختيار التكميم للأجهزة المختلفة**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: منصة الذكاء الاصطناعي المؤسسية

### بنية على مستوى المؤسسة

تمثل Microsoft Foundry Local حلاً مؤسسيًا شاملاً مصممًا خصيصًا لنشر الذكاء الاصطناعي على الحافة مع تكامل عميق في نظام Microsoft البيئي.

**الأساس القائم على ONNX**: يعتمد Foundry Local على ONNX Runtime القياسي في الصناعة، ويوفر أداءً محسنًا عبر بنى الأجهزة المختلفة. تستفيد المنصة من تكامل Windows ML لتحسين Windows الأصلي مع الحفاظ على التوافق عبر المنصات.

**تميز تسريع الأجهزة**: يتميز Foundry Local بالكشف الذكي عن الأجهزة وتحسينها عبر وحدات المعالجة المركزية، وحدات معالجة الرسومات، ووحدات المعالجة العصبية. يضمن التعاون العميق مع موردي الأجهزة (AMD، Intel، NVIDIA، Qualcomm) الأداء الأمثل على تكوينات الأجهزة المؤسسية.

### تجربة المطور المتقدمة

**الوصول متعدد الواجهات**: يوفر Foundry Local واجهات تطوير شاملة بما في ذلك واجهة سطر أوامر قوية لإدارة النماذج والنشر، SDKs متعددة اللغات (Python، NodeJS) للتكامل الأصلي، وواجهات برمجة تطبيقات RESTful متوافقة مع OpenAI للهجرة السلسة.

**تكامل Visual Studio**: تندمج المنصة بسلاسة مع أدوات الذكاء الاصطناعي لـ VS Code، مما يوفر أدوات تحويل النماذج، التكميم، والتحسين داخل بيئة التطوير. يسرع هذا التكامل سير العمل التطويري ويقلل من تعقيد النشر.

**خط أنابيب تحسين النماذج**: يتيح تكامل Microsoft Olive سير عمل تحسين النماذج المتقدم بما في ذلك التكميم الديناميكي، تحسين الرسوم البيانية، وضبط الأجهزة. توفر قدرات التحويل المستندة إلى السحابة عبر Azure ML تحسينًا قابلاً للتوسع للنماذج الكبيرة.

### استراتيجيات التنفيذ للإنتاج

**التثبيت والتكوين**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**عمليات إدارة النماذج**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**تكوين النشر المتقدم**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### تكامل النظام البيئي المؤسسي

**الأمان والامتثال**: يوفر Foundry Local ميزات أمان على مستوى المؤسسة بما في ذلك التحكم في الوصول القائم على الأدوار، تسجيل التدقيق، تقارير الامتثال، وتخزين النماذج المشفر. يضمن التكامل مع بنية أمان Microsoft الالتزام بسياسات الأمان المؤسسية.

**خدمات الذكاء الاصطناعي المدمجة**: تقدم المنصة قدرات ذكاء اصطناعي جاهزة للاستخدام بما في ذلك Phi Silica لمعالجة اللغة المحلية، AI Imaging لتحسين الصور وتحليلها، وواجهات برمجة تطبيقات متخصصة لمهام الذكاء الاصطناعي المؤسسية الشائعة.

## التحليل المقارن: Ollama مقابل Foundry Local

### مقارنة البنية التقنية

| **الجانب** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **تنسيق النموذج** | GGUF (عبر llama.cpp) | ONNX (عبر ONNX Runtime) |
| **تركيز المنصة** | توافق عالمي عبر المنصات | تحسين Windows/المؤسسات |
| **تكامل الأجهزة** | دعم عام لوحدات المعالجة المركزية/وحدات معالجة الرسومات | تكامل عميق مع Windows ML ودعم وحدات المعالجة العصبية |
| **التحسين** | تكميم llama.cpp | Microsoft Olive + ONNX Runtime |
| **ميزات المؤسسات** | مدفوعة بالمجتمع | على مستوى المؤسسات مع اتفاقيات مستوى الخدمة |

### خصائص الأداء

**نقاط قوة أداء Ollama**:
- أداء استثنائي لوحدات المعالجة المركزية عبر تحسينات llama.cpp
- سلوك متسق عبر المنصات والأجهزة المختلفة
- استخدام ذاكرة فعال مع تحميل ذكي للنماذج
- أوقات بدء تشغيل سريعة للتطوير وسيناريوهات الاختبار

**مزايا أداء Foundry Local**:
- استخدام ممتاز لوحدات المعالجة العصبية على أجهزة Windows الحديثة
- تسريع GPU محسن عبر شراكات الموردين
- مراقبة وتحسين الأداء على مستوى المؤسسات
- قدرات نشر قابلة للتوسع لبيئات الإنتاج

### تحليل تجربة المطور

**تجربة مطور Ollama**:
- متطلبات إعداد بسيطة مع إنتاجية فورية
- واجهة سطر أوامر بديهية لجميع العمليات
- دعم مجتمعي واسع ووثائق شاملة
- تخصيص مرن عبر ملفات النماذج

**تجربة مطور Foundry Local**:
- تكامل شامل مع بيئة Visual Studio
- سير عمل تطوير مؤسسي مع ميزات تعاون الفريق
- قنوات دعم احترافية بدعم من Microsoft
- أدوات تصحيح وتحسين متقدمة

### تحسين حالات الاستخدام

**اختر Ollama عندما**:
- تطوير تطبيقات متعددة المنصات تتطلب سلوكًا متسقًا
- إعطاء الأولوية للشفافية مفتوحة المصدر ومساهمات المجتمع
- العمل بموارد محدودة أو قيود الميزانية
- بناء تطبيقات تجريبية أو موجهة للبحث
- الحاجة إلى توافق واسع مع النماذج عبر الهياكل المختلفة

**اختر Foundry Local عندما**:
- نشر تطبيقات مؤسسية بمتطلبات أداء صارمة
- الاستفادة من تحسينات الأجهزة الخاصة بـ Windows (وحدات المعالجة العصبية، Windows ML)
- الحاجة إلى دعم المؤسسات، اتفاقيات مستوى الخدمة، وميزات الامتثال
- بناء تطبيقات إنتاجية مع تكامل نظام Microsoft البيئي
- الحاجة إلى أدوات تحسين متقدمة وسير عمل تطوير احترافي

## استراتيجيات النشر المتقدمة

### أنماط النشر المعبأة

**تعبئة Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**نشر Foundry Local المؤسسي**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### تقنيات تحسين الأداء

**استراتيجيات تحسين Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**تحسين Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## اعتبارات الأمان والامتثال

### تنفيذ الأمان المؤسسي

**أفضل ممارسات أمان Ollama**:
- عزل الشبكة باستخدام قواعد الجدار الناري والوصول عبر VPN
- المصادقة من خلال تكامل الوكيل العكسي
- التحقق من سلامة النموذج وتوزيع النموذج الآمن
- تسجيل التدقيق للوصول إلى واجهات برمجة التطبيقات وعمليات النموذج

**أمان المؤسسات في Foundry Local**:
- التحكم في الوصول القائم على الأدوار مع تكامل Active Directory
- سجلات تدقيق شاملة مع تقارير الامتثال
- تخزين النماذج المشفر ونشر النماذج الآمن
- التكامل مع بنية أمان Microsoft

### الامتثال والمتطلبات التنظيمية

تدعم كلا المنصتين الامتثال التنظيمي من خلال:
- التحكم في إقامة البيانات لضمان المعالجة المحلية
- تسجيل التدقيق لمتطلبات تقارير الامتثال
- التحكم في الوصول للتعامل مع البيانات الحساسة
- التشفير أثناء الراحة وأثناء النقل لحماية البيانات

## أفضل الممارسات لنشر الإنتاج

### المراقبة والملاحظة

**المقاييس الرئيسية للمراقبة**:
- زمن استنتاج النموذج ومعدل الإنتاجية
- استخدام الموارد (وحدة المعالجة المركزية، وحدة معالجة الرسومات، الذاكرة)
- أوقات استجابة واجهات برمجة التطبيقات ومعدلات الخطأ
- دقة النموذج وانحراف الأداء

**تنفيذ المراقبة**:

```yaml
# Prometheus monitoring configuration
- job_name: 'ollama'
  static_configs:
    - targets: ['localhost:11434']
  metrics_path: '/metrics'
  
- job_name: 'foundry-local'
  static_configs:
    - targets: ['localhost:8080']
  metrics_path: '/api/metrics'
```

### التكامل المستمر والنشر

**تكامل خط أنابيب CI/CD**:

```yaml
name: Deploy SLM Models
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Ollama
      run: |
        ollama pull qwen2.5:3b
        ollama create production-model -f Modelfile
        
    - name: Deploy to Foundry Local
      run: |
        foundry-local models pull microsoft/phi-4-mini
        foundry-local deploy --config production.json
```

## الاتجاهات المستقبلية والاعتبارات

### التقنيات الناشئة

يستمر مشهد نشر نماذج SLM المحلية في التطور مع العديد من الاتجاهات الرئيسية:

**هياكل النماذج المتقدمة**: تظهر نماذج SLM من الجيل التالي مع تحسينات في الكفاءة ونسب القدرات، بما في ذلك نماذج الخبراء المختلطة للتوسع الديناميكي والهياكل المتخصصة للنشر على الحافة.

**تكامل الأجهزة**: سيؤدي التكامل الأعمق مع الأجهزة المتخصصة للذكاء الاصطناعي بما في ذلك وحدات المعالجة العصبية، السيليكون المخصص، ومسرعات الحوسبة على الحافة إلى توفير قدرات أداء محسنة.

**تطور النظام البيئي**: ستبسط جهود التوحيد عبر منصات النشر وتحسين التوافق بين الأطر المختلفة عمليات النشر متعددة المنصات.

### أنماط تبني الصناعة

**تبني المؤسسات**: زيادة تبني المؤسسات مدفوعة بمتطلبات الخصوصية، تحسين التكلفة، واحتياجات الامتثال التنظيمي. تركز القطاعات الحكومية والدفاعية بشكل خاص على عمليات النشر المعزولة.

**الاعتبارات العالمية**: تدفع متطلبات سيادة البيانات الدولية تبني النشر المحلي، خاصة في المناطق ذات اللوائح الصارمة لحماية البيانات.

## التحديات والاعتبارات

### التحديات التقنية

**متطلبات البنية التحتية**: يتطلب النشر المحلي تخطيطًا دقيقًا للقدرة واختيار الأجهزة. يجب على المؤسسات تحقيق التوازن بين متطلبات الأداء وقيود التكلفة مع ضمان قابلية التوسع لأعباء العمل المتزايدة.

**🔧 الصيانة والتحديثات**: تتطلب تحديثات النماذج المنتظمة، تصحيحات الأمان، وتحسين الأداء موارد وخبرات مخصصة. تصبح خطوط أنابيب النشر الآلية ضرورية لبيئات الإنتاج.

### اعتبارات الأمان

**أمان النموذج**: حماية النماذج الخاصة من الوصول غير المصرح به أو الاستخراج يتطلب تدابير أمان شاملة بما في ذلك التشفير، التحكم في الوصول، وتسجيل التدقيق.

**حماية البيانات**: ضمان التعامل الآمن مع البيانات طوال خط أنابيب الاستنتاج مع الحفاظ على معايير الأداء وقابلية الاستخدام.

## قائمة التحقق للتنفيذ العملي

### ✅ تقييم ما قبل النشر

- [ ] تحليل متطلبات الأجهزة وتخطيط القدرة
- [ ] تعريف بنية الشبكة ومتطلبات الأمان
- [ ] اختيار النموذج واختبار الأداء
- [ ] التحقق من الامتثال والمتطلبات التنظيمية

### ✅ تنفيذ النشر

- [ ] اختيار المنصة بناءً على تحليل المتطلبات
- [ ] تثبيت وتكوين المنصة المختارة
- [ ] تنفيذ تحسين النموذج والتكميم
- [ ] إكمال تكامل واجهات برمجة التطبيقات والاختبار

### ✅ جاهزية الإنتاج

- [ ] تكوين نظام المراقبة والتنبيه
- [ ] إنشاء إجراءات النسخ الاحتياطي واستعادة الكوارث
- [ ] إكمال ضبط الأداء والتحسين
- [ ] تطوير الوثائق ومواد التدريب

## الخاتمة

يعتمد الاختيار بين Ollama وMicrosoft Foundry Local على متطلبات المنظمة المحددة، القيود التقنية، والأهداف الاستراتيجية. تقدم كلا المنصتين مزايا مقنعة لنشر نماذج SLM محليًا، حيث يتفوق Ollama في التوافق عبر المنصات وسهولة الاستخدام، بينما يوفر Foundry Local تحسينًا على مستوى المؤسسات وتكاملًا مع نظام Microsoft البيئي.

يكمن مستقبل نشر الذكاء الاصطناعي في النهج الهجينة التي تجمع بين فوائد المعالجة المحلية وقدرات السحابة. ستكون المؤسسات التي تتقن نشر نماذج SLM محليًا في وضع جيد للاستفادة من تقنيات الذكاء الاصطناعي مع الحفاظ على السيطرة على بياناتها وبنيتها التحتية.

يتطلب النجاح في نشر نماذج SLM محليًا النظر بعناية في المتطلبات التقنية، الآثار الأمنية، والإجراءات التشغيلية. من خلال اتباع أفضل الممارسات واستغلال نقاط القوة لهذه المنصات، يمكن للمؤسسات بناء حلول ذكاء اصطناعي قوية وقابلة للتوسع وآمنة تلبي احتياجاتها وقيودها المحددة.

## ➡️ ما التالي

- [03: التنفيذ العملي لنماذج SLM](./03.DeployingSLMinCloud.md)

---

**إخلاء المسؤولية**:  
تم ترجمة هذا المستند باستخدام خدمة الترجمة بالذكاء الاصطناعي [Co-op Translator](https://github.com/Azure/co-op-translator). بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو عدم دقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالترجمة البشرية الاحترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.