<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8b994c57f1207012e4d7f58b7c0d1ae7",
  "translation_date": "2025-10-17T09:04:24+00:00",
  "source_file": "Workshop/Readme.md",
  "language_code": "es"
}
-->
# EdgeAI para Principiantes - Taller

> **Ruta de aprendizaje pr√°ctico para construir aplicaciones de Edge AI listas para producci√≥n**
>
> Domina el despliegue de IA local con Microsoft Foundry Local, desde la primera respuesta de chat hasta la orquestaci√≥n de m√∫ltiples agentes en 6 sesiones progresivas.

---

## üéØ Introducci√≥n

Bienvenido al **Taller EdgeAI para Principiantes**: tu gu√≠a pr√°ctica para construir aplicaciones inteligentes que funcionan completamente en hardware local. Este taller transforma conceptos te√≥ricos de Edge AI en habilidades del mundo real a trav√©s de ejercicios progresivamente desafiantes utilizando Microsoft Foundry Local y Modelos de Lenguaje Peque√±os (SLMs).

### ¬øPor qu√© este taller?

**La revoluci√≥n de Edge AI est√° aqu√≠**

Organizaciones de todo el mundo est√°n cambiando de IA dependiente de la nube a computaci√≥n en el borde por tres razones cr√≠ticas:

1. **Privacidad y Cumplimiento** - Procesa datos sensibles localmente sin transmisi√≥n a la nube (HIPAA, GDPR, regulaciones financieras).
2. **Rendimiento** - Elimina la latencia de red (50-500ms local vs 500-2000ms ida y vuelta en la nube).
3. **Control de Costos** - Elimina los costos por token de las API y escala sin gastos en la nube.

**Pero Edge AI es diferente**

Ejecutar IA en instalaciones locales requiere nuevas habilidades:
- Selecci√≥n y optimizaci√≥n de modelos para restricciones de recursos.
- Gesti√≥n de servicios locales y aceleraci√≥n de hardware.
- Ingenier√≠a de prompts para modelos m√°s peque√±os.
- Patrones de despliegue en producci√≥n para dispositivos en el borde.

**Este taller te proporciona esas habilidades**

En 6 sesiones enfocadas (~3 horas en total), avanzar√°s desde "Hola Mundo" hasta desplegar sistemas de m√∫ltiples agentes listos para producci√≥n, todo funcionando localmente en tu m√°quina.

---

## üìö Objetivos de Aprendizaje

Al completar este taller, ser√°s capaz de:

### Competencias B√°sicas
1. **Desplegar y Gestionar Servicios de IA Locales**
   - Instalar y configurar Microsoft Foundry Local.
   - Seleccionar modelos apropiados para despliegue en el borde.
   - Gestionar el ciclo de vida de los modelos (descargar, cargar, almacenar en cach√©).
   - Monitorear el uso de recursos y optimizar el rendimiento.

2. **Construir Aplicaciones Impulsadas por IA**
   - Implementar respuestas de chat compatibles con OpenAI localmente.
   - Dise√±ar prompts efectivos para Modelos de Lenguaje Peque√±os.
   - Manejar respuestas en streaming para una mejor experiencia de usuario.
   - Integrar modelos locales en aplicaciones existentes.

3. **Crear Sistemas RAG (Generaci√≥n Aumentada por Recuperaci√≥n)**
   - Construir b√∫squeda sem√°ntica con embeddings.
   - Fundamentar las respuestas de LLM en conocimiento espec√≠fico del dominio.
   - Evaluar la calidad de RAG con m√©tricas est√°ndar de la industria.
   - Escalar de prototipo a producci√≥n.

4. **Optimizar el Rendimiento de Modelos**
   - Evaluar m√∫ltiples modelos para tu caso de uso.
   - Medir latencia, rendimiento y tiempo del primer token.
   - Seleccionar modelos √≥ptimos basados en compensaciones de velocidad/calidad.
   - Comparar compensaciones entre SLM y LLM en escenarios reales.

5. **Orquestar Sistemas de M√∫ltiples Agentes**
   - Dise√±ar agentes especializados para diferentes tareas.
   - Implementar memoria de agentes y gesti√≥n de contexto.
   - Coordinar agentes en flujos de trabajo complejos.
   - Enrutar solicitudes inteligentemente entre m√∫ltiples modelos.

6. **Desplegar Soluciones Listas para Producci√≥n**
   - Implementar manejo de errores y l√≥gica de reintento.
   - Monitorear el uso de tokens y recursos del sistema.
   - Construir arquitecturas escalables con patrones de modelo como herramientas.
   - Planificar rutas de migraci√≥n del borde a h√≠brido (borde + nube).

---

## üéì Resultados de Aprendizaje

### Lo que construir√°s

Al final de este taller, habr√°s creado:

| Sesi√≥n | Entregable | Habilidades Demostradas |
|--------|------------|-------------------------|
| **1** | Aplicaci√≥n de chat con streaming | Configuraci√≥n del servicio, respuestas b√°sicas, UX de streaming |
| **2** | Sistema RAG con evaluaci√≥n | Embeddings, b√∫squeda sem√°ntica, m√©tricas de calidad |
| **3** | Suite de evaluaci√≥n de modelos m√∫ltiples | Medici√≥n de rendimiento, comparaci√≥n de modelos |
| **4** | Comparador SLM vs LLM | An√°lisis de compensaciones, estrategias de optimizaci√≥n |
| **5** | Orquestador de m√∫ltiples agentes | Dise√±o de agentes, gesti√≥n de memoria, coordinaci√≥n |
| **6** | Sistema de enrutamiento inteligente | Detecci√≥n de intenci√≥n, selecci√≥n de modelos, escalabilidad |

### Matriz de Competencias

| Nivel de Habilidad | Sesi√≥n 1-2 | Sesi√≥n 3-4 | Sesi√≥n 5-6 |
|--------------------|------------|------------|------------|
| **Principiante** | ‚úÖ Configuraci√≥n y conceptos b√°sicos | ‚ö†Ô∏è Desafiante | ‚ùå Demasiado avanzado |
| **Intermedio** | ‚úÖ Revisi√≥n r√°pida | ‚úÖ Aprendizaje central | ‚ö†Ô∏è Metas ambiciosas |
| **Avanzado** | ‚úÖ F√°cil de completar | ‚úÖ Refinamiento | ‚úÖ Patrones de producci√≥n |

### Habilidades Preparadas para la Carrera

**Despu√©s de este taller, estar√°s preparado para:**

‚úÖ **Construir Aplicaciones con Privacidad Primero**
- Aplicaciones de salud que manejan PHI/PII localmente.
- Servicios financieros con requisitos de cumplimiento.
- Sistemas gubernamentales con necesidades de soberan√≠a de datos.

‚úÖ **Optimizar para Entornos en el Borde**
- Dispositivos IoT con recursos limitados.
- Aplicaciones m√≥viles con enfoque offline.
- Sistemas en tiempo real de baja latencia.

‚úÖ **Dise√±ar Arquitecturas Inteligentes**
- Sistemas de m√∫ltiples agentes para flujos de trabajo complejos.
- Despliegues h√≠bridos borde-nube.
- Infraestructura de IA optimizada en costos.

‚úÖ **Liderar Iniciativas de Edge AI**
- Evaluar la viabilidad de Edge AI para proyectos.
- Seleccionar modelos y frameworks apropiados.
- Arquitectar soluciones de IA locales escalables.

---

## üó∫Ô∏è Estructura del Taller

### Resumen de las Sesiones (6 Sesiones √ó 30 Minutos = 3 Horas)

| Sesi√≥n | Tema | Enfoque | Duraci√≥n |
|--------|------|---------|----------|
| **1** | Introducci√≥n a Foundry Local | Instalaci√≥n, validaci√≥n, primeras respuestas | 30 min |
| **2** | Construcci√≥n de Soluciones de IA con RAG | Ingenier√≠a de prompts, embeddings, evaluaci√≥n | 30 min |
| **3** | Modelos Open Source | Descubrimiento de modelos, evaluaci√≥n, selecci√≥n | 30 min |
| **4** | Modelos de √öltima Generaci√≥n | SLM vs LLM, optimizaci√≥n, frameworks | 30 min |
| **5** | Agentes Impulsados por IA | Dise√±o de agentes, orquestaci√≥n, memoria | 30 min |
| **6** | Modelos como Herramientas | Enrutamiento, encadenamiento, estrategias de escalabilidad | 30 min |

---

## üöÄ Inicio R√°pido

### Requisitos Previos

**Requisitos del Sistema:**
- **OS**: Windows 10/11, macOS 11+ o Linux (Ubuntu 20.04+).
- **RAM**: M√≠nimo 8GB, recomendado 16GB+.
- **Almacenamiento**: 10GB+ de espacio libre para modelos.
- **CPU**: Procesador moderno con soporte AVX2.
- **GPU** (opcional): Compatible con CUDA o Qualcomm NPU para aceleraci√≥n.

**Requisitos de Software:**
- **Python 3.8+** ([Descargar](https://www.python.org/downloads/)).
- **Microsoft Foundry Local** ([Gu√≠a de Instalaci√≥n](../../../Workshop)).
- **Git** ([Descargar](https://git-scm.com/downloads)).
- **Visual Studio Code** (recomendado) ([Descargar](https://code.visualstudio.com/)).

### Configuraci√≥n en 3 Pasos

#### 1. Instalar Foundry Local

**Windows:**
```powershell
winget install Microsoft.FoundryLocal
```

**macOS:**
```bash
brew tap microsoft/foundrylocal
brew install foundrylocal
```

**Verificar Instalaci√≥n:**
```bash
foundry --version
foundry service status
```

**Aseg√∫rate de que Azure AI Foundry Local est√© funcionando con un puerto fijo**

```bash
# Set FoundryLocal to use port 58123 (default)
foundry service set --port 58123 --show

# Or use a different port
foundry service set --port 58000 --show
```

**Verificar que funciona:**
```bash
# Check service status
foundry service status

# Test the endpoint
curl http://127.0.0.1:58123/v1/models
```
**Encontrar Modelos Disponibles**
Para ver qu√© modelos est√°n disponibles en tu instancia de Foundry Local, puedes consultar el endpoint de modelos:

```bash
# cmd/bash/powershell
foundry model list
```

Usando el Endpoint Web 

```bash
# Windows PowerShell
powershell -Command "Invoke-RestMethod -Uri 'http://127.0.0.1:58123/v1/models' -Method Get"

# Or using curl (if available)
curl http://127.0.0.1:58123/v1/models
```

#### 2. Clonar el Repositorio e Instalar Dependencias

```bash
# Clone repository
git clone https://github.com/microsoft/edgeai-for-beginners.git
cd edgeai-for-beginners/Workshop

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.\.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### 3. Ejecutar tu Primer Ejemplo

```bash
# Start Foundry Local and load a model
foundry model run phi-4-mini

# Run the chat bootstrap sample
cd samples/session01
python chat_bootstrap.py "What is edge AI?"
```

**‚úÖ ¬°√âxito!** Deber√≠as ver una respuesta en streaming sobre Edge AI.

---

## üì¶ Recursos del Taller

### Ejemplos en Python

Ejemplos pr√°cticos progresivos que demuestran cada concepto:

| Sesi√≥n | Ejemplo | Descripci√≥n | Tiempo de Ejecuci√≥n |
|--------|---------|-------------|----------------------|
| 1 | [`chat_bootstrap.py`](../../../Workshop/samples/session01/chat_bootstrap.py) | Chat b√°sico y streaming | ~30s |
| 2 | [`rag_pipeline.py`](../../../Workshop/samples/session02/rag_pipeline.py) | RAG con embeddings | ~45s |
| 2 | [`rag_eval_ragas.py`](../../../Workshop/samples/session02/rag_eval_ragas.py) | Evaluaci√≥n de calidad RAG | ~60s |
| 3 | [`benchmark_oss_models.py`](../../../Workshop/samples/session03/benchmark_oss_models.py) | Evaluaci√≥n de modelos m√∫ltiples | ~2-3m |
| 4 | [`model_compare.py`](../../../Workshop/samples/session04/model_compare.py) | Comparaci√≥n SLM vs LLM | ~45s |
| 5 | [`agents_orchestrator.py`](../../../Workshop/samples/session05/agents_orchestrator.py) | Sistema de m√∫ltiples agentes | ~60s |
| 6 | [`models_router.py`](../../../Workshop/samples/session06/models_router.py) | Enrutamiento basado en intenci√≥n | ~45s |
| 6 | [`models_pipeline.py`](../../../Workshop/samples/session06/models_pipeline.py) | Pipeline de m√∫ltiples pasos | ~60s |

### Jupyter Notebooks

Exploraci√≥n interactiva con explicaciones y visualizaciones:

| Sesi√≥n | Notebook | Descripci√≥n | Dificultad |
|--------|----------|-------------|------------|
| 1 | [`session01_chat_bootstrap.ipynb`](./notebooks/session01_chat_bootstrap.ipynb) | Conceptos b√°sicos de chat y streaming | ‚≠ê Principiante |
| 2 | [`session02_rag_pipeline.ipynb`](./notebooks/session02_rag_pipeline.ipynb) | Construir sistema RAG | ‚≠ê‚≠ê Intermedio |
| 2 | [`session02_rag_eval_ragas.ipynb`](./notebooks/session02_rag_eval_ragas.ipynb) | Evaluar calidad RAG | ‚≠ê‚≠ê Intermedio |
| 3 | [`session03_benchmark_oss_models.ipynb`](./notebooks/session03_benchmark_oss_models.ipynb) | Evaluaci√≥n de modelos | ‚≠ê‚≠ê Intermedio |
| 4 | [`session04_model_compare.ipynb`](./notebooks/session04_model_compare.ipynb) | Comparaci√≥n de modelos | ‚≠ê‚≠ê Intermedio |
| 5 | [`session05_agents_orchestrator.ipynb`](./notebooks/session05_agents_orchestrator.ipynb) | Orquestaci√≥n de agentes | ‚≠ê‚≠ê‚≠ê Avanzado |
| 6 | [`session06_models_router.ipynb`](./notebooks/session06_models_router.ipynb) | Enrutamiento por intenci√≥n | ‚≠ê‚≠ê‚≠ê Avanzado |
| 6 | [`session06_models_pipeline.ipynb`](./notebooks/session06_models_pipeline.ipynb) | Orquestaci√≥n de pipeline | ‚≠ê‚≠ê‚≠ê Avanzado |

### Documentaci√≥n

Gu√≠as y referencias completas:

| Documento | Descripci√≥n | Usar Cuando |
|-----------|-------------|-------------|
| [QUICK_START.md](./QUICK_START.md) | Gu√≠a de configuraci√≥n r√°pida | Comenzando desde cero |
| [QUICK_REFERENCE.md](./QUICK_REFERENCE.md) | Hoja de referencia de comandos y API | Necesitas respuestas r√°pidas |
| [FOUNDRY_SDK_QUICKREF.md](./FOUNDRY_SDK_QUICKREF.md) | Patrones y ejemplos del SDK | Escribiendo c√≥digo |
| [ENV_CONFIGURATION.md](./ENV_CONFIGURATION.md) | Gu√≠a de variables de entorno | Configurando ejemplos |
| [SAMPLES_UPDATE_SUMMARY.md](./SAMPLES_UPDATE_SUMMARY.md) | √öltimas mejoras en ejemplos | Entendiendo cambios |
| [SDK_MIGRATION_NOTES.md](./SDK_MIGRATION_NOTES.md) | Gu√≠a de migraci√≥n | Actualizando c√≥digo |
| [notebooks/TROUBLESHOOTING.md](./notebooks/TROUBLESHOOTING.md) | Problemas comunes y soluciones | Solucionando problemas |

---

## üéì Recomendaciones de Ruta de Aprendizaje

### Para Principiantes (3-4 horas)
1. ‚úÖ Sesi√≥n 1: Introducci√≥n (enf√≥cate en la configuraci√≥n y chat b√°sico).
2. ‚úÖ Sesi√≥n 2: Conceptos b√°sicos de RAG (omite la evaluaci√≥n inicialmente).
3. ‚úÖ Sesi√≥n 3: Evaluaci√≥n simple (solo 2 modelos).
4. ‚è≠Ô∏è Omite las Sesiones 4-6 por ahora.
5. üîÑ Regresa a las Sesiones 4-6 despu√©s de construir tu primera aplicaci√≥n.

### Para Desarrolladores Intermedios (3 horas)
1. ‚ö° Sesi√≥n 1: Validaci√≥n r√°pida de configuraci√≥n.
2. ‚úÖ Sesi√≥n 2: Completa el pipeline RAG con evaluaci√≥n.
3. ‚úÖ Sesi√≥n 3: Suite completa de evaluaci√≥n.
4. ‚úÖ Sesi√≥n 4: Optimizaci√≥n de modelos.
5. ‚úÖ Sesiones 5-6: Enf√≥cate en patrones de arquitectura.

### Para Practicantes Avanzados (2-3 horas)
1. ‚ö° Sesiones 1-3: Revisi√≥n r√°pida y validaci√≥n.
2. ‚úÖ Sesi√≥n 4: Profundizaci√≥n en optimizaci√≥n.
3. ‚úÖ Sesi√≥n 5: Arquitectura de m√∫ltiples agentes.
4. ‚úÖ Sesi√≥n 6: Patrones de producci√≥n y escalabilidad.
5. üöÄ Extiende: Construye l√≥gica de enrutamiento personalizada y despliegues h√≠bridos.

---

## Paquete de Sesiones del Taller (Laboratorios Enfocados de 30 Minutos)

Si est√°s siguiendo el formato condensado de 6 sesiones del taller, utiliza estas gu√≠as dedicadas (cada una se relaciona y complementa los documentos del m√≥dulo m√°s amplio arriba):

| Sesi√≥n del Taller | Gu√≠a | Enfoque Principal |
|-------------------|------|-------------------|
| 1 | [Session01-GettingStartedFoundryLocal](./Session01-GettingStartedFoundryLocal.md) | Instalaci√≥n, validaci√≥n, ejecuci√≥n phi & GPT-OSS-20B, aceleraci√≥n |
| 2 | [Session02-BuildAISolutionsRAG](./Session02-BuildAISolutionsRAG.md) | Ingenier√≠a de prompts, patrones RAG, CSV y fundamentos de documentos, migraci√≥n |
| 3 | [Session03-OpenSourceModels](./Session03-OpenSourceModels.md) | Integraci√≥n con Hugging Face, evaluaci√≥n, selecci√≥n de modelos |
| 4 | [Session04-CuttingEdgeModels](./Session04-CuttingEdgeModels.md) | SLM vs LLM, WebGPU, Chainlit RAG, aceleraci√≥n ONNX |
| 5 | [Session05-AIPoweredAgents](./Session05-AIPoweredAgents.md) | Roles de agentes, memoria, herramientas, orquestaci√≥n |
| 6 | [Session06-ModelsAsTools](./Session06-ModelsAsTools.md) | Enrutamiento, encadenamiento, escalabilidad en Azure |

Cada archivo de sesi√≥n incluye: resumen, objetivos de aprendizaje, flujo de demostraci√≥n de 30 minutos, proyecto inicial, lista de verificaci√≥n de validaci√≥n, soluci√≥n de problemas y referencias al SDK oficial de Foundry Local Python.

### Scripts de Ejemplo

Instalar dependencias del taller (Windows):

```powershell
cd Workshop
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

macOS / Linux:

```bash
cd Workshop
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Si ejecutas el servicio Foundry Local en una m√°quina o VM diferente (Windows) desde macOS, exporta el endpoint:

```bash
export FOUNDRY_LOCAL_ENDPOINT=http://<windows-host>:5273/v1
```

| Sesi√≥n | Script(s) | Descripci√≥n |
|--------|-----------|-------------|
| 1 | `samples/session01/chat_bootstrap.py` | Servicio inicial y chat en streaming |
| 2 | `samples/session02/rag_pipeline.py` | RAG m√≠nimo (embeddings en memoria) |
|   | `samples/session02/rag_eval_ragas.py` | Evaluaci√≥n RAG con m√©tricas de ragas |
| 3 | `samples/session03/benchmark_oss_models.py` | Benchmarking de latencia y rendimiento multi-modelo |
| 4 | `samples/session04/model_compare.py` | Comparaci√≥n SLM vs LLM (latencia y salida de muestra) |
| 5 | `samples/session05/agents_orchestrator.py` | Pipeline de investigaci√≥n de dos agentes ‚Üí editorial |
| 6 | `samples/session06/models_router.py` | Demostraci√≥n de enrutamiento basado en intenci√≥n |
|   | `samples/session06/models_pipeline.py` | Cadena de planificar/ejecutar/refinar en m√∫ltiples pasos |

### Variables de Entorno (Comunes en los Ejemplos)

| Variable | Prop√≥sito | Ejemplo |
|----------|-----------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Alias de modelo √∫nico predeterminado para ejemplos b√°sicos | `phi-4-mini` |
| `SLM_ALIAS` / `LLM_ALIAS` | SLM expl√≠cito vs modelo m√°s grande para comparaci√≥n | `phi-4-mini` / `gpt-oss-20b` |
| `BENCH_MODELS` | Lista de alias para benchmarking separada por comas | `qwen2.5-0.5b,gemma-2-2b,mistral-7b` |
| `BENCH_ROUNDS` | Repeticiones de benchmarking por modelo | `3` |
| `BENCH_PROMPT` | Prompt utilizado en benchmarking | `Explain retrieval augmented generation briefly.` |
| `EMBED_MODEL` | Modelo de embeddings de sentence-transformers | `sentence-transformers/all-MiniLM-L6-v2` |
| `RAG_QUESTION` | Consulta de prueba para el pipeline RAG | `Why use RAG with local inference?` |
| `AGENT_QUESTION` | Consulta para el pipeline de agentes | `Explain why edge AI matters for compliance.` |
| `AGENT_MODEL_PRIMARY` | Alias de modelo para el agente de investigaci√≥n | `phi-4-mini` |
| `AGENT_MODEL_EDITOR` | Alias de modelo para el agente editor (puede ser diferente) | `gpt-oss-20b` |
| `SHOW_USAGE` | Cuando es `1`, imprime el uso de tokens por cada finalizaci√≥n | `1` |
| `RETRY_ON_FAIL` | Cuando es `1`, reintenta una vez en errores transitorios de chat | `1` |
| `RETRY_BACKOFF` | Segundos de espera antes de reintentar | `1.0` |

Si una variable no est√° configurada, los scripts recurren a valores predeterminados razonables. Para demostraciones de un solo modelo, normalmente solo necesitas `FOUNDRY_LOCAL_ALIAS`.

### M√≥dulo de Utilidad

Todos los ejemplos ahora comparten un helper `samples/workshop_utils.py` que proporciona:

* Creaci√≥n en cach√© de `FoundryLocalManager` + cliente OpenAI
* Helper `chat_once()` con reintento opcional + impresi√≥n de uso
* Informes simples de uso de tokens (habilitar con `SHOW_USAGE=1`)

Esto reduce la duplicaci√≥n y destaca las mejores pr√°cticas para una orquestaci√≥n eficiente de modelos locales.

## Mejoras Opcionales (Entre Sesiones)

| Tema | Mejora | Sesiones | Entorno / Alternar |
|------|--------|----------|--------------------|
| Determinismo | Temperatura fija + conjuntos de prompts estables | 1‚Äì6 | Configurar `temperature=0`, `top_p=1` |
| Visibilidad de Uso de Tokens | Ense√±anza consistente de costos/eficiencia | 1‚Äì6 | `SHOW_USAGE=1` |
| Streaming del Primer Token | M√©trica de latencia percibida | 1,3,4,6 | `BENCH_STREAM=1` (benchmark) |
| Resiliencia de Reintento | Maneja inicio en fr√≠o transitorio | Todas | `RETRY_ON_FAIL=1` + `RETRY_BACKOFF` |
| Agentes Multi-Modelo | Especializaci√≥n de roles heterog√©neos | 5 | `AGENT_MODEL_PRIMARY`, `AGENT_MODEL_EDITOR` |
| Enrutamiento Adaptativo | Heur√≠sticas de intenci√≥n + costo | 6 | Extender el enrutador con l√≥gica de escalamiento |
| Memoria Vectorial | Recuperaci√≥n sem√°ntica a largo plazo | 2,5,6 | Integrar √≠ndice de embeddings FAISS/Chroma |
| Exportaci√≥n de Trazas | Auditor√≠a y evaluaci√≥n | 2,5,6 | Agregar l√≠neas JSON por paso |
| M√©tricas de Calidad | Seguimiento cualitativo | 3‚Äì6 | Prompts de puntuaci√≥n secundaria |
| Pruebas R√°pidas | Validaci√≥n r√°pida previa al taller | Todas | `python Workshop/tests/smoke.py` |

### Inicio R√°pido Determinista

```powershell
set FOUNDRY_LOCAL_ALIAS=phi-4-mini
set SHOW_USAGE=1
python Workshop\tests\smoke.py
```

Espera conteos de tokens estables en entradas id√©nticas repetidas.

### Evaluaci√≥n RAG (Sesi√≥n 2)

Usa `rag_eval_ragas.py` para calcular relevancia de respuestas, fidelidad y precisi√≥n de contexto en un peque√±o conjunto de datos sint√©tico:

```powershell
python samples/session02/rag_eval_ragas.py
```

Ampl√≠a proporcionando un JSONL m√°s grande de preguntas, contextos y verdades base, luego convi√©rtelo en un `Dataset` de Hugging Face.

## Ap√©ndice de Precisi√≥n de Comandos CLI

El taller utiliza deliberadamente solo comandos CLI documentados/estables de Foundry Local.

### Comandos Estables Referenciados

| Categor√≠a | Comando | Prop√≥sito |
|-----------|---------|----------|
| N√∫cleo | `foundry --version` | Mostrar versi√≥n instalada |
| N√∫cleo | `foundry init` | Inicializar configuraci√≥n |
| Servicio | `foundry service start` | Iniciar servicio local (si no es autom√°tico) |
| Servicio | `foundry status` | Mostrar estado del servicio |
| Modelos | `foundry model list` | Listar cat√°logo / modelos disponibles |
| Modelos | `foundry model download <alias>` | Descargar pesos del modelo en cach√© |
| Modelos | `foundry model run <alias>` | Ejecutar (cargar) un modelo localmente; combinar con `--prompt` para una sola ejecuci√≥n |
| Modelos | `foundry model unload <alias>` / `foundry model stop <alias>` | Descargar un modelo de la memoria (si es compatible) |
| Cach√© | `foundry cache list` | Listar modelos en cach√© (descargados) |
| Sistema | `foundry system info` | Instant√°nea de capacidades de hardware y aceleraci√≥n |
| Sistema | `foundry system gpu-info` | Informaci√≥n de diagn√≥stico de GPU |
| Configuraci√≥n | `foundry config list` | Mostrar valores de configuraci√≥n actuales |
| Configuraci√≥n | `foundry config set <key> <value>` | Actualizar configuraci√≥n |

### Patr√≥n de Prompt de Una Sola Ejecuci√≥n

En lugar de un subcomando `model chat` obsoleto, usa:

```powershell
foundry model run <alias> --prompt "Your question here"
```

Esto ejecuta un ciclo de prompt/respuesta √∫nico y luego sale.

### Patrones Eliminados / Evitados

| Obsoleto / No Documentado | Reemplazo / Gu√≠a |
|---------------------------|------------------|
| `foundry model chat <model> "..."` | `foundry model run <model> --prompt "..."` |
| `foundry model list --running` | Usa `foundry model list` + actividad reciente / logs |
| `foundry model list --cached` | `foundry cache list` |
| `foundry model stats <model>` | Usa script de benchmark + herramientas del sistema operativo (Administrador de Tareas / `nvidia-smi`) |
| `foundry model benchmark ...` | `samples/session03/benchmark_oss_models.py` |

### Benchmarking y Telemetr√≠a

- Latencia, p95, tokens/segundo: `samples/session03/benchmark_oss_models.py`
- Latencia del primer token (streaming): configurar `BENCH_STREAM=1`
- Uso de recursos: monitores del sistema operativo (Administrador de Tareas, Monitor de Actividad, `nvidia-smi`) + `foundry system info`.

A medida que nuevos comandos de telemetr√≠a CLI se estabilicen, pueden incorporarse con ediciones m√≠nimas a los archivos markdown de las sesiones.

### Guardia Autom√°tica de Linter

Un linter automatizado previene la reintroducci√≥n de patrones CLI obsoletos dentro de bloques de c√≥digo en archivos markdown:

Script: `Workshop/scripts/lint_markdown_cli.py`

Los patrones obsoletos est√°n bloqueados dentro de bloques de c√≥digo.

Reemplazos recomendados:
| Obsoleto | Reemplazo |
|----------|-----------|
| `foundry model chat <a> "..."` | `foundry model run <a> --prompt "..."` |
| `model list --running` | `model list` |
| `model list --cached` | `cache list` |
| `model stats` | Script de benchmark + herramientas del sistema |
| `model benchmark` | `samples/session03/benchmark_oss_models.py` |
| `model list --available` | `model list` |

Ejecutar localmente:
```powershell
python Workshop\scripts\lint_markdown_cli.py --verbose
```

Acci√≥n de GitHub: `.github/workflows/markdown-cli-lint.yml` se ejecuta en cada push y PR.

Hook opcional de pre-commit:
```bash
echo "python Workshop/scripts/lint_markdown_cli.py" > .git/hooks/pre-commit
chmod +x .git/hooks/pre-commit
```

## Tabla R√°pida de Migraci√≥n CLI ‚Üí SDK

| Tarea | Comando CLI | Equivalente SDK (Python) | Notas |
|-------|-------------|--------------------------|-------|
| Ejecutar un modelo una vez (prompt) | `foundry model run phi-4-mini --prompt "Hello"` | `manager=FoundryLocalManager("phi-4-mini"); client=OpenAI(base_url=manager.endpoint, api_key=manager.api_key or "not-needed"); client.chat.completions.create(model=manager.get_model_info("phi-4-mini").id, messages=[{"role":"user","content":"Hello"}])` | El SDK inicializa autom√°ticamente el servicio y la cach√© |
| Descargar (cach√©) modelo | `foundry model download qwen2.5-0.5b` | `FoundryLocalManager("qwen2.5-0.5b")  # triggers download/load` | El manager elige la mejor variante si el alias mapea a m√∫ltiples versiones |
| Listar cat√°logo | `foundry model list` | `# usar manager para cada alias o mantener lista conocida` | El CLI agrega; el SDK actualmente instancia por alias |
| Listar modelos en cach√© | `foundry cache list` | `manager.list_cached_models()` | Despu√©s de inicializar el manager (cualquier alias) |
| Habilitar aceleraci√≥n GPU | `foundry config set compute.onnx.enable_gpu true` | `# acci√≥n CLI; el SDK asume que la configuraci√≥n ya est√° aplicada` | La configuraci√≥n es un efecto externo |
| Obtener URL del endpoint | (impl√≠cito) | `manager.endpoint` | Usado para crear un cliente compatible con OpenAI |
| Calentar un modelo | `foundry model run <alias>` luego primer prompt | `chat_once(alias, messages=[...])` (utilidad) | Las utilidades manejan el calentamiento inicial de latencia en fr√≠o |
| Medir latencia | `python benchmark_oss_models.py` | `import benchmark_oss_models` (o nuevo script exportador) | Preferir script para m√©tricas consistentes |
| Detener / descargar modelo | `foundry model unload <alias>` | (No expuesto ‚Äì reiniciar servicio / proceso) | Normalmente no es necesario para el flujo del taller |
| Recuperar uso de tokens | (ver salida) | `resp.usage.total_tokens` | Proporcionado si el backend devuelve un objeto de uso |

## Exportaci√≥n Markdown de Benchmark

Usa el script `Workshop/scripts/export_benchmark_markdown.py` para ejecutar un benchmark nuevo (misma l√≥gica que `samples/session03/benchmark_oss_models.py`) y emitir una tabla Markdown amigable para GitHub m√°s JSON sin procesar.

### Ejemplo

```powershell
python Workshop\scripts\export_benchmark_markdown.py --models "qwen2.5-0.5b,gemma-2-2b,mistral-7b" --prompt "Explain retrieval augmented generation briefly." --rounds 3 --output benchmark_report.md
```

Archivos generados:
| Archivo | Contenido |
|---------|-----------|
| `benchmark_report.md` | Tabla Markdown + pistas de interpretaci√≥n |
| `benchmark_report.json` | Array de m√©tricas sin procesar (para comparaci√≥n / seguimiento de tendencias) |

Configura `BENCH_STREAM=1` en el entorno para incluir latencia del primer token si es compatible.

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que surjan del uso de esta traducci√≥n.