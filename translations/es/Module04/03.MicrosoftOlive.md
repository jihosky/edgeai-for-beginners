<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-10-30T10:38:18+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "es"
}
-->
# Sección 3: Microsoft Olive Optimization Suite

## Tabla de Contenidos
1. [Introducción](../../../Module04)
2. [¿Qué es Microsoft Olive?](../../../Module04)
3. [Instalación](../../../Module04)
4. [Guía Rápida](../../../Module04)
5. [Ejemplo: Convertir Qwen3 a ONNX INT4](../../../Module04)
6. [Uso Avanzado](../../../Module04)
7. [Repositorio de Recetas Olive](../../../Module04)
8. [Mejores Prácticas](../../../Module04)
9. [Solución de Problemas](../../../Module04)
10. [Recursos Adicionales](../../../Module04)

## Introducción

Microsoft Olive es una potente y fácil de usar herramienta de optimización de modelos consciente del hardware que simplifica el proceso de optimización de modelos de aprendizaje automático para su implementación en diferentes plataformas de hardware. Ya sea que estés trabajando con CPUs, GPUs o aceleradores de IA especializados, Olive te ayuda a lograr un rendimiento óptimo mientras mantiene la precisión del modelo.

## ¿Qué es Microsoft Olive?

Olive es una herramienta de optimización de modelos consciente del hardware que combina técnicas líderes en la industria para compresión, optimización y compilación de modelos. Funciona con ONNX Runtime como una solución de optimización de inferencia de extremo a extremo.

### Características Principales

- **Optimización Consciente del Hardware**: Selecciona automáticamente las mejores técnicas de optimización para tu hardware objetivo.
- **Más de 40 Componentes de Optimización Integrados**: Incluye compresión de modelos, cuantización, optimización de gráficos y más.
- **Interfaz CLI Fácil de Usar**: Comandos simples para tareas comunes de optimización.
- **Soporte Multi-Framework**: Compatible con PyTorch, modelos de Hugging Face y ONNX.
- **Soporte para Modelos Populares**: Olive puede optimizar automáticamente arquitecturas de modelos populares como Llama, Phi, Qwen, Gemma, etc., de manera predeterminada.

### Beneficios

- **Reducción del Tiempo de Desarrollo**: No es necesario experimentar manualmente con diferentes técnicas de optimización.
- **Mejoras en el Rendimiento**: Incrementos significativos en la velocidad (hasta 6x en algunos casos).
- **Implementación Multiplataforma**: Los modelos optimizados funcionan en diferentes hardware y sistemas operativos.
- **Precisión Mantenida**: Las optimizaciones preservan la calidad del modelo mientras mejoran el rendimiento.

## Instalación

### Requisitos Previos

- Python 3.8 o superior
- Administrador de paquetes pip
- Entorno virtual (recomendado)

### Instalación Básica

Crea y activa un entorno virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instala Olive con funciones de auto-optimización:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependencias Opcionales

Olive ofrece varias dependencias opcionales para funciones adicionales:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verificar Instalación

```bash
olive --help
```

Si es exitoso, deberías ver el mensaje de ayuda de Olive CLI.

## Guía Rápida

### Tu Primera Optimización

Vamos a optimizar un modelo de lenguaje pequeño usando la función de auto-optimización de Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Qué Hace Este Comando

El proceso de optimización incluye: adquirir el modelo desde la caché local, capturar el gráfico ONNX y almacenar los pesos en un archivo de datos ONNX, optimizar el gráfico ONNX y cuantizar el modelo a int4 usando el método RTN.

### Explicación de los Parámetros del Comando

- `--model_name_or_path`: Identificador del modelo de Hugging Face o ruta local.
- `--output_path`: Directorio donde se guardará el modelo optimizado.
- `--device`: Dispositivo objetivo (cpu, gpu).
- `--provider`: Proveedor de ejecución (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider).
- `--use_ort_genai`: Usar ONNX Runtime Generate AI para inferencia.
- `--precision`: Precisión de cuantización (int4, int8, fp16).
- `--log_level`: Nivel de detalle de los registros (0=mínimo, 1=detallado).

## Ejemplo: Convertir Qwen3 a ONNX INT4

Basado en el ejemplo proporcionado por Hugging Face en [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), aquí se muestra cómo optimizar un modelo Qwen3:

### Paso 1: Descargar el Modelo (Opcional)

Para minimizar el tiempo de descarga, cachea solo los archivos esenciales:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Paso 2: Optimizar el Modelo Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Paso 3: Probar el Modelo Optimizado

Crea un script simple en Python para probar tu modelo optimizado:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Estructura de Salida

Después de la optimización, tu directorio de salida contendrá:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Uso Avanzado

### Archivos de Configuración

Para flujos de trabajo de optimización más complejos, puedes usar archivos de configuración JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Ejecutar con configuración:

```bash
olive run --config config.json
```

### Optimización para GPU

Para optimización con CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Para DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ajuste Fino con Olive

Olive también admite el ajuste fino de modelos:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Mejores Prácticas

### 1. Selección de Modelos
- Comienza con modelos más pequeños para pruebas (por ejemplo, 0.5B-7B parámetros).
- Asegúrate de que la arquitectura de tu modelo objetivo sea compatible con Olive.

### 2. Consideraciones de Hardware
- Ajusta tu objetivo de optimización al hardware de implementación.
- Usa optimización para GPU si tienes hardware compatible con CUDA.
- Considera DirectML para máquinas Windows con gráficos integrados.

### 3. Selección de Precisión
- **INT4**: Máxima compresión, ligera pérdida de precisión.
- **INT8**: Buen equilibrio entre tamaño y precisión.
- **FP16**: Mínima pérdida de precisión, reducción moderada de tamaño.

### 4. Pruebas y Validación
- Siempre prueba los modelos optimizados con tus casos de uso específicos.
- Compara métricas de rendimiento (latencia, rendimiento, precisión).
- Usa datos de entrada representativos para la evaluación.

### 5. Optimización Iterativa
- Comienza con auto-optimización para resultados rápidos.
- Usa archivos de configuración para un control más detallado.
- Experimenta con diferentes pases de optimización.

## Solución de Problemas

### Problemas Comunes

#### 1. Problemas de Instalación
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problemas con CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problemas de Memoria
- Usa tamaños de lote más pequeños durante la optimización.
- Prueba la cuantización con mayor precisión primero (int8 en lugar de int4).
- Asegúrate de tener suficiente espacio en disco para la caché del modelo.

#### 4. Errores al Cargar el Modelo
- Verifica la ruta del modelo y los permisos de acceso.
- Comprueba si el modelo requiere `trust_remote_code=True`.
- Asegúrate de que todos los archivos necesarios del modelo estén descargados.

### Obtener Ayuda

- **Documentación**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Problemas en GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Ejemplos**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repositorio de Recetas Olive

### Introducción a las Recetas Olive

El repositorio [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) complementa la herramienta principal Olive proporcionando una colección completa de recetas de optimización listas para usar para modelos de IA populares. Este repositorio sirve como referencia práctica tanto para optimizar modelos disponibles públicamente como para crear flujos de trabajo de optimización para modelos propietarios.

### Características Principales

- **Más de 100 Recetas Preconstruidas**: Configuraciones de optimización listas para usar para modelos populares.
- **Soporte Multi-Arquitectura**: Incluye modelos transformadores, modelos de visión y arquitecturas multimodales.
- **Optimizaciones Específicas de Hardware**: Recetas adaptadas para CPU, GPU y aceleradores especializados.
- **Familias de Modelos Populares**: Incluye Phi, Llama, Qwen, Gemma, Mistral y muchos más.

### Familias de Modelos Compatibles

El repositorio incluye recetas de optimización para:

#### Modelos de Lenguaje
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning.
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B.
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, serie Qwen2.5 (0.5B a 14B).
- **Google Gemma**: Varias configuraciones de modelos Gemma.
- **Mistral AI**: Serie Mistral-7B.
- **DeepSeek**: Modelos de la serie R1-Distill.

#### Modelos de Visión y Multimodales
- **Stable Diffusion**: v1.4, XL-base-1.0.
- **Modelos CLIP**: Varias configuraciones CLIP-ViT.
- **ResNet**: Optimizaciones para ResNet-50.
- **Transformadores de Visión**: ViT-base-patch16-224.

#### Modelos Especializados
- **Whisper**: OpenAI Whisper-large-v3.
- **BERT**: Variantes base y multilingües.
- **Transformadores de Oraciones**: all-MiniLM-L6-v2.

### Uso de Recetas Olive

#### Método 1: Clonar Receta Específica

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Método 2: Usar Receta como Plantilla

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Estructura de la Receta

Cada directorio de receta típicamente contiene:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Ejemplo: Usar la Receta Phi-4-mini

Vamos a usar la receta Phi-4-mini como ejemplo:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

El archivo de configuración típicamente incluye:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Personalización de Recetas

#### Modificar Hardware Objetivo

Para cambiar el hardware objetivo, actualiza la sección `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Ajustar Parámetros de Optimización

Modifica la sección `passes` para diferentes niveles de optimización:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Crear Tu Propia Receta

1. **Comienza con un Modelo Similar**: Encuentra una receta para un modelo con arquitectura similar.
2. **Actualiza la Configuración del Modelo**: Cambia el nombre/ruta del modelo en la configuración.
3. **Ajusta los Parámetros**: Modifica los parámetros de optimización según sea necesario.
4. **Prueba y Valida**: Ejecuta la optimización y valida los resultados.
5. **Contribuye de Vuelta**: Considera contribuir tu receta al repositorio.

### Beneficios de Usar Recetas

#### 1. **Configuraciones Probadas**
- Configuraciones de optimización probadas para modelos específicos.
- Evita el ensayo y error al buscar parámetros óptimos.

#### 2. **Ajuste Específico de Hardware**
- Pre-optimizado para diferentes proveedores de ejecución.
- Configuraciones listas para usar en objetivos CPU, GPU y NPU.

#### 3. **Cobertura Integral**
- Compatible con los modelos de código abierto más populares.
- Actualizaciones regulares con nuevos lanzamientos de modelos.

#### 4. **Contribuciones de la Comunidad**
- Desarrollo colaborativo con la comunidad de IA.
- Conocimiento compartido y mejores prácticas.

### Contribuir a las Recetas Olive

Si has optimizado un modelo que no está cubierto en el repositorio:

1. **Haz un Fork del Repositorio**: Crea tu propio fork de olive-recipes.
2. **Crea un Directorio de Receta**: Agrega un nuevo directorio para tu modelo.
3. **Incluye Configuración**: Agrega olive_config.json y archivos de soporte.
4. **Documenta el Uso**: Proporciona un README claro con instrucciones.
5. **Envía un Pull Request**: Contribuye de vuelta a la comunidad.

### Benchmarks de Rendimiento

Muchas recetas incluyen benchmarks de rendimiento que muestran:
- **Mejoras en Latencia**: Típico aumento de velocidad de 2-6x sobre la línea base.
- **Reducción de Memoria**: Reducción de uso de memoria del 50-75% con cuantización.
- **Preservación de Precisión**: Retención de precisión del 95-99%.

### Integración con Herramientas de IA

Las recetas funcionan perfectamente con:
- **VS Code AI Toolkit**: Integración directa para optimización de modelos.
- **Azure Machine Learning**: Flujos de trabajo de optimización en la nube.
- **ONNX Runtime**: Implementación de inferencia optimizada.

## Recursos Adicionales

### Enlaces Oficiales
- **Repositorio GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repositorio de Recetas Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Documentación de ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Ejemplo de Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Ejemplos de la Comunidad
- **Jupyter Notebooks**: Disponibles en el repositorio de Olive en GitHub — https://github.com/microsoft/Olive/tree/main/examples
- **Extensión VS Code**: Descripción general del AI Toolkit para VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Publicaciones en Blogs**: Blog de Microsoft Open Source — https://opensource.microsoft.com/blog/

### Herramientas Relacionadas
- **ONNX Runtime**: Motor de inferencia de alto rendimiento — https://onnxruntime.ai/
- **Transformers de Hugging Face**: Fuente de muchos modelos compatibles — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Flujos de trabajo de optimización en la nube — https://learn.microsoft.com/azure/machine-learning/

## ➡️ ¿Qué sigue?

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que surjan del uso de esta traducción.