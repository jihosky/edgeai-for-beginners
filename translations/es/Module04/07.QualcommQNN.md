<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "258ecb2013d1666fca92cc5ba7e9a4a0",
  "translation_date": "2025-10-30T15:44:33+00:00",
  "source_file": "Module04/07.QualcommQNN.md",
  "language_code": "es"
}
-->
# Sección 7: Suite de Optimización Qualcomm QNN (Qualcomm Neural Network)

## Tabla de Contenidos
1. [Introducción](../../../Module04)
2. [¿Qué es Qualcomm QNN?](../../../Module04)
3. [Instalación](../../../Module04)
4. [Guía Rápida](../../../Module04)
5. [Ejemplo: Convertir y Optimizar Modelos con QNN](../../../Module04)
6. [Uso Avanzado](../../../Module04)
7. [Mejores Prácticas](../../../Module04)
8. [Solución de Problemas](../../../Module04)
9. [Recursos Adicionales](../../../Module04)

## Introducción

Qualcomm QNN (Qualcomm Neural Network) es un marco integral de inferencia de IA diseñado para aprovechar al máximo los aceleradores de hardware de IA de Qualcomm, incluyendo el Hexagon NPU, Adreno GPU y Kryo CPU. Ya sea que estés trabajando con dispositivos móviles, plataformas de computación en el borde o sistemas automotrices, QNN ofrece capacidades de inferencia optimizadas que utilizan las unidades de procesamiento especializadas de Qualcomm para lograr el máximo rendimiento y eficiencia energética.

## ¿Qué es Qualcomm QNN?

Qualcomm QNN es un marco unificado de inferencia de IA que permite a los desarrolladores implementar modelos de IA de manera eficiente en la arquitectura de computación heterogénea de Qualcomm. Proporciona una interfaz de programación unificada para acceder al Hexagon NPU (Unidad de Procesamiento Neural), Adreno GPU y Kryo CPU, seleccionando automáticamente la unidad de procesamiento óptima para diferentes capas y operaciones del modelo.

### Características Principales

- **Computación Heterogénea**: Acceso unificado al NPU, GPU y CPU con distribución automática de la carga de trabajo.
- **Optimización Basada en Hardware**: Optimizaciones especializadas para plataformas Snapdragon de Qualcomm.
- **Soporte de Cuantización**: Técnicas avanzadas de cuantización INT8, INT16 y de precisión mixta.
- **Herramientas de Conversión de Modelos**: Soporte directo para modelos de TensorFlow, PyTorch, ONNX y Caffe.
- **Optimizado para IA en el Borde**: Diseñado específicamente para escenarios de despliegue móvil y en el borde con enfoque en eficiencia energética.

### Beneficios

- **Máximo Rendimiento**: Aprovecha el hardware especializado de IA para mejoras de rendimiento de hasta 15x.
- **Eficiencia Energética**: Optimizado para dispositivos móviles y alimentados por batería con gestión inteligente de energía.
- **Baja Latencia**: Inferencia acelerada por hardware con mínima sobrecarga para aplicaciones en tiempo real.
- **Despliegue Escalable**: Desde smartphones hasta plataformas automotrices en todo el ecosistema de Qualcomm.
- **Listo para Producción**: Marco probado en millones de dispositivos desplegados.

## Instalación

### Requisitos Previos

- SDK de Qualcomm QNN (requiere registro con Qualcomm)
- Python 3.7 o superior
- Hardware compatible de Qualcomm o simulador
- Android NDK (para despliegue móvil)
- Entorno de desarrollo en Linux o Windows

### Configuración del SDK de QNN

1. **Registro y Descarga**: Visita Qualcomm Developer Network para registrarte y descargar el SDK de QNN.
2. **Extraer SDK**: Descomprime el SDK de QNN en tu directorio de desarrollo.
3. **Configurar Variables de Entorno**: Configura las rutas para las herramientas y bibliotecas de QNN.

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Configuración del Entorno Python

Crea y activa un entorno virtual:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Instala los paquetes de Python requeridos:

```bash
pip install numpy tensorflow torch onnx
```

### Verificar Instalación

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Si es exitoso, deberías ver información de ayuda para cada herramienta de QNN.

## Guía Rápida

### Tu Primera Conversión de Modelo

Vamos a convertir un modelo simple de PyTorch para ejecutarlo en hardware de Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Convertir ONNX a Formato QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generar Biblioteca de Modelo QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Qué Hace Este Proceso

El flujo de trabajo de optimización incluye: convertir el modelo original al formato ONNX, traducir ONNX a una representación intermedia de QNN, aplicar optimizaciones específicas de hardware y generar una biblioteca de modelo compilada para el despliegue.

### Parámetros Clave Explicados

- `--input_network`: Archivo de modelo ONNX fuente.
- `--output_path`: Archivo fuente C++ generado.
- `--input_dim`: Dimensiones del tensor de entrada para la optimización.
- `--quantization_overrides`: Configuración personalizada de cuantización.
- `-t x86_64-linux-clang`: Arquitectura objetivo y compilador.

## Ejemplo: Convertir y Optimizar Modelos con QNN

### Paso 1: Conversión Avanzada de Modelos con Cuantización

Aquí se explica cómo aplicar cuantización personalizada durante la conversión:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Convertir con cuantización personalizada:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Paso 2: Optimización Multi-Backend

Configura la ejecución heterogénea en NPU, GPU y CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Paso 3: Crear Binario de Contexto para Despliegue

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Paso 4: Inferencia con Runtime de QNN

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Estructura de Salida

Después de la optimización, tu directorio de despliegue contendrá:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Uso Avanzado

### Configuración Personalizada de Backend

Configura optimizaciones específicas de backend:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Cuantización Dinámica

Aplica cuantización en tiempo de ejecución para mayor precisión:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Perfilado de Rendimiento

Monitorea el rendimiento en diferentes backends:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Selección Automática de Backend

Implementa selección inteligente de backend basada en las características del modelo:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Mejores Prácticas

### 1. Optimización de Arquitectura de Modelo
- **Fusión de Capas**: Combina operaciones como Conv+BatchNorm+ReLU para mejor utilización del NPU.
- **Convoluciones Separables en Profundidad**: Prefiere estas sobre las convoluciones estándar para despliegue móvil.
- **Diseños Amigables con Cuantización**: Usa activaciones ReLU y evita operaciones que no se cuantizan bien.

### 2. Estrategia de Cuantización
- **Cuantización Post-Entrenamiento**: Comienza con esto para un despliegue rápido.
- **Dataset de Calibración**: Usa datos representativos que cubran todas las variaciones de entrada.
- **Precisión Mixta**: Usa INT8 para la mayoría de las capas, mantén capas críticas en mayor precisión.

### 3. Directrices de Selección de Backend
- **NPU (HTP)**: Ideal para cargas de trabajo CNN, modelos cuantizados y aplicaciones sensibles al consumo de energía.
- **GPU**: Óptimo para operaciones intensivas en cómputo, modelos grandes y precisión FP16.
- **CPU**: Respaldo para operaciones no soportadas y depuración.

### 4. Optimización de Rendimiento
- **Tamaño de Lote**: Usa tamaño de lote 1 para aplicaciones en tiempo real, lotes más grandes para rendimiento.
- **Preprocesamiento de Entrada**: Minimiza la copia y conversión de datos.
- **Reutilización de Contexto**: Precompila contextos para evitar sobrecarga de compilación en tiempo de ejecución.

### 5. Gestión de Memoria
- **Asignación de Tensores**: Usa asignación estática cuando sea posible para evitar sobrecarga en tiempo de ejecución.
- **Pools de Memoria**: Implementa pools de memoria personalizados para tensores asignados frecuentemente.
- **Reutilización de Buffers**: Reutiliza buffers de entrada/salida entre llamadas de inferencia.

### 6. Optimización de Energía
- **Modos de Rendimiento**: Usa modos de rendimiento apropiados según las restricciones térmicas.
- **Escalado Dinámico de Frecuencia**: Permite que el sistema escale la frecuencia según la carga de trabajo.
- **Gestión de Estado Inactivo**: Libera recursos adecuadamente cuando no estén en uso.

## Solución de Problemas

### Problemas Comunes

#### 1. Problemas de Instalación del SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Errores de Conversión de Modelos
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Problemas de Cuantización
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Problemas de Rendimiento
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Problemas de Memoria
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Compatibilidad de Backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Depuración de Rendimiento

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Obtener Ayuda

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Documentación de QNN**: Disponible en el paquete SDK.
- **Foros de la Comunidad**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Soporte Técnico**: A través del portal de desarrolladores de Qualcomm.

## Recursos Adicionales

### Enlaces Oficiales
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Plataformas Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portal de Desarrolladores**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **Motor de IA**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Recursos de Aprendizaje
- **Guía de Inicio**: Disponible en la documentación del SDK de QNN.
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Guía de Optimización**: La documentación del SDK incluye pautas completas de optimización.
- **Tutoriales en Video**: [Canal de YouTube de Qualcomm Developer Network](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Herramientas de Integración
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Modelos pre-optimizados para hardware de Qualcomm.
- **API de Redes Neuronales de Android**: Integración con Android NNAPI.
- **Delegado de TensorFlow Lite**: Delegado de Qualcomm para TFLite.

### Benchmarks de Rendimiento
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Investigación de IA de Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Ejemplos de la Comunidad
- **Aplicaciones de Ejemplo**: Disponibles en el directorio de ejemplos del SDK de QNN.
- **Repositorios de GitHub**: Ejemplos y herramientas contribuidos por la comunidad.
- **Blogs Técnicos**: [Blog de Qualcomm Developer](https://developer.qualcomm.com/blog)

### Herramientas Relacionadas
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Técnicas avanzadas de cuantización y compresión.
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Para comparación y despliegue alternativo.
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Motor de inferencia multiplataforma.

### Especificaciones de Hardware
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Plataformas Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ ¿Qué sigue?

Continúa tu viaje en IA en el borde explorando [Módulo 5: SLMOps y Despliegue en Producción](../Module05/README.md) para aprender sobre aspectos operativos de la gestión del ciclo de vida de modelos de lenguaje pequeño.

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.