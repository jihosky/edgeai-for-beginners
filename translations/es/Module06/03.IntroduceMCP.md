<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8a7765b85f123e8a62aa3847141ca072",
  "translation_date": "2025-10-30T10:37:50+00:00",
  "source_file": "Module06/03.IntroduceMCP.md",
  "language_code": "es"
}
-->
# Sección 03 - Integración del Protocolo de Contexto de Modelo (MCP)

## Introducción al MCP (Protocolo de Contexto de Modelo)

El Protocolo de Contexto de Modelo (MCP) es un estándar de código abierto para conectar aplicaciones de IA con sistemas externos. Usando MCP, aplicaciones de IA como Claude o ChatGPT pueden conectarse a fuentes de datos (por ejemplo, archivos locales, bases de datos), herramientas (por ejemplo, motores de búsqueda, calculadoras) y flujos de trabajo (por ejemplo, indicaciones especializadas), permitiéndoles acceder a información clave y realizar tareas.

Piensa en MCP como un **puerto USB-C para aplicaciones de IA**. Así como el USB-C proporciona una forma estandarizada de conectar dispositivos electrónicos, MCP proporciona una forma estandarizada de conectar aplicaciones de IA con sistemas externos.

### ¿Qué puede habilitar MCP?

MCP desbloquea capacidades poderosas para las aplicaciones de IA:

- **Asistentes de IA personalizados**: Los agentes pueden acceder a tu Google Calendar y Notion, actuando como un asistente de IA más personalizado.
- **Generación avanzada de código**: Claude Code puede generar una aplicación web completa usando un diseño de Figma.
- **Integración de datos empresariales**: Los chatbots empresariales pueden conectarse a múltiples bases de datos en una organización, permitiendo a los usuarios analizar datos mediante chat.
- **Flujos de trabajo creativos**: Los modelos de IA pueden crear diseños 3D en Blender y imprimirlos usando una impresora 3D.
- **Acceso a información en tiempo real**: Conexión a fuentes de datos externas para obtener información actualizada.
- **Operaciones complejas de múltiples pasos**: Realizar flujos de trabajo sofisticados combinando múltiples herramientas y sistemas.

### ¿Por qué importa MCP?

MCP ofrece beneficios en todo el ecosistema:

**Para desarrolladores**: MCP reduce el tiempo y la complejidad de desarrollo al construir o integrar una aplicación o agente de IA.

**Para aplicaciones de IA**: MCP proporciona acceso a un ecosistema de fuentes de datos, herramientas y aplicaciones que mejoran las capacidades y la experiencia del usuario final.

**Para usuarios finales**: MCP resulta en aplicaciones o agentes de IA más capaces que pueden acceder a tus datos y realizar acciones en tu nombre cuando sea necesario.

## Modelos de Lenguaje Pequeños (SLMs) en MCP

Los Modelos de Lenguaje Pequeños representan un enfoque eficiente para el despliegue de IA, ofreciendo varias ventajas:

### Beneficios de los SLMs
- **Eficiencia de recursos**: Menores requisitos computacionales.
- **Tiempos de respuesta más rápidos**: Menor latencia para aplicaciones en tiempo real.  
- **Rentabilidad**: Necesidades mínimas de infraestructura.
- **Privacidad**: Pueden ejecutarse localmente sin transmisión de datos.
- **Personalización**: Más fácil de ajustar para dominios específicos.

### Por qué los SLMs funcionan bien con MCP

Los SLMs combinados con MCP crean una combinación poderosa donde las capacidades de razonamiento del modelo se ven aumentadas por herramientas externas, compensando su menor cantidad de parámetros mediante una funcionalidad mejorada.

## Descripción general del SDK de MCP para Python

El SDK de MCP para Python proporciona la base para construir aplicaciones habilitadas para MCP. El SDK incluye:

- **Bibliotecas cliente**: Para conectarse a servidores MCP.
- **Marco de trabajo del servidor**: Para crear servidores MCP personalizados.
- **Manejadores de protocolo**: Para gestionar la comunicación.
- **Integración de herramientas**: Para ejecutar funciones externas.

## Implementación práctica: Cliente MCP Phi-4

Exploremos una implementación real usando el modelo mini Phi-4 de Microsoft integrado con capacidades MCP.

### Descripción general de la arquitectura MCP

MCP sigue una **arquitectura cliente-servidor** donde un host MCP (una aplicación de IA como Claude Code o Claude Desktop) establece conexiones con uno o más servidores MCP. El host MCP logra esto creando un cliente MCP para cada servidor MCP.

#### Participantes clave

- **Host MCP**: La aplicación de IA que coordina y gestiona uno o varios clientes MCP.
- **Cliente MCP**: Un componente que mantiene una conexión con un servidor MCP y obtiene contexto de un servidor MCP para que lo use el host MCP.
- **Servidor MCP**: Un programa que proporciona contexto a los clientes MCP.

#### Arquitectura de dos capas

MCP consta de dos capas distintas:

**Capa de datos**: Define el protocolo basado en JSON-RPC para la comunicación cliente-servidor, incluyendo:
- Gestión del ciclo de vida (inicialización de conexión, negociación de capacidades).
- Primitivas principales (herramientas, recursos, indicaciones).
- Funciones del cliente (muestreo, obtención de información, registro).
- Funciones de utilidad (notificaciones, seguimiento de progreso).

**Capa de transporte**: Define los mecanismos y canales de comunicación:
- **Transporte STDIO**: Utiliza flujos de entrada/salida estándar para procesos locales (rendimiento óptimo, sin sobrecarga de red).
- **Transporte HTTP transmisible**: Utiliza HTTP POST con eventos enviados por el servidor opcionales para servidores remotos (admite autenticación HTTP estándar).

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### Primitivas principales de MCP

MCP define primitivas que especifican los tipos de información contextual que se pueden compartir con aplicaciones de IA y el rango de acciones que se pueden realizar.

#### Primitivas del servidor

MCP define tres primitivas principales que los servidores pueden exponer:

**Herramientas**: Funciones ejecutables que las aplicaciones de IA pueden invocar para realizar acciones.
- Ejemplos: operaciones de archivos, llamadas a API, consultas a bases de datos.
- Métodos: `tools/list`, `tools/call`.
- Admite descubrimiento y ejecución dinámica.

**Recursos**: Fuentes de datos que proporcionan información contextual a las aplicaciones de IA.
- Ejemplos: contenido de archivos, registros de bases de datos, respuestas de API.
- Métodos: `resources/list`, `resources/read`.
- Permiten acceso a datos estructurados.

**Indicaciones**: Plantillas reutilizables que ayudan a estructurar interacciones con modelos de lenguaje.
- Ejemplos: indicaciones del sistema, ejemplos de pocos disparos.
- Métodos: `prompts/list`, `prompts/get`.
- Estandarizan patrones de interacción con IA.

#### Primitivas del cliente

MCP también define primitivas que los clientes pueden exponer para habilitar interacciones más ricas:

**Muestreo**: Permite a los servidores solicitar completaciones de modelos de lenguaje desde la aplicación de IA del cliente.
- Método: `sampling/complete`.
- Habilita el desarrollo de servidores independientes del modelo.
- Proporciona acceso al modelo de lenguaje del host.

**Obtención de información**: Permite a los servidores solicitar información adicional a los usuarios.
- Método: `elicitation/request`.
- Habilita la interacción y confirmación del usuario.
- Admite la recopilación dinámica de información.

**Registro**: Permite a los servidores enviar mensajes de registro a los clientes.
- Usado para propósitos de depuración y monitoreo.
- Proporciona visibilidad en las operaciones del servidor.

### Ciclo de vida del protocolo MCP

#### Inicialización y negociación de capacidades

MCP es un protocolo con estado que requiere gestión del ciclo de vida. El proceso de inicialización cumple varios propósitos críticos:

1. **Negociación de versión del protocolo**: Garantiza que tanto el cliente como el servidor usen versiones compatibles del protocolo (por ejemplo, "2025-06-18").
2. **Descubrimiento de capacidades**: Cada parte declara las características y primitivas compatibles.
3. **Intercambio de identidad**: Proporciona información de identificación y versiones.

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Descubrimiento y ejecución de herramientas

Después de la inicialización, los clientes pueden descubrir y ejecutar herramientas:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Notificaciones en tiempo real

MCP admite notificaciones en tiempo real para actualizaciones dinámicas:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## Primeros pasos: Guía paso a paso

### Paso 1: Configuración del entorno

Instala las dependencias requeridas:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Paso 2: Configuración básica

Configura tus variables de entorno:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Paso 3: Ejecuta tu primer cliente MCP

**Configuración básica de Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Usando el backend vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Conexión con eventos enviados por el servidor:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Servidor MCP personalizado:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Paso 4: Uso programático

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Funciones avanzadas

### Soporte para múltiples backends

La implementación admite tanto los backends de Ollama como de vLLM, permitiéndote elegir según tus necesidades:

- **Ollama**: Mejor para desarrollo y pruebas locales.
- **vLLM**: Optimizado para producción y escenarios de alto rendimiento.

### Protocolos de conexión flexibles

Se admiten dos modos de conexión:

**Modo STDIO**: Comunicación directa entre procesos.
- Menor latencia.
- Adecuado para herramientas locales.
- Configuración sencilla.

**Modo SSE**: Transmisión basada en HTTP.
- Capacidad de red.
- Mejor para sistemas distribuidos.
- Actualizaciones en tiempo real.

### Capacidades de integración de herramientas

El sistema puede integrarse con varias herramientas:
- Automatización web (Playwright).
- Operaciones de archivos.
- Interacciones con API.
- Comandos del sistema.
- Funciones personalizadas.

## Manejo de errores y mejores prácticas

### Gestión integral de errores

La implementación incluye un manejo robusto de errores para:

**Errores de conexión:**
- Fallos del servidor MCP.
- Tiempos de espera de red.
- Problemas de conectividad.

**Errores de ejecución de herramientas:**
- Herramientas faltantes.
- Validación de parámetros.
- Fallos de ejecución.

**Errores de procesamiento de respuestas:**
- Problemas de análisis JSON.
- Inconsistencias de formato.
- Anomalías en las respuestas de LLM.

### Mejores prácticas

1. **Gestión de recursos**: Usa administradores de contexto asíncronos.
2. **Manejo de errores**: Implementa bloques try-catch completos.
3. **Registro**: Habilita niveles de registro apropiados.
4. **Seguridad**: Valida entradas y sanitiza salidas.
5. **Rendimiento**: Usa agrupación de conexiones y almacenamiento en caché.

## Aplicaciones en el mundo real

### Automatización web
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Procesamiento de datos
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integración de API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optimización del rendimiento

### Gestión de memoria
- Manejo eficiente del historial de mensajes.
- Limpieza adecuada de recursos.
- Agrupación de conexiones.

### Optimización de red
- Operaciones HTTP asíncronas.
- Tiempos de espera configurables.
- Recuperación de errores de manera eficiente.

### Procesamiento concurrente
- I/O no bloqueante.
- Ejecución paralela de herramientas.
- Patrones asíncronos eficientes.

## Consideraciones de seguridad

### Protección de datos
- Gestión segura de claves API.
- Validación de entradas.
- Sanitización de salidas.

### Seguridad de red
- Soporte HTTPS.
- Configuración predeterminada de puntos finales locales.
- Manejo seguro de tokens.

### Seguridad en la ejecución
- Filtrado de herramientas.
- Entornos aislados.
- Registro de auditoría.

## Ecosistema y desarrollo de MCP

### Alcance del proyecto MCP

El ecosistema del Protocolo de Contexto de Modelo incluye varios componentes clave:

- **[Especificación de MCP](https://modelcontextprotocol.io/specification/latest)**: Especificación oficial que describe los requisitos de implementación para clientes y servidores.
- **[SDKs de MCP](https://modelcontextprotocol.io/docs/sdk)**: SDKs para diferentes lenguajes de programación que implementan MCP.
- **Herramientas de desarrollo de MCP**: Herramientas para desarrollar servidores y clientes MCP, incluyendo el [Inspector de MCP](https://github.com/modelcontextprotocol/inspector).
- **[Implementaciones de referencia de servidores MCP](https://github.com/modelcontextprotocol/servers)**: Implementaciones de referencia de servidores MCP.

### Cómo empezar con el desarrollo de MCP

Para comenzar a construir con MCP:

**Construir servidores**: [Crear servidores MCP](https://modelcontextprotocol.io/docs/develop/build-server) para exponer tus datos y herramientas.

**Construir clientes**: [Desarrollar aplicaciones](https://modelcontextprotocol.io/docs/develop/build-client) que se conecten a servidores MCP.

**Aprender conceptos**: [Entender los conceptos básicos](https://modelcontextprotocol.io/docs/learn/architecture) y la arquitectura de MCP.

## Conclusión

Los SLMs integrados con MCP representan un cambio de paradigma en el desarrollo de aplicaciones de IA. Al combinar la eficiencia de los modelos pequeños con el poder de las herramientas externas, los desarrolladores pueden crear sistemas inteligentes que son tanto eficientes en recursos como altamente capaces.

El Protocolo de Contexto de Modelo proporciona una forma estandarizada de conectar aplicaciones de IA con sistemas externos, al igual que el USB-C proporciona un estándar de conexión universal para dispositivos electrónicos. Esta estandarización permite:

- **Integración sin problemas**: Conectar modelos de IA a diversas fuentes de datos y herramientas.
- **Crecimiento del ecosistema**: Construir una vez, usar en múltiples aplicaciones de IA.
- **Capacidades mejoradas**: Aumentar los SLMs con funcionalidad externa.
- **Actualizaciones en tiempo real**: Apoyar aplicaciones de IA dinámicas y receptivas.

Puntos clave:
- MCP es un estándar abierto que conecta aplicaciones de IA con sistemas externos.
- El protocolo admite herramientas, recursos e indicaciones como primitivas principales.
- Las notificaciones en tiempo real permiten aplicaciones dinámicas y receptivas.
- La gestión adecuada del ciclo de vida y el manejo de errores son esenciales para el uso en producción.
- El ecosistema proporciona SDKs y herramientas de desarrollo completas.

## Referencias y lecturas adicionales

### Documentación oficial de MCP

- **[Sitio oficial del Protocolo de Contexto de Modelo](https://modelcontextprotocol.io/)** - Documentación completa y especificaciones.
- **[Guía para empezar con MCP](https://modelcontextprotocol.io/docs/getting-started/intro)** - Introducción y conceptos básicos.
- **[Descripción general de la arquitectura MCP](https://modelcontextprotocol.io/docs/learn/architecture)** - Arquitectura técnica detallada.
- **[Especificación de MCP](https://modelcontextprotocol.io/specification/latest)** - Especificación oficial del protocolo.
- **[Documentación de SDKs de MCP](https://modelcontextprotocol.io/docs/sdk)** - Guías específicas para SDKs por lenguaje.

### Recursos de desarrollo

- **[MCP para principiantes](https://aka.ms/mcp-for-beginners)** - Guía completa para principiantes sobre el Protocolo de Contexto de Modelo.
- **[Organización GitHub de MCP](https://github.com/modelcontextprotocol)** - Repositorios oficiales y ejemplos.
- **[Repositorio de servidores MCP](https://github.com/modelcontextprotocol/servers)** - Implementaciones de referencia de servidores.
- **[Inspector de MCP](https://github.com/modelcontextprotocol/inspector)** - Herramienta de desarrollo y depuración.
- **[Guía para construir servidores MCP](https://modelcontextprotocol.io/docs/develop/build-server)** - Tutorial de desarrollo de servidores.
- **[Guía para construir clientes MCP](https://modelcontextprotocol.io/docs/develop/build-client)** - Tutorial de desarrollo de clientes.

### Modelos de Lenguaje Pequeños y IA en el borde

- **[Modelos Phi de Microsoft](https://aka.ms/phicookbook)** - Familia de modelos Phi.
- **[Documentación de Foundry Local](https://github.com/microsoft/Foundry-Local)** - Runtime de IA en el borde de Microsoft.
- **[Documentación de Ollama](https://ollama.ai/docs)** - Plataforma de implementación local de LLM
- **[Documentación de vLLM](https://docs.vllm.ai/)** - Servicio de LLM de alto rendimiento

### Estándares técnicos y protocolos

- **[Especificación JSON-RPC 2.0](https://www.jsonrpc.org/)** - Protocolo RPC subyacente utilizado por MCP
- **[JSON Schema](https://json-schema.org/)** - Estándar de definición de esquemas para herramientas MCP
- **[Especificación OpenAPI](https://swagger.io/specification/)** - Estándar de documentación de API
- **[Eventos enviados por el servidor (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Estándar web para actualizaciones en tiempo real

### Desarrollo de agentes de IA

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Desarrollo de agentes listo para producción
- **[Documentación de LangChain](https://docs.langchain.com/)** - Marco de integración de agentes y herramientas
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - SDK de orquestación de IA de Microsoft

### Informes de la industria e investigación

- **[Anuncio del Protocolo de Contexto de Modelos de Anthropic](https://www.anthropic.com/news/model-context-protocol)** - Introducción original del MCP
- **[Encuesta sobre Modelos de Lenguaje Pequeños](https://arxiv.org/abs/2410.20011)** - Encuesta académica sobre investigación de SLM
- **[Análisis del mercado de Edge AI](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Tendencias y pronósticos de la industria
- **[Mejores prácticas para el desarrollo de agentes de IA](https://arxiv.org/abs/2309.02427)** - Investigación sobre arquitecturas de agentes

Esta sección proporciona la base para construir tus propias aplicaciones MCP impulsadas por SLM, abriendo posibilidades para automatización, procesamiento de datos e integración de sistemas inteligentes.

## ➡️ ¿Qué sigue?

- [Módulo 7. Ejemplos de Edge AI](../Module07/README.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducción automática [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que surjan del uso de esta traducción.