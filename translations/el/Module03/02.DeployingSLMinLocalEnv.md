<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "0d9a6cd24a08aeac1328626abcc18ba8",
  "translation_date": "2025-10-24T09:35:53+00:00",
  "source_file": "Module03/02.DeployingSLMinLocalEnv.md",
  "language_code": "el"
}
-->
# Ενότητα 2: Ανάπτυξη Τοπικού Περιβάλλοντος - Λύσεις με Προτεραιότητα στην Ιδιωτικότητα

Η τοπική ανάπτυξη Μικρών Γλωσσικών Μοντέλων (SLMs) αντιπροσωπεύει μια αλλαγή παραδείγματος προς λύσεις AI που διατηρούν την ιδιωτικότητα και είναι οικονομικά αποδοτικές. Αυτός ο ολοκληρωμένος οδηγός εξετάζει δύο ισχυρά πλαίσια—Ollama και Microsoft Foundry Local—που επιτρέπουν στους προγραμματιστές να αξιοποιήσουν πλήρως τις δυνατότητες των SLMs, διατηρώντας παράλληλα τον πλήρη έλεγχο του περιβάλλοντος ανάπτυξής τους.

## Εισαγωγή

Σε αυτό το μάθημα, θα εξετάσουμε προηγμένες στρατηγικές ανάπτυξης για Μικρά Γλωσσικά Μοντέλα σε τοπικά περιβάλλοντα. Θα καλύψουμε τις βασικές έννοιες της τοπικής ανάπτυξης AI, θα εξετάσουμε δύο κορυφαίες πλατφόρμες (Ollama και Microsoft Foundry Local) και θα παρέχουμε πρακτική καθοδήγηση για λύσεις έτοιμες για παραγωγή.

## Στόχοι Μάθησης

Μέχρι το τέλος αυτού του μαθήματος, θα μπορείτε να:

- Κατανοήσετε την αρχιτεκτονική και τα οφέλη των πλαισίων τοπικής ανάπτυξης SLM.
- Εφαρμόσετε αναπτύξεις έτοιμες για παραγωγή χρησιμοποιώντας Ollama και Microsoft Foundry Local.
- Συγκρίνετε και επιλέξετε την κατάλληλη πλατφόρμα βάσει συγκεκριμένων απαιτήσεων και περιορισμών.
- Βελτιστοποιήσετε τις τοπικές αναπτύξεις για απόδοση, ασφάλεια και επεκτασιμότητα.

## Κατανόηση Αρχιτεκτονικών Τοπικής Ανάπτυξης SLM

Η τοπική ανάπτυξη SLM αντιπροσωπεύει μια θεμελιώδη αλλαγή από τις υπηρεσίες AI που εξαρτώνται από το cloud σε λύσεις που διατηρούν την ιδιωτικότητα και λειτουργούν τοπικά. Αυτή η προσέγγιση επιτρέπει στους οργανισμούς να διατηρούν πλήρη έλεγχο της υποδομής AI τους, εξασφαλίζοντας παράλληλα την κυριαρχία των δεδομένων και την επιχειρησιακή ανεξαρτησία.

### Κατηγορίες Πλαισίων Ανάπτυξης

Η κατανόηση διαφορετικών προσεγγίσεων ανάπτυξης βοηθά στην επιλογή της σωστής στρατηγικής για συγκεκριμένες περιπτώσεις χρήσης:

- **Εστιασμένο στην Ανάπτυξη**: Απλοποιημένη εγκατάσταση για πειραματισμό και πρωτότυπα.
- **Επιχειρησιακής Κλάσης**: Λύσεις έτοιμες για παραγωγή με δυνατότητες ενσωμάτωσης σε επιχειρήσεις.
- **Δια-πλατφορμικό**: Καθολική συμβατότητα σε διαφορετικά λειτουργικά συστήματα και υλικό.

### Βασικά Πλεονεκτήματα Τοπικής Ανάπτυξης SLM

Η τοπική ανάπτυξη SLM προσφέρει αρκετά θεμελιώδη πλεονεκτήματα που την καθιστούν ιδανική για εφαρμογές επιχειρήσεων και ευαίσθητες στην ιδιωτικότητα:

**Ιδιωτικότητα και Ασφάλεια**: Η τοπική επεξεργασία εξασφαλίζει ότι ευαίσθητα δεδομένα δεν εγκαταλείπουν ποτέ την υποδομή του οργανισμού, επιτρέποντας τη συμμόρφωση με GDPR, HIPAA και άλλες κανονιστικές απαιτήσεις. Είναι δυνατές οι αναπτύξεις με απομόνωση δικτύου για απόρρητα περιβάλλοντα, ενώ πλήρη ίχνη ελέγχου διατηρούν την εποπτεία της ασφάλειας.

**Οικονομική Αποδοτικότητα**: Η εξάλειψη των μοντέλων τιμολόγησης ανά token μειώνει σημαντικά τα λειτουργικά κόστη. Οι χαμηλότερες απαιτήσεις εύρους ζώνης και η μειωμένη εξάρτηση από το cloud παρέχουν προβλέψιμες δομές κόστους για τον προϋπολογισμό των επιχειρήσεων.

**Απόδοση και Αξιοπιστία**: Ταχύτεροι χρόνοι συμπερασμάτων χωρίς καθυστερήσεις δικτύου επιτρέπουν εφαρμογές σε πραγματικό χρόνο. Η λειτουργία εκτός σύνδεσης εξασφαλίζει συνεχή λειτουργία ανεξαρτήτως συνδεσιμότητας στο διαδίκτυο, ενώ η τοπική βελτιστοποίηση πόρων παρέχει σταθερή απόδοση.

## Ollama: Καθολική Πλατφόρμα Τοπικής Ανάπτυξης

### Βασική Αρχιτεκτονική και Φιλοσοφία

Το Ollama έχει σχεδιαστεί ως μια καθολική, φιλική προς τον προγραμματιστή πλατφόρμα που εκδημοκρατίζει την τοπική ανάπτυξη LLM σε διάφορες διαμορφώσεις υλικού και λειτουργικά συστήματα.

**Τεχνική Βάση**: Βασισμένο στο ισχυρό πλαίσιο llama.cpp, το Ollama χρησιμοποιεί τη βελτιστοποιημένη μορφή μοντέλου GGUF για βέλτιστη απόδοση. Η δια-πλατφορμική συμβατότητα εξασφαλίζει συνεπή συμπεριφορά σε περιβάλλοντα Windows, macOS και Linux, ενώ η έξυπνη διαχείριση πόρων βελτιστοποιεί τη χρήση CPU, GPU και μνήμης.

**Φιλοσοφία Σχεδίασης**: Το Ollama δίνει προτεραιότητα στην απλότητα χωρίς να θυσιάζει τη λειτουργικότητα, προσφέροντας ανάπτυξη χωρίς παραμετροποίηση για άμεση παραγωγικότητα. Η πλατφόρμα διατηρεί ευρεία συμβατότητα μοντέλων ενώ παρέχει συνεπείς APIs σε διαφορετικές αρχιτεκτονικές μοντέλων.

### Προηγμένα Χαρακτηριστικά και Δυνατότητες

**Εξαιρετική Διαχείριση Μοντέλων**: Το Ollama παρέχει ολοκληρωμένη διαχείριση κύκλου ζωής μοντέλων με αυτόματη λήψη, προσωρινή αποθήκευση και έκδοση. Η πλατφόρμα υποστηρίζει ένα εκτεταμένο οικοσύστημα μοντέλων, συμπεριλαμβανομένων των Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral και εξειδικευμένων μοντέλων ενσωμάτωσης.

**Προσαρμογή μέσω Modelfiles**: Οι προχωρημένοι χρήστες μπορούν να δημιουργήσουν προσαρμοσμένες διαμορφώσεις μοντέλων με συγκεκριμένες παραμέτρους, προτροπές συστήματος και τροποποιήσεις συμπεριφοράς. Αυτό επιτρέπει βελτιστοποιήσεις για συγκεκριμένους τομείς και εξειδικευμένες απαιτήσεις εφαρμογών.

**Βελτιστοποίηση Απόδοσης**: Το Ollama ανιχνεύει και χρησιμοποιεί αυτόματα διαθέσιμη επιτάχυνση υλικού, συμπεριλαμβανομένων των NVIDIA CUDA, Apple Metal και OpenCL. Η έξυπνη διαχείριση μνήμης εξασφαλίζει βέλτιστη χρήση πόρων σε διαφορετικές διαμορφώσεις υλικού.

### Στρατηγικές Εφαρμογής για Παραγωγή

**Εγκατάσταση και Ρύθμιση**: Το Ollama παρέχει απλοποιημένη εγκατάσταση σε πλατφόρμες μέσω εγγενών εγκαταστάσεων, διαχειριστών πακέτων (WinGet, Homebrew, APT) και κοντέινερ Docker για αναπτύξεις σε κοντέινερ.

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Βασικές Εντολές και Λειτουργίες**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**Προηγμένη Ρύθμιση**: Τα Modelfiles επιτρέπουν εξελιγμένη προσαρμογή για επιχειρησιακές απαιτήσεις:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### Παραδείγματα Ενσωμάτωσης για Προγραμματιστές

**Ενσωμάτωση Python API**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**Ενσωμάτωση JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**Χρήση RESTful API με cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### Βελτιστοποίηση Απόδοσης

**Ρύθμιση Μνήμης και Νημάτων**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**Επιλογή Ποσοτικοποίησης για Διαφορετικό Υλικό**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: Πλατφόρμα AI για Επιχειρήσεις

### Αρχιτεκτονική Επιχειρησιακής Κλάσης

Το Microsoft Foundry Local αντιπροσωπεύει μια ολοκληρωμένη λύση για επιχειρήσεις, σχεδιασμένη ειδικά για αναπτύξεις AI σε περιβάλλοντα edge με βαθιά ενσωμάτωση στο οικοσύστημα της Microsoft.

**Βάση ONNX**: Βασισμένο στο βιομηχανικό πρότυπο ONNX Runtime, το Foundry Local παρέχει βελτιστοποιημένη απόδοση σε διάφορες αρχιτεκτονικές υλικού. Η πλατφόρμα αξιοποιεί την ενσωμάτωση Windows ML για εγγενή βελτιστοποίηση στα Windows, διατηρώντας παράλληλα τη δια-πλατφορμική συμβατότητα.

**Εξαιρετική Επιτάχυνση Υλικού**: Το Foundry Local διαθέτει έξυπνη ανίχνευση και βελτιστοποίηση υλικού σε CPUs, GPUs και NPUs. Η βαθιά συνεργασία με προμηθευτές υλικού (AMD, Intel, NVIDIA, Qualcomm) εξασφαλίζει βέλτιστη απόδοση σε διαμορφώσεις υλικού για επιχειρήσεις.

### Προηγμένη Εμπειρία Προγραμματιστή

**Πρόσβαση Πολλαπλών Διεπαφών**: Το Foundry Local παρέχει ολοκληρωμένες διεπαφές ανάπτυξης, συμπεριλαμβανομένου ενός ισχυρού CLI για διαχείριση και ανάπτυξη μοντέλων, SDKs πολλαπλών γλωσσών (Python, NodeJS) για εγγενή ενσωμάτωση και RESTful APIs με συμβατότητα OpenAI για απρόσκοπτη μετάβαση.

**Ενσωμάτωση Visual Studio**: Η πλατφόρμα ενσωματώνεται άψογα με το AI Toolkit για το VS Code, παρέχοντας εργαλεία μετατροπής μοντέλων, ποσοτικοποίησης και βελτιστοποίησης εντός του περιβάλλοντος ανάπτυξης. Αυτή η ενσωμάτωση επιταχύνει τις ροές εργασίας ανάπτυξης και μειώνει την πολυπλοκότητα της ανάπτυξης.

**Διαδικασία Βελτιστοποίησης Μοντέλων**: Η ενσωμάτωση του Microsoft Olive επιτρέπει εξελιγμένες διαδικασίες βελτιστοποίησης μοντέλων, συμπεριλαμβανομένης της δυναμικής ποσοτικοποίησης, της βελτιστοποίησης γραφημάτων και της προσαρμογής για συγκεκριμένο υλικό. Οι δυνατότητες μετατροπής μέσω cloud μέσω του Azure ML παρέχουν επεκτάσιμη βελτιστοποίηση για μεγάλα μοντέλα.

### Στρατηγικές Εφαρμογής για Παραγωγή

**Εγκατάσταση και Ρύθμιση**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**Λειτουργίες Διαχείρισης Μοντέλων**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**Προηγμένη Ρύθμιση Ανάπτυξης**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### Ενσωμάτωση Οικοσυστήματος Επιχειρήσεων

**Ασφάλεια και Συμμόρφωση**: Το Foundry Local παρέχει χαρακτηριστικά ασφάλειας επιχειρησιακής κλάσης, συμπεριλαμβανομένων ελέγχου πρόσβασης βάσει ρόλων, καταγραφής ελέγχου, αναφοράς συμμόρφωσης και κρυπτογραφημένης αποθήκευσης μοντέλων. Η ενσωμάτωση με την υποδομή ασφάλειας της Microsoft εξασφαλίζει συμμόρφωση με τις πολιτικές ασφάλειας των επιχειρήσεων.

**Ενσωματωμένες Υπηρεσίες AI**: Η πλατφόρμα προσφέρει έτοιμες προς χρήση δυνατότητες AI, συμπεριλαμβανομένων των Phi Silica για τοπική επεξεργασία γλώσσας, AI Imaging για βελτίωση και ανάλυση εικόνων, και εξειδικευμένων APIs για κοινές εργασίες AI στις επιχειρήσεις.

## Συγκριτική Ανάλυση: Ollama vs Foundry Local

### Σύγκριση Τεχνικής Αρχιτεκτονικής

| **Πτυχή** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **Μορφή Μοντέλου** | GGUF (μέσω llama.cpp) | ONNX (μέσω ONNX Runtime) |
| **Εστίαση Πλατφόρμας** | Καθολική δια-πλατφορμική | Βελτιστοποίηση για Windows/Επιχειρήσεις |
| **Ενσωμάτωση Υλικού** | Γενική υποστήριξη GPU/CPU | Βαθιά ενσωμάτωση Windows ML, NPU |
| **Βελτιστοποίηση** | Ποσοτικοποίηση llama.cpp | Microsoft Olive + ONNX Runtime |
| **Χαρακτηριστικά Επιχειρήσεων** | Κοινότητα | Επιχειρησιακής κλάσης με SLAs |

### Χαρακτηριστικά Απόδοσης

**Πλεονεκτήματα Απόδοσης Ollama**:
- Εξαιρετική απόδοση CPU μέσω βελτιστοποίησης llama.cpp.
- Συνεπής συμπεριφορά σε διαφορετικές πλατφόρμες και υλικό.
- Αποτελεσματική χρήση μνήμης με έξυπνη φόρτωση μοντέλων.
- Γρήγοροι χρόνοι εκκίνησης για ανάπτυξη και δοκιμές.

**Πλεονεκτήματα Απόδοσης Foundry Local**:
- Ανώτερη χρήση NPU σε σύγχρονο υλικό Windows.
- Βελτιστοποιημένη επιτάχυνση GPU μέσω συνεργασιών με προμηθευτές.
- Παρακολούθηση και βελτιστοποίηση απόδοσης επιχειρησιακής κλάσης.
- Δυνατότητες επεκτάσιμης ανάπτυξης για περιβάλλοντα παραγωγής.

### Ανάλυση Εμπειρίας Προγραμματιστή

**Εμπειρία Προγραμματιστή Ollama**:
- Ελάχιστες απαιτήσεις εγκατάστασης με άμεση παραγωγικότητα.
- Διαισθητική διεπαφή γραμμής εντολών για όλες τις λειτουργίες.
- Εκτεταμένη υποστήριξη κοινότητας και τεκμηρίωση.
- Ευέλικτη προσαρμογή μέσω Modelfiles.

**Εμπειρία Προγραμματιστή Foundry Local**:
- Ολοκληρωμένη ενσωμάτωση IDE με το οικοσύστημα Visual Studio.
- Ροές εργασίας ανάπτυξης επιχειρήσεων με δυνατότητες συνεργασίας ομάδας.
- Επαγγελματικά κανάλια υποστήριξης με την υποστήριξη της Microsoft.
- Προηγμένα εργαλεία αποσφαλμάτωσης και βελτιστοποίησης.

### Βελτιστοποίηση Περιπτώσεων Χρήσης

**Επιλέξτε το Ollama Όταν**:
- Αναπτύσσετε δια-πλατφορμικές εφαρμογές που απαιτούν συνεπή συμπεριφορά.
- Δίνετε προτεραιότητα στη διαφάνεια ανοιχτού κώδικα και στις συνεισφορές της κοινότητας.
- Εργάζεστε με περιορισμένους πόρους ή προϋπολογισμό.
- Δημιουργείτε εφαρμογές πειραματικού ή ερευνητικού χαρακτήρα.
- Χρειάζεστε ευρεία συμβατότητα μοντέλων σε διαφορετικές αρχιτεκτονικές.

**Επιλέξτε το Foundry Local Όταν**:
- Αναπτύσσετε επιχειρησιακές εφαρμογές με αυστηρές απαιτήσεις απόδοσης.
- Αξιοποιείτε βελτιστοποιήσεις υλικού ειδικά για Windows (NPU, Windows ML).
- Χρειάζεστε υποστήριξη επιχειρήσεων, SLAs και χαρακτηριστικά συμμόρφωσης.
- Δημιουργείτε εφαρμογές παραγωγής με ενσωμάτωση στο οικοσύστημα της Microsoft.
- Χρειάζεστε προηγμένα εργαλεία βελτιστοποίησης και επαγγελματικές ροές εργασίας ανάπτυξης.

## Προηγμένες Στρατηγικές Ανάπτυξης

### Μοτίβα Ανάπτυξης σε Κοντέινερ

**Κοντέινερ Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**Ανάπτυξη Επιχειρήσεων Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### Τεχνικές Βελτιστοποίησης Απόδοσης

**Στρατηγικές Βελτιστοποίησης Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**Βελτιστοποίηση Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## Σκέψεις για Ασφάλεια και Συμμόρφωση

### Εφαρμογή Ασφάλειας Επιχειρήσεων

**Βέλτιστες Πρακτικές Ασφάλειας Ollama

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία αυτόματης μετάφρασης [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.